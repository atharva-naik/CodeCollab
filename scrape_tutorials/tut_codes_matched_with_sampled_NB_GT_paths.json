[
    {
        "id": "750332",
        "GT": "7) How does donor category vary with duration of the project ?->bx = b.plot(kind='bar',figsize=(7,5))\nbx.set_ylabel(\"Project Duration\")",
        "pred": [
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team->win_percent.sort_values().plot.barh(figsize=(6, 12), width=.85, color='k')\nplt.tight_layout()\nsns.despine()\nplt.xlabel(\"Win Percent\")",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "statsmodels->Examples->Time Series Analysis->Time Series Filters->Christiano-Fitzgerald approximate band-pass filter: Inflation and Unemployment->fig = plt.figure(figsize=(14, 10))\nax = fig.add_subplot(111)\ncf_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "statsmodels->Examples->Time Series Analysis->Time Series Filters->Baxter-King approximate band-pass filter: Inflation and Unemployment->Explore the hypothesis that inflation and unemployment are counter-cyclical.->fig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111)\nbk_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))"
        ]
    },
    {
        "id": "114564",
        "GT": "Imputation of NAN.->Mean imputation with raw-direction->imr = Imputer(missing_values='NaN', strategy='mean', axis=1)\nimr = imr.fit(df)\nimputed_data = imr.transform(df.values)\nimputed_data",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Autoregressions->mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach->mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures->summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Macroeconomic data->dta = pd.concat((indprod, income, sales, emp), axis=1)\ndta.columns = ['indprod', 'income', 'sales', 'emp']\ndta.index.freq = dta.index.inferred_freq",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit->mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)"
        ]
    },
    {
        "id": "1438958",
        "GT": "Okay, let's run this thing. \n\n## Because we call a program from within the Jupyter Notebook you have to look at the terminal window that you used to start the notebook to see the screen report of the run.  So, when executing this next block look at your terminal window to see the run.  It will say \"Simulation complete...\" when finished.\n\n### NOTE:  And/or wait until the standard out  reports a \"0\" below this next block (=when the run is finished) before going on.->os.system(\"{0} freyberg.pst\".format(ppp))",
        "pred": [
            "seaborn->User guide and tutorial->Visualizing distributions of data->Empirical cumulative distributions->sns.displot(penguins, x=\"flipper_length_mm\", kind=\"ecdf\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Empirical cumulative distributions->sns.displot(penguins, x=\"flipper_length_mm\", kind=\"ecdf\")\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->sns.displot(penguins, x=\"flipper_length_mm\", kind=\"kde\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->sns.displot(penguins, x=\"flipper_length_mm\", kind=\"kde\")\n",
            "matplotlib->Tutorials->Introductory->Animations using Matplotlib->Animation Writers->Saving Animations->ani.save(filename=\"/tmp/pillow_example.gif\", writer=\"pillow\")\nani.save(filename=\"/tmp/pillow_example.apng\", writer=\"pillow\")",
            "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper->print(res.cusum)\nfig = res.plot_cusum()",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Choosing the smoothing bandwidth->sns.displot(penguins, x=\"flipper_length_mm\", kind=\"kde\", bw_adjust=2)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Choosing the smoothing bandwidth->sns.displot(penguins, x=\"flipper_length_mm\", kind=\"kde\", bw_adjust=2)\n"
        ]
    },
    {
        "id": "826529",
        "GT": "B1.5 Including context information->flights_context_info['oridst'] = flights_context_info.city_origin + '-' + flights_context_info.city_destination\nflights_context_info.groupby('oridst').is_delayed.sum().sort_values(ascending = False).head(10)",
        "pred": [
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()",
            "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures->summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Logit Model->margeff = logit_res.get_margeff()\nprint(margeff.summary())"
        ]
    },
    {
        "id": "859230",
        "GT": "Feature Engineering\n\nIn this part, we will first do the feature engineering, there are mainly three kinds of data in our dataset, numeric data, categorical data(structured data) and also text(unstructured data). For those uneric data which includes date and location information, we choose to first normalize the periodical data -- month, and then comes to the duration between the start date and end date. After we have normalized all the numeric data, we also analysed the correlation between all the processed numeric features and our response -- attending_count. From the correlation map we can see duration and interested_count has a moderate relation with response, while others all have weak correlation. Finally, we still choose to keep all those features because there are a moderate number of features we have, if we can include more features, we will have a better performance in the prediction without put too much load on the complexity of the model, secondly, because of the limitation on the amount of data's we can get from Yelp's API, some correlation between certain features and response may be not so clear, then to make our model more accurate and comprehensive at this point, we choose to include all those numeric features. Then we also considered the categorical data like the category of the event. There are totally 13 distinct event categories, so for each category, we will encode it to a vector of length 13 use one-hot-encoding. Finally, to deal with the text and append it to those features we have already had, we choose to use Bag-of-words, to first seperate the id of the event to a list of words. Then generate a word dictionary to give every unique word an index. It should be noted that, we have tried to include all the unqiue words in our dictionary, while the modelling part was very slow and there will be memory overflow problem, to solve this, we only include a certain number of most frequent unique words appears in id column. Here for each id of one event, we will encode the id to a certain number of 1-D vector, in our case, we choose 200.  \nFollowing is the code of how we realize all the functions stated above:->Code execution and correlation analysis\n\nIn this part we will first call the functions above to corresponding columns in original data set, then analyze the correlation between response and all the numeric features.->category = processCategorial(e['category'])\nids = processText(e['id'],200)\ndata = processData(e)",
        "pred": [
            "scipy->Compressed Sparse Graph Routines (scipy.sparse.csgraph)->Example: Word Ladders->word_list = open('/usr/share/dict/words').readlines()\n word_list = map(str.strip, word_list)",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog->data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)",
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Logit Model->margeff = logit_res.get_margeff()\nprint(margeff.summary())"
        ]
    },
    {
        "id": "67925",
        "GT": "Feature Engineering\nNow, we'll create new attributes that we feel would be more appropriate in prediction from the given data.->#Creating groups in each of the column based on importance with Decision Tree splits \n#First we'll apply this to hour variable in training set with target registered users\ndtree=DecisionTreeRegressor()\nx = train['hour']\ny = train['registered']\nx = x.reshape(-1, 1)\ndtree.fit(x,y)\n\ndot_data = StringIO()\nexport_graphviz(dtree, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())",
        "pred": [
            "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results-># Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data-># Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjQAAAEWCAYAAABxBw2jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOydd5wcdf3/n5/Zdj2XSy7l0jtJIKGEroDSQYoggl9RvnQVBQQL4NcC/vgiqJQvIoJgAVGkiAQQkI50kkACSSC9Xvr12z7z+f3x2c1t7vbudu+23r2fD4bbnZmd+WR2duY176q01giCIAiCIBQzVr4HIAiCIAiC0F9E0AiCIAiCUPSIoBEEQRAEoegRQSMIgiAIQtEjgkYQBEEQhKJHBI0gCIIgCEWPCBpBEARBEIoeETSCIPSIUmqdUiqglGpLmOryPa7eUEpNVEpppZQ732MRBCH7iKARBCEVTtFaVyRM9el8eCCIimT/hnT/XQPhOAhCoSKCRhCEPqOUOlUptVQp1aSUelUpNTNh2Tql1A+VUkuAdqWUWylVp5R6XCm1Qym1Vil1ecL6LqXUdUqp1UqpVqXUQqXUuNiyO5RSG5VSLbH5n0343EFKqQWxZduUUrfGFr0e+9sUsyodmmT8llLqmtg+dymlHlFK1cSWxS08FyqlNgAvJ5vXl+OQqeMvCEIHImgEQegTSqnpwN+AK4Fa4F/AU0opb8JqXwFOBqoBB3gKWAyMAY4GrlRKHR9b96rY+icBVcAFgD+27H1gX6AG+CvwqFKqJLbsDuAOrXUVMAV4JDb/iNjf6phV6e0k/4zLgdOBI4E6oBG4q9M6RwIzgeOTzUv3OGito0nGIQhCP1HSy0kQhJ5QSq0DhgPxG/GrWuvTlVI/BvbRWn85tp4FbAS+qrV+Nfa5G7TWf4gtPxh4VGs9PmHb1wLTtdbnK6U+BX6gtX4yhTE1AkdprRcrpV4HXgHu1FrvTFhnIrAW8HQnIpRSy4Fva61fir0fDWwASoGxsc9P0Vqv6bTNxHlpHQdBELKDWGgEQUiF07XW1bHp9Ni8OmB9fAWttYO5kY9J+NzGhNcTgLqYW6ZJKdUEXAeMjC0fB6xOtnOl1NVKqeVKqebY54ZgRBbAhcB04BOl1PtKqS+k8e+aADyRMJ7lgJ0wps7/hmTz0j0OgiBkAfHlCoLQV+qBfeJvlFIKI0o2J6yTaALeCKzVWk/rZnsbMS6jjxNnxuJlfohxUS3VWjsxC40C0FqvBL4Ss4ycATymlBrWad/dsRG4QGv9ZucFMWtM539Dsn9XusdBEIQsIBYaQRD6yiPAyUqpo5VSHuBqIAS81c367wEtsQDZ0lgQ8N5KqQNjy+8Dfq6UmqYMc2LCpBLj7toBuJVSP8HE2ACglDpXKVUbs4w0xWbbsfUdYHIP/4bfATcqpSbEtlWrlDoty8dBEIQsIIJGEIQ+obX+FDgXuBPYCZyCSe8Od7O+HVtnX0wcyk6MiBkSW+VWjDj4N9AC3I+JZXkeeBZYgXHtBNnThXMCsFQp1YYJED5Hax3UWvuBG4E3Yy6lQ5IM6w5gPvBvpVQr8A5wcDaPgyAI2UGCggVBEARBKHrEQiMIgiAIQtEjgkYQBEEQhKJHBI0gCIIgCEWPCBpBEARBEIqeAV+HZvjw4XrixIn5HoYgCIIgCBlg4cKFO7XWtZ3nD3hBM3HiRBYsWJDvYQiCIAiCkAGUUuuTzReXkyAIgiAIRY8IGkEQBEEQih4RNIIgCIIgFD0iaARBEARBKHpE0AiCIAiCUPSIoBEEQRAEoegRQSMIgiAIQtEjgkYQBEEQhKJHBI0gCIIgCEVBKGp3u0wEjSAIQhHT0wVeEAYSbaEo7SERNIIgCAOOtlCUJn8Efzia76EIQlYxYqbn81wEjSAIQhESCNu7L/CtwSjBiFhqhIFJKmIGRNAIgiAUHcGITUswsse8lkCEcNTJ04gEITukKmZABI0gCEJREbEdWgKRLvM10BQIE7VF1AgDg3TEDIigEQRBKBqitkOjP4zuZrnW0OiPYDvdrSEIxUG6YgZE0AiCIBQFjqNpCkTQvWgVR2ua/GEcETVCkdIXMQMiaARBEAoerY2YSdXyEnU0zYEIujf1IwgFRk9iJhx1+N6ji7v9rAgaQRCEAqc5ECGSJDamNRjhnx9sTlqLJmw7tAQknVsoHnoTM9f8Ywkvf7K928+LoBEEQShgmgMRQkmyl0JRm+8/uoSbnv2Enzy5NKn1Jhjtmg0lCIVIKmLmzVW7+NHJM7vdhggaQRCEAqUtlLy+jKM1189fxgcbmzhm5ghe/XQHt76wIqmLKbFejSAUIqmKmWtO3Isz9x/b7Xbc2RqgIAiC0Hd6EiJ3vLiSlz7ZzuVHT+WrB09g1JCV/OWdDdRW+vjvwyZ2Wb8tFMVlKUo8riyPWhDSIx0x88X9xvS4LRE0QkZxHE1LMILbZVHudaGUyveQBKHoCPXgKvrruxt4+P2NnH3gOP7roPEAXPa5qexsDXP3q6uprfBx8pzRXT7XEoigFPjcImqEwqA3MfPDx5fw1urUxAyIoBEySNR2dmdihKIOgbBNhc9NqVcuoIKQKhHbodmfXMy8sGwbd7y0ks/vNYIrj5m2+4HBUor/+cJMGvxhbnxmOTXlXg6dMmyPz2qg2R9haLnC45JoAyG/ZFrMgMTQCBkiHHVo8If3CEx0tLHW7GoLSUl2QUgB29HdFs5buL6R659ayr7jqvnZqbOwOlk/PS6LX5yxD1NHVHDtPz5iWX1Ll21ooLHT71QQck02xAyIoBEyQCBs0+QPd1vwKxq7SDfJhTRrBCM2De1hWoIRKahWpDhxMZPk61u9vY0fPLaEMdWl3PKlOd26jcp9bm47ey5Dyz1c9ciHbGzwd1nHVBOWwntCfkhVzFybppgBETQZx7YdQoEg/uZWoq1t9FrWs8hpDUZoCUa6LcWeSCjqsKstRGtQCn5lAsfRtIei7GgN7a5TEgjb7GwPSeflIqOnwnnbWoJc8fcPKfFY3HHOfgwp9fS4rWEVPu44ez+0hise/pBdbaEu69i7qw7L71DIHT2JmVDU3kPMnJ6mmAFQA/2Enjdvnl6wYEFGt+nYDtFIFDscIRoO4wRC2MEgdjCICsdv7hqlNeXDhlI6rg7cAytcSWvdbX2MVFAKKnxuyrwD67jkgqjt0B62CUXsHoWk12VRWeLGLfESBU+TP5z0t9QajHDpgwvZ0hzknq8dwPSRlSlv8+PNzVz210VMHFbO3efun/S35nNbVJd5+zV2QUiF3sTMNY9/1KuYUQqGlnnxul0LtdbzuiwvNkGjlDoBuANwAfdprX/R0/p9FTRa6wTREsEOhHCCAaKBmGjRGhQo7aAtF7hd4HKDq5MpOBDA61JUTpqAVVGe9jgKEdsxvWKiGTBZuyxFZYlbMi9SIBixCYRtwml0U1ZAmc8tGWcFTEswQiCcpNJv1OGKhz9gyaZmbj97Xw6cVNNlHaWgqsRDSyC5lfSNVTv5waNLOHDSUH591tyk4rbE4+rV6iMI/SGTYsbjslBKFb+gUUq5gBXAscAm4H3gK1rrZd19pidBo7XGjkQ7hEsojBMIEPUHUOEw2omJFjQaZawsLpeZ0rk5RKOoYICK0SMpGTUCrOJ9Yo7YDk3+CE6Gzxuf26LCJ9aEzmitCURs/GG7X/FHIhwLk/ZQlLYkF3pHa37y5FJeWLaN60+dzQl7j+qyjgKqy7x43RahqE1TN5lRTy2u5/89s5yT9hnFT74wK6mwLfe5qfCJtVTIPJkWM+Z9ckFTbGfwQcAqrfUaAKXUw8BpQLeCBsCO2kTDYSNewhEcf4CoPwjBADga3Vm0WBa6tHS3aOn3rdvtRpdX0LZtB5HWNiomjEWVlPR3qzknGLG7fRKMs6HBz43PLGevUZVc8JlJKT/5haIOoWiYUq+LCq8byxrc1gTb0bSHowTDPbuV0tlekz9CiduhosSNa5Af30IgGLGTihmA37y8iheWbeOyz01JKmYAqko9eN3mAu9zu6gqIWntmlPm1rGjNcQ9r69heIWPyz43tcs67aEolkJcwEJG6T1m5iPeXr2L607ai9P2TU3M9ESxnb1jgI0J7zcBB3deSSl1CXAJwNjRo2n8cClaqZiLSKNjooWSkt3WkqzbqZRCl5UTDIeILF9F5YQxeIZWp2fpySPdPUkmsnB9I9c8vgRba5ZsauJfH2/h4s9M5oz9x6RseQmEbYKRWP0az+Bzk4Sixq3U19ik3ghGbUJtNhUlEr+UT0JR83CQjIff28BD727gSweM5WuHTEi6TlWJp0vV31KvC0frpL/T8w+fyI7WEA+8vZ7aCh9fPnBcl3Vag1EsJdWEhcyQazEDxSdokt3dumgRrfW9wL0A+86arXVlRfcr5xqvD9vt0LRmA6XD26gYV9c17qaA0FrTEkzeTyaR+Yvr+cWznzBuaCm3fnlf/JEod7y4kl+/sILHF23iO0dP4/Apw1ISKFqbi6s/VphvoF9gtdYEIw7+cDQjcUm97g9zfANhm8qSjqd8ITfEC+cl+6ZfWr6N219cyVHTa7nq2Onduoe6K1ZZ7nNja90lJkcpxfeOn8Gu9jC3vrCCYRVejp45ssvnWwIRLKXknBD6RT7EDBRf2vYmIPHRYixQn6ex9B3LgspKAk0tNC5bQbStPd8jSoqpixHpUczYjubOl1dy4zPLmTdhKPefdyBjhpYybUQld35lP375pTnYWnP1I4u54uEPWb29LeX9247JpGpsDxNJIxC2WLAd8zS9oy1ESzDSZzETtR3+vXQrlz20iBueWpY0TTfp52J1T6R2Te6Iu/6SHe0PNjTys/nL2GfsEK4/bXZSt2CJx9VrrEtViQdfEkHishQ3nDabfcYO4afzl7JofWOXdTTQFAgTHYC/NyE35EvMQPEFBbsxQcFHA5sxQcH/pbVe2t1n9p01W7/wxFM5GiG072pi1/KVBFaupXzubMbuP7vnD0QiqGCQ8jGjKR1VWzAuqKjt0NhL8K8/HOWn85fy+oqdfOmAsXz32Gm4kwQ8R2yHxxZu4v431tIeinL6vmO45IjJDC1PL120xOOi0lf88TXhWFuIULR/8TFN/jD//KCexxZtYkdriLrqEna0hvC6Lb555BTO2H9syrEy8WyZgW4NyyeOo7tU046zZkcblzy4kJoyL7//+jyGlHWNPfO6rJR/M1qbh5FkDwLNgQiXPLCAnW1h7vnaAUwdUdFlHUspasq9EmtVgGitidjmHPK4VEG55XMlZgZElhOAUuok4HZM2vYftNY39rR+NgRNqM3Pzk9W075iNfaqNXg2rGPI5vWM2L6Z4e0dTz1Bt5env/0zDr/ky7h7ujBojWpvx1NRTuWEsVglvoyON11CUZvmQKTHmoDbWoJc/chiVu9o46pjp3PWvK4++c40+yPc98YaHl+4mRKvxfmHT+LseePSMm8rjFm9rAjTkIOxbKX+WptWb2/j7ws28tzHWwlFHQ6aVMPZB47jsCnD2NQQ4JfPf8p76xqYObqSH56wFzNHV6W8baldkx16EhjbW4Nc9OcFRG3NfefNo666tMs6bssIjHTO+Z4E1NZms08U3H/ePEZWdU1ScFmKmjJv0T9AFDsR2yFqa8K2Q9R2ulhyLaXwuBRul4XbMn268iFEexUzj33E22t28aOTZnLqvnVJ10vVMjNgBE269FXQRMMRGlauo/nT1URWrsa9fh0VmzcwYttGRjZtx0p4tt5ZPpRtI8fSWjee8PiJuKZOpnz8GKr+9/8xcc1S/nDKpez3o8sZX92LUAmFUNEoFRPHUVJTnfaYM4E/HKU12HPw77L6Fr736GKCUZsbT9+nSxO83li7s53/e2klb63exZjqUr7z+akcNaM2rYu1pUwacqFbFBynI+26P6nujta8uWonf39/I++va8Tntjhx71GcfeA4Jtfu+YSttebfy7Zxx4srafSHOXP/sXzjyClUlKQWMie1azJPd4Xz2oJRLv3LQuqbAvzu3AOYMapr4TxLKYaV901Y2I6moT2c9Nxbtb2NSx5cwIjKEu752gFJMxI9LouhZR45D3KE7WgithObNFHb6ZMVVynwWBZulxE4bktl9SEll2LGrCuCpgvacWhcX0/T8pWEVq5GrV1L+aYNDNu6gdENW/E6HV9Qq6+MLbVjaawbT2jcRKwpkymbPpmamdMoH9aN+AgGCVx2OePffIm/zPsC/ut+zOkzewmMtW3w+ympHUbFmFGoHFYY7q7AVyIvLd/G9U8to6bcy61fntvlZpoO76zZxe0vrmTtznb2H1/NFcdMY69RqVsTwFxwK0vcBdc9ONVqvr3RForyzJItPLJgI5saA4yo9HHWvLGcNndMUrfEHp8NRvnda6t5bOEmasq9XHnMNI6dNTLlm5OlFFWlUrumv3T3u4rYDt/9+4cs2tDErV+eyyGTuz4YKAU1Zd5+3Yyitmkcm+xSv3B9I1c8/AGzRlfxf1/ZL+kDQonb1eu5JqSP48SsLo4mEnWIOE5WO+UoMFYclzICx7Iy4rLKtZgx6w9iQfP43X+gYdlKgitWw9q1lGxcT82WDYzeuZmySEcAZdDtpX5YHQ2jxxMYNwE9eTKlUyczdNY0qkbXovpSEM9x4KabqH3wD7ww9SD+/s3r+f7nJ1NT2rNQUe3tKI+bqskT8JSXpb/fNNDaBCr2VIFWa80f31zHPa+vYc7YIdxy5py0Y2CSEXUcnvygnntfX0NzIMLJc0bzzaOmMLwiPbdbPFgyF6ZWrTVamwBKrXXsL2jM/HSr+SZjU6OfRxZs4qnF9fjDNnPGDuHseeM4akZt2je35Vta+MWzn/DJ1lYOmljD90+Ywfia1M+pErdLatf0kZ4K510/fxnPLd3KT74wi5PnjO6yTmLhvP4SjjqmgWySZS8t38aPnviYI6bXctMZ+yT9nku9LqpKRNT0lXjcS6L7KNPFSfuKy1K7rTlul3mdqjUwH2LGfGaQCpr9LJf+QHfcXKLKYkvNKHaOHEf72InYEyfgmzaV6plTGTppLFaWUqh9Dz1IxY03sHTEZK766vV884TZfHZcL31ZwmEIhigdV0fFyOFZCRi2Y5kuPVWhDUVt/veZT3hu6VZO3HsU1500s8eLrKXU7ht9qrQGI/zxzXX8/f2NeFwW5x02ga8cND4tl1LcVVLitrqIDDq97yxGiC3rWK+rWIn9lzW01ixc38jD72/kjZU7sSzFsTNHcvaB45hVl57lqjO2o/nHok3c/dpqIlHN1w+dwNcPm5Cy9SUeu1Qu1WRTJhgxsWjJuOuVVTzw9nq+ceRkzj98UtJ1hpRmNki7p/H8/f2N3PrCCs7cfwzfP35G0qf2Cvn+UybRbRSxnX5V+c4HqcTlpCJm3lmzi+syLGbM5wapoNmrslrfc+Z/45k6hcq9pjJs6kQ8pfkJuvW+8jIV372cHaVVnHvGT5lz+ByuPHAkpZ4evkzHQfn9uCoqqJwwBncGA4bDUYemQHJTdJyG9jA/fHwJSzY1880jp3DeYRN6NFF6XRbVZZ7dKcnpFojb2ODnNy+v4tUVOxhVVcK3PjeF49JwkxQjwYjNv5du4+/vb2TVjjaqSz18cf8xnLn/WGorM3uu7mwLcfuLK3lh2TbGDi3l+8fPSOrq6A63paR2TQqEona3tWYeXbCRX/17BWfsN4YfnJBcPFRmqfBhTzFyv3l5FQ++k1uRNRCIB+qGY9aXvsa9FDpKgTtmyQG6DU8IRW1+8NgS3l3TwHUnz+TUuZkVM+azg1TQ5DptuzfcH39E1TcuJuwPcMGp17F5n3nccEQds2u7ZjbsQSCAcjTlk8ZTOrR/T+uQWhuD1dvbuPrRxTS0h/npKbOSFuJKJJlZOhS1aQumXzBu4fpG7nhxJZ9ua2XvMVVcecx09hkzJK1tFDrbW4M8vnAzT3ywmeZAhGkjKjj7wHEcN3tk1uNW3l27i1ue+5RNjQGOnTWSK4+ZlpabT1pUdE/EdmhsT+7eefXT7Vzz+Ed8Ztpwbj5zTlL3TpnXRWUW3TutwQj+JDcjR2uuf2oZz328lf85eSanJLkRKWBImWfQx1VFbAd/yCZk21mNeyk2ciFmzOdF0BQM1ubNDLn0Qqx1a/n5qVfywPSjuHDucM6fO7zn9O5o1HTvHj6MyrGjsfp4UenugpbI26t3cd0TH1HmdfGrs+b2mvrb2xNlIGzTGuo5FbwztqP510dbuPvV1exqD3P87JF866ipjBpSfH2wElla38zD723kpU+24ziaI6bXcvaB49h/fHW/LFEuS+E4qbv6QlGbB99ez5/fWo/HrfjGEVM484D0atdU+jzdVq0djPSUVbR4YxPf+dsHTBtZwV3/tX/yANwcdb5uDiQvmBmxHa5+ZDEL1jXyy7PmcPjU4V3WUcDQ8r7fjIqZqO3QHrIJRnu+fmZ1DI7D2p3taA2ThpcXzPeQKzFjtiGCpqBQLS1UXf4tvO+8zfxTLuDymV9k7xGl3HDEGMZV9RBsqzXK3472+qicOI6SitSDO7U2lXd7cgNprXl0wSZue3EFU0dU8Kuz5iatUbH736GMGTqVJzYd6zMTSLPhYnsoyoNvr+ev720A4L8OHs/XD51QVL2IorbDy59s5+8LNvLx5hbKfS5OnVvHWQeMY8zQXqxzveB1WZR6XZR4XMbVF4ymdcHd0OA3tWvWNrDXqEquOVFq1/SFnuq+rNvZzsUPLGBImYf7vj6P6rKuv/G4uzZX7tXG9nDS4PX2UJRvPrSI9bva+e1X92d2XVfLaCayr4qJuAu9txYwmUZrTX1TkGVbWlhW38LS+mY+3dZKMGK+N7elmFxbzrSRlcwYWcn0kRVMG1GZcomGTJFLMWO2I4ImayjA0g5ux8FybFyYp+VwJErA4+u+V1M4TOWPr6PkySdYfcypfOmgCwkqN989aCSnT+/laT0cglAE37g6Kmtres3AMiXXwz26fqK2w60vrODxRZs5cnotPzt1Vo+iwVKKoWWetC9qfbnpgikGdtcrq/j3sm0Mr/DyzaOmcNI+o7EKOL6m2R/hiQ8389hCU8137NBSzp43jpPnjO5XgKUCSrwuyjyupMffND+MppxJobXmxeXbuf3FFexqC3PmAWP5xpGTU3Z9KEzHZ8sy54Wl1B6vB3qGVE+F83a2hbjozwsIRuzdrUE605fCef1Fa2NNSnZN2NUW4qIHFuAP2fz+vHlJs+IGQ+E9x9G0ZbDrfW80tIdZtqWF5fUtLI2JmHggt89tMWNUJTNHVzG7rgpLKVZsa2XFtlY+3dpKo78j4HtMdSnTRlbERE4l00dVUFvhy8r5lWsxY7YlgqZfKMBC43JsXI6DpR1cCtwul7lYezyme7fPB14vuN0QiRBev5FWlxenO8GhNWV33Un5b+6g7aBD+dYXr+X1JosjxlXwo8NH95ze7TgmvbuqiqoJY/D4klt2IrZDUy9tDFqDEa574mPeW9vA1w6ZwLc+N6VHoeBxWVSXevp1MQtHHVr70MPoo03N3PbiCpbWtzBjVCXfPWYa+40f2udxZIMu1Xwn1nD2Qaaab38EmKUUZV4XpR5Xr8dea0172MYfiqZ8MW4LRrnndVO7prrM1K7JVFC2Umb8rpjIUVbH+93LLIWlKPgg8MTUfUdr/N24IdpDUb75l0VsaPBz97n7J7V85bPNQE9WpQ0Nfi7+8wLKfC7u+/o8hiWJsRqoQeKOo/FH0vvtpIs/HOXTra0srTfCZdmWFrY0BwGwFEweXsGsuipmjq5kdt0QptSW9/jwuLMtFBM4bazY2sqK7a1sbAjsXl5d6mH6KGPFmR4TOuNryvp13uVDzJjtiaBJCQuMaNEObsc2xYgshdvtMiLF5+uY3O6OqbsLcFsbzsZNtLm8hHo41L4nHqfyx9dhT5zEA9/7Nb/Y4KLCY/Hjz4zmM72ld/v95ol94ngqOgUMpxL8u6nRz9WPLGZTY4BrTtwraTBgIiUeF1Ul7ozddIIRm9Zg6tYEMDeRF5Zt465XVrGtJcRRM2r5zuenMnZodmv2xIk6DqGIQyjqEIzYu/9ubQ7y+KJNvVbzTRePy6Is5lZKe6y2Q2swmlZtnOVbWrj5uU9YvqWVAycO5QfH78X4Ybk5thB7gLBilh7V6bXq9DrJBbm3WkGppPA7un+p+4nxKL/68hwOm5IkHqUAXDe2o9nVHkoa37a0vplvPbSICcPKufur+3drVXRZilKPOT+L2RqntcYftmkPRzMa7Bu1HVbtaNstXJbVt7B2ZztxHTl6SAmz66qYVVfFrNFVzBhVmRGXensoyqrtbbuFzqfbWlmzo213Lyif22LqiLjAMX+njqhI6TqTLzFjtimCZjcKjUsba4vbcbAUxtpiKVOZ1+frsLYkipa+FNYDaG2FzZsJektoi3ZfDdLz9ptUXX4ZuqSEpb/8LVdvrWZlY4gzZlT3nt4dCxh219ZSWTcCt8fdbVGvRD7Y0MgPH/8IjebmM+aw/4SeLR3ZqkXRF2sCGDH00LsbeODtddiO5uwDx3HWAeNwtN4tNDqLjlDUIRSxCcb+dlkecQhGk/+Nr9OTVWlEpY8vHTCW0/ftvZpvT8TdOGU+V0YuBsGITUsw9cBs29E88cFm7n51NaGozdcOmcB5h00suLRdRYdFJ936R9lCa80NTy/jXx9t5UfdXOwzWTivv/SUmfXW6p1875ElHDBxKLd+eW6v56LPbVHi6Zv4zhdamxYlbaH+CxmtNRsbA3uIlxXbWnfHLg4p9TCrrorZozsETCaKlKZK1HZYu6udldva+HSrcVmt3N62O53fUjC+powZoyr3iM1JjPvKp5gx2x2kgmb/WbP1fx59HBcal6VwgckO8nqNaCkp2VO0ZKmwXlzU2GXltPbQoNC1coXJgGpqYtevbuf2yr156OMGxlV5e0/v3h0wXIJ7TB1hV8/C4+kl9dz0r08YU13Kr788l3E9VI9VQFUOalD0NfhuR2uIu19bzb+WbEn7huZ1Wfg81u4LceJfX6f3vf2t8LmZM3ZIv564lYIyr5uyFNxK6RKPCeitxUUiu9pC3PHSSp5f2rfaNYORu19dzZ/eWsfFn53ERZ+dnHSdQiZ00bgAACAASURBVKvp0lPtnKeX1PPzp5dzwt6j+Okps1JymyoFpR7jHi3k4OFA2AiZvlbu3dkWMuIlFvfyyZYWWmLioMRjMWOkcRnNqjOxL6OHlBScS1VrzZbmoBE5sbicFdta2dbSUUl/RKWP6SMrmTaygqWbW3h/XX7EjNn2IBU08+bO1QtefbUjrsUVcx3lg5YWqK+HigoCUYf2bqwR1vbtVH3jItyfLKftxz/jzaNO56f/qWenP8qF+w7n/Dm9pHeHQqhwBKe0NBa4YHVYlyyFoxR3v7+NP3+4nQPHVHLTCVOoKvWYdZVCx39ssfeWZbIvPG7X7nmJy7u8zwCRmJsk3c7UK7a1smRTc8oixOvOT2faZLgtRZnXTYnHyvoFL2I7tATSi196f20Dtzz/KRsa/BwzcwRXHjM944X/BgL/WLSJm5/7lNP2rePaE/fKaeG8/hIIGyteMv745lp+99oavnbIBL79+alpbdfjsmIuqeyf26kSjFlk+lLFd/mWFh56dwOLNzaxvdXc9F3KZBzFhcusuiomDS/H3VfLfgHQ7I8YcbO9lRVbjetq/S4/jtZ5EzNm+4NV0MybpxcsWJDvYXTQ0gKbN0NlJVFtGtcl/UG1t1N11RX4XnsF/4UXs/XbV3PLe9t5bk0L+9SWcv0RdT2ndzuOaXQJiYEBBKIOP3lrO69u8nPGlAq+v38NbgUqfh7ErzXavHZZiqoSLy5j1+/YvtbdixcVC5IeOhTKy/slIPsSX1Ns+NwWZV53XlwP/nCUtmDqbr5w1OHBd9bzpzfX4XYpLj1iMl+aN7aoL9qZ5LUVO7jm8SUcOmUYt3xpTtLjku3Cef2lO1e11ppfPv8pjy/azFXHTufsA8elvW0F+GJWm3y52oIRm/ZQ+sU+AdbsaOOe19fw6qc7qCp1c/CkYUa8xOJeCsnili2CEZtgxE5aegCyL2bMPkTQFA7NzbBlC1RUoJWiPRQlkMzFEo1S8b8/p/SvfyF4wkm0/uKX/Ls+xC/e3krU0Vx18EhOm5Z6Mbbt7RGufmkTKxqCXHngSM6ZNbTnNgZui6qSPtTF0NrE9IRC5nVlJVRXQ9xilCa7A/WymHGQa+Lm+DJv/ps+9iWNfmODn1/9+1PeWdPAjJGV/PDEGUnrlQwmPtrUzGV/XcSU2gp++9X9kxYcLJbO1d11CLcdzXX/+IjXVuzg/52+N8fM6rl6eE/EA4lTydjLBKGoTXuoe3d/T2xuDPD7/6zhuY+3Uup18dWDx3POQeOpkN5We5ALMWP2I4KmsEgQNVjW7hTmLpYIrSn94/1U3HITkf32p/m397DFU8n1b9SzYIufI8ZV8D+fGc3QXgopLd8Z4OqXNuGPONx45BgOH9dz1k2px0VFJp4itTbCJhIxlpohQ4zA8aXvqnAcTWseiltlEpfVkXZdKKb3OKGosYalaoLXWvPyJ9u59QVTu+a42SOZOKycmnIvNeVehpZ7GRZ7PdCfXDfs8nPRAwuoLHHz+6/PoyZJkGeuC+f1l2Z/JKnIDUZsvvO3D1i+pYXbz96XeRNr+rUfhXl4ylYgcTjm3k8nyy/O9tYgf3xjHU8ursdtKc6aN5avHzIxb6LUbZmGkR6XQqGIOIXTPypXYsbsSwRN4dHUZERNZSVYlgnYDEUJJbmIeJ97lqofXIU9ejQt99xPZMJEHl7WwG8W7KDSZ/Hjw7tP7355XQs/eb2emlI3tx49lqk1PVT+BcqzVc7ecSAYNK4wnw9qaqCsLG2XVMR2aEszDTnfJFbzLWT6VLsmFOXe19fw7EdbdgdDdqbM62JomXe32Nkteso8u18PK/cxtNxDhS9zJQGyhdamGWEw7NDgD3PVIx8SCNv8/uvzkgbX56NwXn/pqVhgcyDCJQ8sYEdbiF+cMYcDJ/Zs7U0VSylKY4K/v5bLiG2ETLoNcgGa/GH+/PZ6Hl+4iaijOX3fOs4/fFJOY8YspfC6TDNIT1zE9HCM400yI7EmmRGn+4zaTJNLMWP2J4KmMGlshG3bjKiJnawmWK1req170UKGXPYN0Jrm395DdP8DWNUQ5Mev17OqMcSZM6q5IiG9W2vNnz7axW8X7mCf2lJ+efRYhvVQqE8pqCrJURppJGIsN2CsVH1wSfUnqC8X9FbNt5DpS+0aME/Djf4wDe1dp87zm7rJqPG6LIaWe2KCp6sIqkmYV1Xq6fbGF0/dD4RtghGHQMQmELEJhu2O18mWx+dHbIJhp8u68deJp12Jx+q2TUA+C+f1l54K721tDnLJgwvY1hJifE0Zp8wdzcn7jE5agK8vxB8CfO70Aon702+pLRjlr+9t4G/vbSAYsTlx79Fc9NlJ1FX3rz1JbygFHsvC47ZwW0bIZMIN5zg6wYpjXmf6eplrMWP2KYKmcEkiamxH0xrs+nRkrV/HkEsuwLVlCy23/JrwCScRth3uXrRjj/TuaTU+/vfNrTyzupnjJ1fx48NH4+tBqLgsxZDSPFx0O7ukqqvNcfCmVpchk/UjMkU61XwLnXRr16RD1HFo9kdoSBA6je2RrmLIH6axmxL9ljK1XIaWebAdvVuYxOsKpYNLKUq8FiVu125rWjzGo8Rr7S4c1zGv4/XeY6qSFk8shMJ5/aWnhpuBsM3Ln2xn/uJ6PtzYhEspDps6jFPn1nHY1GEZCRZXit3HvaebZn/6LQUjNo8u2MQD76yjJRDl83uN4JIjJjNpeHl/hp4UBbvdRsbykttMS601EVsTdRzz1zYipy8/8XyIGbNfETSFTUMDbN++h6gBCISjXYJhVWMDQy77Bp5FC2n7/g8JXHAxKMWCLe38LJbePX6Il7VNYS7dbzgXzh3e4xOOx2WCf/N+83UcCASMS6qkxLikystTqg3Ul/oqnVGx/ylMKf54wbZ4gpcyC7ssI2G5ZZFSo85iIhPHtr9orWkJRhOETyfB4w/jtjpSg0uSiI4Sj7XbnbFbmCQIl95M+ulSSIXz+kvUNq61nm4X63e189TiLTzz0RYa2sMMK/dy8pzRnDKnLmNVpt2WcUmVuDseFvrTbyliO/zzg8388c117GoPc+iUYXzjyMnsNSr15qypjNntsvZwHxUicVdV1EnNZZUvMWP2LYKm8OlG1ERtp2t6dyhE5Q+/R8lz/yLwla/S9qOfgNtNa8jm5ne28tqGVn58eB3HTe75h1niKdAU0kSXVFWVCSYuKenVJRW1nd1P5ipBnHR+30WsFFFsQ77oS+2awUyhFc7rL+GoQ5M/eTXhRKK2w1urdzF/cT1vrdqFrTX7jqvmlLmjOXqvkRmJz4tX0bYsYyVK94yMOg7PfbyV+/6zli3NQfYdV803j5rCvuOq+zUul6ViriOF2+o97qXQseMxOY6x5ERsjaN1XsUMiKDJ9zBSpxtRo7Xumt7tOJTf+kvK7ruX0JGfo+XWO4xFA4g6uufie5g2BqUFWNxrD7Q2gcTRqHFJDR1qYm5SdEkJmccfjhaUi68QKdTCef0lGLF3d39OhZ1tIf710RbmL65nY0OAMq+LY2eN5NS5dcyuq8r5zd7Rmlc+2c69r69h3S4/e42q5JtHTeHgSTVpj0UpYlaXmPvIykzcS6HjxNxT+YwJE0FTTOzcaaZOogZImt5d8vBfqbjhp0T3mknL3b/HGdlzbQgFVJZ6is81YttG3DiOCSCurk7ZJSVkFsfRtKZZu2awUOiF8/qLPxzd3fcnVbTWfLixiacWb+GlT7YRjDhMHl7OqfvWceLeo7ot0pYptNa8tXoX97y2hk+3tTJxWBnfOHIKR82o7ZOQKfe6KfMWXumFwYIImmJj507YtctYIzr9aJKld3tfe4Wq716OM2QIzffcjz19RtLNWkoxpNRT1EGKAITDZoK0XFJCZkm3ds1Ap1gK5/WXtpCJ7evrZ19Yto2nFteztL4Ft6U4Ynotp86t46BJNRl/8l+0vpG7X1vNkk3N1FWXcPFnJ3P87FFp70cBZb7s9FgT0kMETbGhdYeoSWKpga7p3e5lS6m69CJUwE/rrXcQPuwze9R4cbsshhRC8G8mEZdU3ulrp/SBRrEVzusvzYFIv4tcrt7exvzF9Tz78VaaAxFGVvk4eZ/RnDK3rt+p0su3tHD3q6t5d20DtRU+LvjMRE6ZW5d23IcCSr0uyr3ugXXtLGJE0BQjKYiazundVn09Qy69EPfKFWjLwhkxAmfUaKirwz1uLKquDsaMgbo6Mw0b1tG4stixbRNIbNtG1NTUGNeUkDMcxwQN2lqjtTk/Ha1xtBE+5r15PZCuPC5LUVPmHXQ3vCZ/uE+F6zoTjjr8Z+UO5i+u5901DWhg3oShnLpvHUfNqE3LPb5mRxv3vLaGV1fsYEiph/MOm8CZ+49NO0A7XkeqvADakwh7IoKmWImLmoYGI2q6wR+K4g+bJ2TV1orvuWexNm/CtWULnm1bcW2pN1WJQ6E9P+j1GmEzenSHyOkseqqqis+VEwyaTKnSUiPaysqK798wwHESxI75a17bjkZ3ep3Lq1RP6fuWSp66X5KByrbFiNYdtX/60iMpGVubgzy9pJ6nl2xhS3OQyhI3x88exalz65gxqvtr4KZGP7//z1qez0C/pRK3i4oSETKFigiaYkZrk/nU1NSjqOmc3q2AipKE1FGtjTCqrzfT5s0dr+PT1q0dXbrjlJd3FTlxERSfV6iWkHDYiDiPB4YP3907SyguEgWOE7f+xESQdtgdJJ+sXpClFIkp+4nrdVlHRG+fidodVZUzcVtxtGbBukbmL67ntU93ELYdZoys5JS5ozl+9iiqSk2s0vbWIH94Yx3zY/2WvjxvHF87ZEKfYpl8bosKn7v4YwwHOCJoip24qGlsNBaTblfTu/uXVJV60q8TYNumanFnoZM47djR9XNDh+5p3Tn6aDjqqMIRD9GoKdrndhtXVFWVZEcJQhbQWhOKOn2q1twdzYEI/166lfmL61mxrQ2f2+KoGbVUl3l5YtFmHK05fb8xnH/4RIb3ofWC12VRUeIu2KJ3wp6IoBkIpGipMavq7D1thkLGkpPMwlNfDxs3QlsbTJkCF1wAZ521uz5O3rFtI2yUMiJsyBBjvREEIeOYdhTGapOpTLhPtrYw/8N6nl+6DX84yon7jOaiz/St35LHZSwyA6Ga82BCBM1AQWtjQWlu7lXU5I1wGJ5+Gu6/Hz780FhDzjkHzj8fxo/P9+gMiZ2/hwwx4saXu066gjDYCEVNs89QNP3KvskIRmz8YZua8vQzGt2WotznHlCVnAcTaQsapdQZPW1Qa/2PDI2t835/BlwMxP0a12mt/xVbdi1wIWADl2utn+9tewNO0IARNVu3QmuriQkpVLSGhQuNsHnmGfP+uOPgoovgkEMKI0g3nvYdiZhjOWxY4cYDCcIAwHE0wajpXJ7rNhouS1EhQqbo6U7Q9BT+fUoPyzSQFUET4zat9a8SZyilZgHnALOBOuBFpdR0rfXgK1WqFIwaZV4XsqhRCubNM1N9Pfz5z/DQQ/DcczBrlhE2p51mCuLlc4ylpWYKBmH9ejOe2lrJjBKELGBZijKvaQ0RsTu6o2fTWWApFWv1IkJmIFNwLqeYhaYtiaC5FkBrfVPs/fPAz7TWb/e0vQFpoYnjOMZS09ZWuKKmM4EAPPGEsdp88omxiJx7Lnz96x0iLd9IZpQg5JR4IHEgbBPOUPo3mOeRCp+bUo+0KRhI9CuGRil1MsYysvtRWmt9Q0ZH2LGvnwH/DbQAC4CrtdaNSqnfAO9orf8SW+9+4Fmt9WNJtnEJcAnA+PHjD1i/fn02hloYOI6pL9PeXjyiBoyr5803jbB54QWTcXTKKXDhhbDffvkenSGeGeVyGeFVWblH5WVBEDKP7WiT/h229+hZlw7Sb2lg052g6fWxUyn1O+Bs4DuYkg1nARP6OZgXlVIfJ5lOA+4GpgD7AluAX8c/lmRTSc92rfW9Wut5Wut5tbW1/Rlq4WNZph5Mebmx1BQLSsFnPgN//CO88Qacd54RNl/4ghE2Tz5p4lryidttRExJiUlVX7PGFDnM97gEYQATj3OprfRRXeahxO1KevFPhgLKfW6Gl/so97lFzAwyerXQKKWWaK3nJPytAP6htT4u64NTaiLwtNZ6b3E59ULcUuP3F06KdLq0tsIjj8Af/gDr1hkX1HnnGZdUTU2+R2esSn6/OdaSGSUIOaO3QGLptzS46LOFBgjE/vqVUnVABJiUycElopQanfD2i8DHsdfzgXOUUj6l1CRgGvBetsZRdMQtNaWlxv1UjFRWGpfTf/4Df/oTTJsGN98MBx4I3/8+LF+e3/EpZcRiRYU5xuvWwaZNxi1VYLFogjCQiAcSD6vwUVPupdTr2l3ludTrYliFj8qB1nhXSJtUAgKeVkpVA78EFmHcPPdlcUy3KKX2je1nHXApgNZ6qVLqEWAZEAUuG5QZTj1hWaZS7+bNxpJQVpbvEfUNy4JjjzXTp5+aOJvHH4e//hUOP9xkRx19dP4q/cYzo8AED2/YYCw16WRGad0xdX7f0wTGQtTd5HIZ65GkngsDFI/LwuOyqPS5cTTSb0nYTVpZTkopH1CitW7O3pAyy6BxOSVi2yZN2u83N1eXy8SDuN3Fm4bc0AB/+5uJudmyBSZMMIX6zjmnMAoMRiIm7dvjMaJG665iI3EemPepfh+J6yrVMXV+b9smmLm01GRolZYW73cuCIKQhD5nOSmlvp5svtb6gQyNLasMSkED5gYYiZgpEDDiJhjsuDEWq8iJRuHZZ+G++2DBAuMCilchnpQ1T2jq2LY55okiI9mUbcJh8337fB2p58X0PQuCIHRDfwTNnQlvS4CjgUVa6y9ldojZYdAKmmQMNJGzeLERNk89ZYTO0UebGJzPfrY4xp8LEi1H8dRzqakjCEIRk7FeTkqpIcCDWutTMzW4bCKCphcSRU4waIJdi03kbNsGDz5opp07YeZMuPJKOOkkuXnHiTfltCypqSMIQlGTSUHjAZZorWdmanDZRARNHyhWkRMKmfo1d90Fq1YZYfPd78KJJ4qwieM4xjIHJhVeuo0LglBk9Mfl9BQdBewsYBbwqNb6hxkfZRYQQZMhkomcUMjcIAtN5Ng2zJ8Pt90Gq1eLsElGYk2d6mozSU0dQRCKgP4ImiMT3kaB9VrrTRkeX9YQQZNFehM5paX5S62G5MLmqqvghBNE2MSJdxuPRo0bqqYmv81CBUEQeqE/hfVO0lq/Fpve1FpvUkrdnIUxCsWGUuD1mkyjYcNg/HiYOtVkG40YYcRNW5sRFvnA5YIvfhFeeQXuvNOM5+KL4bjj4F//6kifHszEhWdlpRE269aZujp+vxQLFAShqEhF0BybZN6JmR6IMECIi5zqapg8GUaONCnEra35FTZnnAGvvrqnsDn+eJMCLsLGUFICVVXme9q4EdavN4JUhI0gCEVAt4JGKfVNpdRHwF5KqSUJ01rgo9wNUShaLMsEnU6aZNoyxIVNNJqf8cSFzSuvwP/9n7FIXHSRCJvO+HzGYqOUqTq9di20tMjxEQShoOk2hiaWnj0UuAm4JmFRq9a6IQdjywgSQ1NAOI554o93rC4tzW/qcDRqsqJuu83ctGfNMjE2xx8vMTaJRKMm5dvlMq7Fqqr8xkYJgjCoSTuGRmvdrLVeB9wBNGit12ut1wMRpdTB2RuqMGCxLHMznDjRWGyi0fxabNxuOPNM44r6v/8zN+2LLjJBw889J66WOG63sdj4fLBjB6xZ0yFKBUEQCoRUHkPvBtoS3rfH5glC34gLm0mTTDPNuLDJ1w0yUdjccYcJiL3wQmOpEWHTgctlWiiUlkJTkxE227cbV6IgCEKeSUXQKJ3gl9JaO6TWpVsQekYp8+Q/aRKMGWOCUfMtbL70pQ5h097eIWyef16ETRzLMg04KypMbM2aNaYZajCY75EJgjCISUXQrFFKXa6U8sSmK4A12R6YMIhQytwcJ02CsWONcGhtzd+Tf1zYvPYa3H67ETYXXGBcUSJsOlDKCJuqKuOuW7/etKHIVzabIAiDmlQEzTeAw4DNwCbgYODibA5KGKQoZWraTJhghA0YC0A+hc1ZZ3UIm7Y2ETbdEa9l09pqatnE2ysIgiDkiF4FjdZ6u9b6HK31CK31SK31fwETsz80YdCSKGzGjzfz8m2xiQub227bU9j8+98ibBIpKzO9oTZsEGuNIAg5JeXcVKXULKXUDUqplUhQsJAL4i6NCRNg3DgTu9HSYgrj5QO3G7785T2Fzfnni7DpTDwrqqVFrDWCIOSMHns5KaUmAF+JTVFgAjAvls5dFEgdmgFGIGBShwMBU5E4nw0Vo1F4/HGT8r1uHeyzD3z72yZzS+uOQnSOY97H5yUu6zwvvl6y9btblri9uXNhr73ycjiSEomY76qmxtSwkfo1giD0k7SbUyql3gKGAA8DD2utVyql1mqtJ2V3qJlFBM0AJRAwtVDa242wyWdDxbiwueMOExibb44/Hq68EubMyfdIDPHO3i6XqT9UWprvEQmCUMR0J2h6Sr/eAYwFRgK1wEpAbOpCYVBaatxQwaARNi0t+RM2bjecfbZpq/DOOybWRynjIlNqzyk+L1PLEt9Ho/DEE3D//XDiifD5zxthc8ABuT8micRjoiIRY8kaPtxYa6QasyAIGaQ3l9MQ4EyMy2kqUA0cr7V+LzfD6z9ioRkkBIOwa5cJHvZ4BrcVoKUF/vhHuPdeUwDviCOMsDm4AAp8a22sam63WGsEQegTabuckmxgBHA2RtyM01qPy+wQs4MImkFGorDx+fIbY5Nv2tvhgQfgd78zVqxDDzXC5vDDjdUkn4TD5rsaPtzE14i1RhCEFOm3oOm0sQmxvk4FjwiaQUo8eNjvN1YAjyffI8ofgQD85S9w990mlXrePCNsjjoqv8Im0VpTV5ffOChBEIqGtJtT9kSxiBlhEBOPsRk3rqOlwmCtiVJaChdfDG+9BTfeaNoUnHsufOEL+U03j1eItiwTW7NzZ0e2liAIQpqInVcYuMSDUSdNMvEa4bCpHTNYb5olJfDf/w1vvgm33AINDaaOzvHHwzPP5O+4eL2mbk1Dg8kSk55QgiD0ARE0wsBHqY7u3rW1xgXT3j54C+F5vfDVr8Lrr8Ottxq33CWXwDHHwJNP5seSFbfWKCXWGkEQ+kRagkYptShbAxGErGNZMHSoETZDhxprTSCQ71HlD4/HpJu/+ir85jdGQHzrW/C5z8Fjj5k08FyTaK3ZsEGsNYIgpEy6Fpo8p0YIQgZwu012zeTJxiXV0jK4b5xuN3zxi/DyyyYjyuuFK66AI4+Ev/0t9z204tYarY21pqFBrDWCIPRKuoLmmayMQhDygccDo0YZi43Xm9/O3oWAZcEpp5hA4fvvN5aS730PPvtZk/6d6x5aPp8Zw86dxlqTrx5egiAUBWkJGq31/2RrIIKQN3w+GDt2z87e+XC3FAqWZRpuPvusETK1tXDttXDYYUbo5NJNl8xaM1hjnwRB6JG8BAUrpc5SSi1VSjlKqXmdll2rlFqllPpUKXV8wvwTYvNWKaWuyf2ohQFPvLN3XZ0RNIM51RuMmDj6aHjqKeN6mjABfvITU6Dvd7/LbRdtn8+4B3fsEGuNIAhJyVeW08fAGcDriTOVUrOAc4DZwAnAb5VSLqWUC7gLOBGYBXwltq4gZBaljJtj4kTjjgqFBneqN5hjcsQR8I9/mGDh6dPh5z83rRR+8xtzfHKBZZnvxnGMtaaxUaw1giDspldBo5S6IpV56aC1Xq61/jTJotMwnb1DWuu1wCrgoNi0Smu9RmsdxnQAP60/YxCEHrEsGDLExNcMH26sEYM51TvOoYfCI4/AP/8Jc+fCTTcZYXPbbdDcnJsxxK0127cba81gjnuybRNjVF9vYsBCITlHhUFLKhaa85LM++8MjyPOGGBjwvtNsXndzReE7OJymV5DkycbgTPYU73jHHigaafw9NPm9a9+ZWJs/vnP3Ow/bq2xbVi7dnBaa9rbOyxVoRBs3WoKE65aJQJHGJS4u1uglPoK8F/AJKXU/IRFVcCu3jaslHoRGJVk0Y+01k9297Ek8zTJhVe3v1Kl1CXAJQDj44GegtAf3G4YMQKqq80TcUuLNL8E2G8/+NOf4OOP4Zpr4LLL4PnnTYuFmprs77+kxGSobd9uvpMRIwZ+B+9o1MQSNTebuC937DIePxe17nCVQkdgdXm5OV4eT/6bkwpCFuhW0ABvAVuA4cCvE+a3Akt627DW+pg+jGcTkNjFeyxQH3vd3fxk+74XuBdMc8o+jEMQkuP1mqDhmpqOm+hgb34JsPfexjpz112m+vC77xqrzec/n/19x601oZBxQVVVGTfhQPtOtDaB6tu2dVS/ToZSe4ptrU2dpdbWjuUicIQBSLcuJ631eq31q8AxwH+01q9hBM5Ysldgbz5wjlLKp5SaBEwD3gPeB6YppSYppbyYwOH5PWxHELJLSUlH80vHkYwoMJaCK64wfaGGDIGvfc1Ybdrbc7P/eN2a9nZYs2ZgFeQLhWDTJuNKKikxlplUiQucigozlZUZgbN1q3FZrVoFW7Z01GESF5VQpKQSQ/M6UKKUGgO8BJwP/Kk/O1VKfVEptQk4FHhGKfU8gNZ6KfAIsAx4DrhMa21rraPAt4HngeXAI7F1BSF/xJtfTpy4Z/PLcBgiEeMaiEaN0HEcMw2Gm8Xee5saNpdeauJsjjsO3n8/d/svKzPfy65dJr6mtbV4j7vjGGG2bp05p6qqTFxXfxCBIwxQlO7lZFVKLdJa76+U+g5QqrW+RSn1gdZ6v9wMsX/MmzdPL1iwIN/DEAYDjmPiGuKxC3EBE5/i83qzGmjdswugp+WWZVxgheJCePttuPJKY1n41rfgqqtyG3cUjZog7tJSE19TUpK7ffeXQMAIi2jUCLRcfadad4hyMOdUebm4qISCQSm1UGs9r/P8nmJoEj6r8kN+0wAAIABJREFUDgW+ClyYxucEYXARb345dGhq68dFTqLg6W5eqstDIfNEXyjC5tBD4cUX4frrTc2al16CO++EmTNzs3+3uyO+Zt06890MG9YRSFuIxFOxm5o6LCm5pLcYHBE4QoGSyq/6CuBa4Amt9VKl1GTglewOSxAGAfGbQCZvBpWVJhOrqcm4XFyu/AubykoTIHzccfD978NJJ5m/l17af/dJqvh8JqC7tdVY0WprTZyPla/aot0QD/oFI2QKQSgkEziBQIcrz+eDMWMGXhC2UHT06nIqdsTlJAxaIhFTo6SxsTCEDRiR9cMfmhibgw6C2283LRVyieOYQoluN4wcaWJI8n1cIhGTNdfaumcqdjEQDBphM26cEY2CkGW6czmlUim4Vin1S6XUv5RSL8en7AxTEISM4fGYuJHJk42VJF4UMJ8PMcOGwe9/b4TM8uVwzDHw0EO5HZNlGeuH2w0bN5rsoWAwd/tPRGtjMVq71oyhqqq4xAwYt5NlmaJ++TqOgkBqWU4PAZ8Ak4DrgXWYNGpBEIoBj8dYIiZNMnEPra35rXasFJx1lomn2W8/+MEP4LzzjIUil7jdRkBEIuZmvG1bbrusx+vmbN1qrDLFXBAw7tJbvz63TUsFIYFUBM0wrfX9QERr/ZrW+gLgkCyPSxCETOP1moabkyebG2i+hc2YMfDwwyZg+M03TRG+p5/O/ThKSozFprXV1K9pbMxu/RrHMUG/a9ea15WVhRfL0xc8HiPKNmzoCCAWhBySyq8olrvHFqXUyUqp/TDF9QRBKEa8XlM3Z+JEcwNqacmfq8Cy4KKL4LnnTAzGpZfCd76Tu0aXcZQyIq+szLQVWLcuOwUB/X6z7YYGI2QGWusMt9tYATdtyv13KAx6UhE0/08pNQS4GvgecB/w3ayOShCE7OPzmTYOEyea1/kUNtOmwfz5pk7Nk0/C0UfD66/nfhzx+BqXy8TXbNxoXEP9JRo1Lq0NG8y2CyWDKRu4XEasbdlihNsATzwRCgfJchIEwRAMGldIW1t+G29++CFcfjmsXg0XXADXXZe/+JJg0BSZq6kxU7oBu1qb47l1a0dtoMFCvPfUsGGmt9ZAFXBCzulPltOflVLVCe+HKqX+kOkBCoKQZ0pKYOxYk0btdhuLTSasE+my776mY/eFF8If/mDq13zwQe7HAeaYVFaaY7F2rXGjpBpfEw7v2X9pMIkZMAKmstJYabZuHTh9tYSCJRWX0xytdVP8jda6ESiKtgeCIPSB0lITzzJhgnEftLaam3Oux3DDDSZoOBCA004zxfni5fhzSTy+prS0o99RT5k8WpvA4nXrzHGrrMxdAcFCIy5qWluNC2qwN3AVskoqgsZSSu2u5a6UqkFaHwjCwKe0FMaPN+IGOpoV5pLPftakd3/xi3DbbXDKKbBiRW7HEMeyOsTJhg2weXNXC1YwaFKXt28v/lTsTFJRYYTppk25TY0XBhWpCJpfA28ppX6ulLoBeAu4JbvDEgShYCgrM9aauLBpbc2tpWTIELjjDrj3XnNDPOEE8zpfLgyPx9SvCQaNG2rHDiP04tlRMHBSsTNJWZkRMxs35sfSJgx4UgoKVkrNAj4PKOAlrfWybA8sU0hQsCBkEK2Nu2X7dmOdKC3NbQ+f7dtNH6gXXzSNL2+/3cT95Iv48bBtY7kphDYKhU68VcLYsQMvbV3ICX0OCo5RA7Rrre8EdiilJmV0dIIgFAdKmTojEyeaG5JtG4tNrtwII0bAn/5k4mmWLDHp3X//e/5Sg+PHo6rK/M21mFm1Cm6+2aS679iR2333lXirhA0bpFWCkFF6tdAopX4KzANmaK2nK6XqgEe11ofnYoD9RSw0gpBF4mnJO3YYN0JZWe4CYDdsgCuvhHffNW6om24ygmcwEAzCb34Dd921Z1zTjBlw+OFmOuQQ03m9UIlEjJVv7Fhz3ghCinRnoUlF0HyIyWpapLXeLzZvidZ6TlZGmmFE0AhCDnCcjnor8c7eucC2TbPLm282FZC/+11Tu2Ygd31+4w245hoTv3PGGfCjH5kMorfeMi0k3n3XCB6lYJ99OgTOQQcZK1IhEY0al92YMSbuSBBSoD+C5j2t9UFKqUVa6/2VUuXA2yJoBEHoQjhsKuL6/bmthrtmDfzsZyYjasoU0x/qc5/Lzb5zxa5dJpX9sceMy++mm+CII7quFwqZ4oRvvmmmhQuNNcTtNjV+4gLngAOM+yff2LYRw6NHF7ZFSSgY+iNovgdMA44FbgIuAP4ai6cpeETQCEKOiddh2b7d3DBzaS158UX46U9NttFxx5nXEyfmbv/ZQGsTJ/Tzn5v+Ut/6lul3laoVLBCA99/vEDiLFxuLms8H8+Z1CJy5c3Mb4J1I3MI3YgQMHSqB1UKP9FnQxD58LHAcJsvpea31C5kfYnYQQSMIeSIYNFVybTu32T+hENx/v8mAikQ6Gl4WmrslFVauNO6ld94xLqObb4bp0/u3zZYW45aKC5xlsaTV8nI4+OAOgTNrVm4LAsbjsWpqpFWC0D1tbXgqKz+MxEJgEulR0CilXBgBc0w2x5dNRNAIQh6xbdMfqrHRiJp0eyH1h61b4X//Fx5/HEaNgh//2FQcLoYbZTAId95pgn7Ly+F//gfOPjs7tW0aGjrib956y2ROgXH/HHpoh8CZNi37xy7e/2nIEBg5Umr5CB1oDU1NsGULvtmzl4S0ntt5lVRcTvOBr2mti7IXvAgaQSgA2ttN4KpSua+e+/77Rsx89JGxcvz857D33rkdQzq8/jpce61xm515JvzkJ8ZikSu2bu0QOG+8YYoZAtTWwmGHdQicCROyJ3Da2oyQGzVq8LaNEDpwHJNJ2dgIPh++yZP7LGgeAQ4BXgDa4/O11pdneszZQASNIBQI0agJGG5tNQHDuXz6tm3TF+oXvzBPeeeeawr01dTkbgy9sXOnCWb+xz9M3M8vfmFaP+SbDRs6rDdvvmm+QzCZSYcfbpqIZkMg+v0mpmfMmNxa9oTCIho1D0OBgLluhMP4Jk3qs6A5L9l8rfWfMzTcrCKCRhAKCK1NDMe2beZmletKsU1NcOutpjhfZaURNeeem98bpuMYsXXjjcaS9e1vm6kQMpA6ozWsXm0sN/EYnEgE7rsPjjwy8/sLBIzwHTs2fwHLQv4IhUzPNMfpqFXUH0EDoJTyAvFItE+11kXTiEMEjSD8//bOPD7K6vr/75tJyAKEJewgEhQXlhBklwpRJPL9qghUioCKICJaK1pFq0gtKkqhP7S8rKWUIloDBEEqlgoSXOJGBTQgS2QViPAVBIRA9sz9/XFmJgkkIQmzJTnv1+t5ZebZ7n2eO5PnM+ece04Qkpsrv7ry8gKTYTc9XdxQX3wBV18tbqi+ff3bB5BCm08+CV99JYnwZs6UWJXqwtGjMGaMBC/PnQtDhni/DS2VUDvJyhJ357k/fMoRNBe0+RpjEoDdwF+A14BdxphSkh8oiqJUkPBwqeTduLH/i10CXHUVLFsmRS5Pn4bbb4cHHpBfg/4gO1tmLCUmiqiZM0fyy1QnMQMyzXr5cujWTaaTv/mm99vQUgm1j1OnZLwjIiolYitabTvRWjvAWtsfuAl4uYrdVBRFEUJCJNj10kuLMsb6E2Pg5pvhk0/gt7+FDz4Qt8krr/j2wfnJJ3DjjWLRGDpUgoBHjqwes69Ko0EDWLwYbrhBgpn//Gfv19YKD5d8RgcO+P9zovgPayX498gRiZeppCu4IoImzFr7XVF7dhegzkxFUbxDZKQEwdatK9aSwkL/t//YY/Dxx5JdePZs+bt2rXcfzMeOSWzM6NEi5pYtE/EUE+O9NgJFZKTk/hk+HGbNkqzNTqd32wgLk3YOHhSrnlKzKCyUvFUnTkh8WxUmDVTkiE3GmH8YYxJcy9+BzZVuSVEUpSwcDkl937q1uGOys/3fh0sukbpQS5eKqXv8+KL4kIvB6YS33hLrz+rVYg1at05mCNUkwsLEOnPvvRIk/Mgj3nclhoaK8M3IkABvpWaQlydCNTtbxEwVrZUVETQPANuBh4HJwA5gUpVaUxRFKY/69SE2VtwLmZne/5VfEa67TtxPzz0H33wj7qHp08V6VFnS02HYMAn87dhRhMxjjwXnDCZvEBIi92rKFEloOGGC98WpwyGfk//7P/k17233luJfsrPFlWjtRVddL3OWkzGmrbX24EWdPQjQWU6KUg1xZwX98Uf/14MqzvHjEry7eLG4hp5+GkaMuLA5PDtb3Enz5snD9/e/l+Oqa5xMVXjjDakE3qsXvP66xNp4E3ephNBQqf9Uv75O7a5unD4tbqbIyIqPXRVnOf3L/cIYs6LSHS0HY8wIY8x2Y4zTGNOj2Pp2xphsY0yaa5lXbFt3Y8y3xpg9xpi5xtSm/wyKUsswRh5S7doVFS4MxC/xmBiJCfnPfyR4+be/lanJ33xT9jEffSQBsq++Kpl+U1PhV7+qXWIGYOxYeO01+PprmUV29Kh3z2+MiJg6dcRSs3evuKLOng2MZU+pONZKIsnDhyX410tCtDxBU/zb194rrRWxDRgOpJayba+1Nt61FHdt/RWYiFT+7gAM9nKfFEUJNiIiRNQ0bCguqIKCwPQjLg7+9S+JEfnhB7jlFhE3x44V7XP0qExdvvNO+Qf99tsyHTuYshH7myFDJInh/v3iejvoA6O/wyGuiuhoidnJyIB9++SBmZvr/faUi6OwUGYxHT9e5eDfsijvTLaM1xeNtXZn8ZlTF8IY0xKIttZ+acVH9iYw1Jt9UhQlSAkJkXwnl1wiwYOBmrYbEiKWhk8/FeHyzjsSb/O3v0n+lQED4P334fHHJVbm2msD089gIyEBkpPFhTh0KOzc6bu2wsPlIRkRIe3t3y/xGZmZ/p89p5xPfj4cOiRWtIsI/i2L8gRNV2PMaWNMJhDnen3aGJNpjKlCdFyFiTXGfGOM+cQY4y5k0hrIKLZPhmudoii1hbp1xVoTGRm4gGEQE/nUqbB+PfTsKcHDTz0FXbpASgo8+qhmtD2X7t1FABojbriNG33bXkhIkdXGWnFt7NsnVjRNzhcYcnJEXBYWynfZB5SZtcZae1ElTo0xKUCLUjZNtda+W8ZhR4C21trjxpjuwL+MMZ0o6f7ydLGctici7inatm1buY4rihK8hIZCq1YSTPh//yeunUDNGLrsMvjnPyVm5swZcUPVtjiZynDlleK2GzUK7rhDpsjfcIPv261TRxZrRQifPCnvGzeWB6sWvvQ9mZkiKiMifBq47bORtNbeWIVjcoFc1+vNxpi9SA2pDKBNsV3bAIfLOc98YD7ILKfK9kNRlCDGGJkxExkpvvgzZwJTD8rN9dcHpt3qyCWXiKgZMwbGjZOYpKF+ih4wRj4zUFT53Vqx4jRsKA9bFaTexVoRkEePynfUcVF2kgvivWgcL2CMaWqMcbhet0eCf/dZa48AmcaYPq7ZTXcDZVl5FEWpDdSpIw/IQNWDCgbcM8AyMyW2KFBB05WhSRMJmO7ZUzInL1rk/z6EhorrsF49uW8HD0q8zcmTtfNz5AucTrGiHj0q8TI+FjMQIEFjjBlmjMkA+gKrjTFrXZv6A1uNMVuA5cAka+0J17YHgAXAHmAv8L6fu60oSrBRvB5UYaE83GvLlN3sbHkYN20qhT4bNZJ7kJkpS3Z28N6L6GjJnpyYKPFIc+YEZlq+22rjzmHz008Sa+Oe/q1J+6pGQYEE/2Zmylj7yfJVZmK9moIm1lOUWoLTKVV6jx6VX+Bu90JNw13IMzpaxMy5MQkFBTJd+exZEXgFBfJgDguTYOVgcqsUFEhW4WXLxAX13HNencZbZXJzZUZdaKi4o9z5bpQLk5srghB88x0sJ7GeRkMpilIzCAkRK0W9evJL+/Rpnwch+hVrRaSEhoqrrayZIqGhRTWPmjWTB3NenvxadluwQkLkvoSFBVbghIbC//t/Mm5/+5u4fF55JfBjFh4ui9MpSft++kkezo0by+ypYBBdwciZM5KnyV0d3ReUM0tNBY2iKDWLsDApdNmggfjwc3Or/0MoJ0diO2Ji5KFamWtxz/KpV09EUV6enC8zUwQSiKipUycwQiIkBKZNk2t78UURovPnB4eFLSSkSDjm5clMHXdQeoMG5z+0z/V4FH9f0W2VOUdIiIjCkJDACtPipUp8Gfx78KBkoC4DFTSKotRMoqIkb82pU5LR1+EIjodkZSgsFPdSVJRUIr/Y/DbGFFkfGjQQC0Rurgic06dF5IA8JOvU8Usgp6dfv/61uHd+9zuZ1v3GG/I+WHALQ6dT7tXJk9JvaysmJryxX1nbjCkaM/cSFibj5158JXicTvl+nTzpk2R5HjZuhPHjyw18V0GjKErNpbq6oawtqlLdsqXvHhQhISLyIiOLgopzc0VEnT5d1AeHQ0SQr61cY8aIiHnoIUnAt3gxNG/u2zYriztpXzBhrQiL/HwRp6VlRS4ueMLD5X1xwVOVsS0okNQJ2dkS0+Ur3n4bnnhCRP3f/w43lp4VRgWNoig1n3PdUDk5YhoPRjeU2yXUqJHM4PKXlQSK6iJFRUnb+fkicM6ckaWwUIRVWJg8GH0hsm6+WR6O994rOWqWLBFLm1I2xhQJk7IoLCwKGD91qsh95f4bGloUOH6uhcft1ipObq7Eyzid8oPBFzidUu3+1VehXz+Jsyony7DOclIUpXbhng0VbG4op1NiWsLDoUWLwGVALgtriywA7rw31vrOYpGWJoU+Q0MhKQk6dfJ+G0oRTqeInsJCee10lnRxuQPJ3WLnxIkiAeQLzp6Fhx+GNWvEcjdjhrSns5wURVFcnOuGOnVKxEMgp+VmZ8uDpHlzsSIF09RqN+7A4Tp1imok5ebKgy0zU9xi3iQ+XrIK33GHFAVdtAh69/ZuG0oRISFFoqU0rJXPaE6OiNmICN+VjfjhB5nGv3MnTJ8u1roKfCeC0N6qKIriB9xuqEsvLarz4+9EdPn5EqsSFQWxsRI/EoxipjSMkYday5bS78xM7yeiu/xyePddybczerQU/1QCgzvwODxcrJq+EjPffCN10Q4ckMDwCRMq/J1QQaMoSu0mMlJETbNm8sszK8v3bVpbFJNyySUiCoI9ULksjJF75y5B4W1R07o1rFwpxS3Hj4fly717fiV4ePddscZFRMCqVZUuXqqCRlEUxe2Gio2VoMPTpyU41xfk5IiYiYmRYNdyghyrDcaIFaVZM99YumJiJJtwnz4weTIsWODd8yuBxVpJsPjggxAXB//+twjYSqKCRlEUxY0v3VDuOkthYSJkYmKCc5bVxdC4sdy/zMzSpw5fDPXqwZtvwv/+Lzz7LMyerbWWagLZ2SJk5syBESNg6VL5blSBGvZtUhRF8QKRkSI6mje/eDeUtXJ8bi60agVt2vhuZkgw0KCBXOPZs96v/h0RAfPmSTzNK6/Ao49KrIVSPfnxR3ExvfeeFCl9+eWL+m7oLCdFUZTSMEaCXevWrfpsKHdOmcaN5VenP3PKBJL69SU2KCPD+4kMHQ6YNUvu51/+IknXfvELETmDB9dssViT2LYN7rlHvlcLFsjYXSRqoVEURSmPqrihnM6iMgLt2klsSW0RM27q1oW2bYsqV3sTY6REwn//C48/Dt9/L26LHj1kmu+ePd5tT/Eua9ZI0kRjJODbC2IGNLGeoihKxbFWflEePVp2Qjl3TplmzYI3p4w/yc2FQ4eKyif4AqcTUlMlAd8HH4irq1cvsdrcckvwJE+s7VgrVrWXXoJu3WDhQvmeVIZyEuupoFEURaksBQXihvr55yI3VH6+iJkGDaRsQHWdhu0L8vLE/WSt78XFsWPihlq8GPbvlySAw4eLuNFsw4EjN1fqMS1fDrfdJrOaqvJZUEGjgkZRFB+QnS2BjTk5ImpatAi+woXBQn6+ZIAtKPDPPbIWNmwQYbN6tTxQu3YVYTN0qO/qDynnc/y4ZPvduFFchI88UnXLpQoaFTSKovgId2XsiIiaNw3b2xQUiKjJz/ev8Dt5Et55R8RNerq0fdttIm66dVO3oC9JT5fg32PHZBbTkCEXd75yBI1++xRFUS4GY+QBqWLmwoSGypTuiAiZ1u0vGjUSC0FKimSgHTJE6kTdeisMGiSxHD//7L/+1BbWrxfhmJcHK1ZcvJi5APoNVBRFUfyHwyH5eOrVK5oJ5i+Mge7dJX7jm29g5kyJdZo2Tdb/5jfipqrhngufYy3Mny+WmXbtJPNvfLzPm1VBoyiKoviXkBCJN2rYUMpMBEJA1K8Pd90F778Pa9fCr34F69bBL38JAwZIAr/jx/3fr+pOfj48+aRMnx88WKZlt2rll6ZV0CiKoij+x13UskkT3xS1rAydO8tU4q+/lhT8jRrB88+L1eb++2VKuL8rsVdHTp6UuKSkJLF2/e1vfo2V0kzBiqIoSmAwRgRNSIjMFqtfP7CxSFFRMHKkLN99J0HEy5eLy6RtW7jjDtnWokXg+his7NkDY8fC4cMwd65YuvxMrZzllJ+fT0ZGBjk5OQHqlVJbiYiIoE2bNoRpjhJFKcmpU3DkiGQYDqasyjk5ktk2KQm++EL6NnCgCJuWLcVyU9ZibfnbL2afunWlvESbNtC6dWCTB6amwqRJEvT9j39Az56+a0unbZcUNPv376d+/frExMRgdLqe4iestRw/fpzMzExiY2MD3R1FCT4yM2Vad1SUPByDjf37YckSWLZMpiEHE02birhxi5xzX/vK9fPGGxJU3aEDLFokbfqScgRNEH5ifE9OTg7t2rVTMaP4FWMMMTExHAu2f4SKEizUry+unYwMKZMQbJbM2Fh4+mmYMkVmQ2Vni4vM4ZC/xsjfCy3u/co7zpii7aXtc+qU3KeMDCkt4X69dasEOufnl+x7TIyIjdat5W/x123aVD7RYEGBBP4uXCgWq9de832ywsJCyMmhEEoNaKqVggZQMaMEBP3cKcoFiIoSUXPokLhaKlPd3F+EhcF11wW2D/XqiSDp3fv8bU6n1BsrLnTcr3fulHw8ubklj2nUqHwLT3R00b6nT8MDD8DHH0vQ9NSpvncT5ueL+691awqhsLRdaq2gURRFUYKUiIgiS01OjrxXKo57WnyLFqXHszidUovsXMHzww+wezd8+KHc9+I0aFAkcnbtgoMH4U9/glGjfH892dkibi+9tNzPggqaAJGRkcGvf/1rduzYgdPp5JZbbmH27NnU8eGvkRdffJGnn37aZ+d3U69ePc6cOeP18yYkJPCnP/2JHj16lFj/6aefMmnSJMLCwvjyyy+JDHBl3Z9//pnFixfz4IMPBrQfilKtCQ+Xh2dGhjzQtGK29wgJkSnzzZrJ1PRzsRZOnBCR4xY67tfffy/uryVL4Nprfd/Xs2fFSteq1QVdkJqHJgBYaxk+fDhDhw5l9+7d7Nq1izNnzjB16lSftvviiy/69PyBIikpiccff5y0tLQKiZnCwlKtlV7j559/5rXXXvNpG4pSK6hTRyw1DoeIGsU/GCMxN/HxUh5i0iSYMQPefFPKGaSm+l7MWCtB4vXqibCtQDxVrbfQTH9vOzsOn/bqOTu2iubZW8suU//hhx8SERHBuHHjAHA4HLz88svExsYyffp0li1bxqpVq8jKymLv3r0MGzaMWbNmAfDBBx/w7LPPkpuby2WXXcbrr79OvXMCsY4cOcLIkSM5ffo0BQUF/PWvf2X16tVkZ2cTHx9Pp06dSEpK4q233mLu3Lnk5eXRu3dvXnvtNRwOB/Xq1eP+++/no48+olGjRixdupSmTZuedx1Dhw7l0KFD5OTkMHnyZCZOnOjZNnXqVP79738TGRnJu+++S/PmzTl27BiTJk3i4MGDALzyyiv069ePr776ikceeYTs7GwiIyN5/fXXufLKK8nOzmbcuHHs2LGDq6++muxS/qEtWLCAZcuWsXbtWlJSUnjrrbd44okneP/99zHG8MwzzzBy5Eg+/vhjpk+fTsuWLUlLS2PHjh1lXv+aNWt4+umnKSwspEmTJqxfv77MPm7fvp1x48aRl5eH0+lkxYoVTJs2jb179xIfH8+gQYOYPXt25T9EiqIIoaHyQDt8WH6t160b6B4pvqawEM6ckZlbMTEVLh4aEAuNMWa2MSbdGLPVGLPSGNOw2LanjDF7jDHfGWNuKrZ+sGvdHmPM7wLRb2+xfft2up9j5ouOjqZt27bs2bMHgLS0NJKTk/n2229JTk7m0KFD/PTTT7zwwgukpKTw9ddf06NHD+bMmXPe+RcvXsxNN91EWloaW7ZsIT4+npkzZxIZGUlaWhpJSUns3LmT5ORkPv/8c9LS0nA4HCQlJQFw9uxZrrnmGr7++msGDBjA9OnTS72OhQsXsnnzZjZt2sTcuXM57koTfvbsWfr06cOWLVvo378/f//73wGYPHkyjz76KBs3bmTFihVMmDABgKuuuorU1FS++eYbnnvuOY9b7K9//StRUVFs3bqVqVOnsnnz5vP6MGHCBIYMGcLs2bNJSkrinXfe8Vx3SkoKU6ZM4ciRIwB89dVXzJgxgx07dpR5/ceOHeO+++5jxYoVbNmyhbfffrvcPs6bN4/JkyeTlpbGpk2baNOmDTNnzuSyyy4jLS1NxYyieAOHQwJgo6LkQafUXPLzIStLxrtJk0pVQg+UhWYd8JS1tsAY80fgKeBJY0xH4A6gE9AKSDHGXOE65i/AICAD2GiMWWWt3XGxHSnPkuIrrLWlznYpvn7gwIE0aNAAgI4dO3LgwAF+/vlnduzYQb9+/QDIy8ujb9++552nZ8+ejB8/nvz8fIYOHUp8KUXB1q9fz+bNm+npChjLzs6mWbNmAISEhDBy5EgA7rzzToYPH17qdcydO5eVK1cCcOjQIXbv3k1MTAx16tThlltuAaB79+6sW7cOgJSUFHbsKBqy06dPk5mZyalTpxg7diy7d+/GGEO+a7phamqSBMVEAAAVsklEQVQqDz/8MABxcXHExcWVfVNdfPbZZ4waNQqHw0Hz5s0ZMGAAGzduJDo6ml69ennyv5R1/Rs2bKB///6e/Ro3bgxQZh/79u3LjBkzyMjIYPjw4XTo0OGCfVQUpQqEhEgiux9/lCnL9etX6mGnVANyciRguW3bKsVMBUTQWGs/KPZ2A3C76/VtwFJrbS6w3xizB+jl2rbHWrsPwBiz1LXvRQuaQNCpUydWrFhRYt3p06c5dOgQl112GZs3byY8PNyzzeFwUFBQgLWWQYMGsWTJkhLH/ve//+X+++8H4LnnnmPIkCGkpqayevVq7rrrLqZMmcLdd99d4hhrLWPHjuWll166YH+NMRw6dIhbb70VgEmTJnHVVVeRkpLCl19+SVRUFAkJCZ7My2FhYR5h5u47gNPpLDVo9ze/+Q3XX389K1eu5PvvvychIaFE25WhvESRdYuZqsu6/lWrVpXa5rRp00rt4+jRo+nduzerV6/mpptuYsGCBbRv375SfVYUpYK4Z+84HBK0qqKm5nD2rMTJVDBepjSCISh4PPC+63Vr4FCxbRmudWWtr5YMHDiQrKws3nzzTUCCVB977DHuueceosrJ5tinTx8+//xzj1sqKyuLXbt20bt3b9LS0khLS2PIkCEcOHCAZs2acd9993Hvvffy9ddfAyI03JaFgQMHsnz5co4ePQrAiRMnOHDgACDCY/ny5YC4r37xi19wySWXeNqYNGkSp06dolGjRkRFRZGens6GDRsueN2JiYm8+uqrnvdpaWmAWD9at5bhXLRokWd7//79PW6wbdu2sXXr1gu20b9/f5KTkyksLOTYsWOkpqbSq1ev8/Yr6/r79u3LJ598wv79+z3ry+vjvn37aN++PQ8//DBDhgxh69at1K9fn8zMzAv2VVGUKmCMxFY0aSL5UHwc5K/4GHfwb1TURYkZ8KGgMcakGGO2lbLcVmyfqUABkOReVcqpbDnry2p7ojFmkzFmUzBmZTXGsHLlSt5++206dOjAFVdcQURExAVnITVt2pRFixYxatQo4uLi6NOnD+np6eft9/HHHxMfH0+3bt1YsWIFkydPBmDixInExcUxZswYOnbsyAsvvEBiYiJxcXEMGjTIE2tSt25dT5zPhx9+yO9///vz2hg8eDAFBQXExcUxbdo0+vTpc8Hrnjt3Lps2bSIuLo6OHTsyb948AJ544gmeeuop+vXrV2IG0gMPPMCZM2eIi4tj1qxZpQqTcxk2bBhxcXF07dqVG264gVmzZtGilEJyZV1/06ZNmT9/PsOHD6dr164e11tZfUxOTqZz587Ex8eTnp7O3XffTUxMDP369aNz585MmTLlgn1WFKWSuItatm4tMReZmUW5SpTqg9MpYxcTI9OyLzI5X8BqORljxgKTgIHW2izXuqcArLUvud6vBf7gOuQP1tqbStuvPEqr5bRz506uvvpq71xIDcRXeWQUQT9/iuJFrJXYi5Mn5eFojCRfC8ZaUEoR+fkiQlu1KpmFuAIYYzZba3ucuz4gI26MGQw8CQxwixkXq4DFxpg5SFBwB+ArxELTwRgTC/yABA6P9m+vFUVRlKDDGAkgjYyU+kJnz8Lx4/KwDA0VcaNxNsFFTo64Ci+91KsJEwMlYV8FwoF1rgDMDdbaSdba7caYZUiwbwHwa2ttIYAx5iFgLeAAFlprtwem6zUftc4oilItCQ2VFP3R0fLQPHVKFrXaBA9ZWTIObdp4vU5XoGY5XV7OthnAjFLW/wf4jy/7pSiKotQAilttmjRRq00wYK3kEKpXr2immpdRuaooiqLUXEqz2px2ZYdXq41/cDpFzMTEVDpZXmXQkVQURVFqPmq1CQwFBeJmatlShKUPUUGjKIqi1C7UauMffBT8WxbBkFivVuJwODyFIrt27cqcOXNwOp0B6cumTZs8JQYqwqeffkqnTp2Ij48vtWCkv7lQdevs7GwGDBjgyR8zePBgGjZs6CnP4ObDDz/kmmuuoXPnzowdO9aT4Xj27NnEx8cTHx9P586dcTgcnoR7L7/8Mp06daJz586MGjXKky35jjvuYPfu3b64XEVRvIXbatOiBbRvD82bi0VB89pcPFlZcn/9JGZABU3AcBeK3L59O+vWreM///lPmUUgfU2PHj2YO3duhfdPSkri8ccfJy0t7bwyBqVR6ONMnhcSNAsXLmT48OE4XEFoU6ZM4Z///GeJfZxOJ2PHjmXp0qVs27aNSy+9lDfeeMOzvztL8ksvvcSAAQNo3LgxP/zwgydZ4LZt2ygsLGTp0qWAJAV0V0hXFKUa4LbaxMbKQ7huXXFLnTkjIkepGO7Mv5GRUpPJyzOZykMFzSOPQEKCd5dHHqlUF5o1a8b8+fN59dVXsdZy3XXXecoCAPTr14+tW7fyhz/8gfHjx5OQkED79u1LiJChQ4fSvXt3OnXqxPz58z3r69Wrx5NPPkn37t258cYb+eqrrzzHr1q1CpDMwm5rxZkzZxg3bhxdunQhLi7uvJpTCxYsYNmyZTz33HOMGTMGay1Tpkyhc+fOdOnSheTkZM85r7/+ekaPHk2XLl0AeOutt+jVqxfx8fHcf//9HqGzZs0arrnmGrp27crAgQMBqYx97bXX0q1bN6699lq+++47QCqVu88RFxfH7t27+d3vfsfevXuJj48vNTNvUlISt93mSVDNwIEDqV+/fol9jh8/Tnh4OFdcIbVQBw0adN61AyxZsoRRo0Z53hcUFJCdnU1BQQFZWVm0atUKgOuuu46UlBSPlUdRlGqCe4p3ixZw2WVqtakM7sy/jRt7JfNvZVFHYZDQvn17nE4nR48eZcKECSxatIhXXnmFXbt2kZubS1xcHO+88w7p6el89NFHZGZmcuWVV/LAAw8QFhbGwoULady4MdnZ2fTs2ZNf/vKXxMTEcPbsWRISEvjjH//IsGHDeOaZZ1i3bh07duxg7NixDBkypEQ/nn/+eRo0aMC3334LwMmTJ0tsnzBhAp999hm33HILt99+OytWrCAtLY0tW7bw008/0bNnT/r37w+IKNm2bRuxsbHs3LmT5ORkPv/8c8LCwnjwwQdJSkrif/7nf7jvvvtITU0lNjbW48q56qqrSE1NJTQ0lJSUFJ5++mlWrFjBvHnzmDx5MmPGjCEvL4/CwkJmzpzJtm3bSohAN3l5eezbt4927dqVe/+bNGlCfn4+mzZtokePHixfvpxDhw6V2CcrK4s1a9Z46lG1bt2axx9/nLZt2xIZGUliYiKJiYmAVCy//PLL2bJlC927d6/gp0BRlKDC4SiKtcnNhZ9/9n+sTXkCKpiCmAsKRPD5Ifi3LFTQvPJKoHvgwV2GYsSIETz//PPMnj2bhQsXcs8993j2ufnmmwkPDyc8PJxmzZrx448/0qZNG+bOncvKlSsBOHToELt37yYmJoY6deowePBgALp06UJ4eDhhYWF06dKF77///rw+pKSkeNwmAI0aNSq3z5999hmjRo3C4XDQvHlzBgwYwMaNG4mOjqZXr17ExsYCsH79ejZv3kzPnj0BiWtp1qwZGzZsoH///p79GjduDEgxyLFjx7J7926MMZ6imn379mXGjBlkZGQwfPhwOnToUG7/fvrpJxo2bFjuPiD1tZYuXcqjjz5Kbm4uiYmJhJ7zz+q9996jX79+nj6ePHmSd999l/3799OwYUNGjBjBW2+9xZ133gmI5e3w4cMqaBSlulPcatO0adEMKVfMHCDCoyyBUd62imw3pvTt7rhL9/EhIbKEhooY85fgyc0VQXPJJVJkMkCooAkS9u3bh8PhoFmzZhhjGDRoEO+++y7Lli2jeC2q8PBwz2uHw0FBQQEff/wxKSkpfPnll0RFRZGQkOAJTg0LC8OVjZmQkBDP8SEhIaW6Q6y1nv0rQnm1wOrWrVtiv7Fjx/LSSyXLb61atarU9qZNm8b111/PypUr+f7770lISABg9OjR9O7dm9WrV3PTTTexYMEC2rdvX2YfIiMjPffiQvTt25dPP/0UgA8++IBdu3aV2L506dIS7qaUlBRiY2Np2rQpAMOHD+eLL77wCJqcnJwKxRgpilKNcDjEYhMdfX6l7wv977yQaKksTqcIicJCWfLzRVzk5oq1xOksKZZCQqT/7sUbgic7uyj414/xMqWhMTRBwLFjx5g0aRIPPfSQ5+E+YcIEHn74YXr27OmxCJTFqVOnaNSoEVFRUaSnp7Nhw4Yq9yUxMdHjUoHzXU7n0r9/f5KTkyksLOTYsWOkpqaWWhV74MCBLF++nKNHjwJw4sQJDhw4QN++ffnkk0/Yv3+/Z737mlq3bg3AokWLPOfZt28f7du35+GHH2bIkCFs3bqV+vXrk5mZWWr/GjVqRGFhYYVEjbtvubm5/PGPf2TSpEmebadOneKTTz4pEYvTtm1bNmzYQFZWFtZa1q9fX6Lo5K5du+jUqdMF21UUpZpSXBw4HEUWkrIWt6WltKUqhISIiIiMlAy8jRqJFenSS+HyyyUGKDZWLCetWklsS0SEiJzsbIl3OXOmaMnKKrK2XChWyJ35t06doBAzoIImYGRnZ3umbd94440kJiby7LPPerZ3796d6Ohoxo0bd8FzDR48mIKCAuLi4pg2bRp9+vSpcr+eeeYZTp48SefOnenatSsfffRRufsPGzaMuLg4unbtyg033MCsWbNo0aLFeft17NiRF154gcTEROLi4hg0aBBHjhyhadOmzJ8/n+HDh9O1a1dGjhwJwBNPPMFTTz1Fv379SsySSk5OpnPnzsTHx5Oens7dd99NTEwM/fr1o3PnzqUGBScmJvLZZ5953l933XWMGDGC9evX06ZNG9auXQvI9Oyrr76auLg4br31Vm644QbPMStXriQxMbGE1al3797cfvvtXHPNNXTp0gWn08nEiRMB+PHHH4mMjKRly5YVue2Koijex+EoKXgaN5YYF7fgufxyaNdOBE/LliKIIiLEspOVVSR0MjPFzZadXSR4MjOhYUOpyeTn4N+yMOW5DGoCPXr0sMVdNgA7d+4s8Us6GDl8+DAJCQmkp6cTEqK682L45ptvmDNnznlTtX3Jyy+/THR0NPfee+9526rD509RFMXjynK7tfLyZMnNFTFTgfhEX2CM2Wyt7XHueo2hCULefPNNpk6dypw5c1TMeIFu3bpx/fXXU1hY6MlF42saNmzIXXfd5Ze2FEVRfILblRYE7qSKoBYaRfEz+vlTFEWpOmVZaGrtz/+aLuSU4EQ/d4qiKL6hVgqaiIgIjh8/rg8Xxa9Yazl+/DgRERGB7oqiKEqNo1bG0LRp04aMjAyOHTsW6K4otYyIiAjatGkT6G4oiqLUOGqloAkLC/NkplUURVEUpfpTK11OiqIoiqLULFTQKIqiKIpS7VFBoyiKoihKtafG56ExxpwCdgeo+SbATwFquwFwKkBtB7r9QF97bR33QN/3QLZfW8c80O0H+tpr67gH+r53sNY2OHdlbQgKTrbWTgxEw8aYTaUl//FT2/MDdd2Bbj8Irr1WjnsQ3PdAXnutHPNAtx8E114rxz0I7vv80tbXBpfTe4HuQIAI9HUHsv1AX3sgqc33PdDtB4pAX3dt/swFktp830ttv8a7nAJJINW7Ejh03GsfOua1Ex334KI2WGgCSalmMaXGo+Ne+9Axr53ouAcRaqFRFEVRFKXaoxYaRVEURVGqPSpoFEVRFEWp9qigqQTGmIXGmKPGmG3F1nU1xnxpjPnWGPOeMSbatX6MMSat2OI0xsS7to1y7b/VGLPGGNMkUNekXBgvjvtI15hvN8bMCtT1KBWjkuMeZox5w7V+pzHmqWLHDDbGfGeM2WOM+V0grkWpGF4c8/POo/gBa60uFVyA/sA1wLZi6zYCA1yvxwPPl3JcF2Cf63UocBRo4no/C/hDoK9NF5+PewxwEGjqev8GMDDQ16aLd8YdGA0sdb2OAr4H2gEOYC/QHqgDbAE6BvradPHdmJd1Hl18v6iFphJYa1OBE+esvhJIdb1eB/yylENHAUtcr41rqWuMMUA0cNj7vVW8hZfGvT2wy1p7zPU+pYxjlCChkuNuke90KBAJ5AGngV7AHmvtPmttHrAUuM3XfVeqhpfGvKzzKD5GBc3Fsw0Y4no9AriklH1G4nqwWWvzgQeAbxEh0xH4h++7qXiZSo07sAe4yhjTzvUPcGgZxyjBTVnjvhw4CxxBLHF/staeAFoDh4odn+Fap1QfKjvmSoBQQXPxjAd+bYzZDNRHVLoHY0xvIMtau831PgwRNN2AVsBW4CmU6kalxt1aexIZ92TgU8Q8XeDPDiteoaxx7wUUIt/pWOAxY0x7xBp7Lporo3pR2TFXAkRtqOXkU6y16UAigDHmCuDmc3a5g6Jf6QDxruP2uo5ZBmigYDWjCuOOtfY9XCm7jTETkX+GSjWinHEfDaxxWWCPGmM+B3og1pnilrg2qIu5WlGFMd8XkI4qaqG5WIwxzVx/Q4BngHnFtoUgJsqlxQ75AehojGnqej8I2Omf3ireogrjXvyYRsCDwAJ/9VfxDuWM+0HgBiPUBfoA6UhAaQdjTKwxpg4idFf5v+dKVanCmCsBQgVNJTDGLAG+BK40xmQYY+4FRhljdiEf5MPA68UO6Q9kWGs9it1aexiYDqQaY7YiFpsX/XUNSuXxxri7+LMxZgfwOTDTWrvLD91Xqkglx/0vQD0k3mIj8Lq1dqu1tgB4CFiL/HBZZq3d7udLUSqIN8a8nPMoPkZLHyiKoiiKUu1RC42iKIqiKNUeFTSKoiiKolR7VNAoiqIoilLtUUGjKIqiKEq1RwWNoiiKoijVHhU0iqIELcaYQlfV8u3GmC3GmN+68oGUd0w7Y8xof/VRUZTgQAWNoijBTLa1Nt5a2wlJQvm/wLMXOKYdksVVUZRahOahURQlaDHGnLHW1iv2vj2SxKwJcCnwT6Cua/ND1tovjDEbgKuB/cAbwFxgJpAAhAN/sdb+zW8XoSiKX1BBoyhK0HKuoHGtOwlcBWQCTmttjjGmA7DEWtvDGJMAPG6tvcW1/0SgmbX2BWNMOJKpeYS1dr9fL0ZRFJ+ixSkVRaluuCtYhwGvGmPikUKfV5SxfyIQZ4y53fW+AdABseAoilJDUEGjKEq1weVyKgSOIrE0PwJdkXjAnLIOA35jrV3rl04qihIQNChYUZRqgatC/TzgVSu+8gbAEWutE7gLcLh2zQTqFzt0LfCAMSbMdZ4rXNWRFUWpQaiFRlGUYCbSGJOGuJcKkCDgOa5trwErjDEjgI+As671W4ECY8wWYBHwZ2Tm09fGGAMcA4b66wIURfEPGhSsKIqiKEq1R11OiqIoiqJUe1TQKIqiKIpS7VFBoyiKoihKtUcFjaIoiqIo1R4VNIqiKIqiVHtU0CiKoiiKUu1RQaMoiqIoSrXn/wPS9k9u1aGF5QAAAABJRU5ErkJggg==\n\"/>",
            "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard-># 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)",
            "statsmodels->User Guide->Regression and Linear Models->Regression with Discrete Dependent Variable->Examples-># Load the data from Spector and Mazzeo (1980)\nIn [1]: import statsmodels.api as sm\n\nIn [2]: spector_data = sm.datasets.spector.load_pandas()\n\nIn [3]: spector_data.exog = sm.add_constant(spector_data.exog)\n\n# Logit Model\nIn [4]: logit_mod = sm.Logit(spector_data.endog, spector_data.exog)\n\nIn [5]: logit_res = logit_mod.fit()\nOptimization terminated successfully.\n         Current function value: 0.402801\n         Iterations 7\n\nIn [6]: print(logit_res.summary())\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                  GRADE   No. Observations:                   32\nModel:                          Logit   Df Residuals:                       28\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 02 Nov 2022   Pseudo R-squ.:                  0.3740\nTime:                        17:12:32   Log-Likelihood:                -12.890\nconverged:                       True   LL-Null:                       -20.592\nCovariance Type:            nonrobust   LLR p-value:                  0.001502\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -13.0213      4.931     -2.641      0.008     -22.687      -3.356\nGPA            2.8261      1.263      2.238      0.025       0.351       5.301\nTUCE           0.0952      0.142      0.672      0.501      -0.182       0.373\nPSI            2.3787      1.065      2.234      0.025       0.292       4.465\n=============================================================================="
        ]
    },
    {
        "id": "1118014",
        "GT": "Costumers / Open->print(train_df[train_df.open == 0]['sales'].sum())\nprint(train_df[train_df.open == 0]['customers'].sum())",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->means25[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.25)\nprint(means25)",
            "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results->compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "torch->PyTorch Recipes->Timer quick start->6. A/B testing with Callgrind->print(delta.transform(extract_fn_name).filter(lambda fn: \"TensorIterator\" in fn))\n\n\n\n<strong>Instruction count delta (filter)</strong>",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Remainder->print(res_f2.summary())\nprint(res_f.summary())",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit->print(anes_data.exog.head())\nprint(anes_data.endog.head())"
        ]
    },
    {
        "id": "138265",
        "GT": "Project: Investigate a Dataset (Appointment No-Show)\n\n## Table of Contents\n<ul>\n<li><a href=\"#intro\">Introduction</a></li>\n<li><a href=\"#wrangling\">Data Wrangling</a></li>\n<li><a href=\"#eda\">Exploratory Data Analysis</a></li>\n<li><a href=\"#conclusions\">Conclusions</a></li>\n</ul>->Create Datetime Objects-># change the sheduled_day column & appt_day columns to pandas datetime objects\n\ndf.scheduled_day = pd.to_datetime(df.scheduled_day)\ndf.appt_day = pd.to_datetime(df.appt_day)\n\n# check that this worked by returning the weekday using dt from the datetime module\n\n#df.scheduled_day.dt.weekday\ndf.appt_day.dt.weekday[0:5]",
        "pred": [
            "statsmodels->User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->VAR(p) processes->Model fitting-># some example data\nIn [1]: import numpy as np\n\nIn [2]: import pandas\n\nIn [3]: import statsmodels.api as sm\n\nIn [4]: from statsmodels.tsa.api import VAR\n\nIn [5]: mdata = sm.datasets.macrodata.load_pandas().data\n\n# prepare the dates index\nIn [6]: dates = mdata[['year', 'quarter']].astype(int).astype(str)\n\nIn [7]: quarterly = dates[\"year\"] + \"Q\" + dates[\"quarter\"]\n\nIn [8]: from statsmodels.tsa.base.datetools import dates_from_str\n\nIn [9]: quarterly = dates_from_str(quarterly)\n\nIn [10]: mdata = mdata[['realgdp','realcons','realinv']]\n\nIn [11]: mdata.index = pandas.DatetimeIndex(quarterly)\n\nIn [12]: data = np.log(mdata).diff().dropna()\n\n# make a VAR model\nIn [13]: model = VAR(data)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend-># Setup forecasts\nnforecasts = 3\nforecasts = {}\n\n# Get the number of initial training observations\nnobs = len(endog)\nn_init_training = int(nobs * 0.8)\n\n# Create model for initial training sample, fit parameters\ninit_training_endog = endog.iloc[:n_init_training]\nmod = sm.tsa.SARIMAX(training_endog, order=(1, 0, 0), trend='c')\nres = mod.fit()\n\n# Save initial forecast\nforecasts[training_endog.index[-1]] = res.forecast(steps=nforecasts)\n\n# Step through the rest of the sample\nfor t in range(n_init_training, nobs):\n    # Update the results by appending the next observation\n    updated_endog = endog.iloc[t:t+1]\n    res = res.extend(updated_endog)\n\n    # Save the new set of forecasts\n    forecasts[updated_endog.index[0]] = res.forecast(steps=nforecasts)\n\n# Combine all forecasts into a dataframe\nforecasts = pd.concat(forecasts, axis=1)\n\nprint(forecasts.iloc[:5, :5])",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example-># Setup forecasts\nnforecasts = 3\nforecasts = {}\n\n# Get the number of initial training observations\nnobs = len(endog)\nn_init_training = int(nobs * 0.8)\n\n# Create model for initial training sample, fit parameters\ninit_training_endog = endog.iloc[:n_init_training]\nmod = sm.tsa.SARIMAX(training_endog, order=(1, 0, 0), trend='c')\nres = mod.fit()\n\n# Save initial forecast\nforecasts[training_endog.index[-1]] = res.forecast(steps=nforecasts)\n\n# Step through the rest of the sample\nfor t in range(n_init_training, nobs):\n    # Update the results by appending the next observation\n    updated_endog = endog.iloc[t:t+1]\n    res = res.append(updated_endog, refit=False)\n\n    # Save the new set of forecasts\n    forecasts[updated_endog.index[0]] = res.forecast(steps=nforecasts)\n\n# Combine all forecasts into a dataframe\nforecasts = pd.concat(forecasts, axis=1)\n\nprint(forecasts.iloc[:5, :5])",
            "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method-># fit in statsmodels\nmodel = ETSModel(\n    austourists,\n    error=\"add\",\n    trend=\"add\",\n    seasonal=\"add\",\n    damped_trend=True,\n    seasonal_periods=4,\n)\nfit = model.fit()\n\n# fit with R params\nparams_R = [\n    0.35445427,\n    0.03200749,\n    0.39993387,\n    0.97999997,\n    24.01278357,\n    0.97770147,\n    1.76951063,\n    -0.50735902,\n    -6.61171798,\n    5.34956637,\n]\nfit_R = model.smooth(params_R)\n\naustourists.plot(label=\"data\")\nplt.ylabel(\"Australian Tourists\")\n\nfit.fittedvalues.plot(label=\"statsmodels fit\")\nfit_R.fittedvalues.plot(label=\"R fit\", linestyle=\"--\")\nplt.legend()",
            "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation-># plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>"
        ]
    },
    {
        "id": "1127152",
        "GT": "Outliers->pct_change_df['2010'].std()",
        "pred": [
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->pca_model = PCA(dta.T, standardize=False, demean=True)",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->delays.nsmallest(5).sort_values()",
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset->co2_data.index.min(), co2_data.index.max()",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->affair_mod.fittedvalues[1000]",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf.loc[pd.IndexSlice[:, ['ORD', 'DSM']], ['dep_time', 'dep_delay']]"
        ]
    },
    {
        "id": "1002849",
        "GT": "JSON example, with string\n\n+ demonstrates creation of normalized dataframes (tables) from nested json string\n+ source: http://pandas.pydata.org/pandas-docs/stable/io.html#normalization->Solution Part I-># read world_bank_projects into dataframe with only columns related to countries\nworld_bank_projects = pd.read_json('data/world_bank_projects.json')[['countrycode', 'countryshortname']]\n# Sort countries by number of projects\nworld_bank_projects = world_bank_projects.groupby('countryshortname').count().sort_values('countrycode', ascending=False)\n# Print countries by descending order (top 10 only)\nworld_bank_projects.columns = ['Total Projects']\nworld_bank_projects[:10]",
        "pred": [
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 2: computing the \u201cnews\u201d from a new observation-># Compute the impact of the news on the four periods that we previously\n# forecasted: 2008Q3 through 2009Q2\nnews = res_pre.news(res_post, start='2008Q3', end='2009Q2')\n# Note: one alternative way to specify these impact dates is\n# `start='2008Q3', periods=4`",
            "statsmodels->User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Overview of usage-># Load in the example macroeconomic dataset\ndta = sm.datasets.macrodata.load_pandas().data\n# Make sure we have an index with an associated frequency, so that\n# we can refer to time periods with date strings or timestamps\ndta.index = pd.date_range('1959Q1', '2009Q3', freq='QS')\n\n# Step 1: construct an SARIMAX model for US inflation data\nmodel = sm.tsa.SARIMAX(dta.infl, order=(4, 0, 0), trend='c')\n\n# Step 2: fit the model's parameters by maximum likelihood\nresults = model.fit()\n\n# Step 3: explore / use results\n\n# - Print a table summarizing estimation results\nprint(results.summary())\n\n# - Print only the estimated parameters\nprint(results.params)\n\n# - Create diagnostic figures based on standardized residuals:\n#   (1) time series graph\n#   (2) histogram\n#   (3) Q-Q plot\n#   (4) correlogram\nresults.plot_diagnostics()\n\n# - Examine diagnostic hypothesis tests\n# Jarque-Bera: [test_statistic, pvalue, skewness, kurtosis]\nprint(results.test_normality(method='jarquebera'))\n# Goldfeld-Quandt type test: [test_statistic, pvalue]\nprint(results.test_heteroskedasticity(method='breakvar'))\n# Ljung-Box test: [test_statistic, pvalue] for each lag\nprint(results.test_serial_correlation(method='ljungbox'))\n\n# - Forecast the next 4 values\nprint(results.forecast(4))\n\n# - Forecast until 2020Q4\nprint(results.forecast('2020Q4'))\n\n# - Plot in-sample dynamic prediction starting in 2005Q1\n#   and out-of-sample forecasts until 2010Q4 along with\n#   90% confidence intervals\npredict_results = results.get_prediction(start='2005Q1', end='2010Q4', dynamic=True)\npredict_df = predict_results.summary_frame(alpha=0.10)\nfig, ax = plt.subplots()\npredict_df['mean'].plot(ax=ax)\nax.fill_between(predict_df.index, predict_df['mean_ci_lower'],\n                predict_df['mean_ci_upper'], alpha=0.2)\n\n# - Simulate two years of new data after the end of the sample\nprint(results.simulate(8, anchor='end'))\n\n# - Impulse responses for two years\nprint(results.impulse_responses(8))",
            "sklearn->Examples->Miscellaneous->Explicit feature map approximation for RBF kernels->Decision Surfaces of RBF Kernel SVM and Linear SVM-># visualize the decision surface, projected down to the first\n# two principal components of the dataset\npca = (n_components=8).fit(data_train)\n\nX = pca.transform(data_train)\n\n# Generate grid along first two principal components\nmultiples = (-2, 2, 0.1)\n# steps along first component\nfirst = multiples[:, ] * pca.components_[0, :]\n# steps along second component\nsecond = multiples[:, ] * pca.components_[1, :]\n# combine\ngrid = first[, :, :] + second[:, , :]\nflat_grid = grid.reshape(-1, data.shape[1])\n\n# title for the plots\ntitles = [\n    \"SVC with rbf kernel\",\n    \"SVC (linear kernel)\\n with Fourier rbf feature map\\nn_components=100\",\n    \"SVC (linear kernel)\\n with Nystroem rbf feature map\\nn_components=100\",\n]\n\n(figsize=(18, 7.5))\nplt.rcParams.update({\"font.size\": 14})\n# predict and plot\nfor i, clf in enumerate((kernel_svm, nystroem_approx_svm, fourier_approx_svm)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    (1, 3, i + 1)\n    Z = clf.predict(flat_grid)\n\n    # Put the result into a color plot\n    Z = Z.reshape(grid.shape[:-1])\n    levels = (10)\n    lv_eps = 0.01  # Adjust a mapping from calculated contour levels to color.\n    (\n        multiples,\n        multiples,\n        Z,\n        levels=levels - lv_eps,\n        cmap=plt.cm.tab10,\n        vmin=0,\n        vmax=10,\n        alpha=0.7,\n    )\n    (\"off\")\n\n    # Plot also the training points\n    (\n        X[:, 0],\n        X[:, 1],\n        c=targets_train,\n        cmap=plt.cm.tab10,\n        edgecolors=(0, 0, 0),\n        vmin=0,\n        vmax=10,\n    )\n\n    (titles[i])\n()\n()\n\n\n<img alt=\"SVC with rbf kernel, SVC (linear kernel)  with Fourier rbf feature map n_components=100, SVC (linear kernel)  with Nystroem rbf feature map n_components=100\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_approximation_002.png\" srcset=\"../../_images/sphx_glr_plot_kernel_approximation_002.png\"/>",
            "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept-># Get the federal funds rate data\nfrom statsmodels.tsa.regime_switching.tests.test_markov_regression import fedfunds\n\ndta_fedfunds = pd.Series(\n    fedfunds, index=pd.date_range(\"1954-07-01\", \"2010-10-01\", freq=\"QS\")\n)\n\n# Plot the data\ndta_fedfunds.plot(title=\"Federal funds rate\", figsize=(12, 3))\n\n# Fit the model\n# (a switching mean is the default of the MarkovRegession model)\nmod_fedfunds = sm.tsa.MarkovRegression(dta_fedfunds, k_regimes=2)\nres_fedfunds = mod_fedfunds.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_regression_4_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_regression_4_0.png\"/>",
            "pandas_toms_blog->Indexes->Set Operations-># Bring in the flights data\n\nflights = pd.read_hdf('flights.h5', 'flights')\n\nweather_locs = weather.index.levels[0]\n# The `categories` attribute of a Categorical is an Index\norigin_locs = flights.origin.cat.categories\ndest_locs = flights.dest.cat.categories\n\nairports = weather_locs &amp; origin_locs &amp; dest_locs\nairports"
        ]
    },
    {
        "id": "345352",
        "GT": "Part 5: Perform a PCA and Present Findings->Z = linkage(pca_ap, 'ward')\nc, coph_dists = cophenet(Z, pdist(pca_ap))\nplt.title('Dendrogram')\nplt.xlabel('Index Numbers')\nplt.ylabel('Distance')\ndendrogram(\n    Z,\n    leaf_rotation=90.,  \n    leaf_font_size=8.,)\nplt.show()",
        "pred": [
            "sklearn->Examples->Nearest Neighbors->Neighborhood Components Analysis Illustration->Original points->X, y = (\n    n_samples=9,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_classes=3,\n    n_clusters_per_class=1,\n    class_sep=1.0,\n    random_state=0,\n)\n\n(1)\nax = ()\nfor i in range(X.shape[0]):\n    ax.text(X[i, 0], X[i, 1], str(i), va=\"center\", ha=\"center\")\n    ax.scatter(X[i, 0], X[i, 1], s=300, c=cm.Set1(y[[i]]), alpha=0.4)\n\nax.set_title(\"Original points\")\nax.axes.get_xaxis().set_visible(False)\nax.axes.get_yaxis().set_visible(False)\nax.axis(\"equal\")  # so that boundaries are displayed correctly as circles\n\n\ndef link_thickness_i(X, i):\n    diff_embedded = X[i] - X\n    dist_embedded = (\"ij,ij-i\", diff_embedded, diff_embedded)\n    dist_embedded[i] = \n\n    # compute exponentiated distances (use the log-sum-exp trick to\n    # avoid numerical instabilities\n    exp_dist_embedded = (-dist_embedded - (-dist_embedded))\n    return exp_dist_embedded\n\n\ndef relate_point(X, i, ax):\n    pt_i = X[i]\n    for j, pt_j in enumerate(X):\n        thickness = link_thickness_i(X, i)\n        if i != j:\n            line = ([pt_i[0], pt_j[0]], [pt_i[1], pt_j[1]])\n            ax.plot(*line, c=cm.Set1(y[j]), linewidth=5 * thickness[j])\n\n\ni = 3\nrelate_point(X, i, ax)\n()\n\n\n<img alt=\"Original points\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_nca_illustration_001.png\" srcset=\"../../_images/sphx_glr_plot_nca_illustration_001.png\"/>",
            "statsmodels->Examples->Statistics->ANOVA->infl = interM_lm.get_influence()\nresid = infl.resid_studentized_internal\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        resid[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X\")\nplt.ylabel(\"standardized resids\")",
            "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features->cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)",
            "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot->x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights->glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf"
        ]
    },
    {
        "id": "213412",
        "GT": "df_profitability = df_profitability.append(df_failed_midwifing_txns, sort=True)\nlen(df_profitability)",
        "pred": [
            "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures->summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->Estimation->data = sm.datasets.stackloss.load()\ndata.exog = sm.add_constant(data.exog)",
            "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)->fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->dta = sm.datasets.star98.load_pandas().data\nprint(dta.columns)",
            "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper->print(res.recursive_coefficients.filtered[0])\nres.plot_recursive_coefficient(range(mod.k_exog), alpha=None, figsize=(10, 6))"
        ]
    },
    {
        "id": "678314",
        "GT": "Intro to Python\n[NetCDF](#NetCDF) | [Summary stats](#Summary-stats) | [Time slices](#Time-slices) | [Multiple variables](#Multiple-variables) |[Multiple sites](#Multiple-sites) | [Using functions](#Using-functions)\n\nThe first step in using Python is to load in the relevant packages. Assigning abbreviations when importing packages, makes it easier to call the tools later. The abbreviations below are very common.->Summary stats\n[NetCDF](#NetCDF) | [Summary stats](#Summary-stats) | [Time slices](#Time-slices) | [Multiple variables](#Multiple-variables) |[Multiple sites](#Multiple-sites) | [Using functions](#Using-functions)\n\nOnce you have loaded the dataset and chosen a variable to look at, you can access lots more tools by converting it into a pandas.DataFrame object. With a dataframe you can create boxplots and compute the summary statistics on that variable over the entire span of the data.-># convert to a pandas object and then to a dataframe\ndf = ds[variable].to_pandas().to_frame(name=variable)\n\n# get a summary of the data including the percentiles listed\ndf.describe(percentiles=[.1,.25,.5,.75,.9])",
        "pred": [
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example-># Construct the forecast errors\nforecast_errors = forecasts.apply(lambda column: endog - column).reindex(forecasts.index)\n\nprint(forecast_errors.iloc[:5, :5])",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend-># Construct the forecast errors\nforecast_errors = forecasts.apply(lambda column: endog - column).reindex(forecasts.index)\n\nprint(forecast_errors.iloc[:5, :5])",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Annual frequency, using a PeriodIndex\nindex = pd.period_range(start='2000', periods=4, freq='A')\nendog1 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog1.index)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)",
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models-># Construct the model instance\nmod_uc = sm.tsa.UnobservedComponents(inf, \"rwalk\", autoregressive=1)\n\n# Fit the model via maximum likelihood\nres_uc_mle = mod_uc.fit()\nprint(res_uc_mle.summary())"
        ]
    },
    {
        "id": "1061813",
        "GT": "Distribution plots\nAfter line plots and scatterplot, distribution plots are arguably the most used ones. And among them, the most used probably the histogram:\n\n> As we discussed in one of the first lessons, by default Jupyter will show you the output of the last line in the cell, which is very convenient. `hist` function returns bins and heights for each bar it plots, so that you can use this information later. In this case however it clutters the view, so I put `;` after calling `hist` function to suppress output. Try removing it and seeing what happens.->plt.bar([0,1,2,3], [10,11,12,13], yerr=[1,2,1,3])",
        "pred": [
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Working with text->Using mathematical expressions in text->plt.title(r'$\\sigma_i=15$')",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Controlling line properties->plt.plot(x, y, linewidth=2.0)",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Introduction to pyplot->plt.plot([1, 2, 3, 4], [1, 4, 9, 16])\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_002.png\" srcset=\"../../_images/sphx_glr_pyplot_002.png, ../../_images/sphx_glr_pyplot_002_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Controlling the style->plt.style.use('fivethirtyeight')",
            "matplotlib->Tutorials->Text->Text rendering with XeLaTeX/LuaLaTeX via the ``pgf`` backend->plt.savefig('figure.pdf', backend='pgf')"
        ]
    },
    {
        "id": "132599",
        "GT": "Create Revenue and Probability Models->Model Probability Customer Is Active Next Year\nWe create a model which uses RFM statistics from the year of 2014 to predict customer activity for the year 2015.->Use Model To Predict 2016 Customer Activity\nWe use our probability model on 2015 observations to predict the probability of those customers being active in 2016.->out_sample = customers.xs(2015,level='year').copy()\nout_sample['prob_predicted'] = pd.Series(model_prob.predict(out_sample),index=out_sample.index)\nout_sample.drop(['revenue','next_revenue','next_is_active'],axis=1,inplace=True)\nout_sample.head()",
        "pred": [
            "statsmodels->Examples->Statistics->ANOVA->lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings->data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions->last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Pure Premium Modeling via a Product Model vs single TweedieRegressor->res = []\nfor subset_label, X, df in [\n    (\"train\", X_train, df_train),\n    (\"test\", X_test, df_test),\n]:\n    exposure = df[\"Exposure\"].values\n    res.append(\n        {\n            \"subset\": subset_label,\n            \"observed\": df[\"ClaimAmount\"].values.sum(),\n            \"predicted, frequency*severity model\": (\n                exposure * glm_freq.predict(X) * glm_sev.predict(X)\n            ),\n            \"predicted, tweedie, power=%.2f\"\n            % glm_pure_premium.power: (exposure * glm_pure_premium.predict(X)),\n        }\n    )\n\nprint((res).set_index(\"subset\").T)",
            "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance->feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>"
        ]
    },
    {
        "id": "1005762",
        "GT": "formattedRows = [[dt.datetime.fromtimestamp(int(list[0])), list[1], list[2], float(list[3])]  for list in rows]\ndf = pd.DataFrame(formattedRows, columns=['datetime','highway','direction','simpleMPH'])",
        "pred": [
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison->pd.DataFrame(\n    np.column_stack([[r.llf, r.deviance, r.pearson_chi2] for r in results_all]),\n    columns=names,\n    index=[\"llf\", \"deviance\", \"pearson chi2\"],\n)",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->X = (pd.concat([y.shift(i) for i in range(6)], axis=1,\n               keys=['y'] + ['L%s' % i for i in range(1, 6)])\n       .dropna())\nX.head()",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "scipy->Spatial data structures and algorithms (scipy.spatial)->Voronoi diagrams->from scipy.spatial import Voronoi\n vor = Voronoi(points)\n vor.vertices\narray([[0.5, 0.5],\n       [0.5, 1.5],\n       [1.5, 0.5],\n       [1.5, 1.5]])",
            "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests->_ = plt.boxplot([scales[0][0], scales[0][1], scales[1][0], scales[1][1]])\nplt.ylabel(\"Estimated scale\")"
        ]
    },
    {
        "id": "1055213",
        "GT": "Titanic challenge->This is my decision for the Kaggle competition, Titanic: Machine Learning from Disaster.-># -*- coding: utf-8 -*-\n# Import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import Imputer",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Linear Mixed-Effects->%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.tools.sm_exceptions import ConvergenceWarning",
            "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "statsmodels->Examples->State space models - Technical notes->State space models: Chandrasekhar recursions->%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\nfrom pandas_datareader.data import DataReader",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.multivariate.pca import PCA\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "torch->PyTorch Recipes->Loading data in PyTorch->5. [Optional] Visualize the data->import matplotlib.pyplot as plt\n\nprint(data[0][0].numpy())\n\nplt.figure()\nplt.plot(waveform.t().numpy())"
        ]
    },
    {
        "id": "312353",
        "GT": "Programming Experience\n\nHow experienced are our respondents with writing code in Data Science? The Tenure feature of our dataset can provide us answers to this question.->sns.countplot(y='Tenure', data=mcq)",
        "pred": [
            "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->sns.displot(tips, x=\"size\", discrete=True)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->sns.displot(tips, x=\"size\", discrete=True)\n",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->sns.relplot(data=flights_wide, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->sns.relplot(data=flights_wide, kind=\"line\")\n",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->sns.catplot(data=flights_wide, kind=\"box\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->sns.catplot(data=flights_wide, kind=\"box\")\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(tips, x=\"total_bill\", kind=\"kde\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(tips, x=\"total_bill\", kind=\"kde\")\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->sns.displot(tips, x=\"day\", shrink=.8)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->sns.displot(tips, x=\"day\", shrink=.8)\n"
        ]
    },
    {
        "id": "1391789",
        "GT": "genreList = ['Action', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', \\\n 'Documentary', 'Drama', 'Family', 'Fantasy', 'History', 'Horror', \\\n 'Musical', 'Mystery' ,'Romance', 'Sci-Fi', 'Sport', 'Thriller', 'War', 'Western']",
        "pred": [
            "statsmodels->Examples->Statistics->ANOVA->['Intercept',\n 'C(E)[T.2]',\n 'C(E)[T.3]',\n 'C(M)[T.1]',\n 'C(E)[T.2]:C(M)[T.1]',\n 'C(E)[T.3]:C(M)[T.1]',\n 'X']",
            "statsmodels->Examples->State space models->State space modeling: Local Linear Trends-># Perform prediction and forecasting\npredict = res.get_prediction()\nforecast = res.get_forecast('2014')",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack->accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "torch->Introduction to PyTorch->Quickstart->Working with data-># Download training data from open datasets.\n = (\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=(),\n)\n\n# Download test data from open datasets.\n = (\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=(),\n)"
        ]
    },
    {
        "id": "17393",
        "GT": "Graphs with categories\n\nOne of the main uses for visualizations of large graphs is to examine the connectivity patterns from nodes of different categories. Let's consider an artificial example with four groups of highly interconnected nodes:->rd = random_layout(     cnodes, cedges)\nfd = forceatlas2_layout(cnodes, cedges)\n\n\ntf.Images(rd_d,fd_d,rd_b,fd_b).cols(2)",
        "pred": [
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Data generation->X = (0, 5, num=30).reshape(-1, 1)\ny = target_generator(X, add_noise=False)",
            "seaborn->User guide and tutorial->Statistical estimation and error bars->Error bars on regression fits->x = np.random.normal(0, 1, 50)\ny = x * 2 + np.random.normal(0, 2, size=x.size)\nsns.regplot(x=x, y=y)\n",
            "seaborn->Statistical operations->Statistical estimation and error bars->Error bars on regression fits->x = np.random.normal(0, 1, 50)\ny = x * 2 + np.random.normal(0, 2, size=x.size)\nsns.regplot(x=x, y=y)\n",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->dta[\"affair\"] = (dta[\"affairs\"]  0).astype(float)\nprint(dta.head(10))",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "numpy->NumPy Applications->Plotting Fractals->Julia set->x, y = np.meshgrid(np.linspace(-1, 1, 400), np.linspace(-1, 1, 400))\nsmall_mesh = x + (1j * y)\n\nx, y = np.meshgrid(np.linspace(-2, 2, 400), np.linspace(-2, 2, 400))\nmesh = x + (1j * y)"
        ]
    },
    {
        "id": "414066",
        "GT": "<h3>Imports</h3>->keep_vars2=['PRCNT_PV_CUST',\n#'TTL_IBO_DOWN',\n'DEPTH_WIDTH',\n'PRNCT_ENGAGED_FRONTLN',\n'PRNCT_RPT_BUYER',\n'NUMBER_LEGS_BONUS',      \n'PERS_TO_GRP_PV_RATIO',\n'BNS_DIF_TO_PERS',         \n'BNS_DIF_USD_AMT',\n'CNTRY_KEY_NO',        \n'TARGET_RATE_DIA',\n'MONTH_SILVER'  ,     \n'PERS_GRP_PV_RATIO_IBO_LEVEL_1',\n\n'PRIOR_SILVER_6',\n'PRIOR_BONUSAMT_12_CV',\n'PRCNT_PV_FROM_RPT_ABO',\n'PRCNT_PV_LEVEL_1',\n'LEGS_12_PLUS_BNS',\n'IMC_MONTHS_AFTER_SIGNUP',\n'PRIOR_PERS_GRP_PV_RT_6_CV',\n'RATIO_RPT_BUYERS_6MONTH',\n\n'RATIO_LEGS_6MONTH',\n'RATIO_GROUPPV_6MONTH',        \n'IMC_GROUP_PV',\n          \n'PRCNT_PVIBO_MB3',\n'IMC_QUAL_LEGS_QTY',\n'BNS_USD_AMT_IBO_LEVEL_4', \n'IMC_KEY_NO',\n'MO_YR_KEY_NO',\n\n    'STILL_TIME',\n    'RATIO_APPS_3_14',\n            \n       #'PROP_BL_OTHER_ORD_CNT_3', \n       #     'PROP_BL_DURABLES_ORD_CNT_3',\n       #'PROP_BL_HOMECARE_ORD_CNT_3', \n       'PROP_LAST_3DAY_PV_LC_AMT_3', \n        'PROP_RTURN_PV_LC_AMT_3',\n       'AVG_DMD_ORD_CNT_3',\n       #     'PROP_RTURN_PV_LC_AMT_0',\n            'APP_1_AGE',\n            'APP_2_POP', \n            'LPY_FLAGS', \n            'CONTRIBUTORS'\n           ]             ",
        "pred": [
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables->res_mb_hk = metabin(e2i, nei, c2i, nci, data=dat2, sm=\"OR\", Q.Cochrane=FALSE, method=\"MH\", method.tau=\"DL\", hakn=FALSE, backtransf=FALSE)\n res_mb_hk\n     logOR            95%-CI %W(fixed) %W(random)\n1   2.7081 [ 0.5265; 4.8896]       0.3        0.7\n2   1.2567 [ 0.2658; 2.2476]       2.1        3.2\n3   0.3749 [-0.3911; 1.1410]       5.4        5.4\n4   1.6582 [ 0.3245; 2.9920]       0.9        1.8\n5   0.7850 [-0.0673; 1.6372]       3.5        4.4\n6   0.3617 [-0.1528; 0.8762]      12.1       11.8\n7   0.5754 [-0.3861; 1.5368]       3.0        3.4\n8   0.2505 [-0.4881; 0.9892]       6.1        5.8\n9   0.6506 [-0.3877; 1.6889]       2.5        3.0\n10  0.0918 [-0.8067; 0.9903]       4.5        3.9\n11  0.2739 [-0.1047; 0.6525]      23.1       21.4\n12  0.4858 [ 0.0804; 0.8911]      18.6       18.8\n13  0.1823 [-0.6830; 1.0476]       4.6        4.2\n14  0.9808 [-0.4178; 2.3795]       1.3        1.6\n15  1.3122 [-1.0055; 3.6299]       0.4        0.6\n16 -0.2595 [-1.4450; 0.9260]       3.1        2.3\n17  0.1384 [-0.5076; 0.7844]       8.5        7.6\n\nNumber of studies combined: k = 17\n\n                      logOR           95%-CI    z  p-value\nFixed effect model   0.4428 [0.2678; 0.6178] 4.96 &lt; 0.0001\nRandom effects model 0.4295 [0.2504; 0.6086] 4.70 &lt; 0.0001\n\nQuantifying heterogeneity:\n tau^2 = 0.0017 [0.0000; 0.4589]; tau = 0.0410 [0.0000; 0.6774];\n I^2 = 1.1% [0.0%; 51.6%]; H = 1.01 [1.00; 1.44]\n\nTest of heterogeneity:\n     Q d.f. p-value\n 16.18   16  0.4404\n\nDetails on meta-analytical method:\n- Mantel-Haenszel method\n- DerSimonian-Laird estimator for tau^2\n- Jackson method for confidence interval of tau^2 and tau\n\n res_mb_hk$TE.fixed\n[1] 0.4428186730553189\n res_mb_hk$seTE.fixed\n[1] 0.08928560091027186\n c(res_mb_hk$lower.fixed, res_mb_hk$upper.fixed)\n[1] 0.2678221109331694 0.6178152351774684",
            "sklearn->Examples->Support Vector Machines->Support Vector Regression (SVR) using linear and non-linear kernels->Look at the results->lw = 2\n\nsvrs = [svr_rbf, svr_lin, svr_poly]\nkernel_label = [\"RBF\", \"Linear\", \"Polynomial\"]\nmodel_color = [\"m\", \"c\", \"g\"]\n\nfig, axes = (nrows=1, ncols=3, figsize=(15, 10), sharey=True)\nfor ix, svr in enumerate(svrs):\n    axes[ix].plot(\n        X,\n        svr.fit(X, y).predict(X),\n        color=model_color[ix],\n        lw=lw,\n        label=\"{} model\".format(kernel_label[ix]),\n    )\n    axes[ix].scatter(\n        X[svr.support_],\n        y[svr.support_],\n        facecolor=\"none\",\n        edgecolor=model_color[ix],\n        s=50,\n        label=\"{} support vectors\".format(kernel_label[ix]),\n    )\n    axes[ix].scatter(\n        X[((len(X)), svr.support_)],\n        y[((len(X)), svr.support_)],\n        facecolor=\"none\",\n        edgecolor=\"k\",\n        s=50,\n        label=\"other training data\",\n    )\n    axes[ix].legend(\n        loc=\"upper center\",\n        bbox_to_anchor=(0.5, 1.1),\n        ncol=1,\n        fancybox=True,\n        shadow=True,\n    )\n\nfig.text(0.5, 0.04, \"data\", ha=\"center\", va=\"center\")\nfig.text(0.06, 0.5, \"target\", ha=\"center\", va=\"center\", rotation=\"vertical\")\nfig.suptitle(\"Support Vector Regression\", fontsize=14)\n()\n\n\n<img alt=\"Support Vector Regression\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_svm_regression_001.png\" srcset=\"../../_images/sphx_glr_plot_svm_regression_001.png\"/>",
            "torch->Introduction to PyTorch->Quickstart->Loading Models->classes = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\n()\n, y = [0][0], [0][1]\nwith ():\n     = model()\n    predicted, actual = classes[[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')",
            "torch->Model Optimization->PyTorch Profiler With TensorBoard->Steps->1. Prepare the data and model->transform = (\n    [(224),\n     (),\n     ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ntrain_set = (root='./data', train=True, download=True, transform=transform)\ntrain_loader = (train_set, batch_size=32, shuffle=True)",
            "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates->time_s = np.s_[:50]  # After this they basically agree\nfig1 = plt.figure()\nax1 = fig1.add_subplot(111)\nidx = np.asarray(series.index)\nh1, = ax1.plot(idx[time_s], res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh2, = ax1.plot(idx[time_s], res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh3, = ax1.plot(idx[time_s], true_seasonal_10_3[time_s], label='True Seasonal 10(3)')\nplt.legend([h1, h2, h3], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 10(3) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\"/>"
        ]
    },
    {
        "id": "606145",
        "GT": "by_team17.plot.scatter(x = 'Votes', y = 'Avg_Viewers', title = 'All Star Votes vs Average Team Viewership')",
        "pred": [
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team->team\nAtlanta Hawks        0.585366\nBoston Celtics       0.585366\nBrooklyn Nets        0.256098\nCharlotte Hornets    0.585366\nChicago Bulls        0.512195\ndtype: float64",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->flights_wide = flights.pivot(index=\"year\", columns=\"month\", values=\"passengers\")\nflights_wide.head()\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->flights_wide = flights.pivot(index=\"year\", columns=\"month\", values=\"passengers\")\nflights_wide.head()\n",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing long-form data->year = flights_avg.index\npassengers = flights_avg[\"passengers\"]\nsns.relplot(x=year, y=passengers, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing long-form data->year = flights_avg.index\npassengers = flights_avg[\"passengers\"]\nsns.relplot(x=year, y=passengers, kind=\"line\")\n",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog->data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)"
        ]
    },
    {
        "id": "492406",
        "GT": "ROC->y = jobs.salarylevel.values\nX = jobs.iloc[:, 5:17]\n\nX_train, X_test, y_train, y_test = train_test_split(jobs.title, jobs.salarylevel, train_size = .5, random_state=90)\n\nmodel = make_pipeline(CountVectorizer(stop_words='english'),\n                      LogisticRegression())\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nyhat_pp = model.predict_proba(X_test)",
        "pred": [
            "sklearn->Examples->Cross decomposition->Principal Component Regression vs Partial Least Squares Regression->The data->y = X.dot(pca.components_[1]) + rng.normal(size=n_samples) / 2\n\nfig, axes = (1, 2, figsize=(10, 3))\n\naxes[0].scatter(X.dot(pca.components_[0]), y, alpha=0.3)\naxes[0].set(xlabel=\"Projected data onto first PCA component\", ylabel=\"y\")\naxes[1].scatter(X.dot(pca.components_[1]), y, alpha=0.3)\naxes[1].set(xlabel=\"Projected data onto second PCA component\", ylabel=\"y\")\n()\n()\n\n\n<img alt=\"plot pcr vs pls\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\" srcset=\"../../_images/sphx_glr_plot_pcr_vs_pls_002.png\"/>",
            "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation->X = digits.data[indices[:340]]\ny = digits.target[indices[:340]]\nimages = digits.images[indices[:340]]\n\nn_total_samples = len(y)\nn_labeled_points = 40\n\nindices = (n_total_samples)\n\nunlabeled_set = indices[n_labeled_points:]",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = (\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (train set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_003.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_003.png\"/>",
            "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Using robust regression to correct for outliers.->weights = rob_crime_model.weights\nidx = weights  0\nX = rob_crime_model.model.exog[idx.values]\nww = weights[idx] / weights[idx].mean()\nhat_matrix_diag = ww * (X * np.linalg.pinv(X).T).sum(1)\nresid = rob_crime_model.resid\nresid2 = resid ** 2\nresid2 /= resid2.sum()\nnobs = int(idx.sum())\nhm = hat_matrix_diag.mean()\nrm = resid2.mean()",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->train_result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\ntest_results = (\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_importances_idx = train_result.importances_mean.argsort()"
        ]
    },
    {
        "id": "996843",
        "GT": "A new variable created from `earliest_cr_line` and `issue_d` , a Business-driven metrics. \nThe idea is to find since when the customer is taking credit. A longer credit history means the customer is paying off loans without getting defaulted->def dateformat(earliest_cr_line_date):\n    date_split = earliest_cr_line_date.split('-')\n    if int(date_split[1]) > 18:\n        date_split[1] = '19' +  date_split[1]\n    else:\n        date_split[1] = '20' +  date_split[1]\n    return '-'.join(date_split)\nloans_frame['earliest_cr_line_mod'] = loans_frame['earliest_cr_line'].apply(dateformat)",
        "pred": [
            "torch->Learning PyTorch->What is torch.nn really?->Using your GPU->def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(), y.to()\n\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs->def title(y_pred, y_test, target_names, i):\n    pred_name = target_names[y_pred[i]].rsplit(\" \", 1)[-1]\n    true_name = target_names[y_test[i]].rsplit(\" \", 1)[-1]\n    return \"predicted: %s\\ntrue:      %s\" % (pred_name, true_name)\n\n\nprediction_titles = [\n    title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0])\n]\n\nplot_gallery(X_test, prediction_titles, h, w)\n\n\n<img alt=\"predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Blair true:      Blair, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Schroeder true:      Schroeder, predicted: Powell true:      Powell, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Bush true:      Bush, predicted: Bush true:      Bush\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_recognition_002.png\" srcset=\"../../_images/sphx_glr_plot_face_recognition_002.png\"/>",
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Calculating the test statistics->def t_test(x, y):\n    diff = y - x\n    var = np.var(diff, ddof=1)\n    num = np.mean(diff)\n    denom = np.sqrt(var / len(x))\n    return np.divide(num, denom)\n\nt_value = t_test(before_sample, after_sample)",
            "torch->Learning PyTorch->What is torch.nn really?->Wrapping DataLoader->def preprocess(x, y):\n    return x.view(-1, 1, 28, 28), y\n\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "pandas_toms_blog->Indexes->Merging->Merge Version->def mode(x):\n    '''\n    Arbitrarily break ties.\n    '''\n    return x.value_counts().index[0]\n\naggfuncs = {'tmpf': 'mean', 'relh': 'mean',\n            'sped': 'mean', 'mslp': 'mean',\n            'p01i': 'mean', 'vsby': 'mean',\n            'gust_mph': 'mean', 'skyc1': mode,\n            'skyc2': mode, 'skyc3': mode}\n# TimeGrouper works on a DatetimeIndex, so we move `station` to the\n# columns and then groupby it as well.\ndaily = (weather.reset_index(level=\"station\")\n                .groupby([pd.TimeGrouper('1d'), \"station\"])\n                .agg(aggfuncs))\n\ndaily.head()"
        ]
    },
    {
        "id": "967051",
        "GT": "What do all of these column titles mean?\n\nName - this usually contains the name of the investment company and the product name\n\nMorningstar Category - simply how Morningstar have categorised the fund\n\nIMA Sector - as per the above but the category here is decided on by the Investment Association (formerly the IMA)\n\nMorningstar Rating - a star rating system from zero to five stars\n\nMorningstar Analyst Rating - Bronze, Silver or Gold for certain selected funds\n\nYTD Return - Year to Date return\n\nCurr - Currency of the fund\n\nMorningstar Risk (Rel to Category) - a metric devised by Morningstar themselves. Simply describes risk relative to the category. This isn't that useful as it is just a verbalised form of the volatility\n\n3 Year Volatility - 3 year standard deviation of the underlying fund data\n\nYTD Return.1 - Year to Date return, a duplication from before\n\n1 Year Return, 3 Year Annualised Return - self explanatory\n\n5 Year Annualised Return & 10 Year Annualised Return - self explanatory but as Morningstar only has three year volatility data these numbers will be redundant in any analysis of risk adjusted returns\n\nEquity Style Box - representation of the investment style\n\nBond Style Box - as above but for fixed income funds\n\nAverage Market Cap (GBPm) - typical size of the market capitalisation of the companies held within the investment fund\n\nTotal Assets (GBPm) - size of the fund\n\nAverage Credit Quality & Average Duration (Years) - these are risk measures applied to bond funds, not applicable in this case\n\nOngoing charge - this is the total fee charged by the manager per annum\n\nMax initial fee - an upfront fee charged by the manager. Not so important as ongoing charges, as it is these that eat up returns over the long term\n\nDeferred fee - column is blank and of no use, but that is ok for this project\n\nManager Tenure (Years) and Min Initial Purchase - self explanatory->data = data[data['3 Year Annualised Return'] != '-']",
        "pred": [
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data->data[:3]",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison->results_all = [res_o, res_f, res_e, res_a]\nnames = \"res_o res_f res_e res_a\".split()",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ]
    },
    {
        "id": "1124099",
        "GT": "Shopping the sales\n\n### How many items are over 50% off?->Display all items above 50% off.\n\nPandas doesn't want to show you every row. Change its settings so it will display up to **200 rows of content.**->pd.options.display.max_rows = 200\ndf[discount_percentage>50]",
        "pred": [
            "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding->pd.get_dummies(hsb2.race.values, drop_first=False)",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Conditioning on other variables->sns.displot(penguins, x=\"flipper_length_mm\", col=\"sex\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Conditioning on other variables->sns.displot(penguins, x=\"flipper_length_mm\", col=\"sex\")\n",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->sns.relplot(data=flights_wide.transpose(), kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->sns.relplot(data=flights_wide.transpose(), kind=\"line\")\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->sns.pairplot(penguins)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->sns.pairplot(penguins)\n",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->pca_model = PCA(dta.T, standardize=False, demean=True)"
        ]
    },
    {
        "id": "1416954",
        "GT": "Load up Test Data->#pandas is a powerful Python data analysis toolkit\n#fast, flexible, and expressive data structures designed to make \n#working with relational or labeled data\ntrain = pd.read_csv(\"data/train.csv\")\ntrain.head()",
        "pred": [
            "sklearn->Examples->Feature Selection->Model-based and sequential feature selection->Loading the data->from sklearn.datasets import \n\ndiabetes = ()\nX, y = diabetes.data, diabetes.target\nprint(diabetes.DESCR)",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models-># Retrieve the posterior means\nparams = pm.summary(trace_uc)[\"mean\"].values\n\n# Construct results using these posterior means as parameter values\nres_uc_bayes = mod_uc.smooth(params)",
            "matplotlib->Tutorials->Advanced->Path Tutorial->Compound paths-># histogram our data with numpy\ndata = np.random.randn(1000)\nn, bins = np.histogram(data, 100)",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->tidy = pd.melt(games.reset_index(),\n               id_vars=['game_id', 'date'], value_vars=['away_team', 'home_team'],\n               value_name='team')\ntidy.head()"
        ]
    },
    {
        "id": "473630",
        "GT": "msno.bar(df, color=\"blue\", figsize=(30,18))",
        "pred": [
            "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(tips, x=\"total_bill\", kind=\"kde\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(tips, x=\"total_bill\", kind=\"kde\")\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->sns.displot(tips, x=\"day\", shrink=.8)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->sns.displot(tips, x=\"day\", shrink=.8)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(tips, x=\"total_bill\", kind=\"kde\", cut=0)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(tips, x=\"total_bill\", kind=\"kde\", cut=0)\n",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->sns.relplot(data=flights_wide, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->sns.relplot(data=flights_wide, kind=\"line\")\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->sns.displot(tips, x=\"size\", discrete=True)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->sns.displot(tips, x=\"size\", discrete=True)\n"
        ]
    },
    {
        "id": "1425124",
        "GT": "df_oz = pd.read_csv('D:\\\\TeachingMaterials\\\\BusinessAnalytics\\\\Visualization\\\\VizData\\\\ozone.csv')\ndf_con = pd.read_csv('D:\\\\TeachingMaterials\\\\BusinessAnalytics\\\\Visualization\\\\VizData\\\\construction.csv')\ndf_test = pd.read_csv('D:\\\\TeachingMaterials\\\\BusinessAnalytics\\\\Visualization\\\\VizData\\\\test.csv')",
        "pred": [
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->glove = data.fetch('glove.6B.50d.zip')\nemb_path = textproc.unzipper(glove, 'glove.6B.300d.txt')\nemb_matrix = textproc.loadGloveModel(emb_path)",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson->rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']"
        ]
    },
    {
        "id": "311291",
        "GT": "dup2.drop_duplicates(subset=index)",
        "pred": [
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model->exog = sm.add_constant(exog, prepend=True)",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->dta.plot(figsize=(12, 8))",
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time.unique()",
            "statsmodels->Examples->Statistics->ANOVA->lm.model.data.orig_exog[:5]",
            "statsmodels->Examples->Statistics->ANOVA->lm.model.exog[:5]"
        ]
    },
    {
        "id": "1061434",
        "GT": "Oversampling of data using imblearn->###### random UPSAMPLING OF THE UPSAMPLING OF MINORITY CLASS I.E., 'SUICIDES' \n\n#!pip install --user imblearn\nsys.path.append(\"/users/bhavani/appdata/roaming/python/python36/site-packages\")\nfrom imblearn.over_sampling import SMOTE\nsmt = SMOTE()\nX_up_train, y_up_train = smt.fit_sample(X_train, y_train)",
        "pred": [
            "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->The matplotlibrc file->The default matplotlibrc file->#### MATPLOTLIBRC FORMAT\n\n## NOTE FOR END USERS: DO NOT EDIT THIS FILE!\n##\n## This is a sample Matplotlib configuration file - you can find a copy\n## of it on your system in site-packages/matplotlib/mpl-data/matplotlibrc\n## (relative to your Python installation location).\n## DO NOT EDIT IT!\n##\n## If you wish to change your default style, copy this file to one of the\n## following locations:\n##     Unix/Linux:\n##         $HOME/.config/matplotlib/matplotlibrc OR\n##         $XDG_CONFIG_HOME/matplotlib/matplotlibrc (if $XDG_CONFIG_HOME is set)\n##     Other platforms:\n##         $HOME/.matplotlib/matplotlibrc\n## and edit that copy.\n##\n## See https://matplotlib.org/stable/tutorials/introductory/customizing.html#customizing-with-matplotlibrc-files\n## for more details on the paths which are checked for the configuration file.\n##\n## Blank lines, or lines starting with a comment symbol, are ignored, as are\n## trailing comments.  Other lines must have the format:\n##     key: val  # optional comment\n##\n## Formatting: Use PEP8-like style (as enforced in the rest of the codebase).\n## All lines start with an additional '#', so that removing all leading '#'s\n## yields a valid style file.\n##\n## Colors: for the color values below, you can either use\n##     - a Matplotlib color string, such as r, k, or b\n##     - an RGB tuple, such as (1.0, 0.5, 0.0)\n##     - a double-quoted hex string, such as \"#ff00ff\".\n##       The unquoted string ff00ff is also supported for backward\n##       compatibility, but is discouraged.\n##     - a scalar grayscale intensity such as 0.75\n##     - a legal html color name, e.g., red, blue, darkslategray\n##\n## String values may optionally be enclosed in double quotes, which allows\n## using the comment character # in the string.\n##\n## This file (and other style files) must be encoded as utf-8.\n##\n## Matplotlib configuration are currently divided into following parts:\n##     - BACKENDS\n##     - LINES\n##     - PATCHES\n##     - HATCHES\n##     - BOXPLOT\n##     - FONT\n##     - TEXT\n##     - LaTeX\n##     - AXES\n##     - DATES\n##     - TICKS\n##     - GRIDS\n##     - LEGEND\n##     - FIGURE\n##     - IMAGES\n##     - CONTOUR PLOTS\n##     - ERRORBAR PLOTS\n##     - HISTOGRAM PLOTS\n##     - SCATTER PLOTS\n##     - AGG RENDERING\n##     - PATHS\n##     - SAVING FIGURES\n##     - INTERACTIVE KEYMAPS\n##     - ANIMATION\n\n##### CONFIGURATION BEGINS HERE\n\n\n## ***************************************************************************\n## * BACKENDS                                                                *\n## ***************************************************************************\n## The default backend.  If you omit this parameter, the first working\n## backend from the following list is used:\n##     MacOSX QtAgg Gtk4Agg Gtk3Agg TkAgg WxAgg Agg\n## Other choices include:\n##     QtCairo GTK4Cairo GTK3Cairo TkCairo WxCairo Cairo\n##     Qt5Agg Qt5Cairo Wx  # deprecated.\n##     PS PDF SVG Template\n## You can also deploy your own backend outside of Matplotlib by referring to\n## the module name (which must be in the PYTHONPATH) as 'module://my_backend'.\n##backend: Agg\n\n## The port to use for the web server in the WebAgg backend.\n#webagg.port: 8988\n\n## The address on which the WebAgg web server should be reachable\n#webagg.address: 127.0.0.1\n\n## If webagg.port is unavailable, a number of other random ports will\n## be tried until one that is available is found.\n#webagg.port_retries: 50\n\n## When True, open the web browser to the plot that is shown\n#webagg.open_in_browser: True\n\n## If you are running pyplot inside a GUI and your backend choice\n## conflicts, we will automatically try to find a compatible one for\n## you if backend_fallback is True\n#backend_fallback: True\n\n#interactive: False\n#figure.hooks:          # list of dotted.module.name:dotted.callable.name\n#toolbar:     toolbar2  # {None, toolbar2, toolmanager}\n#timezone:    UTC       # a pytz timezone string, e.g., US/Central or Europe/Paris\n\n\n## ***************************************************************************\n## * LINES                                                                   *\n## ***************************************************************************\n## See https://matplotlib.org/stable/api/artist_api.html#module-matplotlib.lines\n## for more information on line properties.\n#lines.linewidth: 1.5               # line width in points\n#lines.linestyle: -                 # solid line\n#lines.color:     C0                # has no affect on plot(); see axes.prop_cycle\n#lines.marker:          None        # the default marker\n#lines.markerfacecolor: auto        # the default marker face color\n#lines.markeredgecolor: auto        # the default marker edge color\n#lines.markeredgewidth: 1.0         # the line width around the marker symbol\n#lines.markersize:      6           # marker size, in points\n#lines.dash_joinstyle:  round       # {miter, round, bevel}\n#lines.dash_capstyle:   butt        # {butt, round, projecting}\n#lines.solid_joinstyle: round       # {miter, round, bevel}\n#lines.solid_capstyle:  projecting  # {butt, round, projecting}\n#lines.antialiased: True            # render lines in antialiased (no jaggies)\n\n## The three standard dash patterns.  These are scaled by the linewidth.\n#lines.dashed_pattern: 3.7, 1.6\n#lines.dashdot_pattern: 6.4, 1.6, 1, 1.6\n#lines.dotted_pattern: 1, 1.65\n#lines.scale_dashes: True\n\n#markers.fillstyle: full  # {full, left, right, bottom, top, none}\n\n#pcolor.shading: auto\n#pcolormesh.snap: True  # Whether to snap the mesh to pixel boundaries. This is\n                        # provided solely to allow old test images to remain\n                        # unchanged. Set to False to obtain the previous behavior.\n\n## ***************************************************************************\n## * PATCHES                                                                 *\n## ***************************************************************************\n## Patches are graphical objects that fill 2D space, like polygons or circles.\n## See https://matplotlib.org/stable/api/artist_api.html#module-matplotlib.patches\n## for more information on patch properties.\n#patch.linewidth:       1.0    # edge width in points.\n#patch.facecolor:       C0\n#patch.edgecolor:       black  # if forced, or patch is not filled\n#patch.force_edgecolor: False  # True to always use edgecolor\n#patch.antialiased:     True   # render patches in antialiased (no jaggies)\n\n\n## ***************************************************************************\n## * HATCHES                                                                 *\n## ***************************************************************************\n#hatch.color:     black\n#hatch.linewidth: 1.0\n\n\n## ***************************************************************************\n## * BOXPLOT                                                                 *\n## ***************************************************************************\n#boxplot.notch:       False\n#boxplot.vertical:    True\n#boxplot.whiskers:    1.5\n#boxplot.bootstrap:   None\n#boxplot.patchartist: False\n#boxplot.showmeans:   False\n#boxplot.showcaps:    True\n#boxplot.showbox:     True\n#boxplot.showfliers:  True\n#boxplot.meanline:    False\n\n#boxplot.flierprops.color:           black\n#boxplot.flierprops.marker:          o\n#boxplot.flierprops.markerfacecolor: none\n#boxplot.flierprops.markeredgecolor: black\n#boxplot.flierprops.markeredgewidth: 1.0\n#boxplot.flierprops.markersize:      6\n#boxplot.flierprops.linestyle:       none\n#boxplot.flierprops.linewidth:       1.0\n\n#boxplot.boxprops.color:     black\n#boxplot.boxprops.linewidth: 1.0\n#boxplot.boxprops.linestyle: -\n\n#boxplot.whiskerprops.color:     black\n#boxplot.whiskerprops.linewidth: 1.0\n#boxplot.whiskerprops.linestyle: -\n\n#boxplot.capprops.color:     black\n#boxplot.capprops.linewidth: 1.0\n#boxplot.capprops.linestyle: -\n\n#boxplot.medianprops.color:     C1\n#boxplot.medianprops.linewidth: 1.0\n#boxplot.medianprops.linestyle: -\n\n#boxplot.meanprops.color:           C2\n#boxplot.meanprops.marker:          ^\n#boxplot.meanprops.markerfacecolor: C2\n#boxplot.meanprops.markeredgecolor: C2\n#boxplot.meanprops.markersize:       6\n#boxplot.meanprops.linestyle:       --\n#boxplot.meanprops.linewidth:       1.0\n\n\n## ***************************************************************************\n## * FONT                                                                    *\n## ***************************************************************************\n## The font properties used by `text.Text`.\n## See https://matplotlib.org/stable/api/font_manager_api.html for more information\n## on font properties.  The 6 font properties used for font matching are\n## given below with their default values.\n##\n## The font.family property can take either a single or multiple entries of any\n## combination of concrete font names (not supported when rendering text with\n## usetex) or the following five generic values:\n##     - 'serif' (e.g., Times),\n##     - 'sans-serif' (e.g., Helvetica),\n##     - 'cursive' (e.g., Zapf-Chancery),\n##     - 'fantasy' (e.g., Western), and\n##     - 'monospace' (e.g., Courier).\n## Each of these values has a corresponding default list of font names\n## (font.serif, etc.); the first available font in the list is used.  Note that\n## for font.serif, font.sans-serif, and font.monospace, the first element of\n## the list (a DejaVu font) will always be used because DejaVu is shipped with\n## Matplotlib and is thus guaranteed to be available; the other entries are\n## left as examples of other possible values.\n##\n## The font.style property has three values: normal (or roman), italic\n## or oblique.  The oblique style will be used for italic, if it is not\n## present.\n##\n## The font.variant property has two values: normal or small-caps.  For\n## TrueType fonts, which are scalable fonts, small-caps is equivalent\n## to using a font size of 'smaller', or about 83 % of the current font\n## size.\n##\n## The font.weight property has effectively 13 values: normal, bold,\n## bolder, lighter, 100, 200, 300, ..., 900.  Normal is the same as\n## 400, and bold is 700.  bolder and lighter are relative values with\n## respect to the current weight.\n##\n## The font.stretch property has 11 values: ultra-condensed,\n## extra-condensed, condensed, semi-condensed, normal, semi-expanded,\n## expanded, extra-expanded, ultra-expanded, wider, and narrower.  This\n## property is not currently implemented.\n##\n## The font.size property is the default font size for text, given in points.\n## 10 pt is the standard value.\n##\n## Note that font.size controls default text sizes.  To configure\n## special text sizes tick labels, axes, labels, title, etc., see the rc\n## settings for axes and ticks.  Special text sizes can be defined\n## relative to font.size, using the following values: xx-small, x-small,\n## small, medium, large, x-large, xx-large, larger, or smaller\n\n#font.family:  sans-serif\n#font.style:   normal\n#font.variant: normal\n#font.weight:  normal\n#font.stretch: normal\n#font.size:    10.0\n\n#font.serif:      DejaVu Serif, Bitstream Vera Serif, Computer Modern Roman, New Century Schoolbook, Century Schoolbook L, Utopia, ITC Bookman, Bookman, Nimbus Roman No9 L, Times New Roman, Times, Palatino, Charter, serif\n#font.sans-serif: DejaVu Sans, Bitstream Vera Sans, Computer Modern Sans Serif, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif\n#font.cursive:    Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive\n#font.fantasy:    Chicago, Charcoal, Impact, Western, Humor Sans, xkcd, fantasy\n#font.monospace:  DejaVu Sans Mono, Bitstream Vera Sans Mono, Computer Modern Typewriter, Andale Mono, Nimbus Mono L, Courier New, Courier, Fixed, Terminal, monospace\n\n\n## ***************************************************************************\n## * TEXT                                                                    *\n## ***************************************************************************\n## The text properties used by `text.Text`.\n## See https://matplotlib.org/stable/api/artist_api.html#module-matplotlib.text\n## for more information on text properties\n#text.color: black\n\n## FreeType hinting flag (\"foo\" corresponds to FT_LOAD_FOO); may be one of the\n## following (Proprietary Matplotlib-specific synonyms are given in parentheses,\n## but their use is discouraged):\n## - default: Use the font's native hinter if possible, else FreeType's auto-hinter.\n##            (\"either\" is a synonym).\n## - no_autohint: Use the font's native hinter if possible, else don't hint.\n##                (\"native\" is a synonym.)\n## - force_autohint: Use FreeType's auto-hinter.  (\"auto\" is a synonym.)\n## - no_hinting: Disable hinting.  (\"none\" is a synonym.)\n#text.hinting: force_autohint\n\n#text.hinting_factor: 8  # Specifies the amount of softness for hinting in the\n                         # horizontal direction.  A value of 1 will hint to full\n                         # pixels.  A value of 2 will hint to half pixels etc.\n#text.kerning_factor: 0  # Specifies the scaling factor for kerning values.  This\n                         # is provided solely to allow old test images to remain\n                         # unchanged.  Set to 6 to obtain previous behavior.\n                         # Values  other than 0 or 6 have no defined meaning.\n#text.antialiased: True  # If True (default), the text will be antialiased.\n                         # This only affects raster outputs.\n#text.parse_math: True  # Use mathtext if there is an even number of unescaped\n                        # dollar signs.\n\n\n## ***************************************************************************\n## * LaTeX                                                                   *\n## ***************************************************************************\n## For more information on LaTeX properties, see\n## https://matplotlib.org/stable/tutorials/text/usetex.html\n#text.usetex: False  # use latex for all text handling. The following fonts\n                     # are supported through the usual rc parameter settings:\n                     # new century schoolbook, bookman, times, palatino,\n                     # zapf chancery, charter, serif, sans-serif, helvetica,\n                     # avant garde, courier, monospace, computer modern roman,\n                     # computer modern sans serif, computer modern typewriter\n#text.latex.preamble:   # IMPROPER USE OF THIS FEATURE WILL LEAD TO LATEX FAILURES\n                        # AND IS THEREFORE UNSUPPORTED. PLEASE DO NOT ASK FOR HELP\n                        # IF THIS FEATURE DOES NOT DO WHAT YOU EXPECT IT TO.\n                        # text.latex.preamble is a single line of LaTeX code that\n                        # will be passed on to the LaTeX system. It may contain\n                        # any code that is valid for the LaTeX \"preamble\", i.e.\n                        # between the \"\\documentclass\" and \"\\begin{document}\"\n                        # statements.\n                        # Note that it has to be put on a single line, which may\n                        # become quite long.\n                        # The following packages are always loaded with usetex,\n                        # so beware of package collisions:\n                        #   geometry, inputenc, type1cm.\n                        # PostScript (PSNFSS) font packages may also be\n                        # loaded, depending on your font settings.\n\n## The following settings allow you to select the fonts in math mode.\n#mathtext.fontset: dejavusans  # Should be 'dejavusans' (default),\n                               # 'dejavuserif', 'cm' (Computer Modern), 'stix',\n                               # 'stixsans' or 'custom'\n## \"mathtext.fontset: custom\" is defined by the mathtext.bf, .cal, .it, ...\n## settings which map a TeX font name to a fontconfig font pattern.  (These\n## settings are not used for other font sets.)\n#mathtext.bf:  sans:bold\n#mathtext.cal: cursive\n#mathtext.it:  sans:italic\n#mathtext.rm:  sans\n#mathtext.sf:  sans\n#mathtext.tt:  monospace\n#mathtext.fallback: cm  # Select fallback font from ['cm' (Computer Modern), 'stix'\n                        # 'stixsans'] when a symbol can not be found in one of the\n                        # custom math fonts. Select 'None' to not perform fallback\n                        # and replace the missing character by a dummy symbol.\n#mathtext.default: it  # The default font to use for math.\n                       # Can be any of the LaTeX font names, including\n                       # the special name \"regular\" for the same font\n                       # used in regular text.\n\n\n## ***************************************************************************\n## * AXES                                                                    *\n## ***************************************************************************\n## Following are default face and edge colors, default tick sizes,\n## default font sizes for tick labels, and so on.  See\n## https://matplotlib.org/stable/api/axes_api.html#module-matplotlib.axes\n#axes.facecolor:     white   # axes background color\n#axes.edgecolor:     black   # axes edge color\n#axes.linewidth:     0.8     # edge line width\n#axes.grid:          False   # display grid or not\n#axes.grid.axis:     both    # which axis the grid should apply to\n#axes.grid.which:    major   # grid lines at {major, minor, both} ticks\n#axes.titlelocation: center  # alignment of the title: {left, right, center}\n#axes.titlesize:     large   # font size of the axes title\n#axes.titleweight:   normal  # font weight of title\n#axes.titlecolor:    auto    # color of the axes title, auto falls back to\n                             # text.color as default value\n#axes.titley:        None    # position title (axes relative units).  None implies auto\n#axes.titlepad:      6.0     # pad between axes and title in points\n#axes.labelsize:     medium  # font size of the x and y labels\n#axes.labelpad:      4.0     # space between label and axis\n#axes.labelweight:   normal  # weight of the x and y labels\n#axes.labelcolor:    black\n#axes.axisbelow:     line    # draw axis gridlines and ticks:\n                             #     - below patches (True)\n                             #     - above patches but below lines ('line')\n                             #     - above all (False)\n\n#axes.formatter.limits: -5, 6  # use scientific notation if log10\n                               # of the axis range is smaller than the\n                               # first or larger than the second\n#axes.formatter.use_locale: False  # When True, format tick labels\n                                   # according to the user's locale.\n                                   # For example, use ',' as a decimal\n                                   # separator in the fr_FR locale.\n#axes.formatter.use_mathtext: False  # When True, use mathtext for scientific\n                                     # notation.\n#axes.formatter.min_exponent: 0  # minimum exponent to format in scientific notation\n#axes.formatter.useoffset: True  # If True, the tick label formatter\n                                 # will default to labeling ticks relative\n                                 # to an offset when the data range is\n                                 # small compared to the minimum absolute\n                                 # value of the data.\n#axes.formatter.offset_threshold: 4  # When useoffset is True, the offset\n                                     # will be used when it can remove\n                                     # at least this number of significant\n                                     # digits from tick labels.\n\n#axes.spines.left:   True  # display axis spines\n#axes.spines.bottom: True\n#axes.spines.top:    True\n#axes.spines.right:  True\n\n#axes.unicode_minus: True  # use Unicode for the minus symbol rather than hyphen.  See\n                           # https://en.wikipedia.org/wiki/Plus_and_minus_signs#Character_codes\n#axes.prop_cycle: cycler('color', ['1f77b4', 'ff7f0e', '2ca02c', 'd62728', '9467bd', '8c564b', 'e377c2', '7f7f7f', 'bcbd22', '17becf'])\n                  # color cycle for plot lines as list of string color specs:\n                  # single letter, long name, or web-style hex\n                  # As opposed to all other parameters in this file, the color\n                  # values must be enclosed in quotes for this parameter,\n                  # e.g. '1f77b4', instead of 1f77b4.\n                  # See also https://matplotlib.org/stable/tutorials/intermediate/color_cycle.html\n                  # for more details on prop_cycle usage.\n#axes.xmargin:   .05  # x margin.  See `axes.Axes.margins`\n#axes.ymargin:   .05  # y margin.  See `axes.Axes.margins`\n#axes.zmargin:   .05  # z margin.  See `axes.Axes.margins`\n#axes.autolimit_mode: data  # If \"data\", use axes.xmargin and axes.ymargin as is.\n                            # If \"round_numbers\", after application of margins, axis\n                            # limits are further expanded to the nearest \"round\" number.\n#polaraxes.grid: True  # display grid on polar axes\n#axes3d.grid:    True  # display grid on 3D axes\n\n#axes3d.xaxis.panecolor:    (0.95, 0.95, 0.95, 0.5)  # background pane on 3D axes\n#axes3d.yaxis.panecolor:    (0.90, 0.90, 0.90, 0.5)  # background pane on 3D axes\n#axes3d.zaxis.panecolor:    (0.925, 0.925, 0.925, 0.5)  # background pane on 3D axes\n\n## ***************************************************************************\n## * AXIS                                                                    *\n## ***************************************************************************\n#xaxis.labellocation: center  # alignment of the xaxis label: {left, right, center}\n#yaxis.labellocation: center  # alignment of the yaxis label: {bottom, top, center}\n\n\n## ***************************************************************************\n## * DATES                                                                   *\n## ***************************************************************************\n## These control the default format strings used in AutoDateFormatter.\n## Any valid format datetime format string can be used (see the python\n## `datetime` for details).  For example, by using:\n##     - '%x' will use the locale date representation\n##     - '%X' will use the locale time representation\n##     - '%c' will use the full locale datetime representation\n## These values map to the scales:\n##     {'year': 365, 'month': 30, 'day': 1, 'hour': 1/24, 'minute': 1 / (24 * 60)}\n\n#date.autoformatter.year:        %Y\n#date.autoformatter.month:       %Y-%m\n#date.autoformatter.day:         %Y-%m-%d\n#date.autoformatter.hour:        %m-%d %H\n#date.autoformatter.minute:      %d %H:%M\n#date.autoformatter.second:      %H:%M:%S\n#date.autoformatter.microsecond: %M:%S.%f\n## The reference date for Matplotlib's internal date representation\n## See https://matplotlib.org/stable/gallery/ticks/date_precision_and_epochs.html\n#date.epoch: 1970-01-01T00:00:00\n## 'auto', 'concise':\n#date.converter:                  auto\n## For auto converter whether to use interval_multiples:\n#date.interval_multiples:         True\n\n## ***************************************************************************\n## * TICKS                                                                   *\n## ***************************************************************************\n## See https://matplotlib.org/stable/api/axis_api.html#matplotlib.axis.Tick\n#xtick.top:           False   # draw ticks on the top side\n#xtick.bottom:        True    # draw ticks on the bottom side\n#xtick.labeltop:      False   # draw label on the top\n#xtick.labelbottom:   True    # draw label on the bottom\n#xtick.major.size:    3.5     # major tick size in points\n#xtick.minor.size:    2       # minor tick size in points\n#xtick.major.width:   0.8     # major tick width in points\n#xtick.minor.width:   0.6     # minor tick width in points\n#xtick.major.pad:     3.5     # distance to major tick label in points\n#xtick.minor.pad:     3.4     # distance to the minor tick label in points\n#xtick.color:         black   # color of the ticks\n#xtick.labelcolor:    inherit # color of the tick labels or inherit from xtick.color\n#xtick.labelsize:     medium  # font size of the tick labels\n#xtick.direction:     out     # direction: {in, out, inout}\n#xtick.minor.visible: False   # visibility of minor ticks on x-axis\n#xtick.major.top:     True    # draw x axis top major ticks\n#xtick.major.bottom:  True    # draw x axis bottom major ticks\n#xtick.minor.top:     True    # draw x axis top minor ticks\n#xtick.minor.bottom:  True    # draw x axis bottom minor ticks\n#xtick.alignment:     center  # alignment of xticks\n\n#ytick.left:          True    # draw ticks on the left side\n#ytick.right:         False   # draw ticks on the right side\n#ytick.labelleft:     True    # draw tick labels on the left side\n#ytick.labelright:    False   # draw tick labels on the right side\n#ytick.major.size:    3.5     # major tick size in points\n#ytick.minor.size:    2       # minor tick size in points\n#ytick.major.width:   0.8     # major tick width in points\n#ytick.minor.width:   0.6     # minor tick width in points\n#ytick.major.pad:     3.5     # distance to major tick label in points\n#ytick.minor.pad:     3.4     # distance to the minor tick label in points\n#ytick.color:         black   # color of the ticks\n#ytick.labelcolor:    inherit # color of the tick labels or inherit from ytick.color\n#ytick.labelsize:     medium  # font size of the tick labels\n#ytick.direction:     out     # direction: {in, out, inout}\n#ytick.minor.visible: False   # visibility of minor ticks on y-axis\n#ytick.major.left:    True    # draw y axis left major ticks\n#ytick.major.right:   True    # draw y axis right major ticks\n#ytick.minor.left:    True    # draw y axis left minor ticks\n#ytick.minor.right:   True    # draw y axis right minor ticks\n#ytick.alignment:     center_baseline  # alignment of yticks\n\n\n## ***************************************************************************\n## * GRIDS                                                                   *\n## ***************************************************************************\n#grid.color:     \"#b0b0b0\"  # grid color\n#grid.linestyle: -          # solid\n#grid.linewidth: 0.8        # in points\n#grid.alpha:     1.0        # transparency, between 0.0 and 1.0\n\n\n## ***************************************************************************\n## * LEGEND                                                                  *\n## ***************************************************************************\n#legend.loc:           best\n#legend.frameon:       True     # if True, draw the legend on a background patch\n#legend.framealpha:    0.8      # legend patch transparency\n#legend.facecolor:     inherit  # inherit from axes.facecolor; or color spec\n#legend.edgecolor:     0.8      # background patch boundary color\n#legend.fancybox:      True     # if True, use a rounded box for the\n                                # legend background, else a rectangle\n#legend.shadow:        False    # if True, give background a shadow effect\n#legend.numpoints:     1        # the number of marker points in the legend line\n#legend.scatterpoints: 1        # number of scatter points\n#legend.markerscale:   1.0      # the relative size of legend markers vs. original\n#legend.fontsize:      medium\n#legend.labelcolor:    None\n#legend.title_fontsize: None    # None sets to the same as the default axes.\n\n## Dimensions as fraction of font size:\n#legend.borderpad:     0.4  # border whitespace\n#legend.labelspacing:  0.5  # the vertical space between the legend entries\n#legend.handlelength:  2.0  # the length of the legend lines\n#legend.handleheight:  0.7  # the height of the legend handle\n#legend.handletextpad: 0.8  # the space between the legend line and legend text\n#legend.borderaxespad: 0.5  # the border between the axes and legend edge\n#legend.columnspacing: 2.0  # column separation\n\n\n## ***************************************************************************\n## * FIGURE                                                                  *\n## ***************************************************************************\n## See https://matplotlib.org/stable/api/figure_api.html#matplotlib.figure.Figure\n#figure.titlesize:   large     # size of the figure title (``Figure.suptitle()``)\n#figure.titleweight: normal    # weight of the figure title\n#figure.labelsize:   large     # size of the figure label (``Figure.sup[x|y]label()``)\n#figure.labelweight: normal    # weight of the figure label\n#figure.figsize:     6.4, 4.8  # figure size in inches\n#figure.dpi:         100       # figure dots per inch\n#figure.facecolor:   white     # figure face color\n#figure.edgecolor:   white     # figure edge color\n#figure.frameon:     True      # enable figure frame\n#figure.max_open_warning: 20   # The maximum number of figures to open through\n                               # the pyplot interface before emitting a warning.\n                               # If less than one this feature is disabled.\n#figure.raise_window : True    # Raise the GUI window to front when show() is called.\n\n## The figure subplot parameters.  All dimensions are a fraction of the figure width and height.\n#figure.subplot.left:   0.125  # the left side of the subplots of the figure\n#figure.subplot.right:  0.9    # the right side of the subplots of the figure\n#figure.subplot.bottom: 0.11   # the bottom of the subplots of the figure\n#figure.subplot.top:    0.88   # the top of the subplots of the figure\n#figure.subplot.wspace: 0.2    # the amount of width reserved for space between subplots,\n                               # expressed as a fraction of the average axis width\n#figure.subplot.hspace: 0.2    # the amount of height reserved for space between subplots,\n                               # expressed as a fraction of the average axis height\n\n## Figure layout\n#figure.autolayout: False  # When True, automatically adjust subplot\n                           # parameters to make the plot fit the figure\n                           # using `tight_layout`\n#figure.constrained_layout.use: False  # When True, automatically make plot\n                                       # elements fit on the figure. (Not\n                                       # compatible with `autolayout`, above).\n#figure.constrained_layout.h_pad:  0.04167  # Padding around axes objects. Float representing\n#figure.constrained_layout.w_pad:  0.04167  # inches. Default is 3/72 inches (3 points)\n#figure.constrained_layout.hspace: 0.02     # Space between subplot groups. Float representing\n#figure.constrained_layout.wspace: 0.02     # a fraction of the subplot widths being separated.\n\n\n## ***************************************************************************\n## * IMAGES                                                                  *\n## ***************************************************************************\n#image.aspect:          equal        # {equal, auto} or a number\n#image.interpolation:   antialiased  # see help(imshow) for options\n#image.cmap:            viridis      # A colormap name (plasma, magma, etc.)\n#image.lut:             256          # the size of the colormap lookup table\n#image.origin:          upper        # {lower, upper}\n#image.resample:        True\n#image.composite_image: True  # When True, all the images on a set of axes are\n                              # combined into a single composite image before\n                              # saving a figure as a vector graphics file,\n                              # such as a PDF.\n\n\n## ***************************************************************************\n## * CONTOUR PLOTS                                                           *\n## ***************************************************************************\n#contour.negative_linestyle: dashed  # string or on-off ink sequence\n#contour.corner_mask:        True    # {True, False}\n#contour.linewidth:          None    # {float, None} Size of the contour line\n                                     # widths. If set to None, it falls back to\n                                     # `line.linewidth`.\n#contour.algorithm:          mpl2014 # {mpl2005, mpl2014, serial, threaded}\n\n\n## ***************************************************************************\n## * ERRORBAR PLOTS                                                          *\n## ***************************************************************************\n#errorbar.capsize: 0  # length of end cap on error bars in pixels\n\n\n## ***************************************************************************\n## * HISTOGRAM PLOTS                                                         *\n## ***************************************************************************\n#hist.bins: 10  # The default number of histogram bins or 'auto'.\n\n\n## ***************************************************************************\n## * SCATTER PLOTS                                                           *\n## ***************************************************************************\n#scatter.marker: o         # The default marker type for scatter plots.\n#scatter.edgecolors: face  # The default edge colors for scatter plots.\n\n\n## ***************************************************************************\n## * AGG RENDERING                                                           *\n## ***************************************************************************\n## Warning: experimental, 2008/10/10\n#agg.path.chunksize: 0  # 0 to disable; values in the range\n                        # 10000 to 100000 can improve speed slightly\n                        # and prevent an Agg rendering failure\n                        # when plotting very large data sets,\n                        # especially if they are very gappy.\n                        # It may cause minor artifacts, though.\n                        # A value of 20000 is probably a good\n                        # starting point.\n\n\n## ***************************************************************************\n## * PATHS                                                                   *\n## ***************************************************************************\n#path.simplify: True  # When True, simplify paths by removing \"invisible\"\n                      # points to reduce file size and increase rendering\n                      # speed\n#path.simplify_threshold: 0.111111111111  # The threshold of similarity below\n                                          # which vertices will be removed in\n                                          # the simplification process.\n#path.snap: True  # When True, rectilinear axis-aligned paths will be snapped\n                  # to the nearest pixel when certain criteria are met.\n                  # When False, paths will never be snapped.\n#path.sketch: None  # May be None, or a 3-tuple of the form:\n                    # (scale, length, randomness).\n                    #     - *scale* is the amplitude of the wiggle\n                    #         perpendicular to the line (in pixels).\n                    #     - *length* is the length of the wiggle along the\n                    #         line (in pixels).\n                    #     - *randomness* is the factor by which the length is\n                    #         randomly scaled.\n#path.effects:\n\n\n## ***************************************************************************\n## * SAVING FIGURES                                                          *\n## ***************************************************************************\n## The default savefig parameters can be different from the display parameters\n## e.g., you may want a higher resolution, or to make the figure\n## background white\n#savefig.dpi:       figure      # figure dots per inch or 'figure'\n#savefig.facecolor: auto        # figure face color when saving\n#savefig.edgecolor: auto        # figure edge color when saving\n#savefig.format:    png         # {png, ps, pdf, svg}\n#savefig.bbox:      standard    # {tight, standard}\n                                # 'tight' is incompatible with pipe-based animation\n                                # backends (e.g. 'ffmpeg') but will work with those\n                                # based on temporary files (e.g. 'ffmpeg_file')\n#savefig.pad_inches:  0.1       # padding to be used, when bbox is set to 'tight'\n#savefig.directory:   ~         # default directory in savefig dialog, gets updated after\n                                # interactive saves, unless set to the empty string (i.e.\n                                # the current directory); use '.' to start at the current\n                                # directory but update after interactive saves\n#savefig.transparent: False     # whether figures are saved with a transparent\n                                # background by default\n#savefig.orientation: portrait  # orientation of saved figure, for PostScript output only\n\n### tk backend params\n#tk.window_focus:   False  # Maintain shell focus for TkAgg\n\n### ps backend params\n#ps.papersize:      letter  # {auto, letter, legal, ledger, A0-A10, B0-B10}\n#ps.useafm:         False   # use of AFM fonts, results in small files\n#ps.usedistiller:   False   # {ghostscript, xpdf, None}\n                            # Experimental: may produce smaller files.\n                            # xpdf intended for production of publication quality files,\n                            # but requires ghostscript, xpdf and ps2eps\n#ps.distiller.res:  6000    # dpi\n#ps.fonttype:       3       # Output Type 3 (Type3) or Type 42 (TrueType)\n\n### PDF backend params\n#pdf.compression:    6  # integer from 0 to 9\n                        # 0 disables compression (good for debugging)\n#pdf.fonttype:       3  # Output Type 3 (Type3) or Type 42 (TrueType)\n#pdf.use14corefonts: False\n#pdf.inheritcolor:   False\n\n### SVG backend params\n#svg.image_inline: True  # Write raster image data directly into the SVG file\n#svg.fonttype: path      # How to handle SVG fonts:\n                         #     path: Embed characters as paths -- supported\n                         #           by most SVG renderers\n                         #     None: Assume fonts are installed on the\n                         #           machine where the SVG will be viewed.\n#svg.hashsalt: None      # If not None, use this string as hash salt instead of uuid4\n\n### pgf parameter\n## See https://matplotlib.org/stable/tutorials/text/pgf.html for more information.\n#pgf.rcfonts: True\n#pgf.preamble:  # See text.latex.preamble for documentation\n#pgf.texsystem: xelatex\n\n### docstring params\n#docstring.hardcopy: False  # set this when you want to generate hardcopy docstring\n\n\n## ***************************************************************************\n## * INTERACTIVE KEYMAPS                                                     *\n## ***************************************************************************\n## Event keys to interact with figures/plots via keyboard.\n## See https://matplotlib.org/stable/users/explain/interactive.html for more\n## details on interactive navigation.  Customize these settings according to\n## your needs. Leave the field(s) empty if you don't need a key-map. (i.e.,\n## fullscreen : '')\n#keymap.fullscreen: f, ctrl+f   # toggling\n#keymap.home: h, r, home        # home or reset mnemonic\n#keymap.back: left, c, backspace, MouseButton.BACK  # forward / backward keys\n#keymap.forward: right, v, MouseButton.FORWARD      # for quick navigation\n#keymap.pan: p                  # pan mnemonic\n#keymap.zoom: o                 # zoom mnemonic\n#keymap.save: s, ctrl+s         # saving current figure\n#keymap.help: f1                # display help about active tools\n#keymap.quit: ctrl+w, cmd+w, q  # close the current figure\n#keymap.quit_all:               # close all figures\n#keymap.grid: g                 # switching on/off major grids in current axes\n#keymap.grid_minor: G           # switching on/off minor grids in current axes\n#keymap.yscale: l               # toggle scaling of y-axes ('log'/'linear')\n#keymap.xscale: k, L            # toggle scaling of x-axes ('log'/'linear')\n#keymap.copy: ctrl+c, cmd+c     # copy figure to clipboard\n\n\n## ***************************************************************************\n## * ANIMATION                                                               *\n## ***************************************************************************\n#animation.html: none  # How to display the animation as HTML in\n                       # the IPython notebook:\n                       #     - 'html5' uses HTML5 video tag\n                       #     - 'jshtml' creates a JavaScript animation\n#animation.writer:  ffmpeg        # MovieWriter 'backend' to use\n#animation.codec:   h264          # Codec to use for writing movie\n#animation.bitrate: -1            # Controls size/quality trade-off for movie.\n                                  # -1 implies let utility auto-determine\n#animation.frame_format: png      # Controls frame format used by temp files\n\n## Path to ffmpeg binary.  Unqualified paths are resolved by subprocess.Popen.\n#animation.ffmpeg_path:  ffmpeg\n## Additional arguments to pass to ffmpeg.\n#animation.ffmpeg_args:\n\n## Path to ImageMagick's convert binary.  Unqualified paths are resolved by\n## subprocess.Popen, except that on Windows, we look up an install of\n## ImageMagick in the registry (as convert is also the name of a system tool).\n#animation.convert_path: convert\n## Additional arguments to pass to convert.\n#animation.convert_args: -layers, OptimizePlus\n#\n#animation.embed_limit:  20.0     # Limit, in MB, of size of base64 encoded\n                                  # animation in HTML (i.e. IPython notebook)",
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Bayesian estimation via MCMC-># Prior hyperparameters\n\n# Prior for obs. cov. is inverse-Wishart(v_1^0=k + 3, S10=I)\nv10 = mod.k_endog + 3\nS10 = np.eye(mod.k_endog)\n\n# Prior for state cov. variances is inverse-Gamma(v_{i2}^0 / 2 = 3, S+{i2}^0 / 2 = 0.005)\nvi20 = 6\nSi20 = 0.01",
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->Confidence interval-># Now create a bootstrap confidence interval around the a LOWESS fit\n\n\ndef lowess_with_confidence_bounds(\n    x, y, eval_x, N=200, conf_interval=0.95, lowess_kw=None\n):\n    \"\"\"\n    Perform Lowess regression and determine a confidence interval by bootstrap resampling\n    \"\"\"\n    # Lowess smoothing\n    smoothed = sm.nonparametric.lowess(exog=x, endog=y, xvals=eval_x, **lowess_kw)\n\n    # Perform bootstrap resamplings of the data\n    # and  evaluate the smoothing at a fixed set of points\n    smoothed_values = np.empty((N, len(eval_x)))\n    for i in range(N):\n        sample = np.random.choice(len(x), len(x), replace=True)\n        sampled_x = x[sample]\n        sampled_y = y[sample]\n\n        smoothed_values[i] = sm.nonparametric.lowess(\n            exog=sampled_x, endog=sampled_y, xvals=eval_x, **lowess_kw\n        )\n\n    # Get the confidence interval\n    sorted_values = np.sort(smoothed_values, axis=0)\n    bound = int(N * (1 - conf_interval) / 2)\n    bottom = sorted_values[bound - 1]\n    top = sorted_values[-bound]\n\n    return smoothed, bottom, top\n\n\n# Compute the 95% confidence interval\neval_x = np.linspace(0, 4 * np.pi, 31)\nsmoothed, bottom, top = lowess_with_confidence_bounds(\n    x, y, eval_x, lowess_kw={\"frac\": 0.1}\n)",
            "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->5. Tracking model training with TensorBoard-># helper functions\n\ndef images_to_probs(net, images):\n    '''\n    Generates predictions and corresponding probabilities from a trained\n    network and a list of images\n    '''\n    output = net(images)\n    # convert output probabilities to predicted class\n    _, preds_tensor = torch.max(output, 1)\n    preds = np.squeeze(preds_tensor.numpy())\n    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n\n\ndef plot_classes_preds(net, images, labels):\n    '''\n    Generates matplotlib Figure using a trained network, along with images\n    and labels from a batch, that shows the network's top prediction along\n    with its probability, alongside the actual label, coloring this\n    information based on whether the prediction was correct or not.\n    Uses the \"images_to_probs\" function.\n    '''\n    preds, probs = images_to_probs(net, images)\n    # plot the images in the batch, along with predicted and true labels\n    fig = plt.figure(figsize=(12, 48))\n    for idx in np.arange(4):\n        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n        matplotlib_imshow(images[idx], one_channel=True)\n        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n            classes[preds[idx]],\n            probs[idx] * 100.0,\n            classes[labels[idx]]),\n                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n    return fig",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>"
        ]
    },
    {
        "id": "292213",
        "GT": "Introducing time effect for future rating->list_of_countries = ['England', 'Italy', 'Spain', 'Germany', 'Brazil']\nn_yr = 15\n\nrating_combine = pd.DataFrame(index = range(0, n_yr), columns = list_of_countries)\n\nfor c in list_of_countries:\n    for n in range(0,n_yr):\n        rating_352_Overall_later, _ = get_best_squad_n_n_yr_later(n, squad_352_strict, c)\n        rating_combine[c].iloc[n] = rating_352_Overall_later\n        \nax = rating_combine.plot(kind = 'line', figsize = (15,10), title = 'Country 3-5-2 best 11 rating by time')\nax.set_xlabel(\"n years later\")\nax.set_ylabel(\"team rating\")\n    ",
        "pred": [
            "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach->Region of Practical Equivalence->cred_intervals = []\nintervals = [0.5, 0.75, 0.95]\n\nfor interval in intervals:\n    cred_interval = list(t_post.interval(interval))\n    cred_intervals.append([interval, cred_interval[0], cred_interval[1]])\n\ncred_int_df = (\n    cred_intervals, columns=[\"interval\", \"lower value\", \"upper value\"]\n).set_index(\"interval\")\ncred_int_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "numpy->NumPy Features->Masked Arrays->Exploring the data->totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Sentiment Analysis on the Speech Data->x_axis = []\ndata = {'positive sentiment': [], 'negative sentiment': []}\nfor speaker in predictions:\n    # The speakers will be used to label the x-axis in our plot\n    x_axis.append(speaker)\n    # number of paras with positive sentiment\n    no_pos_paras = len(predictions[speaker]['pos_paras'])\n    # number of paras with negative sentiment\n    no_neg_paras = len(predictions[speaker]['neg_paras'])\n    # Obtain percentage of paragraphs with positive predicted sentiment\n    pos_perc = no_pos_paras / (no_pos_paras + no_neg_paras)\n    # Store positive and negative percentages\n    data['positive sentiment'].append(pos_perc*100)\n    data['negative sentiment'].append(100*(1-pos_perc))\n\nindex = pd.Index(x_axis, name='speaker')\ndf = pd.DataFrame(data, index=index)\nax = df.plot(kind='bar', stacked=True)\nax.set_ylabel('percentage')\nax.legend(title='labels', bbox_to_anchor=(1, 1), loc='upper left')\nplt.show()",
            "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors->alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "statsmodels->Examples->Statistics->ANOVA->Two-way ANOVA->kidney_lm = ols(\"np.log(Days+1) ~ C(Duration) * C(Weight)\", data=kt).fit()\n\ntable10 = anova_lm(kidney_lm)\n\nprint(\n    anova_lm(ols(\"np.log(Days+1) ~ C(Duration) + C(Weight)\", data=kt).fit(), kidney_lm)\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Duration)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Weight)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)"
        ]
    },
    {
        "id": "779867",
        "GT": "Fill missing with the mean of the dataset. \nThis might not be the _best_ approach, but it should work OK for the time being.  Other approaches could be forward filling the data.->_ = omni_data.fillna({col: omni_data[col].mean() for col in omni_data.columns}, inplace=True)",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->dta[\"affair\"] = (dta[\"affairs\"]  0).astype(float)\nprint(dta.head(10))",
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian Processes regression: basic introductory example->Example with noisy targets->noise_std = 0.75\ny_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog->data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)",
            "sklearn->Examples->Clustering->A demo of the mean-shift clustering algorithm->Generate sample data->centers = [[1, 1], [-1, -1], [1, -1]]\nX, _ = (n_samples=10000, centers=centers, cluster_std=0.6)",
            "sklearn->Tutorials->Working With Text Data->Extracting features from text files->From occurrences to frequencies->tfidf_transformer = TfidfTransformer()\n X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n X_train_tfidf.shape\n(2257, 35788)"
        ]
    },
    {
        "id": "1113305",
        "GT": "Feature Engineering->calculating political knowledge score->answers = np.array('Republican Party; Republican Party; Democratic Party; Representative; Representative; Representative; Senator; Senator; Judge'.split('; '))\n\npk_score = []\nfor row in dems.index:\n    response = np.array(dems.loc[row, 'pk_ideo_baseline':'pk_SCJ_baseline'])\n    pk_score.append(sum(response==answers))\n\nold_pk_list = [col for col in dems.columns if col.startswith('pk_')]\ndems.drop(old_pk_list, axis = 1, inplace = True)\ndems.drop('pol_knowledge', axis =1, inplace = True)\n\ndems['pk_score'] = pk_score\n",
        "pred": [
            "sklearn->Examples->Clustering->Demo of DBSCAN clustering algorithm->Plot results->unique_labels = set(labels)\ncore_samples_mask = (labels, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\n\ncolors = [plt.cm.Spectral(each) for each in (0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = X[class_member_mask & core_samples_mask]\n    (\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    (\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\n(f\"Estimated number of clusters: {n_clusters_}\")\n()\n\n\n<img alt=\"Estimated number of clusters: 3\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_dbscan_002.png\" srcset=\"../../_images/sphx_glr_plot_dbscan_002.png\"/>",
            "statsmodels->Examples->Statistics->ANOVA->resid = interM_lm32.get_influence().summary_frame()[\"standard_resid\"]\n\nplt.figure(figsize=(6, 6))\nresid = resid.reindex(X.index)\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X.loc[idx],\n        resid.loc[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X[~[32]]\")\nplt.ylabel(\"standardized resids\")",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings->data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor->quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance->feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>"
        ]
    },
    {
        "id": "1435816",
        "GT": "Predicting Destinations with the Airbnb Dataset using Naive bayes Classifier\n\nThis notebook demonstrates the entire process of building a predictive model using Naive bayes classifier to suggest the first destination of new Airbnb Users. All the processes involved, such as data wrangling, exploratory data analysis, inferential statistics.->def weightedRandomHelper(pairs):  \n    total = sum(pair[0] for pair in pairs)\n    r = randint(1, total)\n    for (weight, value) in pairs:\n        r -= weight\n        if r <= 0: return value",
        "pred": [
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->But how do you obtain sentiment from the LSTM\u2019s output?->def fp_fc_layer(last_hs, parameters):\n    z2 = (np.dot(parameters['W2'], last_hs)\n          + parameters['b2'])\n    a2 = sigmoid(z2)\n    return a2",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Forward Propagation->def fp_forget_gate(concat, parameters):\n    ft = sigmoid(np.dot(parameters['Wf'], concat)\n                 + parameters['bf'])\n    return ft",
            "torch->Learning PyTorch->What is torch.nn really?->nn.Sequential->class Lambda():\n    def __init__(self, func):\n        super().__init__()\n        self.func = func\n\n    def forward(self, x):\n        return self.func(x)\n\n\ndef preprocess(x):\n    return x.view(-1, 1, 28, 28)",
            "torch->Model Optimization->torch.compile Tutorial->Basic Usage->class MyModule():\n    def __init__(self):\n        super().__init__()\n        self.lin = (100, 10)\n\n    def forward(self, x):\n        return (self.lin(x))\n\nmod = ()\n = (mod)\nprint(((10, 100)))",
            "pandas_toms_blog->Method Chaining->Method Chaining->Inplace?->def dataframe_method(self, inplace=False):\n    data = self.copy()  # regardless of inplace\n    result = ...\n    if inplace:\n        self._update_inplace(data)\n    else:\n        return result"
        ]
    },
    {
        "id": "23260",
        "GT": "Step 2: Generate Features Using Deep Feature Synthesis\n\nWe can specify particular primitives as building blocks to automatically construct features. Featuretools will walk through our EntitySet and apply each primitive as many times as possible using the [Deep Feature Synthesis](https://docs.featuretools.com/automated_feature_engineering/afe.html) (DFS) algorithm. \n\nThe `max_depth` parameter tells DFS how deep to make each feature which is closely associated to how long the algorithm takes. With `max_depth=3` we create 290 features in about 5 minutes. If you are running this notebook yourself, try changing the max depth or the `agg_primitives` to see how that changes the rest of the results.->feature_matrix_encoded, features_encoded = ft.encode_features(feature_matrix, features)\n\npipeline_preprocessing = [(\"imputer\",\n                           SimpleImputer()),\n                          (\"scaler\", RobustScaler(with_centering=True))]\nfeature_matrix_encoded.tail()",
        "pred": [
            "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance->feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = (\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (train set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_003.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_003.png\"/>",
            "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Decision path->node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )",
            "sklearn->Examples->Working with text documents->Clustering text documents using k-means->K-means clustering on text features->Top terms per cluster->original_space_centroids = lsa[0].inverse_transform(kmeans.cluster_centers_)\norder_centroids = original_space_centroids.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names_out()\n\nfor i in range(true_k):\n    print(f\"Cluster {i}: \", end=\"\")\n    for ind in order_centroids[i, :10]:\n        print(f\"{terms[ind]} \", end=\"\")\n    print()",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-steps as categories->one_hot_linear_pipeline = (\n    (\n        transformers=[\n            (\"categorical\", one_hot_encoder, categorical_columns),\n            (\"one_hot_time\", one_hot_encoder, [\"hour\", \"weekday\", \"month\"]),\n        ],\n        remainder=(),\n    ),\n    (alphas=alphas),\n)\n\nevaluate(one_hot_linear_pipeline, X, y, cv=ts_cv)"
        ]
    },
    {
        "id": "190611",
        "GT": "sb.boxplot(x=college['Private'], y=college['Outstate'], data=college, order=[\"No\", \"Yes\"])",
        "pred": [
            "seaborn->User guide and tutorial->Visualizing categorical data->Categorical scatterplots->tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "seaborn->Plotting functions->Visualizing categorical data->Categorical scatterplots->tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots->tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots->tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->sns.jointplot(x='carat', y='price', data=df, size=8, alpha=.25,\n              color='k', marker='.')\nplt.tight_layout()",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines->sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines->sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n"
        ]
    },
    {
        "id": "192037",
        "GT": "Discretization of categorical variables->import pandas as pd\ncat_retained = pd.read_csv(\"/home/data/kaggle/csv_cat_cut1.csv\", nrows=100)",
        "pred": [
            "seaborn->User guide and tutorial->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "numpy->NumPy Applications->X-ray image processing->Examine an X-ray with imageio->import os\nimport imageio\n\nDIR = \"tutorial-x-ray-image-processing\"\n\nxray_image = imageio.v3.imread(os.path.join(DIR, \"00000011_001.png\"))",
            "torch->Model Optimization->Multi-Objective NAS with Ax->Evaluating the results->from ax.service.utils.report_utils import exp_to_df\n\ndf = exp_to_df(experiment)\ndf.head(10)",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->import pandas as pd\n\nfeature_names = rf[:-1].get_feature_names_out()\n\nmdi_importances = (\n    rf[-1].feature_importances_, index=feature_names\n).sort_values(ascending=True)"
        ]
    },
    {
        "id": "1035636",
        "GT": "31. db_creators_url->useless_data.append('db_creators_url')",
        "pred": [
            "seaborn->User guide and tutorial->An introduction to seaborn->Multivariate views on complex datasets->sns.pairplot(data=penguins, hue=\"species\")\n",
            "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Combining multiple views on the data->sns.pairplot(data=penguins, hue=\"species\")\n",
            "seaborn->API Overview->Overview of seaborn plotting functions->Combining multiple views on the data->sns.pairplot(data=penguins, hue=\"species\")\n",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Capturing the Model with Symbolic Tracing->traced_rn18 = (rn18)\nprint()",
            "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Fusing Convolution with Batch Norm->traced_model = (model)\nprint(traced_model.graph)",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->dta.plot(figsize=(12, 8))"
        ]
    },
    {
        "id": "594910",
        "GT": "active_by_phone = udf.groupby('phone')['active'].sum() / udf.groupby('phone')['active'].size()\n\nactive_by_phone = active_by_phone.sort_values()\n_ = active_by_phone.plot(kind = 'bar', rot = 0)\n_ = plt.xlabel(\"\")\n_ = plt.title(\"Proportion of users active by type of phone\")\nplt.show()",
        "pred": [
            "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Accuracy vs alpha for training and testing sets->train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = ()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\n()\n\n\n<img alt=\"Accuracy vs alpha for training and testing sets\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\" srcset=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\"/>",
            "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Decision path->node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )",
            "statsmodels->Examples->Statistics->ANOVA->infl = interM_lm.get_influence()\nresid = infl.resid_studentized_internal\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        resid[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X\")\nplt.ylabel(\"standardized resids\")",
            "numpy->NumPy Features->Masked Arrays->Exploring the data->totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->ROC curve using the OvO macro-average->pair_scores = []\nmean_tpr = dict()\n\nfor ix, (label_a, label_b) in enumerate(pair_list):\n\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = (a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = (label_binarizer.classes_ == label_a)[0]\n    idx_b = (label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = (a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = (b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = (fpr_grid)\n    mean_tpr[ix] += (fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += (fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = (fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = (figsize=(6, 6))\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    (\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    (\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n    )\n    ([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n    (\"square\")\n    (\"False Positive Rate\")\n    (\"True Positive Rate\")\n    (f\"{target_names[idx_a]} vs {label_b} ROC curves\")\n    ()\n    ()\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{(pair_scores):.2f}\")\n\n\n\n<img alt=\"setosa vs versicolor ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_004.png\" srcset=\"../../_images/sphx_glr_plot_roc_004.png\"/>\n<img alt=\"setosa vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_005.png\" srcset=\"../../_images/sphx_glr_plot_roc_005.png\"/>\n<img alt=\"versicolor vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_006.png\" srcset=\"../../_images/sphx_glr_plot_roc_006.png\"/>"
        ]
    },
    {
        "id": "262091",
        "GT": "Collect data from cast section->#Convert the casts of unpopularmove from a list of strings to one string\ncast1=\"\".join(unpopularmovie_casts)",
        "pred": [
            "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Nelder-Mead Simplex algorithm (method='Nelder-Mead')->def rosen(x):\n...     \"\"\"The Rosenbrock function\"\"\"\n...     return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)",
            "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates-># Assign better names for our seasonal terms\ntrue_seasonal_10_3 = terms[0]\ntrue_seasonal_100_2 = terms[1]\ntrue_sum = true_seasonal_10_3 + true_seasonal_100_2\n<br/>",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)->preprocessed_random_frame = frame_preprocessing(random_frame)\nplt.imshow(preprocessed_random_frame, cmap=\"gray\")\nprint(preprocessed_random_frame.shape)"
        ]
    },
    {
        "id": "539624",
        "GT": "2  Designing sample weights->trig05['WTSAF2YR'].sum()",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resf_logit.predict()",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->pca_model = PCA(dta.T, standardize=False, demean=True)",
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time.head()",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->glm_mod.model.data.orig_endog.sum(1)",
            "statsmodels->Examples->Statistics->ANOVA->interM_lm.model.data.orig_exog[:5]"
        ]
    },
    {
        "id": "577184",
        "GT": "map: in lamdba , x is an element->shape,dropna, fillna,drop_dupicates->print(df)\ndf[[\"A\",\"B\"]].drop_duplicates()\n\nprint(\"set the first row to na:\\n\")\ndf[0:1]=np.nan\nprint(\"after dropna:\\n\",df.dropna())\nprint(\"after fillna:\\n\",df.fillna(0))\nprint(\"df.mean\\n\",df.mean())\nvalues=dict(df.mean())\nprint(\"after fillna with mean of each column:\\n\",df.fillna(values))\ndf.head()",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper->print(res.cusum)\nfig = res.plot_cusum()",
            "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper->print(sm.datasets.copper.DESCRLONG)\n\ndta = sm.datasets.copper.load_pandas().data\ndta.index = pd.date_range(\"1951-01-01\", \"1975-01-01\", freq=\"AS\")\nendog = dta[\"WORLDCONSUMPTION\"]\n\n# To the regressors in the dataset, we add a column of ones for an intercept\nexog = sm.add_constant(\n    dta[[\"COPPERPRICE\", \"INCOMEINDEX\", \"ALUMPRICE\", \"INVENTORYINDEX\"]]\n)",
            "scipy->Statistics (scipy.stats)->Analysing one sample->Tails of the distribution->print('tail prob. of normal at 1%%, 5%% and 10%% %8.4f %8.4f %8.4f' %\n...       tuple(stats.norm.sf([crit01, crit05, crit10])*100))\ntail prob. of normal at 1%, 5% and 10%   0.2857   3.4957   8.5003",
            "scipy->Statistics (scipy.stats)->Building specific distributions->Subclassing rv_discrete->print('mean = %6.4f, variance = %6.4f, skew = %6.4f, kurtosis = %6.4f' %\n...       normdiscrete.stats(moments='mvsk'))\nmean = -0.0000, variance = 6.3302, skew = 0.0000, kurtosis = -0.0076",
            "sklearn->Tutorials->Working With Text Data->Loading the 20 newsgroups dataset->print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\nFrom: sd345@city.ac.uk (Michael Collier)\nSubject: Converting images to HP LaserJet III?\nNntp-Posting-Host: hampton\n\n print(twenty_train.target_names[twenty_train.target[0]])\ncomp.graphics"
        ]
    },
    {
        "id": "107315",
        "GT": "df_au = pd.DataFrame()\ndf_au[header_list[0]] = flag\ndf_au[header_list[1]] = state\ndf_au[header_list[2]]=abbrev\ndf_au[header_list[3]]=ISO\ndf_au[header_list[4]]=Postal\ndf_au[header_list[5]]=Type\ndf_au[header_list[6]]=Capital\ndf_au[header_list[7]]=population\ndf_au[header_list[8]]=Area",
        "pred": [
            "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 3: The Dataloader->dataloader = DataLoader(transformed_dataset, batch_size=4,\n                        shuffle=True, num_workers=4)\n\n\n# Helper function to show a batch\ndef show_landmarks_batch(sample_batched):\n    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n    images_batch, landmarks_batch = \\\n            sample_batched['image'], sample_batched['landmarks']\n    batch_size = len(images_batch)\n    im_size = images_batch.size(2)\n\n    grid = utils.make_grid(images_batch)\n    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n\n    for i in range(batch_size):\n        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size,\n                    landmarks_batch[i, :, 1].numpy(),\n                    s=10, marker='.', c='r')\n\n        plt.title('Batch from dataloader')\n\nfor i_batch, sample_batched in enumerate(dataloader):\n    print(i_batch, sample_batched['image'].size(),\n          sample_batched['landmarks'].size())\n\n    # observe 4th batch and stop.\n    if i_batch == 3:\n        plt.figure()\n        show_landmarks_batch(sample_batched)\n        plt.axis('off')\n        plt.ioff()\n        plt.show()\n        break",
            "statsmodels->Examples->Statistics->ANOVA->lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach->Region of Practical Equivalence->cred_intervals = []\nintervals = [0.5, 0.75, 0.95]\n\nfor interval in intervals:\n    cred_interval = list(t_post.interval(interval))\n    cred_intervals.append([interval, cred_interval[0], cred_interval[1]])\n\ncred_int_df = (\n    cred_intervals, columns=[\"interval\", \"lower value\", \"upper value\"]\n).set_index(\"interval\")\ncred_int_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples->face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break",
            "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence->results = (list)\nn_bootstrap = 100\nrng = (seed=0)\n\nfor prevalence, X, y in zip(\n    populations[\"prevalence\"], populations[\"X\"], populations[\"y\"]\n):\n\n    results_for_prevalence = scoring_on_bootstrap(\n        estimator, X, y, rng, n_bootstrap=n_bootstrap\n    )\n    results[\"prevalence\"].append(prevalence)\n    results[\"metrics\"].append(\n        results_for_prevalence.aggregate([\"mean\", \"std\"]).unstack()\n    )\n\nresults = (results[\"metrics\"], index=results[\"prevalence\"])\nresults.index.name = \"prevalence\"\nresults\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ]
    },
    {
        "id": "781315",
        "GT": "Motivation & Theory  <a id=\"background\"></a>->Analysis <a id=\"analysis\"></a>\n\n&emsp;&emsp;&emsp; Despite this issue, I find two results supporting the initial hypotheses. First, as can be observed in the figure below, changes in political affiliation of municipal presidents in favor of SNS continue up to this day, well beyond the time frame imposed by SPS and SNS for their local chapters to match the national coalition. This is strongly indicative of a trend of local aldermen, other than those in SPS, switching their political affiliations to the new nationally dominant party, SNS.-># Figure 1\nImage(\"./exports/figures/Political Affiliation of Municipality Presidents in Serbia 2012-2016.png\", width=1000)",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing->Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_4_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_4_1.png\"/>",
            "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Steps->3. Save model A-># Specify a path to save to\nPATH = \"model.pt\"\n\n(netA.state_dict(), PATH)",
            "torch->PyTorch Recipes->Saving and loading models for inference in PyTorch->Steps->5. Save and load entire model-># Specify a path\nPATH = \"entire_model.pt\"\n\n# Save\n(net, PATH)\n\n# Load\nmodel = (PATH)\nmodel.eval()",
            "statsmodels->Examples->State space models->State space modeling: Local Linear Trends-># Perform prediction and forecasting\npredict = res.get_prediction()\nforecast = res.get_forecast('2014')",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n"
        ]
    },
    {
        "id": "918807",
        "GT": "W205 Query Project: Assignment 05\n\nAnalysis of the publicly avalible Bay Area bikeshare data using Google's BigQuery. The objective of this assignment is to determine a promotion to increase ridership. See conclusion for reccomended promotion. Written by Andrew Walters (andrewfwalters@berkeley.edu).->1) What are the 5 most common commuter trips?->q00_strs = [\"SELECT HOUR(trips.start_date) AS start_hour,\",\n            \"trips.subscriber_type,\"\n            \"COUNT(trips.trip_id) AS num_trips\",\n            \"FROM [bigquery-public-data:san_francisco.bikeshare_trips] AS trips\",\n            \"WHERE DAYOFWEEK(trips.start_date) BETWEEN 2 AND 6\",\n            \"GROUP BY start_hour, trips.subscriber_type\",\n            \"ORDER BY start_hour, trips.subscriber_type ASC\"]\nq00_out = \"q00.csv\"\nq00_cmd = bq_builder(q00_strs,outfile=q00_out,legacy_sql=True)\nexit_stat = os.system(q00_cmd)",
        "pred": [
            "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->5. Quantization-aware training->qat_model = load_model(saved_model_dir + float_model_file)\nqat_model.fuse_model()\n\noptimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nqat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')",
            "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns->n_comps = 2\n\nmethods = [\n    (\"PCA\", ()),\n    (\"Unrotated FA\", ()),\n    (\"Varimax FA\", (rotation=\"varimax\")),\n]\nfig, axes = (ncols=len(methods), figsize=(10, 8), sharey=True)\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks((len(feature_names)))\n    ax.set_yticklabels(feature_names)\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\nfig.suptitle(\"Factors\")\n()\n()\n\n\n<img alt=\"Factors, PCA, Unrotated FA, Varimax FA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_002.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_002.png\"/>",
            "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples->face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions->last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Factor Analysis components - FA->fa_estimator = (n_components=n_components, max_iter=20)\nfa_estimator.fit(faces_centered)\nplot_gallery(\"Factor Analysis (FA)\", fa_estimator.components_[:n_components])\n\n# --- Pixelwise variance\n(figsize=(3.2, 3.6), facecolor=\"white\", tight_layout=True)\nvec = fa_estimator.noise_variance_\nvmax = max(vec.max(), -vec.min())\n(\n    vec.reshape(image_shape),\n    cmap=plt.cm.gray,\n    interpolation=\"nearest\",\n    vmin=-vmax,\n    vmax=vmax,\n)\n(\"off\")\n(\"Pixelwise variance from \\n Factor Analysis (FA)\", size=16, wrap=True)\n(orientation=\"horizontal\", shrink=0.8, pad=0.03)\n()\n\n\n\n<img alt=\"Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\"/>\n<img alt=\"Pixelwise variance from   Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\"/>"
        ]
    },
    {
        "id": "1382528",
        "GT": "Real Data: California Housing->X_CA_H, y_CA_H = utils.shuffle(X_CA_H, y_CA_H, random_state=1)\nX_CA_H_train, X_CA_H_test, y_CA_H_train, y_CA_H_test = model_selection.train_test_split(\n    X_CA_H, y_CA_H, test_size=0.4, random_state=0)\nprint((X_CA_H_train.shape), y_CA_H_train.shape)\nprint((X_CA_H_test.shape), y_CA_H_test.shape)",
        "pred": [
            "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance->feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot->x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Creating listed colormaps->top = mpl.colormaps['Oranges_r'].resampled(128)\nbottom = mpl.colormaps['Blues'].resampled(128)\n\nnewcolors = np.vstack((top(np.linspace(0, 1, 128)),\n                       bottom(np.linspace(0, 1, 128))))\nnewcmp = ListedColormap(newcolors, name='OrangeBlue')\nplot_examples([viridis, newcmp])\n\n\n<img alt=\"colormap manipulation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormap-manipulation_004.png\" srcset=\"../../_images/sphx_glr_colormap-manipulation_004.png, ../../_images/sphx_glr_colormap-manipulation_004_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction->Swiss Roll->sr_lle, sr_err = (\n    sr_points, n_neighbors=12, n_components=2\n)\n\nsr_tsne = (n_components=2, perplexity=40, random_state=0).fit_transform(\n    sr_points\n)\n\nfig, axs = (figsize=(8, 8), nrows=2)\naxs[0].scatter(sr_lle[:, 0], sr_lle[:, 1], c=sr_color)\naxs[0].set_title(\"LLE Embedding of Swiss Roll\")\naxs[1].scatter(sr_tsne[:, 0], sr_tsne[:, 1], c=sr_color)\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss Roll\")\n\n\n<img alt=\"LLE Embedding of Swiss Roll, t-SNE Embedding of Swiss Roll\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_002.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_002.png\"/>",
            "sklearn->Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction->Swiss-Hole->sh_lle, sh_err = (\n    sh_points, n_neighbors=12, n_components=2\n)\n\nsh_tsne = (\n    n_components=2, perplexity=40, init=\"random\", random_state=0\n).fit_transform(sh_points)\n\nfig, axs = (figsize=(8, 8), nrows=2)\naxs[0].scatter(sh_lle[:, 0], sh_lle[:, 1], c=sh_color)\naxs[0].set_title(\"LLE Embedding of Swiss-Hole\")\naxs[1].scatter(sh_tsne[:, 0], sh_tsne[:, 1], c=sh_color)\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss-Hole\")\n\n\n<img alt=\"LLE Embedding of Swiss-Hole, t-SNE Embedding of Swiss-Hole\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_004.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_004.png\"/>"
        ]
    },
    {
        "id": "1268717",
        "GT": "Compute Means->import numpy\nFandango_mean = movies['Fandango_Stars'].mean()\nMetacritic_mean = movies['Metacritic_norm_round'].mean()\n\nFandango_median = movies['Fandango_Stars'].median()\nMetacritic_median = movies['Metacritic_norm_round'].median()\n\nFandango_STD = numpy.std(movies['Fandango_Stars'])\nMetacritic_STD = numpy.std(movies['Metacritic_norm_round'])\n\nprint(Fandango_mean, Metacritic_mean, Fandango_median, Metacritic_median, Fandango_STD, Metacritic_STD)\n",
        "pred": [
            "torch->PyTorch Recipes->PyTorch Benchmark->Steps->6. Saving/Loading benchmark results->import pickle\n\nab_test_results = []\nfor env in ('environment A: mul/sum', 'environment B: bmm'):\n    for b, n in ((1, 1), (1024, 10000), (10000, 1)):\n        x = ((b, n))\n        dot_fn = (batched_dot_mul_sum if env == 'environment A: mul/sum' else batched_dot_bmm)\n        m = (\n            stmt='batched_dot(x, x)',\n            globals={'x': x, 'batched_dot': dot_fn},\n            num_threads=1,\n            label='Batched dot',\n            description=f'[{b}, {n}]',\n            env=env,\n        ).blocked_autorange(min_run_time=1)\n        ab_test_results.append(pickle.dumps(m))\n\nab_results = [pickle.loads(i) for i in ab_test_results]\ncompare = benchmark.Compare(ab_results)\ncompare.trim_significant_figures()\ncompare.colorize()\ncompare.print()\n\n\n\nOutput",
            "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps->import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "sklearn->Examples->Ensemble methods->Feature importances with a forest of trees->Feature importance based on mean decrease in impurity->import pandas as pd\n\nforest_importances = (importances, index=feature_names)\n\nfig, ax = ()\nforest_importances.plot.bar(yerr=std, ax=ax)\nax.set_title(\"Feature importances using MDI\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()\n\n\n<img alt=\"Feature importances using MDI\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_forest_importances_001.png\" srcset=\"../../_images/sphx_glr_plot_forest_importances_001.png\"/>",
            "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Plot the most uncertain predictions->import matplotlib.pyplot as plt\n\nf = (figsize=(7, 5))\nfor index, image_index in enumerate(uncertainty_index):\n    image = images[image_index]\n\n    sub = f.add_subplot(2, 5, index + 1)\n    sub.imshow(image, cmap=plt.cm.gray_r)\n    ([])\n    ([])\n    sub.set_title(\n        \"predict: %i\\ntrue: %i\" % (lp_model.transduction_[image_index], y[image_index])\n    )\n\nf.suptitle(\"Learning with small amount of labeled data\")\n()\n\n\n<img alt=\"Learning with small amount of labeled data, predict: 1 true: 2, predict: 2 true: 2, predict: 8 true: 8, predict: 1 true: 8, predict: 1 true: 8, predict: 1 true: 8, predict: 3 true: 3, predict: 8 true: 8, predict: 2 true: 2, predict: 7 true: 2\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_label_propagation_digits_002.png\" srcset=\"../../_images/sphx_glr_plot_label_propagation_digits_002.png\"/>",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Working with multiple figures and axes->import matplotlib.pyplot as plt\nplt.figure(1)                # the first figure\nplt.subplot(211)             # the first subplot in the first figure\nplt.plot([1, 2, 3])\nplt.subplot(212)             # the second subplot in the first figure\nplt.plot([4, 5, 6])\n\n\nplt.figure(2)                # a second figure\nplt.plot([4, 5, 6])          # creates a subplot() by default\n\nplt.figure(1)                # first figure current;\n                             # subplot(212) still current\nplt.subplot(211)             # make subplot(211) in the first figure\n                             # current\nplt.title('Easy as 1, 2, 3') # subplot 211 title"
        ]
    },
    {
        "id": "692454",
        "GT": "Summary data analysys and preprocessing->students.hist(figsize=(20, 20));",
        "pred": [
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->sns.heatmap(X.corr());",
            "pandas_toms_blog->Time Series->Timeseries->Seasonality->smt.seasonal_decompose(y).plot();",
            "seaborn->User guide and tutorial->Estimating regression fits->Functions for drawing linear regression models->sns.lmplot(x=\"size\", y=\"tip\", data=tips);\n",
            "seaborn->Statistical operations->Estimating regression fits->Functions for drawing linear regression models->sns.lmplot(x=\"size\", y=\"tip\", data=tips);\n",
            "seaborn->User guide and tutorial->Estimating regression fits->Conditioning on other variables->sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", data=tips);\n",
            "seaborn->Statistical operations->Estimating regression fits->Conditioning on other variables->sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", data=tips);\n",
            "seaborn->User guide and tutorial->Estimating regression fits->Functions for drawing linear regression models->sns.lmplot(x=\"total_bill\", y=\"tip\", data=tips);\n",
            "seaborn->Statistical operations->Estimating regression fits->Functions for drawing linear regression models->sns.lmplot(x=\"total_bill\", y=\"tip\", data=tips);\n"
        ]
    },
    {
        "id": "225745",
        "GT": "Filtering the dataframe by Studio->mov3 = mov2[(mov2.Studio == 'Buena Vista Studios') | (mov2.Studio == 'Fox') | (mov2.Studio == 'Paramount Pictures') | (mov2.Studio == 'Sony') | (mov2.Studio == 'Universal') | (mov2.Studio == 'WB')]",
        "pred": [
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Sampling->rng = default_rng()\n\nbefore_sample = rng.choice(before_lock, size=30, replace=False)\nafter_sample = rng.choice(after_lock, size=30, replace=False)",
            "sklearn->Tutorials->Working With Text Data->Parameter tuning using grid search->twenty_train.target_names[gs_clf.predict(['God is love'])[0]]\n'soc.religion.christian'",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team->team\nAtlanta Hawks        0.585366\nBoston Celtics       0.585366\nBrooklyn Nets        0.256098\nCharlotte Hornets    0.585366\nChicago Bulls        0.512195\ndtype: float64",
            "pandas_toms_blog->Indexes->Merging->The merge version->m = pd.merge(flights, daily.reset_index().rename(columns={'date': 'fl_date', 'station': 'origin'}),\n             on=['fl_date', 'origin']).set_index(idx_cols).sort_index()\n\nm.head()",
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()"
        ]
    },
    {
        "id": "587296",
        "GT": "Data preprocessing\n\n## Read  part of GTI dataset and KITTI dataset->Create image path->car_list = glob.glob('datasets/vehicles/**/*.png')\nncar_list = glob.glob('datasets/non-vehicles/**/*.png')\nprint(len(car_list), len(ncar_list))",
        "pred": [
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->diamonds = sns.load_dataset(\"diamonds\")\nsns.displot(diamonds, x=\"carat\", kind=\"kde\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->diamonds = sns.load_dataset(\"diamonds\")\nsns.displot(diamonds, x=\"carat\", kind=\"kde\")\n",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples->tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples->tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n"
        ]
    },
    {
        "id": "645460",
        "GT": "Price Compared to % Vacant->sns.jointplot(x=\"% Vacant\", y=\"PRICE\", data=merged4_MortgageListings, kind = 'reg', size = 7)\nplt.show()",
        "pred": [
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->sns.jointplot(x='carat', y='price', data=df, size=8, alpha=.25,\n              color='k', marker='.')\nplt.tight_layout()",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Showing multiple relationships with facets->sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", hue=\"smoker\", col=\"time\",\n)\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Showing multiple relationships with facets->sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", hue=\"smoker\", col=\"time\",\n)\n",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots->sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", hue=\"smoker\", style=\"smoker\"\n)\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots->sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", hue=\"smoker\", style=\"smoker\"\n)\n",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", errorbar=\"sd\",\n)\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->sns.relplot(\n    data=fmri, kind=\"line\",\n    x=\"timepoint\", y=\"signal\", errorbar=\"sd\",\n)\n",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots->sns.relplot(\n    data=tips, x=\"total_bill\", y=\"tip\",\n    size=\"size\", sizes=(15, 200)\n)\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots->sns.relplot(\n    data=tips, x=\"total_bill\", y=\"tip\",\n    size=\"size\", sizes=(15, 200)\n)\n"
        ]
    },
    {
        "id": "804978",
        "GT": "Customizing the display in pandas\n\nSee how the middle of the table has a `...`? Let's fix that.\n\n## How do you see the number of columns pandas will display?->How do you see the number of rows pandas will display? Then, set it to display 100 rows at a time.\n\nWhat does \"the number rows pandas will display\" actually mean?->pd.set_option('display.max_rows', 100)",
        "pred": [
            "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding->pd.get_dummies(hsb2.race.values, drop_first=False)",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->pd.IndexSlice[:, ['ORD', 'DSM']]",
            "pandas_toms_blog->Indexes->pd.DataFrame(js['features']).head().to_html()",
            "pandas_toms_blog->Indexes->Merging->Concat Version->pd.concat([temp, sped], axis=1, join='inner')",
            "pandas_toms_blog->Indexes->Merging->Concat Version->pd.concat([temp, sped], axis=1).head()"
        ]
    },
    {
        "id": "503330",
        "GT": "Check Estimated Parameters->print(ox.linreg_ridge_lu(y,X, 5.0))",
        "pred": [
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators->print(stats.t.fit(fat_tails, f0=6))",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Small group effects->print(res3.f_test(R))",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators->print(stats.norm.fit(fat_tails))",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->Specifying the number of forecasts->print(res.forecast(steps=2))",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers->print(prestige.head(10))"
        ]
    },
    {
        "id": "390910",
        "GT": "reviews = pd.merge(reviews,\n    (reviews.\n     groupby(['business_id']).\n     size().\n     reset_index().\n     rename(columns={0:'business_review_count'})\n    ),how='left'\n    )",
        "pred": [
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->original data->glm = smf.glm(\n    \"affairs ~ rate_marriage + age + yrs_married\",\n    data=data,\n    family=sm.families.Poisson(),\n)\nres_o = glm.fit()\nprint(res_o.summary())",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->tidy = pd.melt(games.reset_index(),\n               id_vars=['game_id', 'date'], value_vars=['away_team', 'home_team'],\n               value_name='team')\ntidy.head()",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots->sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\",\n    hue=\"size\", palette=\"ch:r=-.5,l=.75\"\n)\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots->sns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\",\n    hue=\"size\", palette=\"ch:r=-.5,l=.75\"\n)\n",
            "pandas_toms_blog->Scaling->Dask->daily = (\n    indiv[['transaction_dt', 'transaction_amt']].dropna()\n        .set_index('transaction_dt')['transaction_amt']\n        .resample(\"D\")\n        .sum()\n).compute()\ndaily",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison->pd.DataFrame(\n    np.column_stack([[r.llf, r.deviance, r.pearson_chi2] for r in results_all]),\n    columns=names,\n    index=[\"llf\", \"deviance\", \"pearson chi2\"],\n)"
        ]
    },
    {
        "id": "59583",
        "GT": "Second block of classes and functions: searching->crawl.createindextables()",
        "pred": [
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->survey.data.info()",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Overriding elements of the seaborn styles->sns.axes_style()\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Overriding elements of the seaborn styles->sns.axes_style()\n",
            "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money->res.plot_cusum()",
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time.head()",
            "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data->res.forecast_components(12)"
        ]
    },
    {
        "id": "1447862",
        "GT": "TIME SERIES ANALYSIS\n\n# Goal: Find outliers, trends and periodicity in the MTA turnstile data\n    \n\n## DATA:\nMTA subway fares. It is a complete dataset of\nrides logged by card swipes for 600 Manhattan stations.->Task 1: \n### Event detection: Identify the most prominent event. There is a very significant drop (>3-sigma) in all time series.\n### Identify it, figure out the date (you know when the data starts and what the cadence is) and figure out what it is due to.->Observe the data:->#draft plot\nax = weeklymean.plot(figsize=(12,4), fontsize=8, linewidth=1.0, legend=False)\nax.set_title('MTA Ridership in NYC by week', fontsize=10)\nax.set_xlabel('Week', fontsize=8)\nax.set_ylabel('Ridership', fontsize=8)",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Dataset\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\ndata['ln_wpi'] = np.log(data['wpi'])\ndata['D.ln_wpi'] = data['ln_wpi'].diff()\n\n\n\n\n\n\n\nIn\u00a0[5]:",
            "matplotlib->Tutorials->Intermediate->Artist tutorial->Object containers->Axis containers-># plt.figure creates a matplotlib.figure.Figure instance\nfig = plt.figure()\nrect = fig.patch  # a rectangle instance\nrect.set_facecolor('lightgoldenrodyellow')\n\nax1 = fig.add_axes([0.1, 0.3, 0.4, 0.4])\nrect = ax1.patch\nrect.set_facecolor('lightslategray')\n\n\nfor label in ax1.xaxis.get_ticklabels():\n    # label is a Text instance\n    label.set_color('red')\n    label.set_rotation(45)\n    label.set_fontsize(16)\n\nfor line in ax1.yaxis.get_ticklines():\n    # line is a Line2D instance\n    line.set_color('green')\n    line.set_markersize(25)\n    line.set_markeredgewidth(3)\n\nplt.show()\n\n\n<img alt=\"artists\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_artists_004.png\" srcset=\"../../_images/sphx_glr_artists_004.png, ../../_images/sphx_glr_artists_004_2_0x.png 2.0x\"/>",
            "numpy->NumPy Features->Saving and sharing your NumPy arrays->Save the data to csv file using-># x, y\n0.000000000000000000e+00,0.000000000000000000e+00\n1.000000000000000000e+00,1.000000000000000000e+00\n2.000000000000000000e+00,4.000000000000000000e+00\n3.000000000000000000e+00,9.000000000000000000e+00\n4.000000000000000000e+00,1.600000000000000000e+01\n5.000000000000000000e+00,2.500000000000000000e+01\n6.000000000000000000e+00,3.600000000000000000e+01\n7.000000000000000000e+00,4.900000000000000000e+01\n8.000000000000000000e+00,6.400000000000000000e+01\n9.000000000000000000e+00,8.100000000000000000e+01",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjQAAAEWCAYAAABxBw2jAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOydd5wcdf3/n5/Zdj2XSy7l0jtJIKGEroDSQYoggl9RvnQVBQQL4NcC/vgiqJQvIoJgAVGkiAQQkI50kkACSSC9Xvr12z7z+f3x2c1t7vbudu+23r2fD4bbnZmd+WR2duY176q01giCIAiCIBQzVr4HIAiCIAiC0F9E0AiCIAiCUPSIoBEEQRAEoegRQSMIgiAIQtEjgkYQBEEQhKJHBI0gCIIgCEWPCBpBEARBEIoeETSCIPSIUmqdUiqglGpLmOryPa7eUEpNVEpppZQ732MRBCH7iKARBCEVTtFaVyRM9el8eCCIimT/hnT/XQPhOAhCoSKCRhCEPqOUOlUptVQp1aSUelUpNTNh2Tql1A+VUkuAdqWUWylVp5R6XCm1Qym1Vil1ecL6LqXUdUqp1UqpVqXUQqXUuNiyO5RSG5VSLbH5n0343EFKqQWxZduUUrfGFr0e+9sUsyodmmT8llLqmtg+dymlHlFK1cSWxS08FyqlNgAvJ5vXl+OQqeMvCEIHImgEQegTSqnpwN+AK4Fa4F/AU0opb8JqXwFOBqoBB3gKWAyMAY4GrlRKHR9b96rY+icBVcAFgD+27H1gX6AG+CvwqFKqJLbsDuAOrXUVMAV4JDb/iNjf6phV6e0k/4zLgdOBI4E6oBG4q9M6RwIzgeOTzUv3OGito0nGIQhCP1HSy0kQhJ5QSq0DhgPxG/GrWuvTlVI/BvbRWn85tp4FbAS+qrV+Nfa5G7TWf4gtPxh4VGs9PmHb1wLTtdbnK6U+BX6gtX4yhTE1AkdprRcrpV4HXgHu1FrvTFhnIrAW8HQnIpRSy4Fva61fir0fDWwASoGxsc9P0Vqv6bTNxHlpHQdBELKDWGgEQUiF07XW1bHp9Ni8OmB9fAWttYO5kY9J+NzGhNcTgLqYW6ZJKdUEXAeMjC0fB6xOtnOl1NVKqeVKqebY54ZgRBbAhcB04BOl1PtKqS+k8e+aADyRMJ7lgJ0wps7/hmTz0j0OgiBkAfHlCoLQV+qBfeJvlFIKI0o2J6yTaALeCKzVWk/rZnsbMS6jjxNnxuJlfohxUS3VWjsxC40C0FqvBL4Ss4ycATymlBrWad/dsRG4QGv9ZucFMWtM539Dsn9XusdBEIQsIBYaQRD6yiPAyUqpo5VSHuBqIAS81c367wEtsQDZ0lgQ8N5KqQNjy+8Dfq6UmqYMc2LCpBLj7toBuJVSP8HE2ACglDpXKVUbs4w0xWbbsfUdYHIP/4bfATcqpSbEtlWrlDoty8dBEIQsIIJGEIQ+obX+FDgXuBPYCZyCSe8Od7O+HVtnX0wcyk6MiBkSW+VWjDj4N9AC3I+JZXkeeBZYgXHtBNnThXMCsFQp1YYJED5Hax3UWvuBG4E3Yy6lQ5IM6w5gPvBvpVQr8A5wcDaPgyAI2UGCggVBEARBKHrEQiMIgiAIQtEjgkYQBEEQhKJHBI0gCIIgCEWPCBpBEARBEIqeAV+HZvjw4XrixIn5HoYgCIIgCBlg4cKFO7XWtZ3nD3hBM3HiRBYsWJDvYQiCIAiCkAGUUuuTzReXkyAIgiAIRY8IGkEQBEEQih4RNIIgCIIgFD0iaARBEARBKHpE0AiCIAiCUPSIoBEEQRAEoegRQSMIgiAIQtEjgkYQBEEQhKJHBI0gCIIgCEVBKGp3u0wEjSAIQhHT0wVeEAYSbaEo7SERNIIgCAOOtlCUJn8Efzia76EIQlYxYqbn81wEjSAIQhESCNu7L/CtwSjBiFhqhIFJKmIGRNAIgiAUHcGITUswsse8lkCEcNTJ04gEITukKmZABI0gCEJREbEdWgKRLvM10BQIE7VF1AgDg3TEDIigEQRBKBqitkOjP4zuZrnW0OiPYDvdrSEIxUG6YgZE0AiCIBQFjqNpCkTQvWgVR2ua/GEcETVCkdIXMQMiaARBEAoerY2YSdXyEnU0zYEIujf1IwgFRk9iJhx1+N6ji7v9rAgaQRCEAqc5ECGSJDamNRjhnx9sTlqLJmw7tAQknVsoHnoTM9f8Ywkvf7K928+LoBEEQShgmgMRQkmyl0JRm+8/uoSbnv2Enzy5NKn1Jhjtmg0lCIVIKmLmzVW7+NHJM7vdhggaQRCEAqUtlLy+jKM1189fxgcbmzhm5ghe/XQHt76wIqmLKbFejSAUIqmKmWtO3Isz9x/b7Xbc2RqgIAiC0Hd6EiJ3vLiSlz7ZzuVHT+WrB09g1JCV/OWdDdRW+vjvwyZ2Wb8tFMVlKUo8riyPWhDSIx0x88X9xvS4LRE0QkZxHE1LMILbZVHudaGUyveQBKHoCPXgKvrruxt4+P2NnH3gOP7roPEAXPa5qexsDXP3q6uprfBx8pzRXT7XEoigFPjcImqEwqA3MfPDx5fw1urUxAyIoBEySNR2dmdihKIOgbBNhc9NqVcuoIKQKhHbodmfXMy8sGwbd7y0ks/vNYIrj5m2+4HBUor/+cJMGvxhbnxmOTXlXg6dMmyPz2qg2R9haLnC45JoAyG/ZFrMgMTQCBkiHHVo8If3CEx0tLHW7GoLSUl2QUgB29HdFs5buL6R659ayr7jqvnZqbOwOlk/PS6LX5yxD1NHVHDtPz5iWX1Ll21ooLHT71QQck02xAyIoBEyQCBs0+QPd1vwKxq7SDfJhTRrBCM2De1hWoIRKahWpDhxMZPk61u9vY0fPLaEMdWl3PKlOd26jcp9bm47ey5Dyz1c9ciHbGzwd1nHVBOWwntCfkhVzFybppgBETQZx7YdQoEg/uZWoq1t9FrWs8hpDUZoCUa6LcWeSCjqsKstRGtQCn5lAsfRtIei7GgN7a5TEgjb7GwPSeflIqOnwnnbWoJc8fcPKfFY3HHOfgwp9fS4rWEVPu44ez+0hise/pBdbaEu69i7qw7L71DIHT2JmVDU3kPMnJ6mmAFQA/2Enjdvnl6wYEFGt+nYDtFIFDscIRoO4wRC2MEgdjCICsdv7hqlNeXDhlI6rg7cAytcSWvdbX2MVFAKKnxuyrwD67jkgqjt0B62CUXsHoWk12VRWeLGLfESBU+TP5z0t9QajHDpgwvZ0hzknq8dwPSRlSlv8+PNzVz210VMHFbO3efun/S35nNbVJd5+zV2QUiF3sTMNY9/1KuYUQqGlnnxul0LtdbzuiwvNkGjlDoBuANwAfdprX/R0/p9FTRa6wTREsEOhHCCAaKBmGjRGhQo7aAtF7hd4HKDq5MpOBDA61JUTpqAVVGe9jgKEdsxvWKiGTBZuyxFZYlbMi9SIBixCYRtwml0U1ZAmc8tGWcFTEswQiCcpNJv1OGKhz9gyaZmbj97Xw6cVNNlHaWgqsRDSyC5lfSNVTv5waNLOHDSUH591tyk4rbE4+rV6iMI/SGTYsbjslBKFb+gUUq5gBXAscAm4H3gK1rrZd19pidBo7XGjkQ7hEsojBMIEPUHUOEw2omJFjQaZawsLpeZ0rk5RKOoYICK0SMpGTUCrOJ9Yo7YDk3+CE6Gzxuf26LCJ9aEzmitCURs/GG7X/FHIhwLk/ZQlLYkF3pHa37y5FJeWLaN60+dzQl7j+qyjgKqy7x43RahqE1TN5lRTy2u5/89s5yT9hnFT74wK6mwLfe5qfCJtVTIPJkWM+Z9ckFTbGfwQcAqrfUaAKXUw8BpQLeCBsCO2kTDYSNewhEcf4CoPwjBADga3Vm0WBa6tHS3aOn3rdvtRpdX0LZtB5HWNiomjEWVlPR3qzknGLG7fRKMs6HBz43PLGevUZVc8JlJKT/5haIOoWiYUq+LCq8byxrc1gTb0bSHowTDPbuV0tlekz9CiduhosSNa5Af30IgGLGTihmA37y8iheWbeOyz01JKmYAqko9eN3mAu9zu6gqIWntmlPm1rGjNcQ9r69heIWPyz43tcs67aEolkJcwEJG6T1m5iPeXr2L607ai9P2TU3M9ESxnb1jgI0J7zcBB3deSSl1CXAJwNjRo2n8cClaqZiLSKNjooWSkt3WkqzbqZRCl5UTDIeILF9F5YQxeIZWp2fpySPdPUkmsnB9I9c8vgRba5ZsauJfH2/h4s9M5oz9x6RseQmEbYKRWP0az+Bzk4Sixq3U19ik3ghGbUJtNhUlEr+UT0JR83CQjIff28BD727gSweM5WuHTEi6TlWJp0vV31KvC0frpL/T8w+fyI7WEA+8vZ7aCh9fPnBcl3Vag1EsJdWEhcyQazEDxSdokt3dumgRrfW9wL0A+86arXVlRfcr5xqvD9vt0LRmA6XD26gYV9c17qaA0FrTEkzeTyaR+Yvr+cWznzBuaCm3fnlf/JEod7y4kl+/sILHF23iO0dP4/Apw1ISKFqbi6s/VphvoF9gtdYEIw7+cDQjcUm97g9zfANhm8qSjqd8ITfEC+cl+6ZfWr6N219cyVHTa7nq2Onduoe6K1ZZ7nNja90lJkcpxfeOn8Gu9jC3vrCCYRVejp45ssvnWwIRLKXknBD6RT7EDBRf2vYmIPHRYixQn6ex9B3LgspKAk0tNC5bQbStPd8jSoqpixHpUczYjubOl1dy4zPLmTdhKPefdyBjhpYybUQld35lP375pTnYWnP1I4u54uEPWb29LeX9247JpGpsDxNJIxC2WLAd8zS9oy1ESzDSZzETtR3+vXQrlz20iBueWpY0TTfp52J1T6R2Te6Iu/6SHe0PNjTys/nL2GfsEK4/bXZSt2CJx9VrrEtViQdfEkHishQ3nDabfcYO4afzl7JofWOXdTTQFAgTHYC/NyE35EvMQPEFBbsxQcFHA5sxQcH/pbVe2t1n9p01W7/wxFM5GiG072pi1/KVBFaupXzubMbuP7vnD0QiqGCQ8jGjKR1VWzAuqKjt0NhL8K8/HOWn85fy+oqdfOmAsXz32Gm4kwQ8R2yHxxZu4v431tIeinL6vmO45IjJDC1PL120xOOi0lf88TXhWFuIULR/8TFN/jD//KCexxZtYkdriLrqEna0hvC6Lb555BTO2H9syrEy8WyZgW4NyyeOo7tU046zZkcblzy4kJoyL7//+jyGlHWNPfO6rJR/M1qbh5FkDwLNgQiXPLCAnW1h7vnaAUwdUdFlHUspasq9EmtVgGitidjmHPK4VEG55XMlZgZElhOAUuok4HZM2vYftNY39rR+NgRNqM3Pzk9W075iNfaqNXg2rGPI5vWM2L6Z4e0dTz1Bt5env/0zDr/ky7h7ujBojWpvx1NRTuWEsVglvoyON11CUZvmQKTHmoDbWoJc/chiVu9o46pjp3PWvK4++c40+yPc98YaHl+4mRKvxfmHT+LseePSMm8rjFm9rAjTkIOxbKX+WptWb2/j7ws28tzHWwlFHQ6aVMPZB47jsCnD2NQQ4JfPf8p76xqYObqSH56wFzNHV6W8baldkx16EhjbW4Nc9OcFRG3NfefNo666tMs6bssIjHTO+Z4E1NZms08U3H/ePEZWdU1ScFmKmjJv0T9AFDsR2yFqa8K2Q9R2ulhyLaXwuBRul4XbMn268iFEexUzj33E22t28aOTZnLqvnVJ10vVMjNgBE269FXQRMMRGlauo/nT1URWrsa9fh0VmzcwYttGRjZtx0p4tt5ZPpRtI8fSWjee8PiJuKZOpnz8GKr+9/8xcc1S/nDKpez3o8sZX92LUAmFUNEoFRPHUVJTnfaYM4E/HKU12HPw77L6Fr736GKCUZsbT9+nSxO83li7s53/e2klb63exZjqUr7z+akcNaM2rYu1pUwacqFbFBynI+26P6nujta8uWonf39/I++va8Tntjhx71GcfeA4Jtfu+YSttebfy7Zxx4srafSHOXP/sXzjyClUlKQWMie1azJPd4Xz2oJRLv3LQuqbAvzu3AOYMapr4TxLKYaV901Y2I6moT2c9Nxbtb2NSx5cwIjKEu752gFJMxI9LouhZR45D3KE7WgithObNFHb6ZMVVynwWBZulxE4bktl9SEll2LGrCuCpgvacWhcX0/T8pWEVq5GrV1L+aYNDNu6gdENW/E6HV9Qq6+MLbVjaawbT2jcRKwpkymbPpmamdMoH9aN+AgGCVx2OePffIm/zPsC/ut+zOkzewmMtW3w+ympHUbFmFGoHFYY7q7AVyIvLd/G9U8to6bcy61fntvlZpoO76zZxe0vrmTtznb2H1/NFcdMY69RqVsTwFxwK0vcBdc9ONVqvr3RForyzJItPLJgI5saA4yo9HHWvLGcNndMUrfEHp8NRvnda6t5bOEmasq9XHnMNI6dNTLlm5OlFFWlUrumv3T3u4rYDt/9+4cs2tDErV+eyyGTuz4YKAU1Zd5+3Yyitmkcm+xSv3B9I1c8/AGzRlfxf1/ZL+kDQonb1eu5JqSP48SsLo4mEnWIOE5WO+UoMFYclzICx7Iy4rLKtZgx6w9iQfP43X+gYdlKgitWw9q1lGxcT82WDYzeuZmySEcAZdDtpX5YHQ2jxxMYNwE9eTKlUyczdNY0qkbXovpSEM9x4KabqH3wD7ww9SD+/s3r+f7nJ1NT2rNQUe3tKI+bqskT8JSXpb/fNNDaBCr2VIFWa80f31zHPa+vYc7YIdxy5py0Y2CSEXUcnvygnntfX0NzIMLJc0bzzaOmMLwiPbdbPFgyF6ZWrTVamwBKrXXsL2jM/HSr+SZjU6OfRxZs4qnF9fjDNnPGDuHseeM4akZt2je35Vta+MWzn/DJ1lYOmljD90+Ywfia1M+pErdLatf0kZ4K510/fxnPLd3KT74wi5PnjO6yTmLhvP4SjjqmgWySZS8t38aPnviYI6bXctMZ+yT9nku9LqpKRNT0lXjcS6L7KNPFSfuKy1K7rTlul3mdqjUwH2LGfGaQCpr9LJf+QHfcXKLKYkvNKHaOHEf72InYEyfgmzaV6plTGTppLFaWUqh9Dz1IxY03sHTEZK766vV884TZfHZcL31ZwmEIhigdV0fFyOFZCRi2Y5kuPVWhDUVt/veZT3hu6VZO3HsU1500s8eLrKXU7ht9qrQGI/zxzXX8/f2NeFwW5x02ga8cND4tl1LcVVLitrqIDDq97yxGiC3rWK+rWIn9lzW01ixc38jD72/kjZU7sSzFsTNHcvaB45hVl57lqjO2o/nHok3c/dpqIlHN1w+dwNcPm5Cy9SUeu1Qu1WRTJhgxsWjJuOuVVTzw9nq+ceRkzj98UtJ1hpRmNki7p/H8/f2N3PrCCs7cfwzfP35G0qf2Cvn+UybRbRSxnX5V+c4HqcTlpCJm3lmzi+syLGbM5wapoNmrslrfc+Z/45k6hcq9pjJs6kQ8pfkJuvW+8jIV372cHaVVnHvGT5lz+ByuPHAkpZ4evkzHQfn9uCoqqJwwBncGA4bDUYemQHJTdJyG9jA/fHwJSzY1880jp3DeYRN6NFF6XRbVZZ7dKcnpFojb2ODnNy+v4tUVOxhVVcK3PjeF49JwkxQjwYjNv5du4+/vb2TVjjaqSz18cf8xnLn/WGorM3uu7mwLcfuLK3lh2TbGDi3l+8fPSOrq6A63paR2TQqEona3tWYeXbCRX/17BWfsN4YfnJBcPFRmqfBhTzFyv3l5FQ++k1uRNRCIB+qGY9aXvsa9FDpKgTtmyQG6DU8IRW1+8NgS3l3TwHUnz+TUuZkVM+azg1TQ5DptuzfcH39E1TcuJuwPcMGp17F5n3nccEQds2u7ZjbsQSCAcjTlk8ZTOrR/T+uQWhuD1dvbuPrRxTS0h/npKbOSFuJKJJlZOhS1aQumXzBu4fpG7nhxJZ9ua2XvMVVcecx09hkzJK1tFDrbW4M8vnAzT3ywmeZAhGkjKjj7wHEcN3tk1uNW3l27i1ue+5RNjQGOnTWSK4+ZlpabT1pUdE/EdmhsT+7eefXT7Vzz+Ed8Ztpwbj5zTlL3TpnXRWUW3TutwQj+JDcjR2uuf2oZz328lf85eSanJLkRKWBImWfQx1VFbAd/yCZk21mNeyk2ciFmzOdF0BQM1ubNDLn0Qqx1a/n5qVfywPSjuHDucM6fO7zn9O5o1HTvHj6MyrGjsfp4UenugpbI26t3cd0TH1HmdfGrs+b2mvrb2xNlIGzTGuo5FbwztqP510dbuPvV1exqD3P87JF866ipjBpSfH2wElla38zD723kpU+24ziaI6bXcvaB49h/fHW/LFEuS+E4qbv6QlGbB99ez5/fWo/HrfjGEVM484D0atdU+jzdVq0djPSUVbR4YxPf+dsHTBtZwV3/tX/yANwcdb5uDiQvmBmxHa5+ZDEL1jXyy7PmcPjU4V3WUcDQ8r7fjIqZqO3QHrIJRnu+fmZ1DI7D2p3taA2ThpcXzPeQKzFjtiGCpqBQLS1UXf4tvO+8zfxTLuDymV9k7xGl3HDEGMZV9RBsqzXK3472+qicOI6SitSDO7U2lXd7cgNprXl0wSZue3EFU0dU8Kuz5iatUbH736GMGTqVJzYd6zMTSLPhYnsoyoNvr+ev720A4L8OHs/XD51QVL2IorbDy59s5+8LNvLx5hbKfS5OnVvHWQeMY8zQXqxzveB1WZR6XZR4XMbVF4ymdcHd0OA3tWvWNrDXqEquOVFq1/SFnuq+rNvZzsUPLGBImYf7vj6P6rKuv/G4uzZX7tXG9nDS4PX2UJRvPrSI9bva+e1X92d2XVfLaCayr4qJuAu9txYwmUZrTX1TkGVbWlhW38LS+mY+3dZKMGK+N7elmFxbzrSRlcwYWcn0kRVMG1GZcomGTJFLMWO2I4ImayjA0g5ux8FybFyYp+VwJErA4+u+V1M4TOWPr6PkySdYfcypfOmgCwkqN989aCSnT+/laT0cglAE37g6Kmtres3AMiXXwz26fqK2w60vrODxRZs5cnotPzt1Vo+iwVKKoWWetC9qfbnpgikGdtcrq/j3sm0Mr/DyzaOmcNI+o7EKOL6m2R/hiQ8389hCU8137NBSzp43jpPnjO5XgKUCSrwuyjyupMffND+MppxJobXmxeXbuf3FFexqC3PmAWP5xpGTU3Z9KEzHZ8sy54Wl1B6vB3qGVE+F83a2hbjozwsIRuzdrUE605fCef1Fa2NNSnZN2NUW4qIHFuAP2fz+vHlJs+IGQ+E9x9G0ZbDrfW80tIdZtqWF5fUtLI2JmHggt89tMWNUJTNHVzG7rgpLKVZsa2XFtlY+3dpKo78j4HtMdSnTRlbERE4l00dVUFvhy8r5lWsxY7YlgqZfKMBC43JsXI6DpR1cCtwul7lYezyme7fPB14vuN0QiRBev5FWlxenO8GhNWV33Un5b+6g7aBD+dYXr+X1JosjxlXwo8NH95ze7TgmvbuqiqoJY/D4klt2IrZDUy9tDFqDEa574mPeW9vA1w6ZwLc+N6VHoeBxWVSXevp1MQtHHVr70MPoo03N3PbiCpbWtzBjVCXfPWYa+40f2udxZIMu1Xwn1nD2Qaaab38EmKUUZV4XpR5Xr8dea0172MYfiqZ8MW4LRrnndVO7prrM1K7JVFC2Umb8rpjIUVbH+93LLIWlKPgg8MTUfUdr/N24IdpDUb75l0VsaPBz97n7J7V85bPNQE9WpQ0Nfi7+8wLKfC7u+/o8hiWJsRqoQeKOo/FH0vvtpIs/HOXTra0srTfCZdmWFrY0BwGwFEweXsGsuipmjq5kdt0QptSW9/jwuLMtFBM4bazY2sqK7a1sbAjsXl5d6mH6KGPFmR4TOuNryvp13uVDzJjtiaBJCQuMaNEObsc2xYgshdvtMiLF5+uY3O6OqbsLcFsbzsZNtLm8hHo41L4nHqfyx9dhT5zEA9/7Nb/Y4KLCY/Hjz4zmM72ld/v95ol94ngqOgUMpxL8u6nRz9WPLGZTY4BrTtwraTBgIiUeF1Ul7ozddIIRm9Zg6tYEMDeRF5Zt465XVrGtJcRRM2r5zuenMnZodmv2xIk6DqGIQyjqEIzYu/9ubQ7y+KJNvVbzTRePy6Is5lZKe6y2Q2swmlZtnOVbWrj5uU9YvqWVAycO5QfH78X4Ybk5thB7gLBilh7V6bXq9DrJBbm3WkGppPA7un+p+4nxKL/68hwOm5IkHqUAXDe2o9nVHkoa37a0vplvPbSICcPKufur+3drVXRZilKPOT+L2RqntcYftmkPRzMa7Bu1HVbtaNstXJbVt7B2ZztxHTl6SAmz66qYVVfFrNFVzBhVmRGXensoyqrtbbuFzqfbWlmzo213Lyif22LqiLjAMX+njqhI6TqTLzFjtimCZjcKjUsba4vbcbAUxtpiKVOZ1+frsLYkipa+FNYDaG2FzZsJektoi3ZfDdLz9ptUXX4ZuqSEpb/8LVdvrWZlY4gzZlT3nt4dCxh219ZSWTcCt8fdbVGvRD7Y0MgPH/8IjebmM+aw/4SeLR3ZqkXRF2sCGDH00LsbeODtddiO5uwDx3HWAeNwtN4tNDqLjlDUIRSxCcb+dlkecQhGk/+Nr9OTVWlEpY8vHTCW0/ftvZpvT8TdOGU+V0YuBsGITUsw9cBs29E88cFm7n51NaGozdcOmcB5h00suLRdRYdFJ936R9lCa80NTy/jXx9t5UfdXOwzWTivv/SUmfXW6p1875ElHDBxKLd+eW6v56LPbVHi6Zv4zhdamxYlbaH+CxmtNRsbA3uIlxXbWnfHLg4p9TCrrorZozsETCaKlKZK1HZYu6udldva+HSrcVmt3N62O53fUjC+powZoyr3iM1JjPvKp5gx2x2kgmb/WbP1fx59HBcal6VwgckO8nqNaCkp2VO0ZKmwXlzU2GXltPbQoNC1coXJgGpqYtevbuf2yr156OMGxlV5e0/v3h0wXIJ7TB1hV8/C4+kl9dz0r08YU13Kr788l3E9VI9VQFUOalD0NfhuR2uIu19bzb+WbEn7huZ1Wfg81u4LceJfX6f3vf2t8LmZM3ZIv564lYIyr5uyFNxK6RKPCeitxUUiu9pC3PHSSp5f2rfaNYORu19dzZ/eWsfFn53ERZ+dnHSdQiZ00bgAACAASURBVKvp0lPtnKeX1PPzp5dzwt6j+Okps1JymyoFpR7jHi3k4OFA2AiZvlbu3dkWMuIlFvfyyZYWWmLioMRjMWOkcRnNqjOxL6OHlBScS1VrzZbmoBE5sbicFdta2dbSUUl/RKWP6SMrmTaygqWbW3h/XX7EjNn2IBU08+bO1QtefbUjrsUVcx3lg5YWqK+HigoCUYf2bqwR1vbtVH3jItyfLKftxz/jzaNO56f/qWenP8qF+w7n/Dm9pHeHQqhwBKe0NBa4YHVYlyyFoxR3v7+NP3+4nQPHVHLTCVOoKvWYdZVCx39ssfeWZbIvPG7X7nmJy7u8zwCRmJsk3c7UK7a1smRTc8oixOvOT2faZLgtRZnXTYnHyvoFL2I7tATSi196f20Dtzz/KRsa/BwzcwRXHjM944X/BgL/WLSJm5/7lNP2rePaE/fKaeG8/hIIGyteMv745lp+99oavnbIBL79+alpbdfjsmIuqeyf26kSjFlk+lLFd/mWFh56dwOLNzaxvdXc9F3KZBzFhcusuiomDS/H3VfLfgHQ7I8YcbO9lRVbjetq/S4/jtZ5EzNm+4NV0MybpxcsWJDvYXTQ0gKbN0NlJVFtGtcl/UG1t1N11RX4XnsF/4UXs/XbV3PLe9t5bk0L+9SWcv0RdT2ndzuOaXQJiYEBBKIOP3lrO69u8nPGlAq+v38NbgUqfh7ErzXavHZZiqoSLy5j1+/YvtbdixcVC5IeOhTKy/slIPsSX1Ns+NwWZV53XlwP/nCUtmDqbr5w1OHBd9bzpzfX4XYpLj1iMl+aN7aoL9qZ5LUVO7jm8SUcOmUYt3xpTtLjku3Cef2lO1e11ppfPv8pjy/azFXHTufsA8elvW0F+GJWm3y52oIRm/ZQ+sU+AdbsaOOe19fw6qc7qCp1c/CkYUa8xOJeCsnili2CEZtgxE5aegCyL2bMPkTQFA7NzbBlC1RUoJWiPRQlkMzFEo1S8b8/p/SvfyF4wkm0/uKX/Ls+xC/e3krU0Vx18EhOm5Z6Mbbt7RGufmkTKxqCXHngSM6ZNbTnNgZui6qSPtTF0NrE9IRC5nVlJVRXQ9xilCa7A/WymHGQa+Lm+DJv/ps+9iWNfmODn1/9+1PeWdPAjJGV/PDEGUnrlQwmPtrUzGV/XcSU2gp++9X9kxYcLJbO1d11CLcdzXX/+IjXVuzg/52+N8fM6rl6eE/EA4lTydjLBKGoTXuoe3d/T2xuDPD7/6zhuY+3Uup18dWDx3POQeOpkN5We5ALMWP2I4KmsEgQNVjW7hTmLpYIrSn94/1U3HITkf32p/m397DFU8n1b9SzYIufI8ZV8D+fGc3QXgopLd8Z4OqXNuGPONx45BgOH9dz1k2px0VFJp4itTbCJhIxlpohQ4zA8aXvqnAcTWseiltlEpfVkXZdKKb3OKGosYalaoLXWvPyJ9u59QVTu+a42SOZOKycmnIvNeVehpZ7GRZ7PdCfXDfs8nPRAwuoLHHz+6/PoyZJkGeuC+f1l2Z/JKnIDUZsvvO3D1i+pYXbz96XeRNr+rUfhXl4ylYgcTjm3k8nyy/O9tYgf3xjHU8ursdtKc6aN5avHzIxb6LUbZmGkR6XQqGIOIXTPypXYsbsSwRN4dHUZERNZSVYlgnYDEUJJbmIeJ97lqofXIU9ejQt99xPZMJEHl7WwG8W7KDSZ/Hjw7tP7355XQs/eb2emlI3tx49lqk1PVT+BcqzVc7ecSAYNK4wnw9qaqCsLG2XVMR2aEszDTnfJFbzLWT6VLsmFOXe19fw7EdbdgdDdqbM62JomXe32Nkteso8u18PK/cxtNxDhS9zJQGyhdamGWEw7NDgD3PVIx8SCNv8/uvzkgbX56NwXn/pqVhgcyDCJQ8sYEdbiF+cMYcDJ/Zs7U0VSylKY4K/v5bLiG2ETLoNcgGa/GH+/PZ6Hl+4iaijOX3fOs4/fFJOY8YspfC6TDNIT1zE9HCM400yI7EmmRGn+4zaTJNLMWP2J4KmMGlshG3bjKiJnawmWK1req170UKGXPYN0Jrm395DdP8DWNUQ5Mev17OqMcSZM6q5IiG9W2vNnz7axW8X7mCf2lJ+efRYhvVQqE8pqCrJURppJGIsN2CsVH1wSfUnqC8X9FbNt5DpS+0aME/Djf4wDe1dp87zm7rJqPG6LIaWe2KCp6sIqkmYV1Xq6fbGF0/dD4RtghGHQMQmELEJhu2O18mWx+dHbIJhp8u68deJp12Jx+q2TUA+C+f1l54K721tDnLJgwvY1hJifE0Zp8wdzcn7jE5agK8vxB8CfO70Aon702+pLRjlr+9t4G/vbSAYsTlx79Fc9NlJ1FX3rz1JbygFHsvC47ZwW0bIZMIN5zg6wYpjXmf6eplrMWP2KYKmcEkiamxH0xrs+nRkrV/HkEsuwLVlCy23/JrwCScRth3uXrRjj/TuaTU+/vfNrTyzupnjJ1fx48NH4+tBqLgsxZDSPFx0O7ukqqvNcfCmVpchk/UjMkU61XwLnXRr16RD1HFo9kdoSBA6je2RrmLIH6axmxL9ljK1XIaWebAdvVuYxOsKpYNLKUq8FiVu125rWjzGo8Rr7S4c1zGv4/XeY6qSFk8shMJ5/aWnhpuBsM3Ln2xn/uJ6PtzYhEspDps6jFPn1nHY1GEZCRZXit3HvaebZn/6LQUjNo8u2MQD76yjJRDl83uN4JIjJjNpeHl/hp4UBbvdRsbykttMS601EVsTdRzz1zYipy8/8XyIGbNfETSFTUMDbN++h6gBCISjXYJhVWMDQy77Bp5FC2n7/g8JXHAxKMWCLe38LJbePX6Il7VNYS7dbzgXzh3e4xOOx2WCf/N+83UcCASMS6qkxLikystTqg3Ul/oqnVGx/ylMKf54wbZ4gpcyC7ssI2G5ZZFSo85iIhPHtr9orWkJRhOETyfB4w/jtjpSg0uSiI4Sj7XbnbFbmCQIl95M+ulSSIXz+kvUNq61nm4X63e189TiLTzz0RYa2sMMK/dy8pzRnDKnLmNVpt2WcUmVuDseFvrTbyliO/zzg8388c117GoPc+iUYXzjyMnsNSr15qypjNntsvZwHxUicVdV1EnNZZUvMWP2LYKm8OlG1ERtp2t6dyhE5Q+/R8lz/yLwla/S9qOfgNtNa8jm5ne28tqGVn58eB3HTe75h1niKdAU0kSXVFWVCSYuKenVJRW1nd1P5ipBnHR+30WsFFFsQ77oS+2awUyhFc7rL+GoQ5M/eTXhRKK2w1urdzF/cT1vrdqFrTX7jqvmlLmjOXqvkRmJz4tX0bYsYyVK94yMOg7PfbyV+/6zli3NQfYdV803j5rCvuOq+zUul6ViriOF2+o97qXQseMxOY6x5ERsjaN1XsUMiKDJ9zBSpxtRo7Xumt7tOJTf+kvK7ruX0JGfo+XWO4xFA4g6uufie5g2BqUFWNxrD7Q2gcTRqHFJDR1qYm5SdEkJmccfjhaUi68QKdTCef0lGLF3d39OhZ1tIf710RbmL65nY0OAMq+LY2eN5NS5dcyuq8r5zd7Rmlc+2c69r69h3S4/e42q5JtHTeHgSTVpj0UpYlaXmPvIykzcS6HjxNxT+YwJE0FTTOzcaaZOogZImt5d8vBfqbjhp0T3mknL3b/HGdlzbQgFVJZ6is81YttG3DiOCSCurk7ZJSVkFsfRtKZZu2awUOiF8/qLPxzd3fcnVbTWfLixiacWb+GlT7YRjDhMHl7OqfvWceLeo7ot0pYptNa8tXoX97y2hk+3tTJxWBnfOHIKR82o7ZOQKfe6KfMWXumFwYIImmJj507YtctYIzr9aJKld3tfe4Wq716OM2QIzffcjz19RtLNWkoxpNRT1EGKAITDZoK0XFJCZkm3ds1Ap1gK5/WXtpCJ7evrZ19Yto2nFteztL4Ft6U4Ynotp86t46BJNRl/8l+0vpG7X1vNkk3N1FWXcPFnJ3P87FFp70cBZb7s9FgT0kMETbGhdYeoSWKpga7p3e5lS6m69CJUwE/rrXcQPuwze9R4cbsshhRC8G8mEZdU3ulrp/SBRrEVzusvzYFIv4tcrt7exvzF9Tz78VaaAxFGVvk4eZ/RnDK3rt+p0su3tHD3q6t5d20DtRU+LvjMRE6ZW5d23IcCSr0uyr3ugXXtLGJE0BQjKYiazundVn09Qy69EPfKFWjLwhkxAmfUaKirwz1uLKquDsaMgbo6Mw0b1tG4stixbRNIbNtG1NTUGNeUkDMcxwQN2lqjtTk/Ha1xtBE+5r15PZCuPC5LUVPmHXQ3vCZ/uE+F6zoTjjr8Z+UO5i+u5901DWhg3oShnLpvHUfNqE3LPb5mRxv3vLaGV1fsYEiph/MOm8CZ+49NO0A7XkeqvADakwh7IoKmWImLmoYGI2q6wR+K4g+bJ2TV1orvuWexNm/CtWULnm1bcW2pN1WJQ6E9P+j1GmEzenSHyOkseqqqis+VEwyaTKnSUiPaysqK798wwHESxI75a17bjkZ3ep3Lq1RP6fuWSp66X5KByrbFiNYdtX/60iMpGVubgzy9pJ6nl2xhS3OQyhI3x88exalz65gxqvtr4KZGP7//z1qez0C/pRK3i4oSETKFigiaYkZrk/nU1NSjqOmc3q2AipKE1FGtjTCqrzfT5s0dr+PT1q0dXbrjlJd3FTlxERSfV6iWkHDYiDiPB4YP3907SyguEgWOE7f+xESQdtgdJJ+sXpClFIkp+4nrdVlHRG+fidodVZUzcVtxtGbBukbmL67ntU93ELYdZoys5JS5ozl+9iiqSk2s0vbWIH94Yx3zY/2WvjxvHF87ZEKfYpl8bosKn7v4YwwHOCJoip24qGlsNBaTblfTu/uXVJV60q8TYNumanFnoZM47djR9XNDh+5p3Tn6aDjqqMIRD9GoKdrndhtXVFWVZEcJQhbQWhOKOn2q1twdzYEI/166lfmL61mxrQ2f2+KoGbVUl3l5YtFmHK05fb8xnH/4RIb3ofWC12VRUeIu2KJ3wp6IoBkIpGipMavq7D1thkLGkpPMwlNfDxs3QlsbTJkCF1wAZ521uz5O3rFtI2yUMiJsyBBjvREEIeOYdhTGapOpTLhPtrYw/8N6nl+6DX84yon7jOaiz/St35LHZSwyA6Ga82BCBM1AQWtjQWlu7lXU5I1wGJ5+Gu6/Hz780FhDzjkHzj8fxo/P9+gMiZ2/hwwx4saXu066gjDYCEVNs89QNP3KvskIRmz8YZua8vQzGt2WotznHlCVnAcTaQsapdQZPW1Qa/2PDI2t835/BlwMxP0a12mt/xVbdi1wIWADl2utn+9tewNO0IARNVu3QmuriQkpVLSGhQuNsHnmGfP+uOPgoovgkEMKI0g3nvYdiZhjOWxY4cYDCcIAwHE0wajpXJ7rNhouS1EhQqbo6U7Q9BT+fUoPyzSQFUET4zat9a8SZyilZgHnALOBOuBFpdR0rfXgK1WqFIwaZV4XsqhRCubNM1N9Pfz5z/DQQ/DcczBrlhE2p51mCuLlc4ylpWYKBmH9ejOe2lrJjBKELGBZijKvaQ0RsTu6o2fTWWApFWv1IkJmIFNwLqeYhaYtiaC5FkBrfVPs/fPAz7TWb/e0vQFpoYnjOMZS09ZWuKKmM4EAPPGEsdp88omxiJx7Lnz96x0iLd9IZpQg5JR4IHEgbBPOUPo3mOeRCp+bUo+0KRhI9CuGRil1MsYysvtRWmt9Q0ZH2LGvnwH/DbQAC4CrtdaNSqnfAO9orf8SW+9+4Fmt9WNJtnEJcAnA+PHjD1i/fn02hloYOI6pL9PeXjyiBoyr5803jbB54QWTcXTKKXDhhbDffvkenSGeGeVyGeFVWblH5WVBEDKP7WiT/h229+hZlw7Sb2lg052g6fWxUyn1O+Bs4DuYkg1nARP6OZgXlVIfJ5lOA+4GpgD7AluAX8c/lmRTSc92rfW9Wut5Wut5tbW1/Rlq4WNZph5Mebmx1BQLSsFnPgN//CO88Qacd54RNl/4ghE2Tz5p4lryidttRExJiUlVX7PGFDnM97gEYQATj3OprfRRXeahxO1KevFPhgLKfW6Gl/so97lFzAwyerXQKKWWaK3nJPytAP6htT4u64NTaiLwtNZ6b3E59ULcUuP3F06KdLq0tsIjj8Af/gDr1hkX1HnnGZdUTU2+R2esSn6/OdaSGSUIOaO3QGLptzS46LOFBgjE/vqVUnVABJiUycElopQanfD2i8DHsdfzgXOUUj6l1CRgGvBetsZRdMQtNaWlxv1UjFRWGpfTf/4Df/oTTJsGN98MBx4I3/8+LF+e3/EpZcRiRYU5xuvWwaZNxi1VYLFogjCQiAcSD6vwUVPupdTr2l3ludTrYliFj8qB1nhXSJtUAgKeVkpVA78EFmHcPPdlcUy3KKX2je1nHXApgNZ6qVLqEWAZEAUuG5QZTj1hWaZS7+bNxpJQVpbvEfUNy4JjjzXTp5+aOJvHH4e//hUOP9xkRx19dP4q/cYzo8AED2/YYCw16WRGad0xdX7f0wTGQtTd5HIZ65GkngsDFI/LwuOyqPS5cTTSb0nYTVpZTkopH1CitW7O3pAyy6BxOSVi2yZN2u83N1eXy8SDuN3Fm4bc0AB/+5uJudmyBSZMMIX6zjmnMAoMRiIm7dvjMaJG665iI3EemPepfh+J6yrVMXV+b9smmLm01GRolZYW73cuCIKQhD5nOSmlvp5svtb6gQyNLasMSkED5gYYiZgpEDDiJhjsuDEWq8iJRuHZZ+G++2DBAuMCilchnpQ1T2jq2LY55okiI9mUbcJh8337fB2p58X0PQuCIHRDfwTNnQlvS4CjgUVa6y9ldojZYdAKmmQMNJGzeLERNk89ZYTO0UebGJzPfrY4xp8LEi1H8dRzqakjCEIRk7FeTkqpIcCDWutTMzW4bCKCphcSRU4waIJdi03kbNsGDz5opp07YeZMuPJKOOkkuXnHiTfltCypqSMIQlGTSUHjAZZorWdmanDZRARNHyhWkRMKmfo1d90Fq1YZYfPd78KJJ4qwieM4xjIHJhVeuo0LglBk9Mfl9BQdBewsYBbwqNb6hxkfZRYQQZMhkomcUMjcIAtN5Ng2zJ8Pt90Gq1eLsElGYk2d6mozSU0dQRCKgP4ImiMT3kaB9VrrTRkeX9YQQZNFehM5paX5S62G5MLmqqvghBNE2MSJdxuPRo0bqqYmv81CBUEQeqE/hfVO0lq/Fpve1FpvUkrdnIUxCsWGUuD1mkyjYcNg/HiYOtVkG40YYcRNW5sRFvnA5YIvfhFeeQXuvNOM5+KL4bjj4F//6kifHszEhWdlpRE269aZujp+vxQLFAShqEhF0BybZN6JmR6IMECIi5zqapg8GUaONCnEra35FTZnnAGvvrqnsDn+eJMCLsLGUFICVVXme9q4EdavN4JUhI0gCEVAt4JGKfVNpdRHwF5KqSUJ01rgo9wNUShaLMsEnU6aZNoyxIVNNJqf8cSFzSuvwP/9n7FIXHSRCJvO+HzGYqOUqTq9di20tMjxEQShoOk2hiaWnj0UuAm4JmFRq9a6IQdjywgSQ1NAOI554o93rC4tzW/qcDRqsqJuu83ctGfNMjE2xx8vMTaJRKMm5dvlMq7Fqqr8xkYJgjCoSTuGRmvdrLVeB9wBNGit12ut1wMRpdTB2RuqMGCxLHMznDjRWGyi0fxabNxuOPNM44r6v/8zN+2LLjJBw889J66WOG63sdj4fLBjB6xZ0yFKBUEQCoRUHkPvBtoS3rfH5glC34gLm0mTTDPNuLDJ1w0yUdjccYcJiL3wQmOpEWHTgctlWiiUlkJTkxE227cbV6IgCEKeSUXQKJ3gl9JaO6TWpVsQekYp8+Q/aRKMGWOCUfMtbL70pQ5h097eIWyef16ETRzLMg04KypMbM2aNaYZajCY75EJgjCISUXQrFFKXa6U8sSmK4A12R6YMIhQytwcJ02CsWONcGhtzd+Tf1zYvPYa3H67ETYXXGBcUSJsOlDKCJuqKuOuW7/etKHIVzabIAiDmlQEzTeAw4DNwCbgYODibA5KGKQoZWraTJhghA0YC0A+hc1ZZ3UIm7Y2ETbdEa9l09pqatnE2ysIgiDkiF4FjdZ6u9b6HK31CK31SK31fwETsz80YdCSKGzGjzfz8m2xiQub227bU9j8+98ibBIpKzO9oTZsEGuNIAg5JeXcVKXULKXUDUqplUhQsJAL4i6NCRNg3DgTu9HSYgrj5QO3G7785T2Fzfnni7DpTDwrqqVFrDWCIOSMHns5KaUmAF+JTVFgAjAvls5dFEgdmgFGIGBShwMBU5E4nw0Vo1F4/HGT8r1uHeyzD3z72yZzS+uOQnSOY97H5yUu6zwvvl6y9btblri9uXNhr73ycjiSEomY76qmxtSwkfo1giD0k7SbUyql3gKGAA8DD2utVyql1mqtJ2V3qJlFBM0AJRAwtVDa242wyWdDxbiwueMOExibb44/Hq68EubMyfdIDPHO3i6XqT9UWprvEQmCUMR0J2h6Sr/eAYwFRgK1wEpAbOpCYVBaatxQwaARNi0t+RM2bjecfbZpq/DOOybWRynjIlNqzyk+L1PLEt9Ho/DEE3D//XDiifD5zxthc8ABuT8micRjoiIRY8kaPtxYa6QasyAIGaQ3l9MQ4EyMy2kqUA0cr7V+LzfD6z9ioRkkBIOwa5cJHvZ4BrcVoKUF/vhHuPdeUwDviCOMsDm4AAp8a22sam63WGsEQegTabuckmxgBHA2RtyM01qPy+wQs4MImkFGorDx+fIbY5Nv2tvhgQfgd78zVqxDDzXC5vDDjdUkn4TD5rsaPtzE14i1RhCEFOm3oOm0sQmxvk4FjwiaQUo8eNjvN1YAjyffI8ofgQD85S9w990mlXrePCNsjjoqv8Im0VpTV5ffOChBEIqGtJtT9kSxiBlhEBOPsRk3rqOlwmCtiVJaChdfDG+9BTfeaNoUnHsufOEL+U03j1eItiwTW7NzZ0e2liAIQpqInVcYuMSDUSdNMvEa4bCpHTNYb5olJfDf/w1vvgm33AINDaaOzvHHwzPP5O+4eL2mbk1Dg8kSk55QgiD0ARE0wsBHqY7u3rW1xgXT3j54C+F5vfDVr8Lrr8Ottxq33CWXwDHHwJNP5seSFbfWKCXWGkEQ+kRagkYptShbAxGErGNZMHSoETZDhxprTSCQ71HlD4/HpJu/+ir85jdGQHzrW/C5z8Fjj5k08FyTaK3ZsEGsNYIgpEy6Fpo8p0YIQgZwu012zeTJxiXV0jK4b5xuN3zxi/DyyyYjyuuFK66AI4+Ev/0t9z204tYarY21pqFBrDWCIPRKuoLmmayMQhDygccDo0YZi43Xm9/O3oWAZcEpp5hA4fvvN5aS730PPvtZk/6d6x5aPp8Zw86dxlqTrx5egiAUBWkJGq31/2RrIIKQN3w+GDt2z87e+XC3FAqWZRpuPvusETK1tXDttXDYYUbo5NJNl8xaM1hjnwRB6JG8BAUrpc5SSi1VSjlKqXmdll2rlFqllPpUKXV8wvwTYvNWKaWuyf2ohQFPvLN3XZ0RNIM51RuMmDj6aHjqKeN6mjABfvITU6Dvd7/LbRdtn8+4B3fsEGuNIAhJyVeW08fAGcDriTOVUrOAc4DZwAnAb5VSLqWUC7gLOBGYBXwltq4gZBaljJtj4kTjjgqFBneqN5hjcsQR8I9/mGDh6dPh5z83rRR+8xtzfHKBZZnvxnGMtaaxUaw1giDspldBo5S6IpV56aC1Xq61/jTJotMwnb1DWuu1wCrgoNi0Smu9RmsdxnQAP60/YxCEHrEsGDLExNcMH26sEYM51TvOoYfCI4/AP/8Jc+fCTTcZYXPbbdDcnJsxxK0127cba81gjnuybRNjVF9vYsBCITlHhUFLKhaa85LM++8MjyPOGGBjwvtNsXndzReE7OJymV5DkycbgTPYU73jHHigaafw9NPm9a9+ZWJs/vnP3Ow/bq2xbVi7dnBaa9rbOyxVoRBs3WoKE65aJQJHGJS4u1uglPoK8F/AJKXU/IRFVcCu3jaslHoRGJVk0Y+01k9297Ek8zTJhVe3v1Kl1CXAJQDj44GegtAf3G4YMQKqq80TcUuLNL8E2G8/+NOf4OOP4Zpr4LLL4PnnTYuFmprs77+kxGSobd9uvpMRIwZ+B+9o1MQSNTebuC937DIePxe17nCVQkdgdXm5OV4eT/6bkwpCFuhW0ABvAVuA4cCvE+a3Akt627DW+pg+jGcTkNjFeyxQH3vd3fxk+74XuBdMc8o+jEMQkuP1mqDhmpqOm+hgb34JsPfexjpz112m+vC77xqrzec/n/19x601oZBxQVVVGTfhQPtOtDaB6tu2dVS/ToZSe4ptrU2dpdbWjuUicIQBSLcuJ631eq31q8AxwH+01q9hBM5Ysldgbz5wjlLKp5SaBEwD3gPeB6YppSYppbyYwOH5PWxHELJLSUlH80vHkYwoMJaCK64wfaGGDIGvfc1Ybdrbc7P/eN2a9nZYs2ZgFeQLhWDTJuNKKikxlplUiQucigozlZUZgbN1q3FZrVoFW7Z01GESF5VQpKQSQ/M6UKKUGgO8BJwP/Kk/O1VKfVEptQk4FHhGKfU8gNZ6KfAIsAx4DrhMa21rraPAt4HngeXAI7F1BSF/xJtfTpy4Z/PLcBgiEeMaiEaN0HEcMw2Gm8Xee5saNpdeauJsjjsO3n8/d/svKzPfy65dJr6mtbV4j7vjGGG2bp05p6qqTFxXfxCBIwxQlO7lZFVKLdJa76+U+g5QqrW+RSn1gdZ6v9wMsX/MmzdPL1iwIN/DEAYDjmPiGuKxC3EBE5/i83qzGmjdswugp+WWZVxgheJCePttuPJKY1n41rfgqqtyG3cUjZog7tJSE19TUpK7ffeXQMAIi2jUCLRcfadad4hyMOdUebm4qISCQSm1UGs9r/P8nmJoEj6r8kN+0wAAIABJREFUDgW+ClyYxucEYXARb345dGhq68dFTqLg6W5eqstDIfNEXyjC5tBD4cUX4frrTc2al16CO++EmTNzs3+3uyO+Zt06890MG9YRSFuIxFOxm5o6LCm5pLcYHBE4QoGSyq/6CuBa4Amt9VKl1GTglewOSxAGAfGbQCZvBpWVJhOrqcm4XFyu/AubykoTIHzccfD978NJJ5m/l17af/dJqvh8JqC7tdVY0WprTZyPla/aot0QD/oFI2QKQSgkEziBQIcrz+eDMWMGXhC2UHT06nIqdsTlJAxaIhFTo6SxsTCEDRiR9cMfmhibgw6C2283LRVyieOYQoluN4wcaWJI8n1cIhGTNdfaumcqdjEQDBphM26cEY2CkGW6czmlUim4Vin1S6XUv5RSL8en7AxTEISM4fGYuJHJk42VJF4UMJ8PMcOGwe9/b4TM8uVwzDHw0EO5HZNlGeuH2w0bN5rsoWAwd/tPRGtjMVq71oyhqqq4xAwYt5NlmaJ++TqOgkBqWU4PAZ8Ak4DrgXWYNGpBEIoBj8dYIiZNMnEPra35rXasFJx1lomn2W8/+MEP4LzzjIUil7jdRkBEIuZmvG1bbrusx+vmbN1qrDLFXBAw7tJbvz63TUsFIYFUBM0wrfX9QERr/ZrW+gLgkCyPSxCETOP1moabkyebG2i+hc2YMfDwwyZg+M03TRG+p5/O/ThKSozFprXV1K9pbMxu/RrHMUG/a9ea15WVhRfL0xc8HiPKNmzoCCAWhBySyq8olrvHFqXUyUqp/TDF9QRBKEa8XlM3Z+JEcwNqacmfq8Cy4KKL4LnnTAzGpZfCd76Tu0aXcZQyIq+szLQVWLcuOwUB/X6z7YYGI2QGWusMt9tYATdtyv13KAx6UhE0/08pNQS4GvgecB/w3ayOShCE7OPzmTYOEyea1/kUNtOmwfz5pk7Nk0/C0UfD66/nfhzx+BqXy8TXbNxoXEP9JRo1Lq0NG8y2CyWDKRu4XEasbdlihNsATzwRCgfJchIEwRAMGldIW1t+G29++CFcfjmsXg0XXADXXZe/+JJg0BSZq6kxU7oBu1qb47l1a0dtoMFCvPfUsGGmt9ZAFXBCzulPltOflVLVCe+HKqX+kOkBCoKQZ0pKYOxYk0btdhuLTSasE+my776mY/eFF8If/mDq13zwQe7HAeaYVFaaY7F2rXGjpBpfEw7v2X9pMIkZMAKmstJYabZuHTh9tYSCJRWX0xytdVP8jda6ESiKtgeCIPSB0lITzzJhgnEftLaam3Oux3DDDSZoOBCA004zxfni5fhzSTy+prS0o99RT5k8WpvA4nXrzHGrrMxdAcFCIy5qWluNC2qwN3AVskoqgsZSSu2u5a6UqkFaHwjCwKe0FMaPN+IGOpoV5pLPftakd3/xi3DbbXDKKbBiRW7HEMeyOsTJhg2weXNXC1YwaFKXt28v/lTsTFJRYYTppk25TY0XBhWpCJpfA28ppX6ulLoBeAu4JbvDEgShYCgrM9aauLBpbc2tpWTIELjjDrj3XnNDPOEE8zpfLgyPx9SvCQaNG2rHDiP04tlRMHBSsTNJWZkRMxs35sfSJgx4UgoKVkrNAj4PKOAlrfWybA8sU0hQsCBkEK2Nu2X7dmOdKC3NbQ+f7dtNH6gXXzSNL2+/3cT95Iv48bBtY7kphDYKhU68VcLYsQMvbV3ICX0OCo5RA7Rrre8EdiilJmV0dIIgFAdKmTojEyeaG5JtG4tNrtwII0bAn/5k4mmWLDHp3X//e/5Sg+PHo6rK/M21mFm1Cm6+2aS679iR2333lXirhA0bpFWCkFF6tdAopX4KzANmaK2nK6XqgEe11ofnYoD9RSw0gpBF4mnJO3YYN0JZWe4CYDdsgCuvhHffNW6om24ygmcwEAzCb34Dd921Z1zTjBlw+OFmOuQQ03m9UIlEjJVv7Fhz3ghCinRnoUlF0HyIyWpapLXeLzZvidZ6TlZGmmFE0AhCDnCcjnor8c7eucC2TbPLm282FZC/+11Tu2Ygd31+4w245hoTv3PGGfCjH5kMorfeMi0k3n3XCB6lYJ99OgTOQQcZK1IhEY0al92YMSbuSBBSoD+C5j2t9UFKqUVa6/2VUuXA2yJoBEHoQjhsKuL6/bmthrtmDfzsZyYjasoU0x/qc5/Lzb5zxa5dJpX9sceMy++mm+CII7quFwqZ4oRvvmmmhQuNNcTtNjV+4gLngAOM+yff2LYRw6NHF7ZFSSgY+iNovgdMA44FbgIuAP4ai6cpeETQCEKOiddh2b7d3DBzaS158UX46U9NttFxx5nXEyfmbv/ZQGsTJ/Tzn5v+Ut/6lul3laoVLBCA99/vEDiLFxuLms8H8+Z1CJy5c3Mb4J1I3MI3YgQMHSqB1UKP9FnQxD58LHAcJsvpea31C5kfYnYQQSMIeSIYNFVybTu32T+hENx/v8mAikQ6Gl4WmrslFVauNO6ld94xLqObb4bp0/u3zZYW45aKC5xlsaTV8nI4+OAOgTNrVm4LAsbjsWpqpFWC0D1tbXgqKz+MxEJgEulR0CilXBgBc0w2x5dNRNAIQh6xbdMfqrHRiJp0eyH1h61b4X//Fx5/HEaNgh//2FQcLoYbZTAId95pgn7Ly+F//gfOPjs7tW0aGjrib956y2ROgXH/HHpoh8CZNi37xy7e/2nIEBg5Umr5CB1oDU1NsGULvtmzl4S0ntt5lVRcTvOBr2mti7IXvAgaQSgA2ttN4KpSua+e+/77Rsx89JGxcvz857D33rkdQzq8/jpce61xm515JvzkJ8ZikSu2bu0QOG+8YYoZAtTWwmGHdQicCROyJ3Da2oyQGzVq8LaNEDpwHJNJ2dgIPh++yZP7LGgeAQ4BXgDa4/O11pdneszZQASNIBQI0agJGG5tNQHDuXz6tm3TF+oXvzBPeeeeawr01dTkbgy9sXOnCWb+xz9M3M8vfmFaP+SbDRs6rDdvvmm+QzCZSYcfbpqIZkMg+v0mpmfMmNxa9oTCIho1D0OBgLluhMP4Jk3qs6A5L9l8rfWfMzTcrCKCRhAKCK1NDMe2beZmletKsU1NcOutpjhfZaURNeeem98bpuMYsXXjjcaS9e1vm6kQMpA6ozWsXm0sN/EYnEgE7rsPjjwy8/sLBIzwHTs2fwHLQv4IhUzPNMfpqFXUH0EDoJTyAvFItE+11kXTiEMEjSD8//bOPD7K6vr/75tJyAKEJewgEhQXlhBklwpRJPL9qghUioCKICJaK1pFq0gtKkqhP7S8rKWUIloDBEEqlgoSXOJGBTQgS2QViPAVBIRA9sz9/XFmJgkkIQmzJTnv1+t5ZebZ7n2eO5PnM+ece04Qkpsrv7ry8gKTYTc9XdxQX3wBV18tbqi+ff3bB5BCm08+CV99JYnwZs6UWJXqwtGjMGaMBC/PnQtDhni/DS2VUDvJyhJ357k/fMoRNBe0+RpjEoDdwF+A14BdxphSkh8oiqJUkPBwqeTduLH/i10CXHUVLFsmRS5Pn4bbb4cHHpBfg/4gO1tmLCUmiqiZM0fyy1QnMQMyzXr5cujWTaaTv/mm99vQUgm1j1OnZLwjIiolYitabTvRWjvAWtsfuAl4uYrdVBRFEUJCJNj10kuLMsb6E2Pg5pvhk0/gt7+FDz4Qt8krr/j2wfnJJ3DjjWLRGDpUgoBHjqwes69Ko0EDWLwYbrhBgpn//Gfv19YKD5d8RgcO+P9zovgPayX498gRiZeppCu4IoImzFr7XVF7dhegzkxFUbxDZKQEwdatK9aSwkL/t//YY/Dxx5JdePZs+bt2rXcfzMeOSWzM6NEi5pYtE/EUE+O9NgJFZKTk/hk+HGbNkqzNTqd32wgLk3YOHhSrnlKzKCyUvFUnTkh8WxUmDVTkiE3GmH8YYxJcy9+BzZVuSVEUpSwcDkl937q1uGOys/3fh0sukbpQS5eKqXv8+KL4kIvB6YS33hLrz+rVYg1at05mCNUkwsLEOnPvvRIk/Mgj3nclhoaK8M3IkABvpWaQlydCNTtbxEwVrZUVETQPANuBh4HJwA5gUpVaUxRFKY/69SE2VtwLmZne/5VfEa67TtxPzz0H33wj7qHp08V6VFnS02HYMAn87dhRhMxjjwXnDCZvEBIi92rKFEloOGGC98WpwyGfk//7P/k17233luJfsrPFlWjtRVddL3OWkzGmrbX24EWdPQjQWU6KUg1xZwX98Uf/14MqzvHjEry7eLG4hp5+GkaMuLA5PDtb3Enz5snD9/e/l+Oqa5xMVXjjDakE3qsXvP66xNp4E3ephNBQqf9Uv75O7a5unD4tbqbIyIqPXRVnOf3L/cIYs6LSHS0HY8wIY8x2Y4zTGNOj2Pp2xphsY0yaa5lXbFt3Y8y3xpg9xpi5xtSm/wyKUsswRh5S7doVFS4MxC/xmBiJCfnPfyR4+be/lanJ33xT9jEffSQBsq++Kpl+U1PhV7+qXWIGYOxYeO01+PprmUV29Kh3z2+MiJg6dcRSs3evuKLOng2MZU+pONZKIsnDhyX410tCtDxBU/zb194rrRWxDRgOpJayba+1Nt61FHdt/RWYiFT+7gAM9nKfFEUJNiIiRNQ0bCguqIKCwPQjLg7+9S+JEfnhB7jlFhE3x44V7XP0qExdvvNO+Qf99tsyHTuYshH7myFDJInh/v3iejvoA6O/wyGuiuhoidnJyIB9++SBmZvr/faUi6OwUGYxHT9e5eDfsijvTLaM1xeNtXZn8ZlTF8IY0xKIttZ+acVH9iYw1Jt9UhQlSAkJkXwnl1wiwYOBmrYbEiKWhk8/FeHyzjsSb/O3v0n+lQED4P334fHHJVbm2msD089gIyEBkpPFhTh0KOzc6bu2wsPlIRkRIe3t3y/xGZmZ/p89p5xPfj4cOiRWtIsI/i2L8gRNV2PMaWNMJhDnen3aGJNpjKlCdFyFiTXGfGOM+cQY4y5k0hrIKLZPhmudoii1hbp1xVoTGRm4gGEQE/nUqbB+PfTsKcHDTz0FXbpASgo8+qhmtD2X7t1FABojbriNG33bXkhIkdXGWnFt7NsnVjRNzhcYcnJEXBYWynfZB5SZtcZae1ElTo0xKUCLUjZNtda+W8ZhR4C21trjxpjuwL+MMZ0o6f7ydLGctici7inatm1buY4rihK8hIZCq1YSTPh//yeunUDNGLrsMvjnPyVm5swZcUPVtjiZynDlleK2GzUK7rhDpsjfcIPv261TRxZrRQifPCnvGzeWB6sWvvQ9mZkiKiMifBq47bORtNbeWIVjcoFc1+vNxpi9SA2pDKBNsV3bAIfLOc98YD7ILKfK9kNRlCDGGJkxExkpvvgzZwJTD8rN9dcHpt3qyCWXiKgZMwbGjZOYpKF+ih4wRj4zUFT53Vqx4jRsKA9bFaTexVoRkEePynfUcVF2kgvivWgcL2CMaWqMcbhet0eCf/dZa48AmcaYPq7ZTXcDZVl5FEWpDdSpIw/IQNWDCgbcM8AyMyW2KFBB05WhSRMJmO7ZUzInL1rk/z6EhorrsF49uW8HD0q8zcmTtfNz5AucTrGiHj0q8TI+FjMQIEFjjBlmjMkA+gKrjTFrXZv6A1uNMVuA5cAka+0J17YHgAXAHmAv8L6fu60oSrBRvB5UYaE83GvLlN3sbHkYN20qhT4bNZJ7kJkpS3Z28N6L6GjJnpyYKPFIc+YEZlq+22rjzmHz008Sa+Oe/q1J+6pGQYEE/2Zmylj7yfJVZmK9moIm1lOUWoLTKVV6jx6VX+Bu90JNw13IMzpaxMy5MQkFBTJd+exZEXgFBfJgDguTYOVgcqsUFEhW4WXLxAX13HNencZbZXJzZUZdaKi4o9z5bpQLk5srghB88x0sJ7GeRkMpilIzCAkRK0W9evJL+/Rpnwch+hVrRaSEhoqrrayZIqGhRTWPmjWTB3NenvxadluwQkLkvoSFBVbghIbC//t/Mm5/+5u4fF55JfBjFh4ui9MpSft++kkezo0by+ypYBBdwciZM5KnyV0d3ReUM0tNBY2iKDWLsDApdNmggfjwc3Or/0MoJ0diO2Ji5KFamWtxz/KpV09EUV6enC8zUwQSiKipUycwQiIkBKZNk2t78UURovPnB4eFLSSkSDjm5clMHXdQeoMG5z+0z/V4FH9f0W2VOUdIiIjCkJDACtPipUp8Gfx78KBkoC4DFTSKotRMoqIkb82pU5LR1+EIjodkZSgsFPdSVJRUIr/Y/DbGFFkfGjQQC0Rurgic06dF5IA8JOvU8Usgp6dfv/61uHd+9zuZ1v3GG/I+WHALQ6dT7tXJk9JvaysmJryxX1nbjCkaM/cSFibj5158JXicTvl+nTzpk2R5HjZuhPHjyw18V0GjKErNpbq6oawtqlLdsqXvHhQhISLyIiOLgopzc0VEnT5d1AeHQ0SQr61cY8aIiHnoIUnAt3gxNG/u2zYriztpXzBhrQiL/HwRp6VlRS4ueMLD5X1xwVOVsS0okNQJ2dkS0+Ur3n4bnnhCRP3f/w43lp4VRgWNoig1n3PdUDk5YhoPRjeU2yXUqJHM4PKXlQSK6iJFRUnb+fkicM6ckaWwUIRVWJg8GH0hsm6+WR6O994rOWqWLBFLm1I2xhQJk7IoLCwKGD91qsh95f4bGloUOH6uhcft1ipObq7Eyzid8oPBFzidUu3+1VehXz+Jsyony7DOclIUpXbhng0VbG4op1NiWsLDoUWLwGVALgtriywA7rw31vrOYpGWJoU+Q0MhKQk6dfJ+G0oRTqeInsJCee10lnRxuQPJ3WLnxIkiAeQLzp6Fhx+GNWvEcjdjhrSns5wURVFcnOuGOnVKxEMgp+VmZ8uDpHlzsSIF09RqN+7A4Tp1imok5ebKgy0zU9xi3iQ+XrIK33GHFAVdtAh69/ZuG0oRISFFoqU0rJXPaE6OiNmICN+VjfjhB5nGv3MnTJ8u1roKfCeC0N6qKIriB9xuqEsvLarz4+9EdPn5EqsSFQWxsRI/EoxipjSMkYday5bS78xM7yeiu/xyePddybczerQU/1QCgzvwODxcrJq+EjPffCN10Q4ckMDwCRMq/J1QQaMoSu0mMlJETbNm8sszK8v3bVpbFJNyySUiCoI9ULksjJF75y5B4W1R07o1rFwpxS3Hj4fly717fiV4ePddscZFRMCqVZUuXqqCRlEUxe2Gio2VoMPTpyU41xfk5IiYiYmRYNdyghyrDcaIFaVZM99YumJiJJtwnz4weTIsWODd8yuBxVpJsPjggxAXB//+twjYSqKCRlEUxY0v3VDuOkthYSJkYmKCc5bVxdC4sdy/zMzSpw5fDPXqwZtvwv/+Lzz7LMyerbWWagLZ2SJk5syBESNg6VL5blSBGvZtUhRF8QKRkSI6mje/eDeUtXJ8bi60agVt2vhuZkgw0KCBXOPZs96v/h0RAfPmSTzNK6/Ao49KrIVSPfnxR3ExvfeeFCl9+eWL+m7oLCdFUZTSMEaCXevWrfpsKHdOmcaN5VenP3PKBJL69SU2KCPD+4kMHQ6YNUvu51/+IknXfvELETmDB9dssViT2LYN7rlHvlcLFsjYXSRqoVEURSmPqrihnM6iMgLt2klsSW0RM27q1oW2bYsqV3sTY6REwn//C48/Dt9/L26LHj1kmu+ePd5tT/Eua9ZI0kRjJODbC2IGNLGeoihKxbFWflEePVp2Qjl3TplmzYI3p4w/yc2FQ4eKyif4AqcTUlMlAd8HH4irq1cvsdrcckvwJE+s7VgrVrWXXoJu3WDhQvmeVIZyEuupoFEURaksBQXihvr55yI3VH6+iJkGDaRsQHWdhu0L8vLE/WSt78XFsWPihlq8GPbvlySAw4eLuNFsw4EjN1fqMS1fDrfdJrOaqvJZUEGjgkZRFB+QnS2BjTk5ImpatAi+woXBQn6+ZIAtKPDPPbIWNmwQYbN6tTxQu3YVYTN0qO/qDynnc/y4ZPvduFFchI88UnXLpQoaFTSKovgId2XsiIiaNw3b2xQUiKjJz/ev8Dt5Et55R8RNerq0fdttIm66dVO3oC9JT5fg32PHZBbTkCEXd75yBI1++xRFUS4GY+QBqWLmwoSGypTuiAiZ1u0vGjUSC0FKimSgHTJE6kTdeisMGiSxHD//7L/+1BbWrxfhmJcHK1ZcvJi5APoNVBRFUfyHwyH5eOrVK5oJ5i+Mge7dJX7jm29g5kyJdZo2Tdb/5jfipqrhngufYy3Mny+WmXbtJPNvfLzPm1VBoyiKoviXkBCJN2rYUMpMBEJA1K8Pd90F778Pa9fCr34F69bBL38JAwZIAr/jx/3fr+pOfj48+aRMnx88WKZlt2rll6ZV0CiKoij+x13UskkT3xS1rAydO8tU4q+/lhT8jRrB88+L1eb++2VKuL8rsVdHTp6UuKSkJLF2/e1vfo2V0kzBiqIoSmAwRgRNSIjMFqtfP7CxSFFRMHKkLN99J0HEy5eLy6RtW7jjDtnWokXg+his7NkDY8fC4cMwd65YuvxMrZzllJ+fT0ZGBjk5OQHqlVJbiYiIoE2bNoRpjhJFKcmpU3DkiGQYDqasyjk5ktk2KQm++EL6NnCgCJuWLcVyU9ZibfnbL2afunWlvESbNtC6dWCTB6amwqRJEvT9j39Az56+a0unbZcUNPv376d+/frExMRgdLqe4iestRw/fpzMzExiY2MD3R1FCT4yM2Vad1SUPByDjf37YckSWLZMpiEHE02birhxi5xzX/vK9fPGGxJU3aEDLFokbfqScgRNEH5ifE9OTg7t2rVTMaP4FWMMMTExHAu2f4SKEizUry+unYwMKZMQbJbM2Fh4+mmYMkVmQ2Vni4vM4ZC/xsjfCy3u/co7zpii7aXtc+qU3KeMDCkt4X69dasEOufnl+x7TIyIjdat5W/x123aVD7RYEGBBP4uXCgWq9de832ywsJCyMmhEEoNaKqVggZQMaMEBP3cKcoFiIoSUXPokLhaKlPd3F+EhcF11wW2D/XqiSDp3fv8bU6n1BsrLnTcr3fulHw8ubklj2nUqHwLT3R00b6nT8MDD8DHH0vQ9NSpvncT5ueL+691awqhsLRdaq2gURRFUYKUiIgiS01OjrxXKo57WnyLFqXHszidUovsXMHzww+wezd8+KHc9+I0aFAkcnbtgoMH4U9/glGjfH892dkibi+9tNzPggqaAJGRkcGvf/1rduzYgdPp5JZbbmH27NnU8eGvkRdffJGnn37aZ+d3U69ePc6cOeP18yYkJPCnP/2JHj16lFj/6aefMmnSJMLCwvjyyy+JDHBl3Z9//pnFixfz4IMPBrQfilKtCQ+Xh2dGhjzQtGK29wgJkSnzzZrJ1PRzsRZOnBCR4xY67tfffy/uryVL4Nprfd/Xs2fFSteq1QVdkJqHJgBYaxk+fDhDhw5l9+7d7Nq1izNnzjB16lSftvviiy/69PyBIikpiccff5y0tLQKiZnCwlKtlV7j559/5rXXXvNpG4pSK6hTRyw1DoeIGsU/GCMxN/HxUh5i0iSYMQPefFPKGaSm+l7MWCtB4vXqibCtQDxVrbfQTH9vOzsOn/bqOTu2iubZW8suU//hhx8SERHBuHHjAHA4HLz88svExsYyffp0li1bxqpVq8jKymLv3r0MGzaMWbNmAfDBBx/w7LPPkpuby2WXXcbrr79OvXMCsY4cOcLIkSM5ffo0BQUF/PWvf2X16tVkZ2cTHx9Pp06dSEpK4q233mLu3Lnk5eXRu3dvXnvtNRwOB/Xq1eP+++/no48+olGjRixdupSmTZuedx1Dhw7l0KFD5OTkMHnyZCZOnOjZNnXqVP79738TGRnJu+++S/PmzTl27BiTJk3i4MGDALzyyiv069ePr776ikceeYTs7GwiIyN5/fXXufLKK8nOzmbcuHHs2LGDq6++muxS/qEtWLCAZcuWsXbtWlJSUnjrrbd44okneP/99zHG8MwzzzBy5Eg+/vhjpk+fTsuWLUlLS2PHjh1lXv+aNWt4+umnKSwspEmTJqxfv77MPm7fvp1x48aRl5eH0+lkxYoVTJs2jb179xIfH8+gQYOYPXt25T9EiqIIoaHyQDt8WH6t160b6B4pvqawEM6ckZlbMTEVLh4aEAuNMWa2MSbdGLPVGLPSGNOw2LanjDF7jDHfGWNuKrZ+sGvdHmPM7wLRb2+xfft2up9j5ouOjqZt27bs2bMHgLS0NJKTk/n2229JTk7m0KFD/PTTT7zwwgukpKTw9ddf06NHD+bMmXPe+RcvXsxNN91EWloaW7ZsIT4+npkzZxIZGUlaWhpJSUns3LmT5ORkPv/8c9LS0nA4HCQlJQFw9uxZrrnmGr7++msGDBjA9OnTS72OhQsXsnnzZjZt2sTcuXM57koTfvbsWfr06cOWLVvo378/f//73wGYPHkyjz76KBs3bmTFihVMmDABgKuuuorU1FS++eYbnnvuOY9b7K9//StRUVFs3bqVqVOnsnnz5vP6MGHCBIYMGcLs2bNJSkrinXfe8Vx3SkoKU6ZM4ciRIwB89dVXzJgxgx07dpR5/ceOHeO+++5jxYoVbNmyhbfffrvcPs6bN4/JkyeTlpbGpk2baNOmDTNnzuSyyy4jLS1NxYyieAOHQwJgo6LkQafUXPLzIStLxrtJk0pVQg+UhWYd8JS1tsAY80fgKeBJY0xH4A6gE9AKSDHGXOE65i/AICAD2GiMWWWt3XGxHSnPkuIrrLWlznYpvn7gwIE0aNAAgI4dO3LgwAF+/vlnduzYQb9+/QDIy8ujb9++552nZ8+ejB8/nvz8fIYOHUp8KUXB1q9fz+bNm+npChjLzs6mWbNmAISEhDBy5EgA7rzzToYPH17qdcydO5eVK1cCcOjQIXbv3k1MTAx16tThlltuAaB79+6sW7cOgJSUFHbsKBqy06dPk5mZyalTpxg7diy7d+/GGEO+a7phamqSBMVEAAAVsklEQVQqDz/8MABxcXHExcWVfVNdfPbZZ4waNQqHw0Hz5s0ZMGAAGzduJDo6ml69ennyv5R1/Rs2bKB///6e/Ro3bgxQZh/79u3LjBkzyMjIYPjw4XTo0OGCfVQUpQqEhEgiux9/lCnL9etX6mGnVANyciRguW3bKsVMBUTQWGs/KPZ2A3C76/VtwFJrbS6w3xizB+jl2rbHWrsPwBiz1LXvRQuaQNCpUydWrFhRYt3p06c5dOgQl112GZs3byY8PNyzzeFwUFBQgLWWQYMGsWTJkhLH/ve//+X+++8H4LnnnmPIkCGkpqayevVq7rrrLqZMmcLdd99d4hhrLWPHjuWll166YH+NMRw6dIhbb70VgEmTJnHVVVeRkpLCl19+SVRUFAkJCZ7My2FhYR5h5u47gNPpLDVo9ze/+Q3XX389K1eu5PvvvychIaFE25WhvESRdYuZqsu6/lWrVpXa5rRp00rt4+jRo+nduzerV6/mpptuYsGCBbRv375SfVYUpYK4Z+84HBK0qqKm5nD2rMTJVDBepjSCISh4PPC+63Vr4FCxbRmudWWtr5YMHDiQrKws3nzzTUCCVB977DHuueceosrJ5tinTx8+//xzj1sqKyuLXbt20bt3b9LS0khLS2PIkCEcOHCAZs2acd9993Hvvffy9ddfAyI03JaFgQMHsnz5co4ePQrAiRMnOHDgACDCY/ny5YC4r37xi19wySWXeNqYNGkSp06dolGjRkRFRZGens6GDRsueN2JiYm8+uqrnvdpaWmAWD9at5bhXLRokWd7//79PW6wbdu2sXXr1gu20b9/f5KTkyksLOTYsWOkpqbSq1ev8/Yr6/r79u3LJ598wv79+z3ry+vjvn37aN++PQ8//DBDhgxh69at1K9fn8zMzAv2VVGUKmCMxFY0aSL5UHwc5K/4GHfwb1TURYkZ8KGgMcakGGO2lbLcVmyfqUABkOReVcqpbDnry2p7ojFmkzFmUzBmZTXGsHLlSt5++206dOjAFVdcQURExAVnITVt2pRFixYxatQo4uLi6NOnD+np6eft9/HHHxMfH0+3bt1YsWIFkydPBmDixInExcUxZswYOnbsyAsvvEBiYiJxcXEMGjTIE2tSt25dT5zPhx9+yO9///vz2hg8eDAFBQXExcUxbdo0+vTpc8Hrnjt3Lps2bSIuLo6OHTsyb948AJ544gmeeuop+vXrV2IG0gMPPMCZM2eIi4tj1qxZpQqTcxk2bBhxcXF07dqVG264gVmzZtGilEJyZV1/06ZNmT9/PsOHD6dr164e11tZfUxOTqZz587Ex8eTnp7O3XffTUxMDP369aNz585MmTLlgn1WFKWSuItatm4tMReZmUW5SpTqg9MpYxcTI9OyLzI5X8BqORljxgKTgIHW2izXuqcArLUvud6vBf7gOuQP1tqbStuvPEqr5bRz506uvvpq71xIDcRXeWQUQT9/iuJFrJXYi5Mn5eFojCRfC8ZaUEoR+fkiQlu1KpmFuAIYYzZba3ucuz4gI26MGQw8CQxwixkXq4DFxpg5SFBwB+ArxELTwRgTC/yABA6P9m+vFUVRlKDDGAkgjYyU+kJnz8Lx4/KwDA0VcaNxNsFFTo64Ci+91KsJEwMlYV8FwoF1rgDMDdbaSdba7caYZUiwbwHwa2ttIYAx5iFgLeAAFlprtwem6zUftc4oilItCQ2VFP3R0fLQPHVKFrXaBA9ZWTIObdp4vU5XoGY5XV7OthnAjFLW/wf4jy/7pSiKotQAilttmjRRq00wYK3kEKpXr2immpdRuaooiqLUXEqz2px2ZYdXq41/cDpFzMTEVDpZXmXQkVQURVFqPmq1CQwFBeJmatlShKUPUUGjKIqi1C7UauMffBT8WxbBkFivVuJwODyFIrt27cqcOXNwOp0B6cumTZs8JQYqwqeffkqnTp2Ij48vtWCkv7lQdevs7GwGDBjgyR8zePBgGjZs6CnP4ObDDz/kmmuuoXPnzowdO9aT4Xj27NnEx8cTHx9P586dcTgcnoR7L7/8Mp06daJz586MGjXKky35jjvuYPfu3b64XEVRvIXbatOiBbRvD82bi0VB89pcPFlZcn/9JGZABU3AcBeK3L59O+vWreM///lPmUUgfU2PHj2YO3duhfdPSkri8ccfJy0t7bwyBqVR6ONMnhcSNAsXLmT48OE4XEFoU6ZM4Z///GeJfZxOJ2PHjmXp0qVs27aNSy+9lDfeeMOzvztL8ksvvcSAAQNo3LgxP/zwgydZ4LZt2ygsLGTp0qWAJAV0V0hXFKUa4LbaxMbKQ7huXXFLnTkjIkepGO7Mv5GRUpPJyzOZykMFzSOPQEKCd5dHHqlUF5o1a8b8+fN59dVXsdZy3XXXecoCAPTr14+tW7fyhz/8gfHjx5OQkED79u1LiJChQ4fSvXt3OnXqxPz58z3r69Wrx5NPPkn37t258cYb+eqrrzzHr1q1CpDMwm5rxZkzZxg3bhxdunQhLi7uvJpTCxYsYNmyZTz33HOMGTMGay1Tpkyhc+fOdOnSheTkZM85r7/+ekaPHk2XLl0AeOutt+jVqxfx8fHcf//9HqGzZs0arrnmGrp27crAgQMBqYx97bXX0q1bN6699lq+++47QCqVu88RFxfH7t27+d3vfsfevXuJj48vNTNvUlISt93mSVDNwIEDqV+/fol9jh8/Tnh4OFdcIbVQBw0adN61AyxZsoRRo0Z53hcUFJCdnU1BQQFZWVm0atUKgOuuu46UlBSPlUdRlGqCe4p3ixZw2WVqtakM7sy/jRt7JfNvZVFHYZDQvn17nE4nR48eZcKECSxatIhXXnmFXbt2kZubS1xcHO+88w7p6el89NFHZGZmcuWVV/LAAw8QFhbGwoULady4MdnZ2fTs2ZNf/vKXxMTEcPbsWRISEvjjH//IsGHDeOaZZ1i3bh07duxg7NixDBkypEQ/nn/+eRo0aMC3334LwMmTJ0tsnzBhAp999hm33HILt99+OytWrCAtLY0tW7bw008/0bNnT/r37w+IKNm2bRuxsbHs3LmT5ORkPv/8c8LCwnjwwQdJSkrif/7nf7jvvvtITU0lNjbW48q56qqrSE1NJTQ0lJSUFJ5++mlWrFjBvHnzmDx5MmPGjCEvL4/CwkJmzpzJtm3bSohAN3l5eezbt4927dqVe/+bNGlCfn4+mzZtokePHixfvpxDhw6V2CcrK4s1a9Z46lG1bt2axx9/nLZt2xIZGUliYiKJiYmAVCy//PLL2bJlC927d6/gp0BRlKDC4SiKtcnNhZ9/9n+sTXkCKpiCmAsKRPD5Ifi3LFTQvPJKoHvgwV2GYsSIETz//PPMnj2bhQsXcs8993j2ufnmmwkPDyc8PJxmzZrx448/0qZNG+bOncvKlSsBOHToELt37yYmJoY6deowePBgALp06UJ4eDhhYWF06dKF77///rw+pKSkeNwmAI0aNSq3z5999hmjRo3C4XDQvHlzBgwYwMaNG4mOjqZXr17ExsYCsH79ejZv3kzPnj0BiWtp1qwZGzZsoH///p79GjduDEgxyLFjx7J7926MMZ6imn379mXGjBlkZGQwfPhwOnToUG7/fvrpJxo2bFjuPiD1tZYuXcqjjz5Kbm4uiYmJhJ7zz+q9996jX79+nj6ePHmSd999l/3799OwYUNGjBjBW2+9xZ133gmI5e3w4cMqaBSlulPcatO0adEMKVfMHCDCoyyBUd62imw3pvTt7rhL9/EhIbKEhooY85fgyc0VQXPJJVJkMkCooAkS9u3bh8PhoFmzZhhjGDRoEO+++y7Lli2jeC2q8PBwz2uHw0FBQQEff/wxKSkpfPnll0RFRZGQkOAJTg0LC8OVjZmQkBDP8SEhIaW6Q6y1nv0rQnm1wOrWrVtiv7Fjx/LSSyXLb61atarU9qZNm8b111/PypUr+f7770lISABg9OjR9O7dm9WrV3PTTTexYMEC2rdvX2YfIiMjPffiQvTt25dPP/0UgA8++IBdu3aV2L506dIS7qaUlBRiY2Np2rQpAMOHD+eLL77wCJqcnJwKxRgpilKNcDjEYhMdfX6l7wv977yQaKksTqcIicJCWfLzRVzk5oq1xOksKZZCQqT/7sUbgic7uyj414/xMqWhMTRBwLFjx5g0aRIPPfSQ5+E+YcIEHn74YXr27OmxCJTFqVOnaNSoEVFRUaSnp7Nhw4Yq9yUxMdHjUoHzXU7n0r9/f5KTkyksLOTYsWOkpqaWWhV74MCBLF++nKNHjwJw4sQJDhw4QN++ffnkk0/Yv3+/Z737mlq3bg3AokWLPOfZt28f7du35+GHH2bIkCFs3bqV+vXrk5mZWWr/GjVqRGFhYYVEjbtvubm5/PGPf2TSpEmebadOneKTTz4pEYvTtm1bNmzYQFZWFtZa1q9fX6Lo5K5du+jUqdMF21UUpZpSXBw4HEUWkrIWt6WltKUqhISIiIiMlAy8jRqJFenSS+HyyyUGKDZWLCetWklsS0SEiJzsbIl3OXOmaMnKKrK2XChWyJ35t06doBAzoIImYGRnZ3umbd94440kJiby7LPPerZ3796d6Ohoxo0bd8FzDR48mIKCAuLi4pg2bRp9+vSpcr+eeeYZTp48SefOnenatSsfffRRufsPGzaMuLg4unbtyg033MCsWbNo0aLFeft17NiRF154gcTEROLi4hg0aBBHjhyhadOmzJ8/n+HDh9O1a1dGjhwJwBNPPMFTTz1Fv379SsySSk5OpnPnzsTHx5Oens7dd99NTEwM/fr1o3PnzqUGBScmJvLZZ5953l933XWMGDGC9evX06ZNG9auXQvI9Oyrr76auLg4br31Vm644QbPMStXriQxMbGE1al3797cfvvtXHPNNXTp0gWn08nEiRMB+PHHH4mMjKRly5YVue2Koijex+EoKXgaN5YYF7fgufxyaNdOBE/LliKIIiLEspOVVSR0MjPFzZadXSR4MjOhYUOpyeTn4N+yMOW5DGoCPXr0sMVdNgA7d+4s8Us6GDl8+DAJCQmkp6cTEqK682L45ptvmDNnznlTtX3Jyy+/THR0NPfee+9526rD509RFMXjynK7tfLyZMnNFTFTgfhEX2CM2Wyt7XHueo2hCULefPNNpk6dypw5c1TMeIFu3bpx/fXXU1hY6MlF42saNmzIXXfd5Ze2FEVRfILblRYE7qSKoBYaRfEz+vlTFEWpOmVZaGrtz/+aLuSU4EQ/d4qiKL6hVgqaiIgIjh8/rg8Xxa9Yazl+/DgRERGB7oqiKEqNo1bG0LRp04aMjAyOHTsW6K4otYyIiAjatGkT6G4oiqLUOGqloAkLC/NkplUURVEUpfpTK11OiqIoiqLULFTQKIqiKIpS7VFBoyiKoihKtafG56ExxpwCdgeo+SbATwFquwFwKkBtB7r9QF97bR33QN/3QLZfW8c80O0H+tpr67gH+r53sNY2OHdlbQgKTrbWTgxEw8aYTaUl//FT2/MDdd2Bbj8Irr1WjnsQ3PdAXnutHPNAtx8E114rxz0I7vv80tbXBpfTe4HuQIAI9HUHsv1AX3sgqc33PdDtB4pAX3dt/swFktp830ttv8a7nAJJINW7Ejh03GsfOua1Ex334KI2WGgCSalmMaXGo+Ne+9Axr53ouAcRaqFRFEVRFKXaoxYaRVEURVGqPSpoFEVRFEWp9qigqQTGmIXGmKPGmG3F1nU1xnxpjPnWGPOeMSbatX6MMSat2OI0xsS7to1y7b/VGLPGGNMkUNekXBgvjvtI15hvN8bMCtT1KBWjkuMeZox5w7V+pzHmqWLHDDbGfGeM2WOM+V0grkWpGF4c8/POo/gBa60uFVyA/sA1wLZi6zYCA1yvxwPPl3JcF2Cf63UocBRo4no/C/hDoK9NF5+PewxwEGjqev8GMDDQ16aLd8YdGA0sdb2OAr4H2gEOYC/QHqgDbAE6BvradPHdmJd1Hl18v6iFphJYa1OBE+esvhJIdb1eB/yylENHAUtcr41rqWuMMUA0cNj7vVW8hZfGvT2wy1p7zPU+pYxjlCChkuNuke90KBAJ5AGngV7AHmvtPmttHrAUuM3XfVeqhpfGvKzzKD5GBc3Fsw0Y4no9AriklH1G4nqwWWvzgQeAbxEh0xH4h++7qXiZSo07sAe4yhjTzvUPcGgZxyjBTVnjvhw4CxxBLHF/staeAFoDh4odn+Fap1QfKjvmSoBQQXPxjAd+bYzZDNRHVLoHY0xvIMtau831PgwRNN2AVsBW4CmU6kalxt1aexIZ92TgU8Q8XeDPDiteoaxx7wUUIt/pWOAxY0x7xBp7Lporo3pR2TFXAkRtqOXkU6y16UAigDHmCuDmc3a5g6Jf6QDxruP2uo5ZBmigYDWjCuOOtfY9XCm7jTETkX+GSjWinHEfDaxxWWCPGmM+B3og1pnilrg2qIu5WlGFMd8XkI4qaqG5WIwxzVx/Q4BngHnFtoUgJsqlxQ75AehojGnqej8I2Omf3ireogrjXvyYRsCDwAJ/9VfxDuWM+0HgBiPUBfoA6UhAaQdjTKwxpg4idFf5v+dKVanCmCsBQgVNJTDGLAG+BK40xmQYY+4FRhljdiEf5MPA68UO6Q9kWGs9it1aexiYDqQaY7YiFpsX/XUNSuXxxri7+LMxZgfwOTDTWrvLD91Xqkglx/0vQD0k3mIj8Lq1dqu1tgB4CFiL/HBZZq3d7udLUSqIN8a8nPMoPkZLHyiKoiiKUu1RC42iKIqiKNUeFTSKoiiKolR7VNAoiqIoilLtUUGjKIqiKEq1RwWNoiiKoijVHhU0iqIELcaYQlfV8u3GmC3GmN+68oGUd0w7Y8xof/VRUZTgQAWNoijBTLa1Nt5a2wlJQvm/wLMXOKYdksVVUZRahOahURQlaDHGnLHW1iv2vj2SxKwJcCnwT6Cua/ND1tovjDEbgKuB/cAbwFxgJpAAhAN/sdb+zW8XoSiKX1BBoyhK0HKuoHGtOwlcBWQCTmttjjGmA7DEWtvDGJMAPG6tvcW1/0SgmbX2BWNMOJKpeYS1dr9fL0ZRFJ+ixSkVRaluuCtYhwGvGmPikUKfV5SxfyIQZ4y53fW+AdABseAoilJDUEGjKEq1weVyKgSOIrE0PwJdkXjAnLIOA35jrV3rl04qihIQNChYUZRqgatC/TzgVSu+8gbAEWutE7gLcLh2zQTqFzt0LfCAMSbMdZ4rXNWRFUWpQaiFRlGUYCbSGJOGuJcKkCDgOa5trwErjDEjgI+As671W4ECY8wWYBHwZ2Tm09fGGAMcA4b66wIURfEPGhSsKIqiKEq1R11OiqIoiqJUe1TQKIqiKIpS7VFBoyiKoihKtUcFjaIoiqIo1R4VNIqiKIqiVHtU0CiKoiiKUu1RQaMoiqIoSrXn/wPS9k9u1aGF5QAAAABJRU5ErkJggg==\n\"/>",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Personal consumption', xlabel='Date', ylabel='Billions of dollars')\n\n# Plot data points\ndata.loc['1977-07-01':, 'consump'].plot(ax=ax, style='o', label='Observed')\n\n# Plot predictions\npredict.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='r--', label='One-step-ahead forecast')\nci = predict_ci.loc['1977-07-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\npredict_dy.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='g', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-07-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='g', alpha=0.1)\n\nlegend = ax.legend(loc='lower right')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>"
        ]
    },
    {
        "id": "1245662",
        "GT": "Project: Train a Quadcopter How to Fly\n\nDesign an agent that can fly a quadcopter, and then train it using a reinforcement learning algorithm of your choice! Try to apply the techniques you have learnt, but also feel free to come up with innovative ideas and test them.\n\n![Quadcopter doing a flip trying to takeoff from the ground](images/quadcopter_tumble.png)\n\n## Instructions\n\n> **Note**: If you haven't done so already, follow the steps in this repo's README to install ROS, and ensure that the simulator is running and correctly connecting to ROS.\n\nWhen you are ready to start coding, take a look at the `quad_controller_rl/src/` (source) directory to better understand the structure. Here are some of the salient items:\n\n- `src/`: Contains all the source code for the project.\n  - `quad_controller_rl/`: This is the root of the Python package you'll be working in.\n  - ...\n  - `tasks/`: Define your tasks (environments) in this sub-directory.\n    - `__init__.py`: When you define a new task, you'll have to import it here.\n    - `base_task.py`: Generic base class for all tasks, with documentation.\n    - `takeoff.py`: This is the first task, already defined for you, and set to run by default.\n  - ...\n  - `agents/`: Develop your reinforcement learning agents here.\n    - `__init__.py`: When you define a new agent, you'll have to import it here, just like tasks.\n    - `base_agent.py`: Generic base class for all agents, with documentation.\n    - `policy_search.py`: A sample agent has been provided here, and is set to run by default.\n  - ...\n\n### Tasks\n\nOpen up the base class for tasks, `BaseTask`, defined in `tasks/base_task.py`:\n\n```python\nclass BaseTask:\n    \"\"\"Generic base class for reinforcement learning tasks.\"\"\"\n\n    def __init__(self):\n        \"\"\"Define state and action spaces, initialize other task parameters.\"\"\"\n        pass\n    \n    def set_agent(self, agent):\n        \"\"\"Set an agent to carry out this task; to be called from update.\"\"\"\n        self.agent = agent\n    \n    def reset(self):\n        \"\"\"Reset task and return initial condition.\"\"\"\n        raise NotImplementedError\n    \n    def update(self, timestamp, pose, angular_velocity, linear_acceleration):\n        \"\"\"Process current data, call agent, return action and done flag.\"\"\"\n        raise NotImplementedError            \n```\n\nAll tasks must inherit from this class to function properly. You will need to override the `reset()` and `update()` methods when defining a task, otherwise you will get `NotImplementedError`'s. Besides these two, you should define the state (observation) space and the action space for the task in the constructor, `__init__()`, and initialize any other variables you may need to run the task.\n\nNow compare this with the first concrete task `Takeoff`, defined in `tasks/takeoff.py`:\n\n```python\nclass Takeoff(BaseTask):\n    \"\"\"Simple task where the goal is to lift off the ground and reach a target height.\"\"\"\n    ...\n```\n\nIn `__init__()`, notice how the state and action spaces are defined using [OpenAI Gym spaces](https://gym.openai.com/docs/#spaces), like [`Box`](https://github.com/openai/gym/blob/master/gym/spaces/box.py). These objects provide a clean and powerful interface for agents to explore. For instance, they can inspect the dimensionality of a space (`shape`), ask for the limits (`high` and `low`), or even sample a bunch of observations using the `sample()` method, before beginning to interact with the environment. We also set a time limit (`max_duration`) for each episode here, and the height (`target_z`) that the quadcopter needs to reach for a successful takeoff.\n\nThe `reset()` method is meant to give you a chance to reset/initialize any variables you need in order to prepare for the next episode. You do not need to call it yourself; it will be invoked externally. And yes, it will be called once before each episode, including the very first one. Here `Takeoff` doesn't have any episode variables to initialize, but it must return a valid _initial condition_ for the task, which is a tuple consisting of a [`Pose`](http://docs.ros.org/api/geometry_msgs/html/msg/Pose.html) and [`Twist`](http://docs.ros.org/api/geometry_msgs/html/msg/Twist.html) object. These are ROS message types used to convey the pose (position, orientation) and velocity (linear, angular) you want the quadcopter to have at the beginning of an episode. You may choose to supply the same initial values every time, or change it a little bit, e.g. `Takeoff` drops the quadcopter off from a small height with a bit of randomness.\n\n> **Tip**: Slightly randomized initial conditions can help the agent explore the state space faster.\n\nFinally, the `update()` method is perhaps the most important. This is where you define the dynamics of the task and engage the agent. It is called by a ROS process periodically (roughly 30 times a second, by default), with current data from the simulation. A number of arguments are available: `timestamp` (you can use this to check for timeout, or compute velocities), `pose` (position, orientation of the quadcopter), `angular_velocity`, and `linear_acceleration`. You do not have to include all these variables in every task, e.g. `Takeoff` only uses pose information, and even that requires a 7-element state vector.\n\nOnce you have prepared the state you want to pass on to your agent, you will need to compute the reward, and check whether the episode is complete (e.g. agent crossed the time limit, or reached a certain height). Note that these two things (`reward` and `done`) are based on actions that the agent took in the past. When you are writing your own agents, you have to be mindful of this.\n\nNow you can pass in the `state`, `reward` and `done` values to the agent's `step()` method and expect an action vector back that matches the action space that you have defined, in this case a `Box(6,)`. After checking that the action vector is non-empty, and clamping it to the space limits, you have to convert it into a ROS `Wrench` message. The first 3 elements of the action vector are interpreted as force in x, y, z directions, and the remaining 3 elements convey the torque to be applied around those axes, respectively.\n\nReturn the `Wrench` object (or `None` if you don't want to take any action) and the `done` flag from your `update()` method (note that when `done` is `True`, the `Wrench` object is ignored, so you can return `None` instead). This will be passed back to the simulation as a control command, and will affect the quadcopter's pose, orientation, velocity, etc. You will be able to gauge the effect when the `update()` method is called in the next time step.\n\n### Agents\n\nReinforcement learning agents are defined in a similar way. Open up the generic agent class, `BaseAgent`, defined in `agents/base_agent.py`, and the sample agent `RandomPolicySearch` defined in `agents/policy_search.py`. They are actually even simpler to define - you only need to implement the `step()` method that is discussed above. It needs to consume `state` (vector), `reward` (scalar value) and `done` (boolean), and produce an `action` (vector). The state and action vectors must match the respective space indicated by the task. And that's it!\n\nWell, that's just to get things working correctly! The sample agent given `RandomPolicySearch` uses a very simplistic linear policy to directly compute the action vector as a dot product of the state vector and a matrix of weights. Then, it randomly perturbs the parameters by adding some Gaussian noise, to produce a different policy. Based on the average reward obtained in each episode (\"score\"), it keeps track of the best set of parameters found so far, how the score is changing, and accordingly tweaks a scaling factor to widen or tighten the noise.->Task 3: Landing\n\nWhat goes up, must come down! But safely!\n\n### Implement landing agent\n\nThis time, you will need to edit the starting state of the quadcopter to place it at a position above the ground (at least 10 units). And change the reward function to make the agent learn to settle down _gently_. Again, create a new task for this (e.g. `Landing` in `tasks/landing.py`), and implement the changes. Note that you will have to modify the `reset()` method to return a position in the air, perhaps with some upward velocity to mimic a recent takeoff.\n\nOnce you're satisfied with your task definition, create another agent or repurpose an existing one to learn this task. This might be a good chance to try out a different approach or algorithm.\n\n### Initial condition, states and rewards\n\n**Q**: How did you change the initial condition (starting state), state representation and/or reward function? Please explain below what worked best for you, and why you chose that scheme. Were you able to build in a reward mechanism for landing gently?\n\n**A**: Initial z position was 10 units above. State representation and reward function used for the hover task was reused, but the taget was set to a linear fuction of time starting from the initial z position to the ground. Setting target to a linear function made landing gently.\n\n### Implementation notes\n\n**Q**: Discuss your implementation below briefly, using the same questions as before to guide you.\n\n**A**: For the agent, same implementaion was used as hover task.\n\n### Plot episode rewards\n\nAs before, plot the episode rewards, either from a single run, or averaged over multiple runs. This task is a little different from the previous ones, since you're starting in the air. Was it harder to learn? Why/why not?-># TODO: Read and plot episode rewards\ndf_stats = pd.read_csv('landing_stats.csv')\ndf_stats[['total_reward']].plot(title=\"Episode Rewards\")",
        "pred": [
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)"
        ]
    },
    {
        "id": "792057",
        "GT": "Parametric: Normal and $t$ Distributions\n* Another simple approach is to\nassume $R_{t+1}\\sim N(\\mu ,\\sigma ^{2})$, and to estimate $\\mu $\nand $\\sigma ^{2}$ from historical data (for daily data, $\\mu \\approx 0$). With $\\Phi (z)$ denoting the distribution function of the standard normal, the VaR is then determined from\n\n\\begin{align*}\n\\Pr \\left( R_{t+1}<-VaR_{t+1}^{p}\\right) &=\\Pr \\left( \\frac{R_{t+1}-\\mu }{\n\\sigma }<\\frac{-VaR_{t+1}^{p}-\\mu }{\\sigma }\\right) \\\\\n&=\\Pr \\left( z_{t+1}<\\frac{-VaR_{t+1}^{p}-\\mu }{\\sigma }\\right) \\\\\n&=\\Phi \\left( \\frac{-VaR_{t+1}^{p}-\\mu }{\\sigma }\\right) =p.\n\\end{align*}->df, m, h = stats.t.fit(r)  #Fit a location-scale t distribution to r.\nVaR_t = -stats.t.ppf(0.01, df, loc=m, scale=h)\nVaR_t",
        "pred": [
            "scipy->Statistics (scipy.stats)->Analysing one sample->Special tests for normal distributions->print('normaltest teststat = %6.3f pvalue = %6.4f' %\n...       stats.normaltest((x-x.mean())/x.std()))\nnormaltest teststat = 30.379 pvalue = 0.0000  # random",
            "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Fitting with the default arguments->kde = sm.nonparametric.KDEUnivariate(obs_dist)\nkde.fit()  # Estimate the densities",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS estimation->X = sm.add_constant(X)\ny = np.dot(X, beta) + e",
            "sklearn->Examples->Clustering->Segmenting the picture of greek coins in regions->Spectral clustering: kmeans, 2.04s\nSpectral clustering: discretize, 1.83s\nSpectral clustering: cluster_qr, 1.82s",
            "scipy->Sparse eigenvalue problems with ARPACK->Examples->evals_all, evecs_all = eigh(X)"
        ]
    },
    {
        "id": "696137",
        "GT": "df['hour'] = df['date'].map(lambda x: x.hour).astype('category')\ndf_hour = pd.get_dummies(df['hour'],drop_first=False)",
        "pred": [
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first = df.groupby('airline_id')[['fl_date', 'unique_carrier']].first()\nfirst.head()"
        ]
    },
    {
        "id": "575836",
        "GT": "Fitting the classifier to the Training Set->from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)",
        "pred": [
            "sklearn->Examples->Ensemble methods->IsolationForest example->Training of the model->from sklearn.ensemble import \n\nclf = (max_samples=100, random_state=0)\nclf.fit(X_train)",
            "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation->from sklearn import datasets\nimport numpy as np\n\ndigits = ()\nrng = (2)\nindices = (len(digits.data))\nrng.shuffle(indices)",
            "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data->from statsmodels.tsa.forecasting.theta import ThetaModel\n\ntm = ThetaModel(housing)\nres = tm.fit()\nprint(res.summary())",
            "sklearn->Examples->Generalized Linear Models->Fitting an Elastic Net with a precomputed Gram Matrix and Weighted Samples->from sklearn.linear_model import \n\nlm = (alpha=0.01, precompute=gram)\nlm.fit(X_centered, y, sample_weight=normalized_weights)",
            "sklearn->Examples->Generalized Linear Models->Sparsity Example: Fitting only features 1  and 2->from sklearn import linear_model\n\nols = ()\n_ = ols.fit(X_train, y_train)"
        ]
    },
    {
        "id": "1088479",
        "GT": "Create a KFBS Experiment->Optimize parameters->observations_2 = test_data['observations_2'].unstack(level=0).values\nobservation_dim = test_data['observations_2'].ndim\nobservations_2 = observations.reshape(-1, observation_dim, num_test_episodes)\n\nevaluation.setup_evaluation(train_data=train_matrices, test_observations=observations,\n                            test_groundtruth=groundtruth)",
        "pred": [
            "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance->feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "sklearn->Examples->Working with text documents->Clustering text documents using k-means->K-means clustering on text features->Top terms per cluster->original_space_centroids = lsa[0].inverse_transform(kmeans.cluster_centers_)\norder_centroids = original_space_centroids.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names_out()\n\nfor i in range(true_k):\n    print(f\"Cluster {i}: \", end=\"\")\n    for ind in order_centroids[i, :10]:\n        print(f\"{terms[ind]} \", end=\"\")\n    print()",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, with regularization, normalized variables\")\n(\"Raw coefficient values\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, with regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\"/>",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot(kind=\"barh\", figsize=(9, 7))\n(\"Lasso model, optimum regularization, normalized variables\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Lasso model, optimum regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_016.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_016.png\"/>",
            "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->OLS vs.\u00a0WLS->covb = res_ols.cov_params()\nprediction_var = res_ols.mse_resid + (X * np.dot(covb, X.T).T).sum(1)\nprediction_std = np.sqrt(prediction_var)\ntppf = stats.t.ppf(0.975, res_ols.df_resid)"
        ]
    },
    {
        "id": "1191539",
        "GT": "pca = PCA(n_components=0.95)\nX_pca = pca.fit_transform(X)\nprint(X_pca.shape)",
        "pred": [
            "statsmodels->Examples->User Notes->Prediction->Estimation->olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach->mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->mod = TVRegression(y_t, x_t, w_t)\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())"
        ]
    },
    {
        "id": "226200",
        "GT": "7. Quick Exploratory Data Analysis\n\nNext, we perform some quick EDA on our data.->Combined Dataset->color = ['red', 'blue']\nplt.figure()\nfor color, i, name in zip(color, [0,1], ['no_churn', 'churn']):\n    plt.scatter(df_fa[df_fa['is_churn'] == i]['date_featuresdatelistening_tenure'],\n               df_fa[df_fa['is_churn'] == i]['within_days_7num_unqmean'], color = color, alpha = 0.2, label = name)\nplt.legend(loc = 'best')\nplt.xlabel('Listening Tenure')\nplt.ylabel('Mean Number of Unique listening Periods in the last 7 days')",
        "pred": [
            "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Define a function to visualize cross-validation behavior->cvs = [, , ]\n\nfor cv in cvs:\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(cv(n_splits), X, y, groups, ax, n_splits)\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n\n\n\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_003.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_003.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_004.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_004.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_005.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_005.png\"/>",
            "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Visualize cross-validation indices for many CV objects->cvs = [\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n]\n\n\nfor cv in cvs:\n    this_cv = cv(n_splits=n_splits)\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n()\n\n\n\n<img alt=\"KFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_006.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_006.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_007.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_007.png\"/>\n<img alt=\"ShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_008.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_008.png\"/>\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_009.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_009.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_010.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_010.png\"/>\n<img alt=\"GroupShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_011.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_011.png\"/>\n<img alt=\"StratifiedShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_012.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_012.png\"/>\n<img alt=\"TimeSeriesSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_013.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_013.png\"/>",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->cmap = sns.cubehelix_palette(as_cmap=True, dark=0, light=1, reverse=True)\n\n(df.select_dtypes(include=[np.number])\n   .pipe(core)\n   .pipe(sns.PairGrid)\n   .map_upper(plt.scatter, marker='.', alpha=.25)\n   .map_diag(sns.kdeplot)\n   .map_lower(plt.hexbin, cmap=cmap, gridsize=20)\n);",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings->data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "pandas_toms_blog->Indexes->stations = pd.io.json.json_normalize(js['features']).id\nurl = (\"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n       \"&amp;data=tmpf&amp;data=relh&amp;data=sped&amp;data=mslp&amp;data=p01i&amp;data=vsby&amp;data=gust_mph&amp;data=skyc1&amp;data=skyc2&amp;data=skyc3\"\n       \"&amp;tz=Etc/UTC&amp;format=comma&amp;latlon=no\"\n       \"&amp;{start:year1=%Y&amp;month1=%m&amp;day1=%d}\"\n       \"&amp;{end:year2=%Y&amp;month2=%m&amp;day2=%d}&amp;{stations}\")\nstations = \"&amp;\".join(\"station=%s\" % s for s in stations)\nstart = pd.Timestamp('2014-01-01')\nend=pd.Timestamp('2014-01-31')\n\nweather = (pd.read_csv(url.format(start=start, end=end, stations=stations),\n                       comment=\"#\"))"
        ]
    },
    {
        "id": "1307607",
        "GT": "02_03 Data Transformation\n\n## Import the initial dataset (without expired records)->import pandas as pd\n\npath=r'noExpired.xlsx'\nall=pd.read_excel(path).dropna()",
        "pred": [
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "statsmodels->Examples->User Notes->Contrasts->Example Data->import pandas as pd\n\nurl = \"https://stats.idre.ucla.edu/stat/data/hsb2.csv\"\nhsb2 = pd.read_table(url, delimiter=\",\")",
            "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Custom Deterministic Terms->import numpy as np\n\ngen = np.random.default_rng(98765432101234567890)\nexog = pd.DataFrame(gen.integers(100, size=(300, 2)), columns=[\"exog1\", \"exog2\"])\nexog.head()",
            "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime->import onnx\n\nonnx_model = onnx.load(\"super_resolution.onnx\")\nonnx.checker.check_model(onnx_model)",
            "statsmodels->Examples->Time Series Analysis->ARMA: Artificial Data->import numpy as np\nimport pandas as pd\n\nfrom statsmodels.graphics.tsaplots import plot_predict\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.arima.model import ARIMA\n\nnp.random.seed(12345)"
        ]
    },
    {
        "id": "1421220",
        "GT": "Tf-idf in Scikit-Learn->tf_and = 1\ndf_and = 1 \ntf_and * (np.log(n_docs / df_and) + 1)",
        "pred": [
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Calculating the test statistics->dof = len(before_sample) - 1\n\np_value = stats.distributions.t.cdf(t_value, dof)\n\nprint(\"The t value is {} and the p value is {}.\".format(t_value, p_value))",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->delta_hat, rho_hat = sarima_res.params[:2]\ndelta_hat + rho_hat * y[0]",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "numpy->NumPy Features->Masked Arrays->Exploring the data->totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "torch->Learning PyTorch->What is torch.nn really?->Refactor using optim->tensor(2.2155, grad_fn=&lt;NllLossBackward0&gt;)\ntensor(0.0802, grad_fn=&lt;NllLossBackward0&gt;)"
        ]
    },
    {
        "id": "1043714",
        "GT": "Problem Statement\n\nhttps://discuss.leetcode.com/topic/21691/rolling-average-and-rolling-maximum-over-two-different-window-size\n\nGiven a stream of numbers, write code to calculate the rolling average and rolling maximum over two different window sizes. We provide an illustrative example below. It illustrates the expected output for window sizes of 3 and 5. For your solution please implement a solution that uses window sizes of 3 and 20.\n\n*For example*, given a stream containing the following values:\n\n[1, 2, 3, 4, 5, 6]\n\nthe following tuples would be expected, where \u2018None\u2019 indicates that a value is not available, and would be \u2018NaN\u2019 in some languages.->\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport random as rand",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.tsa.arima.model import ARIMA",
            "statsmodels->Examples->Statistics->Mediation analysis with duration data->import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.stats.mediation import Mediation",
            "numpy->NumPy Applications->Plotting Fractals->What you\u2019ll need->import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable",
            "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests->import pandas as pd\nimport numpy as np\nfrom scipy.stats.distributions import norm, poisson\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt"
        ]
    },
    {
        "id": "421411",
        "GT": "Implementation using Python\n\n#### The dataset we are gonna use has 3000 entries with 3 clusters. So we already know the value of K.->Getting the values and plotting it->f1 = data[\"V1\"].values\nf2 = data[\"V2\"].values\nX = np.array(list(zip(f1,f2)))  \nplt.scatter(f1,f2,c=\"black\",s=7) # \u2018c\u2019\t\u2018color\u2019 ",
        "pred": [
            "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Factor Analysis components - FA->fa_estimator = (n_components=n_components, max_iter=20)\nfa_estimator.fit(faces_centered)\nplot_gallery(\"Factor Analysis (FA)\", fa_estimator.components_[:n_components])\n\n# --- Pixelwise variance\n(figsize=(3.2, 3.6), facecolor=\"white\", tight_layout=True)\nvec = fa_estimator.noise_variance_\nvmax = max(vec.max(), -vec.min())\n(\n    vec.reshape(image_shape),\n    cmap=plt.cm.gray,\n    interpolation=\"nearest\",\n    vmin=-vmax,\n    vmax=vmax,\n)\n(\"off\")\n(\"Pixelwise variance from \\n Factor Analysis (FA)\", size=16, wrap=True)\n(orientation=\"horizontal\", shrink=0.8, pad=0.03)\n()\n\n\n\n<img alt=\"Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\"/>\n<img alt=\"Pixelwise variance from   Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\"/>",
            "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance->feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()->pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "scipy->Statistics (scipy.stats)->Comparing two samples->Comparing means->rvs1 = stats.norm.rvs(loc=5, scale=10, size=500)\n rvs2 = stats.norm.rvs(loc=5, scale=10, size=500)\n stats.ttest_ind(rvs1, rvs2)\nTtest_indResult(statistic=-0.5489036175088705, pvalue=0.5831943748663959)  # random",
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Building the dataset->pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)"
        ]
    },
    {
        "id": "1468887",
        "GT": "Advanced Data Manipulation with Pandas \n## Stack, Unstack (Melting), Split-apply-combine, Pivot Tables ... You name it !!->Importing Modules and Reading the Tables->How many years have been \"Batman years\", with more Batman characters than Superman characters?->(cast\n .loc[cast.character.isin(['Superman', 'Batman'])]\n .pivot_table(index='year', columns='character', aggfunc='size')\n .fillna(0)\n .apply(lambda x: 1 if x['Batman'] > x['Superman'] else 0, axis=1)\n .sum())",
        "pred": [
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team->(sm.Logit.from_formula('home_win ~ strength_diff + rest_spread',\n                       df.assign(strength_diff=df.home_strength - df.away_strength))\n    .fit().summary())",
            "pandas_toms_blog->Method Chaining->Method Chaining->Application->(df.dropna(subset=['dep_time', 'unique_carrier'])\n   .loc[df['unique_carrier']\n       .isin(df['unique_carrier'].value_counts().index[:5])]\n   .set_index('dep_time')\n   # TimeGrouper to resample &amp; groupby at once\n   .groupby(['unique_carrier', pd.TimeGrouper(\"H\")])\n   .fl_num.count()\n   .unstack(0)\n   .fillna(0)\n   .rolling(24)\n   .sum()\n   .rename_axis(\"Flights per Day\", axis=1)\n   .plot()\n)\nsns.despine()",
            "seaborn->User guide and tutorial->The seaborn.objects interface->Customizing the appearance->Parameterizing scales->(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"carat\", marker=\"cut\")\n    .add(so.Dots())\n    .scale(\n        color=so.Continuous(\"crest\", norm=(0, 3), trans=\"sqrt\"),\n        marker=so.Nominal([\"o\", \"+\", \"x\"], order=[\"Ideal\", \"Premium\", \"Good\"]),\n    )\n)\n",
            "seaborn->Objects interface->The seaborn.objects interface->Customizing the appearance->Parameterizing scales->(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"carat\", marker=\"cut\")\n    .add(so.Dots())\n    .scale(\n        color=so.Continuous(\"crest\", norm=(0, 3), trans=\"sqrt\"),\n        marker=so.Nominal([\"o\", \"+\", \"x\"], order=[\"Ideal\", \"Premium\", \"Good\"]),\n    )\n)\n",
            "seaborn->User guide and tutorial->The seaborn.objects interface->Customizing the appearance->Customizing legends and ticks->(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"carat\")\n    .add(so.Dots())\n    .scale(\n        x=so.Continuous().tick(every=0.5),\n        y=so.Continuous().label(like=\"${x:.0f}\"),\n        color=so.Continuous().tick(at=[1, 2, 3, 4]),\n    )\n)\n",
            "seaborn->Objects interface->The seaborn.objects interface->Customizing the appearance->Customizing legends and ticks->(\n    so.Plot(diamonds, x=\"carat\", y=\"price\", color=\"carat\")\n    .add(so.Dots())\n    .scale(\n        x=so.Continuous().tick(every=0.5),\n        y=so.Continuous().label(like=\"${x:.0f}\"),\n        color=so.Continuous().tick(at=[1, 2, 3, 4]),\n    )\n)\n",
            "pandas_toms_blog->Indexes->Merging->The merge version->(weather.reset_index(level='station')\n .query('station in @locs')\n .groupby(['station', pd.TimeGrouper('H')])).mean()"
        ]
    },
    {
        "id": "740194",
        "GT": "KS investigation (with negative weights)->Check how sPlot weights change real data pdfs->for feature in features_for_ks:\n    hist(data_agreement[data_agreement.signal == 1][feature].values,\n         weights=data_agreement[data_agreement.signal == 1]['weight'].values, label='MC', **hist_kw)\n    hist(data_agreement[data_agreement.signal == 0][feature].values,\n         weights=data_agreement[data_agreement.signal == 0]['weight'].values, label='real', **hist_kw)\n    title(feature)\n    legend()\n    show()",
        "pred": [
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->for name, importances in zip([\"train\", \"test\"], [train_importances, test_importances]):\n    ax = importances.plot.box(vert=False, whis=10)\n    ax.set_title(f\"Permutation Importances ({name} set)\")\n    ax.set_xlabel(\"Decrease in accuracy score\")\n    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n    ax.figure.tight_layout()\n\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_004.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_004.png\"/>\n<img alt=\"Permutation Importances (test set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_005.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_005.png\"/>",
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Bayesian estimation via MCMC->for i in range(niter):\n    mod.update_variances(store_obs_cov[i], store_state_cov[i])\n    sim.simulate()\n\n    # 1. Sample states\n    store_states[i + 1] = sim.simulated_state.T\n\n    # 2. Simulate obs cov\n    fitted = np.matmul(mod['design'].transpose(2, 0, 1), store_states[i + 1][..., None])[..., 0]\n    resid = mod.endog - fitted\n    store_obs_cov[i + 1] = invwishart.rvs(v10 + mod.nobs, S10 + resid.T @ resid)\n\n    # 3. Simulate state cov variances\n    resid = store_states[i + 1, 1:] - store_states[i + 1, :-1]\n    sse = np.sum(resid**2, axis=0)\n\n    for j in range(mod.k_states):\n        rv = invgamma.rvs((vi20 + mod.nobs - 1) / 2, scale=(Si20 + sse[j]) / 2)\n        store_state_cov[i + 1, j] = rv",
            "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits->for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>",
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data->for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>",
            "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Steps->5. Zero the gradients while training the network->for epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n\nprint('Finished Training')"
        ]
    },
    {
        "id": "113426",
        "GT": "Comparison of level-2 sub-criteria of *Prices*\nThere are 2 criteria to compare:\n\n* Ticket prices\n* Additional services: additional checked luggages, sport equipement and special luggages\n* Reductions with the company's partners: car rental, hotel reservation\n\nA cross-comparison of 3 criteria involves 3 independant comparisons. Here is my personal opinion:\n\n* Ticket price has a very strong importance compared to the price of additional services\n* Ticket price has an extreme importance compared to reductions with the company's partners\n* The price of additional services is slightly more important than reductions with the company's partners->labels = ['Ticket', 'Add. services', 'Reductions']\ncomp_mat = np.array([[1,7,9],[1/7,1,3],[1/9,1/3,1]])\ncomp_df = pd.DataFrame(comp_mat, index = labels, columns=labels)\ndisplay(comp_df)\nind, mat_E = inconsistency(comp_df)\nprint('The consistency index is:',ind,'\\n')\nprint('The consistency deviation matrix:')\nprint(mat_E-1,'\\n')",
        "pred": [
            "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors->alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor->quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings->data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with categorical variables->names = ['group_a', 'group_b', 'group_c']\nvalues = [1, 10, 100]\n\nplt.figure(figsize=(9, 3))\n\nplt.subplot(131)\nplt.bar(names, values)\nplt.subplot(132)\nplt.scatter(names, values)\nplt.subplot(133)\nplt.plot(names, values)\nplt.suptitle('Categorical Plotting')\nplt.show()\n\n\n<img alt=\"Categorical Plotting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_006.png\" srcset=\"../../_images/sphx_glr_pyplot_006.png, ../../_images/sphx_glr_pyplot_006_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features->cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)"
        ]
    },
    {
        "id": "567628",
        "GT": "Dimensionality Reduction on faces\n\nIn this notebook, we visualize a small data set of faces.->import matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn \nimport pandas as pd",
        "pred": [
            "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->What you\u2019ll need->import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm",
            "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm",
            "matplotlib->Tutorials->Introductory->Animations using Matplotlib->import matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np",
            "matplotlib->Tutorials->Introductory->Quick start guide->import matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n",
            "statsmodels->Examples->State space models->VARMAX: Introduction->import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "statsmodels->Examples->State space models->Trends and cycles in unemployment->import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt"
        ]
    },
    {
        "id": "1481291",
        "GT": "Multivariate Analysis->grid = sns.FacetGrid(df, col='quality',col_wrap = 2)\ngrid.map(plt.scatter,'pH','total sulfur dioxide',alpha = 0.2)",
        "pred": [
            "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)->fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)",
            "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Partial Regression Plots (Duncan)->fig = sm.graphics.plot_partregress_grid(prestige_model)\nfig.tight_layout(pad=1.0)",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->sns.jointplot(x='carat', y='price', data=df, size=8, alpha=.25,\n              color='k', marker='.')\nplt.tight_layout()",
            "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Partial Regression Plots (Duncan)->fig = sm.graphics.plot_partregress(\"prestige\", \"income\", [\"education\"], data=prestige)\nfig.tight_layout(pad=1.0)",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dynamics->fig = plt.figure(figsize=(16, 9))\nfig = res.plot_diagnostics(fig=fig, lags=30)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_29_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_29_0.png\"/>"
        ]
    },
    {
        "id": "335237",
        "GT": "df = pd.concat(df_list, ignore_index = True, join = 'inner')",
        "pred": [
            "statsmodels->Examples->Statistics->ANOVA->df_infl = infl.summary_frame()",
            "pandas_toms_blog->Scaling->Dask->df.visualize(rankdir='LR')",
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis->df = generate_nested()",
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis->df = generate_crossed()",
            "pandas_toms_blog->Indexes->Merging->Merge Version->pd.merge(temp.to_frame(), sped.to_frame(), left_index=True, right_index=True).head()"
        ]
    },
    {
        "id": "925431",
        "GT": "2-2- Working on Null Values, Duplicates\n\nFirst steps in cleaning the data is dealing with null-values and duplicates. First let's deal with the duplicate rows. We don't want to over train our model by data points that carry exactly the same information. Let's see how many duplicates do we have?->class_data.info()",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resf_logit.predict()",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data->data.describe()",
            "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Filardo (1994) Time-Varying Transition Probabilities->res_filardo.summary()",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Overriding elements of the seaborn styles->sns.axes_style()\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Overriding elements of the seaborn styles->sns.axes_style()\n",
            "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money->res.plot_cusum()"
        ]
    },
    {
        "id": "618636",
        "GT": "# count null values by sex and class and use unstack() to make each sex a column in the \n# resulting table\nnull_ages = df[df.Age.isnull()].groupby(['Pclass', 'Sex']).count().Name.unstack()\nnull_ages",
        "pred": [
            "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Gradient boosting estimator with native categorical support-># The ordinal encoder will first output the categorical features, and then the\n# continuous (passed-through) features\n\nhist_native = (\n    ordinal_encoder,\n    (\n        random_state=42,\n        categorical_features=categorical_columns,\n    ),\n).set_output(transform=\"pandas\")",
            "statsmodels->Examples->User Notes->Least squares fitting of models to data->Linear models->Check against self-written cost function-># You can also use `scipy.optimize.minimize` and write your own cost function.\n# This does not give you the parameter errors though ... you'd have\n# to estimate the HESSE matrix separately ...\nfrom scipy.optimize import minimize\n\n\ndef chi2(pars):\n    \"\"\"Cost function.\"\"\"\n    y_model = pars[0] * data[\"x\"] + pars[1]\n    chi = (data[\"y\"] - y_model) / data[\"y_err\"]\n    return np.sum(chi ** 2)\n\n\nresult = minimize(fun=chi2, x0=[0, 0])\npopt = result.x\nprint(\"a = {0:10.3f}\".format(popt[0]))\nprint(\"b = {0:10.3f}\".format(popt[1]))",
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models-># Here we follow the same procedure as above, but now we instantiate the\n# Theano wrapper `Loglike` with the UC model instance instead of the\n# SARIMAX model instance\nloglike_uc = Loglike(mod_uc)\n\nwith pm.Model():\n    # Priors\n    sigma2level = pm.InverseGamma(\"sigma2.level\", 1, 1)\n    sigma2ar = pm.InverseGamma(\"sigma2.ar\", 1, 1)\n    arL1 = pm.Uniform(\"ar.L1\", -0.99, 0.99)\n\n    # convert variables to tensor vectors\n    theta_uc = tt.as_tensor_variable([sigma2level, sigma2ar, arL1])\n\n    # use a DensityDist (use a lamdba function to \"call\" the Op)\n    pm.DensityDist(\"likelihood\", loglike_uc, observed=theta_uc)\n\n    # Draw samples\n    trace_uc = pm.sample(\n        ndraws,\n        tune=nburn,\n        return_inferencedata=True,\n        cores=1,\n        compute_convergence_checks=False,\n    )",
            "torch->PyTorch Recipes->Dynamic Quantization->Steps->1: Set Up-># import the modules used here in this recipe\nimport torch\nimport torch.quantization\nimport torch.nn as nn\nimport copy\nimport os\nimport time\n\n# define a very, very simple LSTM for demonstration purposes\n# in this case, we are wrapping nn.LSTM, one layer, no pre or post processing\n# inspired by\n# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html, by Robert Guthrie\n# and https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html\nclass lstm_for_demonstration():\n  \"\"\"Elementary Long Short Term Memory style model which simply wraps nn.LSTM\n     Not to be used for anything other than demonstration.\n  \"\"\"\n  def __init__(self,in_dim,out_dim,depth):\n     super(lstm_for_demonstration,self).__init__()\n     self.lstm = (in_dim,out_dim,depth)\n\n  def forward(self,inputs,hidden):\n     out,hidden = self.lstm(inputs,hidden)\n     return out, hidden\n\n\n(29592)  # set the seed for reproducibility\n\n#shape parameters\nmodel_dimension=8\nsequence_length=20\nbatch_size=1\nlstm_depth=1\n\n# random data for input\ninputs = (sequence_length,batch_size,model_dimension)\n# hidden is actually is a tuple of the initial hidden state and the initial cell state\nhidden = ((lstm_depth,batch_size,model_dimension), (lstm_depth,batch_size,model_dimension))",
            "torch->PyTorch Recipes->Timer quick start->2. Wall time: Timer.blocked_autorange(\u2026)-># Measurement objects store the results of multiple repeats, and provide\n# various utility features.\nfrom torch.utils.benchmark import \n\nm:  = timer.blocked_autorange(min_run_time=1)\nprint(m)\n\n\n\n<strong>Snippet wall time.</strong>"
        ]
    },
    {
        "id": "1277599",
        "GT": "Step 1 - Read two ECVs\nWe read in 12 x monthly year of OC and SST at once->oc_pattern = os.path.join(ecv_base, 'occci-v2.0/data/geographic/netcdf/monthly/chlor_a/2010/*.nc')\nst_pattern = os.path.join(ecv_base, 'sst/data/lt/Analysis/L4/v01.1/2010/*.nc')\n\noc_ds = xr.open_mfdataset(oc_pattern)\nsst_ds = xr.open_mfdataset(st_pattern)",
        "pred": [
            "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples->face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Binary Model compared to Logit->ex = add_constant(data2[['pared', 'public', 'gpa']], prepend=False)\nmod_logit = Logit(data2['apply'].cat.codes, ex)\n\nres_logit = mod_logit.fit(method='bfgs', disp=False)",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Binary Model compared to Logit->mod_log = OrderedModel(data2['apply'],\n                        data2[['pared', 'public', 'gpa']],\n                        distr='logit')\n\nres_log = mod_log.fit(method='bfgs', disp=False)\nres_log.summary()",
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Testing->url = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/csv/COUNT/medpar.csv'\nmedpar = read.csv(url)\nf = los~factor(type)+hmo+white\n\nlibrary(MASS)\nmod = glm.nb(f, medpar)\ncoef(summary(mod))\n                 Estimate Std. Error   z value      Pr(|z|)\n(Intercept)    2.31027893 0.06744676 34.253370 3.885556e-257\nfactor(type)2  0.22124898 0.05045746  4.384861  1.160597e-05\nfactor(type)3  0.70615882 0.07599849  9.291748  1.517751e-20\nhmo           -0.06795522 0.05321375 -1.277024  2.015939e-01\nwhite         -0.12906544 0.06836272 -1.887951  5.903257e-02",
            "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests->rslt, scales = [], []\n\nfor hyp in 0, 1:\n    s, t = dosim(hyp, sm.cov_struct.Exchangeable(), mcrep=100)\n    rslt.append(s)\n    scales.append(t)\n\nrslt = pd.DataFrame(rslt, index=[\"H0\", \"H1\"], columns=[\"Mean\", \"Prop(p&lt;0.1)\"])\n\nprint(rslt)"
        ]
    },
    {
        "id": "905025",
        "GT": "Clustering on Second 5-Blob Configuration ##\nIn the K-means cluster it was unable to correctly estimate the number of clusters.  It estimated two clusters when there was actually five clusters.  For the spectral cluster, it was able to correctly identify the clusters.  But it was able to correctly identify the clusters because we had the expected number of clusters. As expected, the Affinity Propagation cluster over-estimated the number of clusters, it estimated 35 clusters almost 7 times the number of actual clusters.  I thought that the number of estimated clusters by the Affinity Propagation would be less since the clusters were closer together, but it actually produced more clusters.-># Here we set the bandwidth. This function automatically derives a bandwidth\n# number based on an inspection of the distances among points in the data.\nbandwidth = estimate_bandwidth(X_train2, quantile=0.2, n_samples=500)\n\n# Declare and fit the model.\nms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\nms.fit(X_train2)\n\n# Extract cluster assignments for each data point.\nlabels = ms.labels_\n\n# Coordinates of the cluster centers.\ncluster_centers = ms.cluster_centers_\n\n# Count our clusters.\nn_clusters_ = len(np.unique(labels))\n\nprint(\"Number of estimated clusters: {}\".format(n_clusters_))",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models-># Here we follow the same procedure as above, but now we instantiate the\n# Theano wrapper `Loglike` with the UC model instance instead of the\n# SARIMAX model instance\nloglike_uc = Loglike(mod_uc)\n\nwith pm.Model():\n    # Priors\n    sigma2level = pm.InverseGamma(\"sigma2.level\", 1, 1)\n    sigma2ar = pm.InverseGamma(\"sigma2.ar\", 1, 1)\n    arL1 = pm.Uniform(\"ar.L1\", -0.99, 0.99)\n\n    # convert variables to tensor vectors\n    theta_uc = tt.as_tensor_variable([sigma2level, sigma2ar, arL1])\n\n    # use a DensityDist (use a lamdba function to \"call\" the Op)\n    pm.DensityDist(\"likelihood\", loglike_uc, observed=theta_uc)\n\n    # Draw samples\n    trace_uc = pm.sample(\n        ndraws,\n        tune=nburn,\n        return_inferencedata=True,\n        cores=1,\n        compute_convergence_checks=False,\n    )",
            "torch->Model Optimization->torch.compile Tutorial->Demonstrating Speedups-># Returns the result of running `fn()` and the time it took for `fn()` to run,\n# in seconds. We use CUDA events and synchronization for the most accurate\n# measurements.\ndef timed(fn):\n    start = (enable_timing=True)\n    end = (enable_timing=True)\n    start.record()\n    result = fn()\n    end.record()\n    ()\n    return result, start.elapsed_time(end) / 1000\n\n# Generates random input and targets data for the model, where `b` is\n# batch size.\ndef generate_data(b):\n    return (\n        (b, 3, 128, 128).to().cuda(),\n        (1000, (b,)).cuda(),\n    )\n\nN_ITERS = 10\n\nfrom torchvision.models import \ndef init_model():\n    return ().to().cuda()",
            "sklearn->Examples->Tutorial exercises->Cross-validation on diabetes Dataset Exercise->Bonus: how much can you trust the selection of alpha?-># To answer this question we use the LassoCV object that sets its alpha\n# parameter automatically from the data by internal cross-validation (i.e. it\n# performs cross-validation on the training data it receives).\n# We use external cross-validation to see how much the automatically obtained\n# alphas differ across different cross-validation folds.\n\nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \n\nlasso_cv = (alphas=alphas, random_state=0, max_iter=10000)\nk_fold = (3)\n\nprint(\"Answer to the bonus question:\", \"how much can you trust the selection of alpha?\")\nprint()\nprint(\"Alpha parameters maximising the generalization score on different\")\nprint(\"subsets of the data:\")\nfor k, (train, test) in enumerate(k_fold.split(X, y)):\n    lasso_cv.fit(X[train], y[train])\n    print(\n        \"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".format(\n            k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])\n        )\n    )\nprint()\nprint(\"Answer: Not very much since we obtained different alphas for different\")\nprint(\"subsets of the data and moreover, the scores for these alphas differ\")\nprint(\"quite substantially.\")\n\n()",
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Preliminary investigation with ad-hoc parameters in H, Q-># Here, for illustration purposes only, we plot the time-varying\n# coefficients conditional on an ad-hoc parameterization\n\n# Recall that `initial_res` contains the Kalman filtering and smoothing,\n# and the `states.smoothed` attribute contains the smoothed states\nplot_coefficients_by_equation(initial_res.states.smoothed);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_27_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_27_0.png\"/>",
            "statsmodels->Examples->User Notes->Least squares fitting of models to data->Linear models->Check against self-written cost function-># You can also use `scipy.optimize.minimize` and write your own cost function.\n# This does not give you the parameter errors though ... you'd have\n# to estimate the HESSE matrix separately ...\nfrom scipy.optimize import minimize\n\n\ndef chi2(pars):\n    \"\"\"Cost function.\"\"\"\n    y_model = pars[0] * data[\"x\"] + pars[1]\n    chi = (data[\"y\"] - y_model) / data[\"y_err\"]\n    return np.sum(chi ** 2)\n\n\nresult = minimize(fun=chi2, x0=[0, 0])\npopt = result.x\nprint(\"a = {0:10.3f}\".format(popt[0]))\nprint(\"b = {0:10.3f}\".format(popt[1]))"
        ]
    },
    {
        "id": "950328",
        "GT": "Load Data\n Expecting an excel->Define colors for data->pal = sns.diverging_palette(220,10, sep=80, n=2)\ncmap = sns.diverging_palette(220,10, sep=80, as_cmap=True)\nsns.palplot(pal)",
        "pred": [
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines->sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines->sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n",
            "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions->Violinplots->g = sns.catplot(data=tips, x=\"day\", y=\"total_bill\", kind=\"violin\", inner=None)\nsns.swarmplot(data=tips, x=\"day\", y=\"total_bill\", color=\"k\", size=3, ax=g.ax)\n",
            "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions->Violinplots->g = sns.catplot(data=tips, x=\"day\", y=\"total_bill\", kind=\"violin\", inner=None)\nsns.swarmplot(data=tips, x=\"day\", y=\"total_bill\", color=\"k\", size=3, ax=g.ax)\n",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->sns.jointplot(x='carat', y='price', data=df, size=8, alpha=.25,\n              color='k', marker='.')\nplt.tight_layout()",
            "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Getting colormaps and accessing their values->LinearSegmentedColormap->copper = mpl.colormaps['copper'].resampled(8)\n\nprint('copper(range(8))', copper(range(8)))\nprint('copper(np.linspace(0, 1, 8))', copper(np.linspace(0, 1, 8)))"
        ]
    },
    {
        "id": "167514",
        "GT": "Predicting the Movie Genre from its Plot\n\n## tl;dr\n\nI built a NLP classifier that predicts a movie genre for a given plot. \n\nIf you don't care about the model development, you can scroll down to the last cells, where I test my algorithm with 5 TV Shows and plot the results. \n\n## Introduction\n\nFor this project, I will be using a database from IMDB that consists of the genre and plot for around 4.5k movies. I will develop a model that is capable of predicting the movie genre for a given plot using machine learning algorithms (no word matching RegEx will be used).->data['plot_clean'] = data['plot'].apply(clean_sentence)",
        "pred": [
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->pca_model = PCA(dta.T, standardize=False, demean=True)",
            "statsmodels->Examples->User Notes->Dates in Time-Series Models->Using Pandas->data.endog.index = dates\nendog = data.endog\nendog",
            "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns->data = ()\nX = ().fit_transform(data[\"data\"])\nfeature_names = data[\"feature_names\"]",
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model->sm_probit_canned = sm.Probit(endog, exog).fit()"
        ]
    },
    {
        "id": "778867",
        "GT": "Maximum likelihood estimators\n___\n\nLets the data matrix $\\mathbf{X}_{n \\times p}$ such that each row of this data set it's independen one each other, the data matrix could be represent independt sample of the muultivariated normal distribution, since the rows are independent the jointly distribution density function could be obtained multiplying the individual densities.\n\n$$\n\\begin{align*}\nf_{\\mathbf{x_1,...,x_n}}(X_1,..,X_p)&=\\prod_{i=0}^{n} \\left[\\frac{\\left| \\Sigma_i \\right|^{-\\frac{1}{2}}}{\\left[\\sqrt{2 \\pi}\\right]^p} exp\\left( \\frac{1}{2} \\left[\\textbf{x}_i-\\mu \\right]' \\Sigma^{-1} \\left[\\textbf{x}_i - \\mu\\right] \\right)\\right]\\\\\n&=\\left[\\frac{\\left|\\Sigma\\right|^{-\\frac{1}{2}}}{\\left[\\sqrt{2\\pi}\\right]^p}\\right]^n \\prod_{i=0}^{n}\\left[exp\\left(\\frac{1}{2} \\left[\\textbf{x}_i-\\mu\\right]'\\Sigma^{-1}\\left[\\textbf{x}_i-\\mu\\right]\\right)\\right]\\\\\n\\mathcal{L}\\left(\\mathbf{\\mu,\\Sigma}|\\mathbf{x_1,...,x_n}\\right) &=\\frac{\\left|\\Sigma\\right|^{-\\frac{n}{2}}}{\\left[\\sqrt{2\\pi}\\right]^{np}} exp\\left(\\sum_{i=1}^n\\frac{1}{2}\\left[\\textbf{x}_i-\\mu\\right]'\\Sigma^{-1}\\left[\\textbf{x}_i-\\mu\\right]\\right)\n\\end{align*}\n$$\n\nDue to $\\mathbf{x_1,...,x_n}$ it's fixed values in the data matrix if $\\mu$ and $\\mathbf{\\Sigma}$ are unknown then the density function could be put as function of the parameters given de data $\\mathbf{x_1,...,x_n}$, this function $\\mathcal{L}\\left(\\mathbf{\\mu,\\Sigma}|\\mathbf{x_1,...,x_n}\\right)$ is called the likelihood function. The maximum likehood estimators of $\\mu$ and $\\Sigma$b are the values $\\hat{\\mu}$ and $\\hat{\\mathbf{\\Sigma}}$ which their maximum of $\\mathcal{L}$ is reached. For the multivariate normal distribution the maximum likehood estimators is:\n\n$$\\hat{\\mu}=\\bar{\\mathbf{x}} \\qquad \\hat{\\mathbf{\\Sigma}}=\\mathbf{S}$$\n\nMaximum likehood estimators possess and invariance property. So if $\\mathbf{\\theta}$ is a unknow parameter and $\\hat{\\mathbf{\\theta}}$ is its ml-estimator and $g$ a function of $\\mathbf{\\theta}$ then the ml-estimator of $\\widehat{g(\\mathbf{\\theta})}$ is $g(\\widehat{\\mathbf{\\theta}})$, using this propertie the following estimators could be computing as functions of $\\mathbf{\\bar{x}}$ and $\\mathbf{S}$.\n\n\n* $\\widehat{\\mathbf{\\Sigma}^{-1}}=\\mathbf{S}^{-1}$\n* $\\widehat{\\mathbf{\\Gamma\\Lambda\\Gamma^{-1}}}=\\mathbf{QLQ^{-1}}$ where $\\mathbf{S=QLQ^{-1}}$\n\n**Example 2.1 - 2 \n[Estimating the bivariated normal distribution parameters and vizualizing constants probability density contours for two characteristics of three distinct plant populations]**\n\n**step 1:**  Loading iris data set->normalSampleDF = sc.parallelize(np.ones(n))\\\n                 .map(lambda y: multivariateNormalVector(y,mu,mormalizerMatrix))\\\n                 .toDF(['x_1','x_2'])",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis->oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis->oo = np.ones(df.shape[0])\nmodel4 = sm.MixedLM(df.y, oo[:, None], exog_re=None, groups=oo, exog_vc=vcs)\nresult4 = model4.fit()\nprint(result4.summary())",
            "statsmodels->Examples->Linear Regression Models->Generalized Least Squares->resid_fit = sm.OLS(\n    np.asarray(ols_resid)[1:], sm.add_constant(np.asarray(ols_resid)[:-1])\n).fit()\nprint(resid_fit.tvalues[1])\nprint(resid_fit.pvalues[1])",
            "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Fit and summary->glm_binom = sm.GLM(data.endog, data.exog, family=sm.families.Binomial())\nres = glm_binom.fit()\nprint(res.summary())",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson->rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)"
        ]
    },
    {
        "id": "253067",
        "GT": "(Q8)->cuisine = df[['CUISINE DESCRIPTION','RESTAURANT']].drop_duplicates(subset='RESTAURANT')\ncuisine['CUISINE DESCRIPTION'].value_counts()[:20].plot(kind = 'bar')",
        "pred": [
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "pandas_toms_blog->Scaling->Using Dask->df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->X = (pd.concat([y.shift(i) for i in range(6)], axis=1,\n               keys=['y'] + ['L%s' % i for i in range(1, 6)])\n       .dropna())\nX.head()",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)"
        ]
    },
    {
        "id": "687339",
        "GT": "Training a Classifier\nAt this point you might have employed the following during feature engineering:\n1. Dealt with contractions and abbreviations\n2. normalized your vocabulary via\n    - Case Norming\n    - Stemming\n    - Lemmatization\n3. included a stop words list\n    - words to exclude such as \"it\" or \"the\" \n4. Parts of Speech Tagging\n5. Created \"N-Grams\" where your token looks like \n    - [\"Ice Cream\", \"New York City\"] rather than\n    - [New, york, city, ice, cream]\n6. Dimensionality reduction\n7. included meta data such as time and critic->print(\"Accuracy on Training Data: %f\" % clf.score(X_train,y_train))\nprint(\"Accuracy on Test Data: %f\" % clf.score(X_test,y_test))",
        "pred": [
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Accuracy of the Model->print(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->print(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")",
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding->print(\"The data type of training labels: {}\".format(y_train.dtype))\nprint(\"The data type of test labels: {}\".format(y_test.dtype))",
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding->print(\"The data type of training labels: {}\".format(training_labels.dtype))\nprint(\"The data type of test labels: {}\".format(test_labels.dtype))",
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the image data to the floating-point format->print(\"The data type of training images: {}\".format(x_train.dtype))\nprint(\"The data type of test images: {}\".format(x_test.dtype))",
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the image data to the floating-point format->print(\"The data type of training images: {}\".format(training_images.dtype))\nprint(\"The data type of test images: {}\".format(test_images.dtype))"
        ]
    },
    {
        "id": "219007",
        "GT": "Clustering->We will check if 5 is the best number of emergency stations or if we could reallocate the funding to another area / request more stations, if possible->wss = []  # within-cluster sum of squares\nns = range(2, 11)\nfor i in ns:\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=500, n_init=20, random_state=0)\n    kmeans.fit(X)\n    wss.append(kmeans.inertia_)",
        "pred": [
            "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->ROC curve using the OvO macro-average->pair_scores = []\nmean_tpr = dict()\n\nfor ix, (label_a, label_b) in enumerate(pair_list):\n\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = (a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = (label_binarizer.classes_ == label_a)[0]\n    idx_b = (label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = (a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = (b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = (fpr_grid)\n    mean_tpr[ix] += (fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += (fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = (fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = (figsize=(6, 6))\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    (\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    (\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n    )\n    ([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n    (\"square\")\n    (\"False Positive Rate\")\n    (\"True Positive Rate\")\n    (f\"{target_names[idx_a]} vs {label_b} ROC curves\")\n    ()\n    ()\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{(pair_scores):.2f}\")\n\n\n\n<img alt=\"setosa vs versicolor ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_004.png\" srcset=\"../../_images/sphx_glr_plot_roc_004.png\"/>\n<img alt=\"setosa vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_005.png\" srcset=\"../../_images/sphx_glr_plot_roc_005.png\"/>\n<img alt=\"versicolor vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_006.png\" srcset=\"../../_images/sphx_glr_plot_roc_006.png\"/>",
            "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors->alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Total impurity of leaves vs effective alphas of pruned tree->clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = (random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\n    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n        clfs[-1].tree_.node_count, ccp_alphas[-1]\n    )\n)",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator->all_betas = []\nfor i in range(mc_iter):\n    y = np.dot(X, beta_true) + np.random.normal(size=200)\n    random_idx = np.random.randint(0, nobs, size=int(contaminate * nobs))\n    y[random_idx] = np.random.uniform(-750, 750)\n    beta_hat = sm.RLM(y, X).fit().params\n    all_betas.append(beta_hat)",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->Condition number->norm_x = X.values\nfor i, name in enumerate(X):\n    if name == \"const\":\n        continue\n    norm_x[:, i] = X[name] / np.linalg.norm(X[name])\nnorm_xtx = np.dot(norm_x.T, norm_x)"
        ]
    },
    {
        "id": "383696",
        "GT": "Collaboration\nBy: Bethany Bailey->Part 1: Kaggle->Kaggle Dataset\nMy dataset consisted of information on post-graduation salaries for college graduates. My graph plots the distribution of the mid-career 10th percentile and 90th percentile salaries for college graduates. I chose this comparison in order to compare the distribution of the 10th percentile and 90th percentile salaries. The data can be found at: https://www.kaggle.com/wsj/college-salaries->import pandas as pd\nimport matplotlib.pyplot as plt\n\nsal_by_reg = pd.read_csv('salaries-by-region.csv', index_col='Region')\nclean_df = sal_by_reg.dropna(axis=0).replace({'\\,':''}, regex = True).replace({'\\$':''}, regex = True)\nten_num = pd.to_numeric(clean_df['Mid-Career 10th Percentile Salary'], errors='coerce')",
        "pred": [
            "sklearn->Examples->Working with text documents->Clustering text documents using k-means->Clustering evaluation summary->import pandas as pd\nimport matplotlib.pyplot as plt\n\nfig, (ax0, ax1) = (ncols=2, figsize=(16, 6), sharey=True)\n\ndf = (evaluations[::-1]).set_index(\"estimator\")\ndf_std = (evaluations_std[::-1]).set_index(\"estimator\")\n\ndf.drop(\n    [\"train_time\"],\n    axis=\"columns\",\n).plot.barh(ax=ax0, xerr=df_std)\nax0.set_xlabel(\"Clustering scores\")\nax0.set_ylabel(\"\")\n\ndf[\"train_time\"].plot.barh(ax=ax1, xerr=df_std[\"train_time\"])\nax1.set_xlabel(\"Clustering time (s)\")\n()\n\n\n<img alt=\"plot document clustering\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_document_clustering_001.png\" srcset=\"../../_images/sphx_glr_plot_document_clustering_001.png\"/>",
            "matplotlib->Tutorials->Intermediate->Autoscaling->import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-2 * np.pi, 2 * np.pi, 100)\ny = np.sinc(x)\n\nfig, ax = plt.subplots()\nax.plot(x, y)\n\n\n<img alt=\"autoscale\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_autoscale_001.png\" srcset=\"../../_images/sphx_glr_autoscale_001.png, ../../_images/sphx_glr_autoscale_001_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Support Vector Machines->Non-linear SVM->import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\n\nxx, yy = ((-3, 3, 500), (-3, 3, 500))\n(0)\nX = (300, 2)\nY = (X[:, 0]  0, X[:, 1]  0)\n\n# fit the model\nclf = (gamma=\"auto\")\nclf.fit(X, Y)\n\n# plot the decision function for each datapoint on the grid\nZ = clf.decision_function([xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n(\n    Z,\n    interpolation=\"nearest\",\n    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n    aspect=\"auto\",\n    origin=\"lower\",\n    cmap=plt.cm.PuOr_r,\n)\ncontours = (xx, yy, Z, levels=[0], linewidths=2, linestyles=\"dashed\")\n(X[:, 0], X[:, 1], s=30, c=Y, cmap=plt.cm.Paired, edgecolors=\"k\")\n(())\n(())\n([-3, 3, -3, 3])\n()",
            "matplotlib->Tutorials->Intermediate->Artist tutorial->Object containers->Tick containers->import numpy as np\nimport matplotlib.pyplot as plt\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\nfig, ax = plt.subplots()\nax.plot(100*np.random.rand(20))\n\n# Use automatic StrMethodFormatter\nax.yaxis.set_major_formatter('${x:1.2f}')\n\nax.yaxis.set_tick_params(which='major', labelcolor='green',\n                         labelleft=False, labelright=True)\n\nplt.show()\n\n\n<img alt=\"dollar ticks\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_dollar_ticks_001.png\" srcset=\"../../_images/sphx_glr_dollar_ticks_001.png, ../../_images/sphx_glr_dollar_ticks_001_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Plot the BIC scores->import pandas as pd\n\ndf = (grid_search.cv_results_)[\n    [\"param_n_components\", \"param_covariance_type\", \"mean_test_score\"]\n]\ndf[\"mean_test_score\"] = -df[\"mean_test_score\"]\ndf = df.rename(\n    columns={\n        \"param_n_components\": \"Number of components\",\n        \"param_covariance_type\": \"Type of covariance\",\n        \"mean_test_score\": \"BIC score\",\n    }\n)\ndf.sort_values(by=\"BIC score\").head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ]
    },
    {
        "id": "1363504",
        "GT": "Capstone Project\n\nAnalyzing Crime Occurance in Los Angeles From 2010 to Present->import pandas as pd \nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc \nfrom datetime import datetime as dt \nfrom matplotlib.font_manager import FontProperties\nimport seaborn as sns \nimport numpy as np\nfrom mapsplotlib import mapsplot as mplt",
        "pred": [
            "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.neural_network import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.tree import \nfrom sklearn.inspection import PartialDependenceDisplay",
            "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "sklearn->Examples->Ensemble methods->Plot individual and voting regression predictions->import matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.ensemble import \nfrom sklearn.linear_model import \nfrom sklearn.ensemble import",
            "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation->import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)",
            "statsmodels->Examples->Time Series Analysis->Deterministic Terms->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nplt.rc(\"figure\", figsize=(16, 9))\nplt.rc(\"font\", size=16)"
        ]
    },
    {
        "id": "798429",
        "GT": "compare how dof is equivalent to 1 in formula\n\nIn the NumPy `std()` method, there's an argument called `ddof` which means delta degrees of freedom. The divisor $n-1$ component in the equation above is the same as `n - ddof` for use in the `std()` method. By default, `ddof` is zero - the proper divisor for the population standard deviation equation. A `ddof` of 1 is equivalent to the $n-1$ componenet in the sample standard deviation equation.\n\nWith the code below, for degrees of freedom values from 0 to 6 with a step of 1, I perform 500 simulations of sampling 60 values from `mass_values`. For each simulation, I calculate the standard deviation value and store all those standard deviation values in the `df3` DataFrame.->df3['degree_of_freedom'] = df3['degree_of_freedom'].astype(int)",
        "pred": [
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 1: Create an outcome variable->df['home_win'] = df.home_points &gt; df.away_points",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->pca_model = PCA(dta.T, standardize=False, demean=True)"
        ]
    },
    {
        "id": "1328929",
        "GT": "candperc = pd.DataFrame({c : OH[c] / OH[u'Total Voters'] for c in candidates})\ncandperc[['County Name','Precinct Name','Total Voters']] = OH[['County Name','Precinct Name','Total Voters']]\ncandperc = candperc.set_index(['County Name','Precinct Name'])",
        "pred": [
            "scipy->Sparse eigenvalue problems with ARPACK->Use of LinearOperator->N = 21\n D = np.diag(0.5*np.ones(N-1), k=1) - np.diag(0.5*np.ones(N-1), k=-1)\n D[0] = D[-1] = 0 # take away edge effects\n Dop = FirstDerivative(N, dtype=np.float64)",
            "pandas_toms_blog->Scaling->Dask->daily = (\n    indiv[['transaction_dt', 'transaction_amt']].dropna()\n        .set_index('transaction_dt')['transaction_amt']\n        .resample(\"D\")\n        .sum()\n).compute()\ndaily",
            "torch->Text->Language Modeling with nn.Transformer and TorchText->Evaluate the best model on the test dataset->test_loss = evaluate(model, )\ntest_ppl = math.exp(test_loss)\nprint('=' * 89)\nprint(f'| End of training | test loss {test_loss:5.2f} | '\n      f'test ppl {test_ppl:8.2f}')\nprint('=' * 89)",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->tidy = pd.melt(games.reset_index(),\n               id_vars=['game_id', 'date'], value_vars=['away_team', 'home_team'],\n               value_name='team')\ntidy.head()",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Exercise: Logit vs Probit->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nsupport = np.linspace(-6, 6, 1000)\nax.plot(support, stats.logistic.cdf(support), \"r-\", label=\"Logistic\")\nax.plot(support, stats.norm.cdf(support), label=\"Probit\")\nax.legend()"
        ]
    },
    {
        "id": "1203352",
        "GT": "Model 4 - Multiple Large Layers\nWe will add two layers with a size of 1000 each.-># Establish and fit the model, with default settings.\nmlp4 = MLPClassifier(activation='logistic', hidden_layer_sizes=(1000, 1000))\nmlp4.fit(X, Y)",
        "pred": [
            "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Fine-tuning HF T5->def setup_model(model_name):\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    tokenizer =  T5Tokenizer.from_pretrained(model_name)\n    return model, tokenizer",
            "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding->from patsy.contrasts import Treatment\n\nlevels = [1, 2, 3, 4]\ncontrast = Treatment(reference=0).code_without_intercept(levels)\nprint(contrast.matrix)",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "sklearn->Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Evaluation->from sklearn import metrics\n\nY_pred = rbm_features_classifier.predict(X_test)\nprint(\n    \"Logistic regression using RBM features:\\n%s\\n\"\n    % ((Y_test, Y_pred))\n)",
            "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding->Backward Difference Coding->from patsy.contrasts import Diff\n\ncontrast = Diff().code_without_intercept(levels)\nprint(contrast.matrix)"
        ]
    },
    {
        "id": "809529",
        "GT": "Mean Value Baseline\n\nAs a baseline for the facial keypoints RMSE, we\n\n1. Take only the images that have all 30 labels\n2. Calculate the average value of each label using 80% of the images from 1.\n3. Compare the average values from 2. with the true values of the labels in the remaining 20% of the images from 1.\n\nThe RMSE from the comparison in 3. is *3.1379662*. This is better than we thought it would be given the 0 to 96 range of the labels. Though when looking at the worst cases, it is clear the baseline is not a great fit.\n\nThe kaggle reported \"Averages Benchmark\" is 3.96242. This higher number probably reflects the fact that there are more different images in the holdout set for the leaderboar, whereas the images that contain all their lablels likely were coded similarly and are the most clean.\n\nThe code for this baseline calculation is below.->Create Mean Predictions->y_pred = np.tile(np.mean(y_train, axis=0),(y_test.shape[0], 1))",
        "pred": [
            "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Gaussian gradient magnitude method->x_ray_image_gaussian_gradient = ndimage.gaussian_gradient_magnitude(xray_image, sigma=2)",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->pca_model = PCA(dta.T, standardize=False, demean=True)",
            "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Model Support->Simulate Some Data->auto_reg_forecast = res.predict(200, 211)\nauto_reg_forecast",
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model->sm_probit_canned = sm.Probit(endog, exog).fit()",
            "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->n_neighbors = 12  # neighborhood which is used to recover the locally linear structure\nn_components = 2  # number of coordinates for the manifold"
        ]
    },
    {
        "id": "1328995",
        "GT": "Linear Regression->The Example Data\n\nFirst, let's just look at the data, in both a table and graphed as a scatter plot.->def setax(ax):\n    ax.set_title(\"Height vs Weight\", fontsize=20)\n    ax.set_xlabel(\"Height\", fontsize=15)\n    ax.set_ylabel(\"Weight\", fontsize=15)",
        "pred": [
            "matplotlib->Tutorials->Intermediate->Arranging multiple Axes in a Figure->High-level methods for making grids->Basic 2x2 grid->def annotate_axes(ax, text, fontsize=18):\n    ax.text(0.5, 0.5, text, transform=ax.transAxes,\n            ha=\"center\", va=\"center\", fontsize=fontsize, color=\"darkgrey\")",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Histogram of standardized deviance residuals with Kernel Density Estimate overlaid->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, title=\"Standardized Deviance Residuals\")\nax.hist(resid_std, bins=25, density=True)\nax.plot(kde_resid.support, kde_resid.density, \"r\")",
            "torch->Introduction to PyTorch->Quickstart->Optimizing the Model Parameters->epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))"
        ]
    },
    {
        "id": "1166229",
        "GT": "\n#the residual has a lot of NaN Values, eliminating all of them\nress1 = ress1.fillna(0)\n\n#assigning index values to train normal data\n\ncd1 = np.arange(35000,59991)\ncd = np.arange(0,13500)\nc = np.concatenate((cd,cd1))",
        "pred": [
            "scipy->Fourier Transforms (scipy.fft)->Discrete Cosine Transforms->DCT and IDCT-># Normalized inverse: no scaling factor\n idct(dct(x, type=2), type=2)\narray([ 1. ,  2. ,  1. , -1. ,  1.5])",
            "sklearn->Examples->Generalized Linear Models->Lasso and Elastic Net for Sparse Signals->ElasticNet->ElasticNet(alpha=0.1, l1_ratio=0.7)\nr^2 on test data : 0.642515",
            "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features->cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Fusing Convolution with Batch Norm->traced_model = (model)\nprint(traced_model.graph)"
        ]
    },
    {
        "id": "256272",
        "GT": "from IPython.display import HTML\ns = \"\"\"<table>\n<tr>\n<th>Model</th>\n<th>RLMSE</th>\n<th># exact predictions</th>\n<th>most extreme difference</th>\n</tr>\n<tr>\n<td>GradientBoostingRegressor</td>\n<td>0.705</td>\n<td>31</td>\n<td>440</td>\n</tr>\n<tr>\n<td>KNeighboursRegressor</td>\n<td>1.027</td>\n<td>13</td>\n<td>598</td>\n</tr>\n<tr>\n<td>RandomForestRegressor</td>\n<td>0.319</td>\n<td>67</td>\n<td>456</td>\n</tr>\n<tr>\n<td>AdaBoostRegressor</td>\n<td>0.799</td>\n<td>11</td>\n<td>589</td>\n</tr>\n<tr>\n<td>DecisionTreeRegressor</td>\n<td>0.394</td>\n<td>57</td>\n<td>603</td>\n</tr>\n<tr>\n<td>BaggingRegressor</td>\n<td>0.315</td>\n<td>77</td>\n<td>442</td>\n</tr>\n</table>\"\"\"\nh = HTML(s); h",
        "pred": [
            "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution->from sklearn.linear_model import \n\n\nmask_train = df_train[\"ClaimAmount\"]  0\nmask_test = df_test[\"ClaimAmount\"]  0\n\nglm_sev = (alpha=10.0, solver=\"newton-cholesky\")\n\nglm_sev.fit(\n    X_train[mask_train.values],\n    df_train.loc[mask_train, \"AvgClaimAmount\"],\n    sample_weight=df_train.loc[mask_train, \"ClaimNb\"],\n)\n\nscores = score_estimator(\n    glm_sev,\n    X_train[mask_train.values],\n    X_test[mask_test.values],\n    df_train[mask_train],\n    df_test[mask_test],\n    target=\"AvgClaimAmount\",\n    weights=\"ClaimNb\",\n)\nprint(\"Evaluation of GammaRegressor on target AvgClaimAmount\")\nprint(scores)",
            "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Model comparison->from sklearn.model_selection import \nimport matplotlib.pyplot as plt\n\nscoring = \"neg_mean_absolute_percentage_error\"\nn_cv_folds = 3\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\n\ndef plot_results(figure_title):\n    fig, (ax1, ax2) = (1, 2, figsize=(12, 8))\n\n    plot_info = [\n        (\"fit_time\", \"Fit times (s)\", ax1, None),\n        (\"test_score\", \"Mean Absolute Percentage Error\", ax2, None),\n    ]\n\n    x, width = (4), 0.9\n    for key, title, ax, y_limit in plot_info:\n        items = [\n            dropped_result[key],\n            one_hot_result[key],\n            ordinal_result[key],\n            native_result[key],\n        ]\n\n        mape_cv_mean = [(np.abs(item)) for item in items]\n        mape_cv_std = [(item) for item in items]\n\n        ax.bar(\n            x=x,\n            height=mape_cv_mean,\n            width=width,\n            yerr=mape_cv_std,\n            color=[\"C0\", \"C1\", \"C2\", \"C3\"],\n        )\n        ax.set(\n            xlabel=\"Model\",\n            title=title,\n            xticks=x,\n            xticklabels=[\"Dropped\", \"One Hot\", \"Ordinal\", \"Native\"],\n            ylim=y_limit,\n        )\n    fig.suptitle(figure_title)\n\n\nplot_results(\"Gradient Boosting on Ames Housing\")\n\n\n<img alt=\"Gradient Boosting on Ames Housing, Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\"/>",
            "statsmodels->Examples->Statistics->ANOVA->from urllib.request import urlopen\nimport numpy as np\n\nnp.set_printoptions(precision=4, suppress=True)\n\nimport pandas as pd\n\npd.set_option(\"display.width\", 100)\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\nfrom statsmodels.graphics.api import interaction_plot, abline_plot\nfrom statsmodels.stats.anova import anova_lm\n\ntry:\n    salary_table = pd.read_csv(\"salary.table\")\nexcept:  # recent pandas can read URL without urlopen\n    url = \"http://stats191.stanford.edu/data/salary.table\"\n    fh = urlopen(url)\n    salary_table = pd.read_table(fh)\n    salary_table.to_csv(\"salary.table\")\n\nE = salary_table.E\nM = salary_table.M\nX = salary_table.X\nS = salary_table.S",
            "sklearn->Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Visualize the learning curves->from sklearn.model_selection import LearningCurveDisplay\n\n_, ax = ()\n\nsvr = (kernel=\"rbf\", C=1e1, gamma=0.1)\nkr = (kernel=\"rbf\", alpha=0.1, gamma=0.1)\n\ncommon_params = {\n    \"X\": X[:100],\n    \"y\": y[:100],\n    \"train_sizes\": (0.1, 1, 10),\n    \"scoring\": \"neg_mean_squared_error\",\n    \"negate_score\": True,\n    \"score_name\": \"Mean Squared Error\",\n    \"std_display_style\": None,\n    \"ax\": ax,\n}\n\n(svr, **common_params)\n(kr, **common_params)\nax.set_title(\"Learning curves\")\nax.legend(handles=ax.get_legend_handles_labels()[0], labels=[\"SVR\", \"KRR\"])\n\n()\n\n\n<img alt=\"Learning curves\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\" srcset=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\"/>",
            "torch->Text->Language Translation with nn.Transformer and torchtext->Collation->from timeit import default_timer as timer\nNUM_EPOCHS = 18\n\nfor epoch in range(1, NUM_EPOCHS+1):\n    start_time = timer()\n    train_loss = train_epoch(transformer, optimizer)\n    end_time = timer()\n    val_loss = evaluate(transformer)\n    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n\n\n# function to generate output sequence using greedy algorithm\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n\n    memory = model.encode(src, src_mask)\n    ys = (1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n    for i in range(max_len-1):\n        memory = memory.to(DEVICE)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                    .type(torch.bool)).to(DEVICE)\n        out = model.decode(ys, memory, tgt_mask)\n        out = out.transpose(0, 1)\n        prob = model.generator(out[:, -1])\n        _, next_word = (prob, dim=1)\n        next_word = next_word.item()\n\n        ys = ([ys,\n                        (1, 1).type_as(src.data).fill_(next_word)], dim=0)\n        if next_word == EOS_IDX:\n            break\n    return ys\n\n\n# actual function to translate input sentence into target language\ndef translate(model: , src_sentence: str):\n    model.eval()\n    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n    num_tokens = src.shape[0]\n    src_mask = ((num_tokens, num_tokens)).type(torch.bool)\n    tgt_tokens = greedy_decode(\n        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"&lt;bos&gt;\", \"\").replace(\"&lt;eos&gt;\", \"\")"
        ]
    },
    {
        "id": "364571",
        "GT": "Code 5.33->sum_blbr = trace_5_8['br'] + trace_5_8['bl']\npm.kdeplot(sum_blbr);",
        "pred": [
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team->g = sns.FacetGrid(wins.reset_index(), hue='team', size=7, aspect=.5, palette=['k'])\ng.map(sns.pointplot, 'is_home', 'win_pct').set(ylim=(0, 1));",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->respondent1000 = dta.iloc[1000]\nprint(respondent1000)",
            "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data->dta.plot(figsize=(12,4));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_arma_0_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_arma_0_9_0.png\"/>",
            "sklearn->Examples->Clustering->Segmenting the picture of greek coins in regions->Spectral clustering: kmeans, 2.04s\nSpectral clustering: discretize, 1.83s\nSpectral clustering: cluster_qr, 1.82s"
        ]
    },
    {
        "id": "1239657",
        "GT": "Monte Carlo Assignment Codes->Calculate pi using Monte Carlo Integration.\n\nRandom samples will be taken in a quarter circle of unit radius. \n\npi = 4 * (hits/shots)->First, we'll use a loop (slower performance)->def calculate_pi_loop(N): \n    hits = np.zeros(N)\n    for i in range(N): \n        point = np.random.rand(2)\n        dist = point[0]**2 + point[1]**2\n        hits[i] = dist < 1\n    return 4*hits.sum()/N",
        "pred": [
            "statsmodels->Examples->Statistics->Mediation analysis with duration data->def gen_outcome(otype, mtime0):\n    if otype == \"full\":\n        lp = 0.5 * mtime0\n    elif otype == \"no\":\n        lp = exp\n    else:\n        lp = exp + mtime0\n    mn = np.exp(-lp)\n    ytime0 = -mn * np.log(np.random.uniform(size=n))\n    ctime = -2 * mn * np.log(np.random.uniform(size=n))\n    ystatus = (ctime = ytime0).astype(int)\n    ytime = np.where(ytime0 &lt;= ctime, ytime0, ctime)\n    return ytime, ystatus",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Training the Network->def loss_f(A, Y):\n    # define value of epsilon to prevent zero division error inside a log\n    epsilon = 1e-5\n    # Implement formula for negative log likelihood\n    loss = (- Y * np.log(A + epsilon)\n            - (1 - Y) * np.log(1 - A + epsilon))\n    # Return loss\n    return np.squeeze(loss)",
            "statsmodels->Examples->Statistics->Mediation analysis with duration data->def gen_mediator():\n    mn = np.exp(exp)\n    mtime0 = -mn * np.log(np.random.uniform(size=n))\n    ctime = -2 * mn * np.log(np.random.uniform(size=n))\n    mstatus = (ctime = mtime0).astype(int)\n    mtime = np.where(mtime0 &lt;= ctime, mtime0, ctime)\n    return mtime0, mtime, mstatus",
            "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)->def frame_preprocessing(observation_frame):\n    # Crop the frame.\n    observation_frame = observation_frame[35:195]\n    # Downsample the frame by a factor of 2.\n    observation_frame = observation_frame[::2, ::2, 0]\n    # Remove the background and apply other enhancements.\n    observation_frame[observation_frame == 144] = 0  # Erase the background (type 1).\n    observation_frame[observation_frame == 109] = 0  # Erase the background (type 2).\n    observation_frame[observation_frame != 0] = 1  # Set the items (rackets, ball) to 1.\n    # Return the preprocessed frame as a 1D floating-point array.\n    return observation_frame.astype(float)",
            "pandas_toms_blog->Indexes->def get_ids(network):\n    url = \"http://mesonet.agron.iastate.edu/geojson/network.php?network={}\"\n    r = requests.get(url.format(network))\n    md = pd.io.json.json_normalize(r.json()['features'])\n    md['network'] = network\n    return md"
        ]
    },
    {
        "id": "967874",
        "GT": "Principal component analysis\n\nWe extract all voxels for each subject and region directly from the nifti images and the masks generated from Freesurfer.\n\nWe use principal component analysis to transform each subject into the vector space spanned by the eigenvectors of the covariance matrix. The plots show the components along each axis in this subspace for each patient. The axis are ordered according to the value of the eigenvectors.\n\nWe choose a few statistical quantities to describe each region->masked_region_df_pbr['subject_id'] = pd.Categorical(masked_region_df_pbr['subject_id'], pca_results.subject_id)",
        "pred": [
            "numpy->NumPy Features->Masked Arrays->Missing data->china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison->results_all = [res_o, res_f, res_e, res_a]\nnames = \"res_o res_f res_e res_a\".split()",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->means25[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.25)\nprint(means25)",
            "sklearn->Examples->Decomposition->Kernel PCA->Projecting into the original feature space->X_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))\nX_reconstructed_kernel_pca = kernel_pca.inverse_transform(kernel_pca.transform(X_test))"
        ]
    },
    {
        "id": "1209585",
        "GT": "data.number_emergency.value_counts().head(20)",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.head(5)",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data->data.describe()",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data->data = sm.datasets.fair.load_pandas().data",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.dtypes",
            "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data->res.forecast_components(12)"
        ]
    },
    {
        "id": "319711",
        "GT": "Pivot Table method->f = pd.read_csv('foods.csv')\nf.head()",
        "pred": [
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->data = sm.datasets.fertility.load_pandas().data\ndata.head()",
            "pandas_toms_blog->Indexes->weather = pd.read_hdf(\"weather.h5\", \"weather\").sort_index()\n\nweather.head()",
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples->tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples->tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n"
        ]
    },
    {
        "id": "59750",
        "GT": "df_mw['tot pop'] = np.round(df_mw['tot unemp'].values / (df_mw['tot unemp quote'].values/100)).astype(int)\ndf_mw.sort_values(by='canton',ascending=True).head()",
        "pred": [
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures->summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->tidy = pd.melt(games.reset_index(),\n               id_vars=['game_id', 'date'], value_vars=['away_team', 'home_team'],\n               value_name='team')\ntidy.head()",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]"
        ]
    },
    {
        "id": "603991",
        "GT": "Salary_Data[\"salary\"].iloc[6179] = (387500 + 350000) / 2\nSalary_Data[\"salary\"].iloc[12007] = 170000",
        "pred": [
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Remainder->Intercept        3.756282e-141\nrate_marriage     0.000000e+00\nage               2.221918e-11\nyrs_married       1.219200e-02\ndtype: float64",
            "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->sigma = 0.5 + X.ravel() / 10\nnoise = rng.lognormal(sigma=sigma) - (sigma**2 / 2)\ny = expected_y + noise",
            "sklearn->Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Dataset and Gaussian process generation->rng = (4)\nX_train = rng.uniform(0, 5, 10).reshape(-1, 1)\ny_train = ((X_train[:, 0] - 2.5) ** 2)\nn_samples = 5",
            "statsmodels->Examples->User Notes->Formulas->Multiplicative interactions->Literacy:Wealth    0.018176\ndtype: float64\n\nLiteracy           0.427386\nWealth             1.080987\nLiteracy:Wealth   -0.013609\ndtype: float64",
            "sklearn->Examples->Generalized Linear Models->Lasso on dense and sparse data->Comparing the two Lasso implementations on Sparse data->Matrix density : 0.626%\nSparse Lasso done in 0.117s\nDense Lasso done in  0.805s\nDistance between coefficients : 8.65e-12"
        ]
    },
    {
        "id": "700685",
        "GT": "Analysing \"How ISIS Uses Twitter\" using social network cluster analysis\n## Approach\nThe general approach will be to extract each unique user by their username to act as a node. \nThe username has been chosen as the associated data on each node as it is unique unlike names which\nmight not be. The scale of the node will be influenced by the a combination of the number of \nfollowers and the number of tweets they produce. This combination will ensure that active and \npopular users are identified rather than identifying those who tweet a lot and have a small amount \nof followers or vice versa.\n\nCurrently the relation between each user is yet to be decided, as well as the number of followers, \nwho those followers are would be useful to identify the relation between different users. \nOne relation criteria might be to scrape the tweets of users for mentions and then link nodes via \nthis metric with numerous mentions increasing the weight of an edge between two users. In this \ncontext two types of mentions could be identified, those that result in direct communication with \na user and those mentions that come from retweeting a user. The former could be combined with \nlanguage processing to determine the emotive qualities of the tweets to see if there are inner \nhostilities between ISIS supporters.->Who talks about who?\nNow we have separated the retweets and actual tweets and grouped them with their usernames, we can \nproceed to perform some analysis on who is talking about who! \n\nFirstly we iterate through each tweet and scrape mentioned usernames from them (where the user isn't\nmentioning themself). These usernames are then determined to either be users from within tweets.csv\nor not within tweets.csv. It is clear to see from the below bar chart that the majority of users \nmentioned are outside the scope of the dataset. This opens up another avenue \nfor producing the social graph where a graph of all users mentioned can be constructed with colour \ndefining those that are known (within the dataset) and those that aren't known (not within the \ndataset). For now we'll focus on those contained only within the dataset as this is a smaller set \nof users to work with.\n\nThe second chart shows how many users in tweets.csv have been mentioned by other users within\ntweets.csv. There seems to be a reasonable amount of communication between the users but what this\ncommunication is, is yet to be seen.->sender_count = Counter(in_set[:,0])\nreceiver_count = Counter(in_set[:,1])\ntop_5_senders = sender_count.most_common(5)\ntop_5_receivers = receiver_count.most_common(5)\n\nprint(top_5_senders)\nprint(top_5_receivers)",
        "pred": [
            "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Train tree classifier->iris = ()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = (X, y, random_state=0)\n\nclf = (max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)",
            "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression->model1 = sm.GLM.from_formula(\n    \"blotch ~ 0 + C(variety) + C(site)\", family=sm.families.Binomial(), data=df\n)\nresult1 = model1.fit(scale=\"X2\")\nprint(result1.summary())",
            "statsmodels->Examples->User Notes->Formulas->Multiplicative interactions->res1 = ols(formula=\"Lottery ~ Literacy : Wealth - 1\", data=df).fit()\nres2 = ols(formula=\"Lottery ~ Literacy * Wealth - 1\", data=df).fit()\nprint(res1.params, \"\\n\")\nprint(res2.params)",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->feature_names = model[:-1].get_feature_names_out()\n\ncoefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients\"],\n    index=feature_names,\n)\n\ncoefs\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Plotting subsets of data with semantic mappings->dots = sns.load_dataset(\"dots\").query(\"align == 'dots'\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\",\n    hue=\"coherence\", style=\"choice\",\n)\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Plotting subsets of data with semantic mappings->dots = sns.load_dataset(\"dots\").query(\"align == 'dots'\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\",\n    hue=\"coherence\", style=\"choice\",\n)\n"
        ]
    },
    {
        "id": "269949",
        "GT": "Crawler\n### Selangor Data\nOpen the json file that you want scrawl, we will be running selangor first:-># output\ndf.to_csv(pwd + 'output/selangor.csv', sep=';', index=False )",
        "pred": [
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "sklearn->Examples->Model Selection->Sample pipeline for text feature extraction and evaluation->Pipeline with hyperparameter tuning->from sklearn.feature_extraction.text import \nfrom sklearn.naive_bayes import \nfrom sklearn.pipeline import \n\npipeline = (\n    [\n        (\"vect\", ()),\n        (\"clf\", ()),\n    ]\n)\npipeline",
            "sklearn->Examples->Generalized Linear Models->Lasso model selection: AIC-BIC / cross-validation->Dataset->from sklearn.datasets import \n\nX, y = (return_X_y=True, as_frame=True)\nX.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ]
    },
    {
        "id": "424934",
        "GT": "Why numpy?\n\n-- Vectorization\n -ufunc() that are faster->np where\n\nnumpy.where(condition, x, y) - Return elements chosen from x or y depending on condition->import numpy as np\nimport pandas as pd\n\nd_data = {\n    'names' : ['tom' , 'tim', 'jim', 'ali'] ,\n    'age' : [12, 44, 56, 9]   \n}\n\ndf = pd.DataFrame(d_data)\ndf",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->ARMA: Artificial Data->import numpy as np\nimport pandas as pd\n\nfrom statsmodels.graphics.tsaplots import plot_predict\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.arima.model import ARIMA\n\nnp.random.seed(12345)",
            "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation->import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)",
            "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->import sys\nfrom time import \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.random_projection import \nfrom sklearn.random_projection import \nfrom sklearn.datasets import \nfrom sklearn.datasets import \nfrom sklearn.metrics.pairwise import euclidean_distances",
            "sklearn->Examples->Semi Supervised Classification->Label Propagation learning a complex structure->import numpy as np\nfrom sklearn.datasets import \n\nn_samples = 200\nX, y = (n_samples=n_samples, shuffle=False)\nouter, inner = 0, 1\nlabels = (n_samples, -1.0)\nlabels[0] = outer\nlabels[-1] = inner",
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)"
        ]
    },
    {
        "id": "381195",
        "GT": "Adam Ruby - HiPy Nov 1st Registration Competition->\nreg_labels = reg_list[0]\n\nreg_list = reg_list[1:]\nreg_list[:5]  #To view first five rows as example",
        "pred": [
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->feature_names = model[:-1].get_feature_names_out()\n\ncoefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients\"],\n    index=feature_names,\n)\n\ncoefs\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots->tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots->tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-based cross-validation->all_splits = list(ts_cv.split(X, y))\ntrain_0, test_0 = all_splits[0]",
            "statsmodels->Examples->Linear Regression Models->Regression Diagnostics->Linearity->name = [\"t value\", \"p value\"]\ntest = sms.linear_harvey_collier(results)\nlzip(name, test)",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n"
        ]
    },
    {
        "id": "93910",
        "GT": "Gender Classification by Voice->Data Overview->voices.isnull().values.any()",
        "pred": [
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time.unique()",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_mask.nonzero()",
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time.head()",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->delays.nsmallest(5).sort_values()",
            "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data->res.forecast_components(12)"
        ]
    },
    {
        "id": "1508176",
        "GT": "Worst categories in inception compared to resnet->breed_acc_df.sort_values(by='inception_accuracy', ascending=True).head(10).plot.barh(figsize=(8, 10))",
        "pred": [
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Pandas Built-in Plotting->df.plot.scatter(x='carat', y='depth', c='k', alpha=.15)\nplt.tight_layout()",
            "statsmodels->Examples->User Notes->Dates in Time-Series Models->Using Pandas->fig = pandas_ar_res.plot_predict(start=\"2005\", end=\"2027\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\" src=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\"/>",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production->fig = res_glob.plot_predict(start=714, end=732)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\"/>",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->scores = pd.DataFrame(est.cv_results_)\nscores.head()"
        ]
    },
    {
        "id": "70396",
        "GT": "Q2 Answer: How to Train your Dragon 2 with highest coefficient of correlation of 0.19728895166240468->Question 3 (Bonus): Ratings for rest of unseen movies->### Create personal ratings vector\nmy_ratings = M_copy.ix['Daniel Lee']\nmy_ratings[np.isnan(my_ratings)] = 0\nmy_ratings",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->Bayesian estimation-># Set sampling params\nndraws = 3000  #  3000 number of draws from the distribution\nnburn = 600  # 600 number of \"burn-in points\" (which will be discarded)",
            "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding->Backward Difference Coding->from patsy.contrasts import Diff\n\ncontrast = Diff().code_without_intercept(levels)\nprint(contrast.matrix)"
        ]
    },
    {
        "id": "1382167",
        "GT": "change yes no to ones and zeros->X['international_plan'] = pd.get_dummies(X.international_plan)[' yes']\nX['voice_mail'] = pd.get_dummies(X.voice_mail)[' yes']\n#pd.get_dummies(X.voice_mail)",
        "pred": [
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog->data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)",
            "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.1->Grouping infrequent categories in OneHotEncoder->encoded = enc.transform(([[\"dog\"], [\"snake\"], [\"cat\"], [\"rabbit\"]]))\n(encoded, columns=enc.get_feature_names_out())\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()"
        ]
    },
    {
        "id": "833317",
        "GT": "Off-grid->tstrt = 20.00 * 60  # time in minutes at which the battery inverter is switched on\nratingPV = 6000.  # W minimum installed panel capacity\nbatvolt = 48 # nominal battery voltage\nbatCap = 800. * batvolt  # Wh  battery capacity in Ah * voltage\nbatDoD = 0.27 # depth of dischage\nbatACEff = 0.9 # battery discharge efficiency only this much is available as AC power\nbatDCEff = 0.9 # battery charge efficiency only this much is available as charge\nmaxInverter = 6000  # W maximum that the inverter can supply\nmaxBatCharge = 40 # maximum battery charge current\n\ncalcProfile(tstrt, ratingPV, batvolt, batCap, batDoD, batACEff, batDCEff,\n            maxInverter,maxBatCharge,reqAC,removedAC,cost=1.5, storedinGrid=False);",
        "pred": [
            "pandas_toms_blog->Indexes->stations = pd.io.json.json_normalize(js['features']).id\nurl = (\"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n       \"&amp;data=tmpf&amp;data=relh&amp;data=sped&amp;data=mslp&amp;data=p01i&amp;data=vsby&amp;data=gust_mph&amp;data=skyc1&amp;data=skyc2&amp;data=skyc3\"\n       \"&amp;tz=Etc/UTC&amp;format=comma&amp;latlon=no\"\n       \"&amp;{start:year1=%Y&amp;month1=%m&amp;day1=%d}\"\n       \"&amp;{end:year2=%Y&amp;month2=%m&amp;day2=%d}&amp;{stations}\")\nstations = \"&amp;\".join(\"station=%s\" % s for s in stations)\nstart = pd.Timestamp('2014-01-01')\nend=pd.Timestamp('2014-01-31')\n\nweather = (pd.read_csv(url.format(start=start, end=end, stations=stations),\n                       comment=\"#\"))",
            "matplotlib->Tutorials->Colors->Colormap Normalization->Centered->delta = 0.1\nx = np.arange(-3.0, 4.001, delta)\ny = np.arange(-4.0, 3.001, delta)\nX, Y = np.meshgrid(x, y)\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (0.9*Z1 - 0.5*Z2) * 2\n\n# select a divergent colormap\ncmap = cm.coolwarm\n\nfig, (ax1, ax2) = plt.subplots(ncols=2)\npc = ax1.pcolormesh(Z, cmap=cmap)\nfig.colorbar(pc, ax=ax1)\nax1.set_title('Normalize()')\n\npc = ax2.pcolormesh(Z, norm=colors.CenteredNorm(), cmap=cmap)\nfig.colorbar(pc, ax=ax2)\nax2.set_title('CenteredNorm()')\n\nplt.show()\n\n\n<img alt=\"Normalize(), CenteredNorm()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_002.png\" srcset=\"../../_images/sphx_glr_colormapnorms_002.png, ../../_images/sphx_glr_colormapnorms_002_2_0x.png 2.0x\"/>",
            "numpy->NumPy Applications->Plotting Fractals->Julia set->output = julia(mesh, c=-0.75 + 0.4j, num_iter=20)\nkwargs = {'title': 'f(z) = z^2 - \\dfrac{3}{4} + 0.4i', 'cmap': 'Greens_r'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/bfb120678f65f1892658c8e3121fc93f5e95b61f9b4acd4ff184d041d5903eac.png\" src=\"../_images/bfb120678f65f1892658c8e3121fc93f5e95b61f9b4acd4ff184d041d5903eac.png\"/>",
            "numpy->NumPy Applications->Plotting Fractals->Mandelbrot set->output = mandelbrot(mesh, num_iter=50)\nkwargs = {'title': 'Mandelbrot \\ set', 'cmap': 'hot'}\n\nplot_fractal(output, **kwargs);\n\n\n\n\n<img alt=\"../_images/20dd449f2a6134d4842d5337133c4150c22d59046f8a6076cf707296b245644c.png\" src=\"../_images/20dd449f2a6134d4842d5337133c4150c22d59046f8a6076cf707296b245644c.png\"/>",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings->data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>"
        ]
    },
    {
        "id": "901914",
        "GT": "The Saltenis estimator->It is then time to define the sample and scrambled matrices->p = 13\nrun = 50\n\nn = [3,4,6]\n\ndf = pd.DataFrame(sobol_seq.i4_sobol_generate(6*k, run*2**p))",
        "pred": [
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "sklearn->Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Dataset and Gaussian process generation->rng = (4)\nX_train = rng.uniform(0, 5, 10).reshape(-1, 1)\ny_train = ((X_train[:, 0] - 2.5) ** 2)\nn_samples = 5",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "statsmodels->Examples->User Notes->Formulas->Multiplicative interactions->res1 = ols(formula=\"Lottery ~ Literacy : Wealth - 1\", data=df).fit()\nres2 = ols(formula=\"Lottery ~ Literacy * Wealth - 1\", data=df).fit()\nprint(res1.params, \"\\n\")\nprint(res2.params)"
        ]
    },
    {
        "id": "860767",
        "GT": "2-  Which teams pay higher salaries based on performance? Which teams pay higher for its players?  Which teams tend to over-pay their players?->Median_Salary_Sorted = Median.sort_values('Salary', ascending=False)\nMedian_Salary_Sorted.Salary.plot(kind='bar', figsize = (10, 5))\nplt.ylabel('Median Salary per team (in terms of $10M)')",
        "pred": [
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "scipy->Sparse eigenvalue problems with ARPACK->Use of LinearOperator->N = 21\n D = np.diag(0.5*np.ones(N-1), k=1) - np.diag(0.5*np.ones(N-1), k=-1)\n D[0] = D[-1] = 0 # take away edge effects\n Dop = FirstDerivative(N, dtype=np.float64)",
            "sklearn->Examples->Classification->Plot classification probability->Accuracy (train) for L1 logistic: 83.3%\nAccuracy (train) for L2 logistic (Multinomial): 82.7%\nAccuracy (train) for L2 logistic (OvR): 79.3%\nAccuracy (train) for Linear SVC: 82.0%\nAccuracy (train) for GPC: 82.7%\n\n\n\n<br/>",
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian Processes regression: basic introductory example->Example with noisy targets->gaussian_process = (\n    kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=9\n)\ngaussian_process.fit(X_train, y_train_noisy)\nmean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)"
        ]
    },
    {
        "id": "657375",
        "GT": "7. Visualize Relationships Between Numerical and Categorical Attributes->Does having online backup service increase monthly charges?-># Jitter plot\n# How does online backup service affect monthly charges?\nfrom plotnine import *\n(ggplot(df_churn, aes(x='OnlineBackup', y='MonthlyCharges')) + geom_jitter(position=position_jitter(0.4)))",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Dataset\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\ndata['ln_wpi'] = np.log(data['wpi'])\ndata['D.ln_wpi'] = data['ln_wpi'].diff()\n\n\n\n\n\n\n\nIn\u00a0[5]:",
            "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Transformer Block With a Novel Normalization-># Profile rms_norm\nfunc = functools.partial(\n    with_rms_norm,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"Eager Mode - RMS Norm\")",
            "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Appendix 1: Extending the dynamic factor model-># Create the model\nextended_mod = ExtendedDFM(endog)\ninitial_extended_res = extended_mod.fit(maxiter=1000, disp=False)\nextended_res = extended_mod.fit(initial_extended_res.params, method='nm', maxiter=1000)\nprint(extended_res.summary(separate_params=False))",
            "torch->Image and Video->Adversarial Example Generation->Results->Sample Adversarial Examples-># Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()\n\n\n<img alt=\"2 -&gt; 2, 6 -&gt; 6, 4 -&gt; 4, 6 -&gt; 6, 2 -&gt; 2, 8 -&gt; 3, 4 -&gt; 2, 9 -&gt; 5, 7 -&gt; 9, 8 -&gt; 9, 6 -&gt; 0, 1 -&gt; 8, 6 -&gt; 8, 9 -&gt; 8, 8 -&gt; 6, 5 -&gt; 8, 5 -&gt; 2, 4 -&gt; 8, 4 -&gt; 8, 5 -&gt; 8, 3 -&gt; 5, 4 -&gt; 8, 8 -&gt; 2, 8 -&gt; 6, 3 -&gt; 8, 6 -&gt; 1, 1 -&gt; 3, 1 -&gt; 8, 4 -&gt; 8, 8 -&gt; 2, 7 -&gt; 8, 1 -&gt; 2, 0 -&gt; 2, 7 -&gt; 8, 7 -&gt; 8\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_002.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_002.png\"/>",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>"
        ]
    },
    {
        "id": "995131",
        "GT": "sns.barplot(x='Month',y='RSPM/PM10',data=df[df['City/Town/Village/Area']=='Bongaigaon']).set_title('Mean RSPM/PM10 level in Bonaigaon')",
        "pred": [
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Plots for categorical data->sns.catplot(data=tips, kind=\"violin\", x=\"day\", y=\"total_bill\", hue=\"smoker\", split=True)\n",
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations->sns.displot(data=tips, kind=\"ecdf\", x=\"total_bill\", col=\"time\", hue=\"smoker\", rug=True)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", thresh=.2, levels=4)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", thresh=.2, levels=4)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics->sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"density\", common_norm=False)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics->sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"density\", common_norm=False)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", levels=[.01, .05, .1, .8])\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", levels=[.01, .05, .1, .8])\n"
        ]
    },
    {
        "id": "715594",
        "GT": "Rolling mean with window of one day:->ax = df_count.rolling(96).mean().plot()\nplt.show()",
        "pred": [
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Pandas Built-in Plotting->df.plot.scatter(x='carat', y='depth', c='k', alpha=.15)\nplt.tight_layout()",
            "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure->ax = res.plot_predict(24, theta=2)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_20_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_20_0.png\"/>",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()",
            "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions->Violinplots->g = sns.catplot(data=tips, x=\"day\", y=\"total_bill\", kind=\"violin\", inner=None)\nsns.swarmplot(data=tips, x=\"day\", y=\"total_bill\", color=\"k\", size=3, ax=g.ax)\n",
            "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions->Violinplots->g = sns.catplot(data=tips, x=\"day\", y=\"total_bill\", kind=\"violin\", inner=None)\nsns.swarmplot(data=tips, x=\"day\", y=\"total_bill\", color=\"k\", size=3, ax=g.ax)\n"
        ]
    },
    {
        "id": "1054586",
        "GT": "sns.distplot(cdf['Fare'], bins=5, kde=False)\nplt.show()",
        "pred": [
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations->sns.displot(data=tips, kind=\"ecdf\", x=\"total_bill\", col=\"time\", hue=\"smoker\", rug=True)\n",
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Plots for categorical data->sns.catplot(data=tips, kind=\"violin\", x=\"day\", y=\"total_bill\", hue=\"smoker\", split=True)\n",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics->sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"density\", common_norm=False)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics->sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"density\", common_norm=False)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", thresh=.2, levels=4)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", thresh=.2, levels=4)\n"
        ]
    },
    {
        "id": "969048",
        "GT": "for c in ['Gold', 'Silver', 'Bronze', 'Total']:\n    dfn[c]=dfn[c].astype(int)",
        "pred": [
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_wide_list = [col for _, col in flights_wide.items()]\nsns.relplot(data=flights_wide_list, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_wide_list = [col for _, col in flights_wide.items()]\nsns.relplot(data=flights_wide_list, kind=\"line\")\n",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Products with n-dimensional arrays->Sigma = np.zeros((3, 768, 1024))\nfor j in range(3):\n    np.fill_diagonal(Sigma[j, :, :], s[j, :])",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->X = (pd.concat([y.shift(i) for i in range(6)], axis=1,\n               keys=['y'] + ['L%s' % i for i in range(1, 6)])\n       .dropna())\nX.head()"
        ]
    },
    {
        "id": "1514550",
        "GT": "import pymc3 as pm  \nfrom scipy.optimize import fmin_powell",
        "pred": [
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team->import statsmodels.formula.api as sm\n\ndf['home_win'] = df.home_win.astype(int)  # for statsmodels",
            "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Nelder-Mead Simplex algorithm (method='Nelder-Mead')->import numpy as np\n from scipy.optimize import minimize",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n",
            "sklearn->Examples->Model Selection->Plotting Cross-Validated Predictions->from sklearn.datasets import \nfrom sklearn.linear_model import \n\nX, y = (return_X_y=True)\nlr = ()",
            "sklearn->Examples->Model Selection->Plotting Learning Curves and Checking Models' Scalability->from sklearn.datasets import \nfrom sklearn.linear_model import \n\nX, y = (return_X_y=True)\nlr = ()",
            "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->import torch\nimport torch.fx\nimport torchvision.models as models\n\nrn18 = ()\n()"
        ]
    },
    {
        "id": "87548",
        "GT": "Data Cleaning->Convert to `str`  \n    i. PatientId  \n    ii. AppointmentID->noshowappointments['PatientId'] = str(noshowappointments['PatientId'])\nnoshowappointments['AppointmentID'] = str(noshowappointments['AppointmentID'])",
        "pred": [
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding->training_labels = one_hot_encoding(y_train[:training_sample])\ntest_labels = one_hot_encoding(y_test[:test_sample])",
            "torch->PyTorch Recipes->PyTorch Benchmark->Steps->4. Benchmarking with Blocked Autorange->m0 = t0.blocked_autorange()\nm1 = t1.blocked_autorange()\n\nprint(m0)\nprint(m1)\n\n\n\nOutput",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA->sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()",
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the image data to the floating-point format->print(\"The data type of training images: {}\".format(x_train.dtype))\nprint(\"The data type of test images: {}\".format(x_test.dtype))"
        ]
    },
    {
        "id": "1243566",
        "GT": "import pandas as pd\n\n# #downloading dataset\n## !wget -nv -O /resources/data/china_gdp.csv https://ibm.box.com/shared/static/ccd2tu4wvkwi1f6yp4mm1ztsyt1ygphv.csv\n# df = pd.read_csv(\"/resources/data/china_gdp.csv\")\n\n# JAG this is to be switche with DSWB path\ndf = pd.read_csv(\"https://ibm.box.com/shared/static/7tr6tai74kdk3vik815k2frl54kfuw4h.csv\")\n\ndf.head()",
        "pred": [
            "sklearn->Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Plot the BIC scores->import pandas as pd\n\ndf = (grid_search.cv_results_)[\n    [\"param_n_components\", \"param_covariance_type\", \"mean_test_score\"]\n]\ndf[\"mean_test_score\"] = -df[\"mean_test_score\"]\ndf = df.rename(\n    columns={\n        \"param_n_components\": \"Number of components\",\n        \"param_covariance_type\": \"Type of covariance\",\n        \"mean_test_score\": \"BIC score\",\n    }\n)\ndf.sort_values(by=\"BIC score\").head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->import pandas as pd\n\nresults_df = (search.cv_results_)\nresults_df = results_df.sort_values(by=[\"rank_test_score\"])\nresults_df = results_df.set_index(\n    results_df[\"params\"].apply(lambda x: \"_\".join(str(val) for val in x.values()))\n).rename_axis(\"kernel\")\nresults_df[[\"params\", \"rank_test_score\", \"mean_test_score\", \"std_test_score\"]]\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types->import pandas as pd\n\ncv_results = (search_cv.cv_results_)\ncv_results = cv_results.sort_values(\"mean_test_score\", ascending=False)\ncv_results[\n    [\n        \"mean_test_score\",\n        \"std_test_score\",\n        \"param_preprocessor__num__imputer__strategy\",\n        \"param_preprocessor__cat__selector__percentile\",\n        \"param_classifier__C\",\n    ]\n].head(5)\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "scipy->Linear Algebra (scipy.linalg)->Decompositions->Singular value decomposition->import numpy as np\n from scipy import linalg\n A = np.array([[1,2,3],[4,5,6]])\n A\narray([[1, 2, 3],\n      [4, 5, 6]])\n M,N = A.shape\n U,s,Vh = linalg.svd(A)\n Sig = linalg.diagsvd(s,M,N)\n U, Vh = U, Vh\n U\narray([[-0.3863177 , -0.92236578],\n      [-0.92236578,  0.3863177 ]])\n Sig\narray([[ 9.508032  ,  0.        ,  0.        ],\n      [ 0.        ,  0.77286964,  0.        ]])\n Vh\narray([[-0.42866713, -0.56630692, -0.7039467 ],\n      [ 0.80596391,  0.11238241, -0.58119908],\n      [ 0.40824829, -0.81649658,  0.40824829]])\n U.dot(Sig.dot(Vh)) #check computation\narray([[ 1.,  2.,  3.],\n      [ 4.,  5.,  6.]])\n\n\n<aside class=\"footnote-list brackets\">\n<aside class=\"footnote brackets\" id=\"id3\" role=\"note\">\n[1]",
            "sklearn->Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors->Models robustness to recover the ground truth weights->Fit the regressors->import pandas as pd\nfrom sklearn.linear_model import , , \n\nolr = ().fit(X, y)\nbrr = (compute_score=True, n_iter=30).fit(X, y)\nard = (compute_score=True, n_iter=30).fit(X, y)\ndf = (\n    {\n        \"Weights of true generative process\": true_weights,\n        \"ARDRegression\": ard.coef_,\n        \"BayesianRidge\": brr.coef_,\n        \"LinearRegression\": olr.coef_,\n    }\n)"
        ]
    },
    {
        "id": "110251",
        "GT": "Name of Columns->df1.columns",
        "pred": [
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables->dta_c.T[0]",
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time",
            "statsmodels->Examples->Statistics->ANOVA->df_infl[:5]",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.dtypes",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Remainder->res_o.pvalues"
        ]
    },
    {
        "id": "665484",
        "GT": "boundary_distances2.columns = [\"boundaries_geo2\", \"sub_geo2\", \"ccb_geo2\", \"crb_geo2\", \"ctf_geo2\", \"ocb_geo2\", \n                               #\"osr_geo2\", \"otf_geo2\"]\nearthquakes_dist2 = pd.concat([earthquakes2_gdf, boundary_distances2], axis=1)\n#print(len(earthquakes_dist2))\n#boundary_distances\n#print(len(earthquakes2_gdf))\n#earthquakes_dist.to_csv(\"MR_Data/Earthquake_boundary_distances\")\n#boundary_distances2\n#earthquakes_dist2\nearthquakes_dist2.to_csv(\"MR_Data/Earthquake_boundary_distances2\")->boundary_distances2 = pd.read_csv(\"MR_Data/Earthquake_boundary_distances2\")\nboundary_distances2 = boundary_distances2.drop(\"Unnamed: 0\", axis = 1)",
        "pred": [
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "pandas_toms_blog->Time Series->Timeseries->gs = web.DataReader(\"GS\", data_source='yahoo', start='2006-01-01',\n                    end='2010-01-01')\ngs.head().round(2)",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)"
        ]
    },
    {
        "id": "102825",
        "GT": "Step 5: a4 Feature Analysis\n##### Use modules built for a2 and a7 (evolve if necessary)->import pandas as pd\nimport sys\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nsys.path.append('../utils')\nimport DataAggregation as da\nimport AlgoUtils as au\ncmap_bold = ListedColormap(['#00FF00','#FF0000'])",
        "pred": [
            "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->from collections import \n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import \nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import \n\nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.inspection import \nfrom sklearn.model_selection import",
            "torch->Extending PyTorch->Custom C++ and CUDA Extensions->Motivation and Example->import torch\n\nX = torch.randn(batch_size, input_features)\nh = torch.randn(batch_size, state_size)\nC = torch.randn(batch_size, state_size)\n\nrnn = LLTM(input_features, state_size)\n\nnew_h, new_C = rnn(X, (h, C))",
            "numpy->NumPy Applications->X-ray image processing->Combine images into a multidimensional array to demonstrate progression->import numpy as np\nnum_imgs = 9\n\ncombined_xray_images_1 = np.array(\n    [imageio.v3.imread(os.path.join(DIR, f\"00000011_00{i}.png\")) for i in range(num_imgs)]\n)",
            "sklearn->Examples->Model Selection->Successive Halving Iterations->import pandas as pd\nfrom sklearn import datasets\nimport matplotlib.pyplot as plt\nfrom scipy.stats import \nimport numpy as np\n\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import \nfrom sklearn.ensemble import",
            "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Description of the simulated data->from sklearn.model_selection import \n\ntarget_name = \"hourly wage\"\nX, y = df.drop(columns=target_name), df[target_name]\nX_train, X_test, y_train, y_test = (X, y, test_size=0.2, random_state=0)"
        ]
    },
    {
        "id": "451367",
        "GT": "Create synthetic dataset 1\nFor the first technology, where \"JDBC\" was used.->Treat first commit separetely\nSet a fixed value because we have to start with some code at the beginning->df_jdbc.loc[0, 'lines'] = 250\ndf_jdbc.head()",
        "pred": [
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team->df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "sklearn->Examples->Ensemble methods->Monotonic Constraints->gbdt_no_cst = ()\ngbdt_no_cst.fit(X, y)",
            "statsmodels->Examples->Statistics->ANOVA->df_infl = infl.summary_frame()"
        ]
    },
    {
        "id": "185810",
        "GT": "Choosing a Data Visualization Package\n\n- Back to [Table of Contents](#Table-of-Contents)\n\nThere are many excellent data visualiation modules available in Python, but for the tutorial we will stick to the tried and true combination of `matplotlib` and `seaborn`. You can read more about different options for data visualization in Python in the [More Resources](#More-Resources) section at the bottom of this notebook. \n\n`matplotlib` is very expressive, meaning it has functionality that can easily account for fine-tuned graph creation and adjustment. However, this also means that `matplotlib` is somewhat more complex to code.\n\n`seaborn` is a higher-level visualization module, which means it is much less expressive and flexible than matplotlib, but far more concise and easier to code.\n\nIt may seem like we need to choose between these two approaches, but this is not the case! Since `seaborn` is itself written in `matplotlib` (you will sometimes see `seaborn` be called a `matplotlib` 'wrapper'), we can use `seaborn` for making graphs quickly and then `matplotlib` for specific adjustments. When you see `plt` referenced in the code below, we are using `matplotlib`'s pyplot submodule.\n\n\n`seaborn` also improves on `matplotlib` in important ways, such as the ability to more easily visualize regression model results, creating small multiples, enabling better color palettes, and improve default aesthetics. From [`seaborn`'s documentation](https://seaborn.pydata.org/introduction.html):\n\n> If matplotlib 'tries to make easy things easy and hard things possible', seaborn tries to make a well-defined set of hard things easy too.->## Seaborn offers a powerful tool called FacetGrid for making small multiples of matplotlib graphs:\n\n### Create an empty set of grids:\nfacet_histograms = sns.FacetGrid(ein_empl_lim, col='year', hue='year')\n\n## \"map' a histogram to each grid:\nfacet_histograms = facet_histograms.map(plt.hist, 'avg_wage')\n\n## Data Sourcing:\nplt.annotate('Source: MO Department of Labor', xy=(0.6,-0.35), xycoords=\"axes fraction\")\nplt.show()",
        "pred": [
            "statsmodels->User Guide->Regression and Linear Models->Linear Regression->Examples-># Load modules and data\nIn [1]: import numpy as np\n\nIn [2]: import statsmodels.api as sm\n\nIn [3]: spector_data = sm.datasets.spector.load()\n\nIn [4]: spector_data.exog = sm.add_constant(spector_data.exog, prepend=False)\n\n# Fit and summarize OLS model\nIn [5]: mod = sm.OLS(spector_data.endog, spector_data.exog)\n\nIn [6]: res = mod.fit()\n\nIn [7]: print(res.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  GRADE   R-squared:                       0.416\nModel:                            OLS   Adj. R-squared:                  0.353\nMethod:                 Least Squares   F-statistic:                     6.646\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):            0.00157\nTime:                        17:12:47   Log-Likelihood:                -12.978\nNo. Observations:                  32   AIC:                             33.96\nDf Residuals:                      28   BIC:                             39.82\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGPA            0.4639      0.162      2.864      0.008       0.132       0.796\nTUCE           0.0105      0.019      0.539      0.594      -0.029       0.050\nPSI            0.3786      0.139      2.720      0.011       0.093       0.664\nconst         -1.4980      0.524     -2.859      0.008      -2.571      -0.425\n==============================================================================\nOmnibus:                        0.176   Durbin-Watson:                   2.346\nProb(Omnibus):                  0.916   Jarque-Bera (JB):                0.167\nSkew:                           0.141   Prob(JB):                        0.920\nKurtosis:                       2.786   Cond. No.                         176.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.",
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Preliminary investigation with ad-hoc parameters in H, Q-># Here, for illustration purposes only, we plot the time-varying\n# coefficients conditional on an ad-hoc parameterization\n\n# Recall that `initial_res` contains the Kalman filtering and smoothing,\n# and the `states.smoothed` attribute contains the smoothed states\nplot_coefficients_by_equation(initial_res.states.smoothed);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_27_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_27_0.png\"/>",
            "statsmodels->User Guide->Regression and Linear Models->Regression with Discrete Dependent Variable->Examples-># Load the data from Spector and Mazzeo (1980)\nIn [1]: import statsmodels.api as sm\n\nIn [2]: spector_data = sm.datasets.spector.load_pandas()\n\nIn [3]: spector_data.exog = sm.add_constant(spector_data.exog)\n\n# Logit Model\nIn [4]: logit_mod = sm.Logit(spector_data.endog, spector_data.exog)\n\nIn [5]: logit_res = logit_mod.fit()\nOptimization terminated successfully.\n         Current function value: 0.402801\n         Iterations 7\n\nIn [6]: print(logit_res.summary())\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                  GRADE   No. Observations:                   32\nModel:                          Logit   Df Residuals:                       28\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 02 Nov 2022   Pseudo R-squ.:                  0.3740\nTime:                        17:12:32   Log-Likelihood:                -12.890\nconverged:                       True   LL-Null:                       -20.592\nCovariance Type:            nonrobust   LLR p-value:                  0.001502\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -13.0213      4.931     -2.641      0.008     -22.687      -3.356\nGPA            2.8261      1.263      2.238      0.025       0.351       5.301\nTUCE           0.0952      0.142      0.672      0.501      -0.182       0.373\nPSI            2.3787      1.065      2.234      0.025       0.292       4.465\n==============================================================================",
            "statsmodels->User Guide->Regression and Linear Models->Robust Linear Models->Examples-># Load modules and data\nIn [1]: import statsmodels.api as sm\n\nIn [2]: data = sm.datasets.stackloss.load()\n\nIn [3]: data.exog = sm.add_constant(data.exog)\n\n# Fit model and print summary\nIn [4]: rlm_model = sm.RLM(data.endog, data.exog, M=sm.robust.norms.HuberT())\n\nIn [5]: rlm_results = rlm_model.fit()\n\nIn [6]: print(rlm_results.params)\nconst       -41.026498\nAIRFLOW       0.829384\nWATERTEMP     0.926066\nACIDCONC     -0.127847\ndtype: float64",
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Implementation in Statsmodels->Local level model-># Plot the inflation data along with simulated trends\nfig, axes = plt.subplots(2, figsize=(15, 6))\n\n# Plot data and KFS simulations\ndta.infl.plot(ax=axes[0], color='k')\naxes[0].set_title('Simulations based on KFS approach, MLE parameters')\nsimulated_state_kfs.plot(ax=axes[0], color='C0', alpha=0.25, legend=False)\n\n# Plot data and CFA simulations\ndta.infl.plot(ax=axes[1], color='k')\naxes[1].set_title('Simulations based on CFA approach, MLE parameters')\nsimulated_state_cfa.plot(ax=axes[1], color='C0', alpha=0.25, legend=False)\n\n# Add a legend, clean up layout\nhandles, labels = axes[0].get_legend_handles_labels()\naxes[0].legend(handles[:2], ['Data', 'Simulated state'])\nfig.tight_layout();\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_9_0.png\"/>"
        ]
    },
    {
        "id": "609621",
        "GT": "selected_vars = np.array(selected_vars)\nimportances = result.feature_importances_\nimportant_names = selected_vars[importances > np.mean(importances)]\nprint (important_names)",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis->oo = np.ones(df.shape[0])\nmodel4 = sm.MixedLM(df.y, oo[:, None], exog_re=None, groups=oo, exog_vc=vcs)\nresult4 = model4.fit()\nprint(result4.summary())",
            "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach-># initialize random variable\nt_post = (\n    df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n)",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit->mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)",
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis->oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->Condition number->eigs = np.linalg.eigvals(norm_xtx)\ncondition_number = np.sqrt(eigs.max() / eigs.min())\nprint(condition_number)"
        ]
    },
    {
        "id": "432689",
        "GT": "Filtering->cold_days = freezing_days[freezing_days.min_temp >= 20]\ncold_days.info()",
        "pred": [
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Sampling->rng = default_rng()\n\nbefore_sample = rng.choice(before_lock, size=30, replace=False)\nafter_sample = rng.choice(after_lock, size=30, replace=False)",
            "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results->compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ]
    },
    {
        "id": "135317",
        "GT": "dfWAR.Nationality.value_counts()",
        "pred": [
            "statsmodels->Examples->Statistics->ANOVA->interM_lm.model.data.orig_exog[:5]",
            "scipy->Sparse eigenvalue problems with ARPACK->Examples->evals_all, evecs_all = eigh(X)",
            "sklearn->Examples->Inspection->Partial Dependence and Individual Conditional Expectation Plots->Bike sharing dataset preprocessing->X_train.info()",
            "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types->X_train.info()",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Data exploration on the Bike Sharing Demand dataset->df[\"count\"].max()",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Data exploration on the Bike Sharing Demand dataset->X[\"weather\"].value_counts()",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Data exploration on the Bike Sharing Demand dataset->X[\"weather\"].value_counts()",
            "sklearn->Examples->Inspection->Partial Dependence and Individual Conditional Expectation Plots->Bike sharing dataset preprocessing->X[\"weather\"].value_counts()"
        ]
    },
    {
        "id": "1161768",
        "GT": "Load a csv with no headers->df = pd.read_csv('pandas_dataframe_importing_csv/example.csv', header=None)\ndf",
        "pred": [
            "statsmodels->Examples->State space models->ETS models->Predictions->df = pred.summary_frame(alpha=0.05)\ndf",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->SettingWithCopy->f = pd.DataFrame({'a':[1,2,3,4,5], 'b':[10,20,30,40,50]})\nf",
            "statsmodels->Examples->User Notes->Dates in Time-Series Models->Using Pandas->data.endog.index = dates\nendog = data.endog\nendog",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Data exploration on the Bike Sharing Demand dataset->X = df.drop(\"count\", axis=\"columns\")\nX"
        ]
    },
    {
        "id": "1409703",
        "GT": "Read data->classes = list(set(data['activity']))\nfor activity in classes:\n    nb = np.sum(data['activity'] == activity)\n    print(\"{:<15}{:<9d}{:<5.2f} %\".format(activity, nb, 100. * nb / data.shape[0]))\nprint()\nprint(\"Number of objects: {:d}\".format(data.shape[0]))",
        "pred": [
            "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Total impurity of leaves vs effective alphas of pruned tree->clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = (random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\n    \"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n        clfs[-1].tree_.node_count, ccp_alphas[-1]\n    )\n)",
            "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor->quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Define a function to visualize cross-validation behavior->cvs = [, , ]\n\nfor cv in cvs:\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(cv(n_splits), X, y, groups, ax, n_splits)\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n\n\n\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_003.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_003.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_004.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_004.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_005.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_005.png\"/>",
            "pandas_toms_blog->Fast Pandas->Constructors->files = glob.glob('weather/*.csv')\ncolumns = ['station', 'date', 'tmpf', 'relh', 'sped', 'mslp',\n           'p01i', 'vsby', 'gust_mph', 'skyc1', 'skyc2', 'skyc3']\n\n# init empty DataFrame, like you might for a list\nweather = pd.DataFrame(columns=columns)\n\nfor fp in files:\n    city = pd.read_csv(fp, columns=columns)\n    weather.append(city)",
            "sklearn->Examples->Clustering->Demo of DBSCAN clustering algorithm->Plot results->unique_labels = set(labels)\ncore_samples_mask = (labels, dtype=bool)\ncore_samples_mask[db.core_sample_indices_] = True\n\ncolors = [plt.cm.Spectral(each) for each in (0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = labels == k\n\n    xy = X[class_member_mask & core_samples_mask]\n    (\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=14,\n    )\n\n    xy = X[class_member_mask & ~core_samples_mask]\n    (\n        xy[:, 0],\n        xy[:, 1],\n        \"o\",\n        markerfacecolor=tuple(col),\n        markeredgecolor=\"k\",\n        markersize=6,\n    )\n\n(f\"Estimated number of clusters: {n_clusters_}\")\n()\n\n\n<img alt=\"Estimated number of clusters: 3\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_dbscan_002.png\" srcset=\"../../_images/sphx_glr_plot_dbscan_002.png\"/>"
        ]
    },
    {
        "id": "850244",
        "GT": "one_tenth = google.sample(frac = .1, random_state=np.random.randint(10))",
        "pred": [
            "sklearn->Tutorials->Working With Text Data->Loading the 20 newsgroups dataset->twenty_train.target[:10]\narray([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines->sinplot()\nsns.despine()\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines->sinplot()\nsns.despine()\n",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first.ix[10:15, ['fl_date', 'tail_num']]",
            "scipy->Multidimensional image processing (scipy.ndimage)->Object measurements->ndi_sum(image, labels, [0, 2])\narray([178.0, 80.0])",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->Prediction with MA components->t = y.shape[0]\nma_res.predict(t - 3, t - 1)"
        ]
    },
    {
        "id": "982874",
        "GT": "indices = []\nfor i in range(0,len(data['Zip_code'])):\n\n    if(data['Zip_code'][i].isdigit()):\n        x = int(data['Zip_code'][i])\n    else:\n        indices.append(i)",
        "pred": [
            "torch->Introduction to PyTorch->Quickstart->Optimizing the Model Parameters->epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")",
            "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack->accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "scipy->Optimization (scipy.optimize)->Unconstrained minimization of multivariate scalar functions (minimize)->Nelder-Mead Simplex algorithm (method='Nelder-Mead')->def rosen(x):\n...     \"\"\"The Rosenbrock function\"\"\"\n...     return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)",
            "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach->mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())"
        ]
    },
    {
        "id": "384142",
        "GT": "Company Analysis and Filtering->Read from File->import dill\nwith open('tutorial_results.pkl', 'rb') as f:\n    successes = dill.load(f)\n    failures = dill.load(f)",
        "pred": [
            "numpy->NumPy Applications->X-ray image processing->Examine an X-ray with imageio->import os\nimport imageio\n\nDIR = \"tutorial-x-ray-image-processing\"\n\nxray_image = imageio.v3.imread(os.path.join(DIR, \"00000011_001.png\"))",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation->from sklearn import datasets\nimport numpy as np\n\ndigits = ()\nrng = (2)\nindices = (len(digits.data))\nrng.shuffle(indices)",
            "statsmodels->Examples->State space models->Trends and cycles in unemployment->from pandas_datareader.data import DataReader\nendog = DataReader('UNRATE', 'fred', start='1954-01-01')\nendog.index.freq = endog.index.inferred_freq",
            "matplotlib->Tutorials->Toolkits->The axisartist toolkit->axisartist->import mpl_toolkits.axisartist as AA\nfig = plt.figure()\nfig.add_axes([0.1, 0.1, 0.8, 0.8], axes_class=AA.Axes)"
        ]
    },
    {
        "id": "642078",
        "GT": "Plotting perceived randomness/control/deception \n\nCombined dataset, all subjects:->dfs_piv_1 = dfs_piv[dfs_piv['group']==1]\nfig, axs = plt.subplots(1,3)\ndfs_piv_1.boxplot(column='randomness',ax=axs[0],return_type='dict')\ndfs_piv_1.boxplot(column='real_person',ax=axs[1],return_type='dict')\ndfs_piv_1.boxplot(column='control',ax=axs[2],return_type='dict')",
        "pred": [
            "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Train models on the diabetes dataset->diabetes = ()\nX = (diabetes.data, columns=diabetes.feature_names)\ny = diabetes.target\n\ntree = ()\nmlp = (\n    (),\n    (hidden_layer_sizes=(100, 100), tol=1e-2, max_iter=500, random_state=0),\n)\ntree.fit(X, y)\nmlp.fit(X, y)",
            "statsmodels->Examples->Statistics->ANOVA->lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights->glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production->res_ar5 = AutoReg(ind_prod, 5, old_names=False).fit()\npredictions = pd.DataFrame(\n    {\n        \"AR(5)\": res_ar5.predict(start=714, end=726),\n        \"AR(13)\": res.predict(start=714, end=726),\n        \"Restr. AR(13)\": res_glob.predict(start=714, end=726),\n    }\n)\n_, ax = plt.subplots()\nax = predictions.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\"/>",
            "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features->cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)"
        ]
    },
    {
        "id": "994549",
        "GT": "Which direction has the highest passenger traffic? (Passenger traffic is determined by the sum of the number of people getting on at any stop along a route in a given direction)->route = data.groupby(data['route_name']).sum()[['ons','offs']]\nroute['avg']=route.mean(axis=1)\nroute['avg'][route['avg']==route['avg'].max()].index[0]",
        "pred": [
            "pandas_toms_blog->Scaling->Dask->subset = daily.loc['2011':'2016']\nax = subset.div(1000).plot(figsize=(12, 6))\nax.set(ylim=0, title=\"Daily Donations\", ylabel=\"$ (thousands)\",)\nsns.despine();",
            "pandas_toms_blog->Scaling->Using Dask->df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->df['home_win'] = df['home_points'] &gt; df['away_points']\ndf['rest_spread'] = df['home_rest'] - df['away_rest']\ndf.dropna().head()",
            "statsmodels->Examples->State space models->VARMAX: Introduction->Caution: VARMA(p,q) specifications->mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(1,1))\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()"
        ]
    },
    {
        "id": "687027",
        "GT": "Competions per month->fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(comps_sqrt_2diff_sdiff.dropna(), lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(comps_sqrt_2diff_sdiff.dropna(), lags=40, ax=ax2)",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->fig = plt.figure(figsize=(12, 8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(dta.values.squeeze(), lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(dta, lags=40, ax=ax2)",
            "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data->fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(dta.values.squeeze(), lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(dta, lags=40, ax=ax2)",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->fig = plt.figure(figsize=(12, 8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(resid.values.squeeze(), lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(resid, lags=40, ax=ax2)",
            "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data->fig = plt.figure(figsize=(12,4))\nax = fig.add_subplot(111)\nfig = qqplot(resid, line='q', ax=ax, fit=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_arma_0_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_arma_0_21_0.png\"/>",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nfig = qqplot(resid, line=\"q\", ax=ax, fit=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_arma_0_21_0.png\" src=\"../../../_images/examples_notebooks_generated_tsa_arma_0_21_0.png\"/>"
        ]
    },
    {
        "id": "1150175",
        "GT": "9. (Grid) Search for hyper-parameter optimizations->imgfiles = import_data(datafolder)\ndf, feature_names = extract_features(imgfiles, feature_funcs, 10, 10)",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Data->spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit->mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)",
            "statsmodels->Examples->User Notes->Prediction->Estimation->olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())"
        ]
    },
    {
        "id": "353538",
        "GT": "-------------------------------------------------------------------------------------------------------->for i in range(2,24,1): \n    LIMIT_L = 1990\n    LIMIT_H = LIMIT_L+3040 #2 TAU\n    PULSE_R = 1738\n    hdf5_file     = ''.join([path,'cal_1u.h5.z'])\n    Channel       = i\n    event_range      = range(0,500,1)\n\n    coeff[i,0] = find_coeff_II( LIMIT_L, LIMIT_H, PULSE_R,\n                                hdf5_file, Channel, event_range )\n    print(\"CH\",i,'=',coeff[i,0])",
        "pred": [
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->for name, importances in zip([\"train\", \"test\"], [train_importances, test_importances]):\n    ax = importances.plot.box(vert=False, whis=10)\n    ax.set_title(f\"Permutation Importances ({name} set)\")\n    ax.set_xlabel(\"Decrease in accuracy score\")\n    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n    ax.figure.tight_layout()\n\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_004.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_004.png\"/>\n<img alt=\"Permutation Importances (test set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_005.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_005.png\"/>",
            "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Steps->5. Zero the gradients while training the network->for epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n\nprint('Finished Training')",
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Bayesian estimation via MCMC->for i in range(niter):\n    mod.update_variances(store_obs_cov[i], store_state_cov[i])\n    sim.simulate()\n\n    # 1. Sample states\n    store_states[i + 1] = sim.simulated_state.T\n\n    # 2. Simulate obs cov\n    fitted = np.matmul(mod['design'].transpose(2, 0, 1), store_states[i + 1][..., None])[..., 0]\n    resid = mod.endog - fitted\n    store_obs_cov[i + 1] = invwishart.rvs(v10 + mod.nobs, S10 + resid.T @ resid)\n\n    # 3. Simulate state cov variances\n    resid = store_states[i + 1, 1:] - store_states[i + 1, :-1]\n    sse = np.sum(resid**2, axis=0)\n\n    for j in range(mod.k_states):\n        rv = invgamma.rvs((vi20 + mod.nobs - 1) / 2, scale=(Si20 + sse[j]) / 2)\n        store_state_cov[i + 1, j] = rv",
            "torch->PyTorch Recipes->Automatic Mixed Precision->Adding autocast->for epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        # Runs the forward pass under autocast.\n        with (device_type='cuda', dtype=torch.float16):\n            output = net(input)\n            # output is float16 because linear layers autocast to float16.\n            assert output.dtype is torch.float16\n\n            loss = loss_fn(output, target)\n            # loss is float32 because mse_loss layers autocast to float32.\n            assert loss.dtype is torch.float32\n\n        # Exits autocast before backward().\n        # Backward passes under autocast are not recommended.\n        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance",
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data->for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>"
        ]
    },
    {
        "id": "64732",
        "GT": "plot = df.plot(x='I',y='V', kind='scatter', \n               title=r'$V$ $\\rm vs.$ $I$ (with error bars and trendline)',\n              xerr = 0.05, yerr = 0.1);\n\nplot.set_xlabel(r'$I$ $\\rm (Amperes)$')\nplot.set_ylabel(r'$V$ $\\rm (Volts)$')\n\nx1 = 1; y1 = a + b*x1\nx2 = 12; y2 = a + b*x2\n\nplt.plot([x1,x2],[y1,y2],'k-')\nplt.text(7, 6000, r'$y = %.2f x + (%.2f) $' % (b, a), fontsize=14)",
        "pred": [
            "statsmodels->Examples->State space models->Trends and cycles in unemployment->Graphical comparison->fig, axes = plt.subplots(2, figsize=(13,5));\naxes[0].set(title='Level/trend component')\naxes[0].plot(endog.index, res_uc.level.smoothed, label='UC')\naxes[0].plot(endog.index, res_ucarima.level.smoothed, label='UC-ARIMA(2,0)')\naxes[0].plot(hp_trend, label='HP Filter')\naxes[0].legend(loc='upper left')\naxes[0].grid()\n\naxes[1].set(title='Cycle component')\naxes[1].plot(endog.index, res_uc.cycle.smoothed, label='UC')\naxes[1].plot(endog.index, res_ucarima.autoregressive.smoothed, label='UC-ARIMA(2,0)')\naxes[1].plot(hp_cycle, label='HP Filter')\naxes[1].legend(loc='upper left')\naxes[1].grid()\n\nfig.tight_layout();\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_cycles_11_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_cycles_11_0.png\"/>",
            "pandas_toms_blog->Scaling->Dask->ax = occupation_avg.sort_values(ascending=False).plot.barh(color='k', width=0.9);\nlim = ax.get_ylim()\nax.vlines(total_avg, *lim, color='C1', linewidth=3)\nax.legend(['Average donation'])\nax.set(xlabel=\"Donation Amount\", title=\"Average Dontation by Occupation\")\nsns.despine()",
            "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec->fig = plt.figure(figsize=(4, 6), layout=\"constrained\")\n\ngs0 = fig.add_gridspec(6, 2)\n\nax1 = fig.add_subplot(gs0[:3, 0])\nax2 = fig.add_subplot(gs0[3:, 0])\n\nexample_plot(ax1)\nexample_plot(ax2)\n\nax = fig.add_subplot(gs0[0:2, 1])\nexample_plot(ax, hide_labels=True)\nax = fig.add_subplot(gs0[2:4, 1])\nexample_plot(ax, hide_labels=True)\nax = fig.add_subplot(gs0[4:, 1])\nexample_plot(ax, hide_labels=True)\nfig.suptitle('Overlapping Gridspecs')\n\n\n<img alt=\"Overlapping Gridspecs, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_021.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_021.png, ../../_images/sphx_glr_constrainedlayout_guide_021_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Intermediate->Arranging multiple Axes in a Figure->Low-level and advanced grid methods->Nested layouts with SubplotSpec->fig = plt.figure(layout=\"constrained\")\ngs0 = fig.add_gridspec(1, 2)\n\ngs00 = gs0[0].subgridspec(2, 2)\ngs01 = gs0[1].subgridspec(3, 1)\n\nfor a in range(2):\n    for b in range(2):\n        ax = fig.add_subplot(gs00[a, b])\n        annotate_axes(ax, f'axLeft[{a}, {b}]', fontsize=10)\n        if a == 1 and b == 1:\n            ax.set_xlabel('xlabel')\nfor a in range(3):\n    ax = fig.add_subplot(gs01[a])\n    annotate_axes(ax, f'axRight[{a}, {b}]')\n    if a == 2:\n        ax.set_ylabel('ylabel')\n\nfig.suptitle('nested gridspecs')\n\n\n<img alt=\"nested gridspecs\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_arranging_axes_012.png\" srcset=\"../../_images/sphx_glr_arranging_axes_012.png, ../../_images/sphx_glr_arranging_axes_012_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec->fig = plt.figure(layout=\"constrained\")\nsfigs = fig.subfigures(1, 2, width_ratios=[1, 2])\n\naxs_left = sfigs[0].subplots(2, 1)\nfor ax in axs_left.flat:\n    example_plot(ax)\n\naxs_right = sfigs[1].subplots(2, 2)\nfor ax in axs_right.flat:\n    pcm = ax.pcolormesh(arr, **pc_kwargs)\n    ax.set_xlabel('x-label')\n    ax.set_ylabel('y-label')\n    ax.set_title('title')\nfig.colorbar(pcm, ax=axs_right)\nfig.suptitle('Nested plots using subfigures')\n\n\n<img alt=\"Nested plots using subfigures, Title, Title, title, title, title, title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_023.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_023.png, ../../_images/sphx_glr_constrainedlayout_guide_023_2_0x.png 2.0x\"/>"
        ]
    },
    {
        "id": "642959",
        "GT": "Big Entropy and the Generalized Linear Model->import pandas as pd\nimport numpy as np\n\nimport collections\n\nimport matplotlib.pyplot as plt\nimport pymc3 as pm\n\nimport scipy.stats as stats",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel",
            "statsmodels->Examples->Time Series Analysis->Time Series Filters->import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->import numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "statsmodels->Examples->State space models->VARMAX: Introduction->import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "statsmodels->Examples->State space models->Trends and cycles in unemployment->import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n"
        ]
    },
    {
        "id": "1133605",
        "GT": "4. Visualisation of results for different energy source levels->4.1 Energy source level 1->4.1.1 Table->pivot_capacity_level1 = pd.pivot_table(data_selection[data_selection.energy_source_level_1 == True],\n                                       index=('country','year','source'),\n                                       columns='technology',\n                                       values='capacity',\n                                       aggfunc=sum,\n                                       margins=False)\n\npivot_capacity_level1",
        "pred": [
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights->glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "statsmodels->Examples->Statistics->ANOVA->lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "statsmodels->Examples->Statistics->ANOVA->Two-way ANOVA->kidney_lm = ols(\"np.log(Days+1) ~ C(Duration) * C(Weight)\", data=kt).fit()\n\ntable10 = anova_lm(kidney_lm)\n\nprint(\n    anova_lm(ols(\"np.log(Days+1) ~ C(Duration) + C(Weight)\", data=kt).fit(), kidney_lm)\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Duration)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Weight)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)",
            "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure->forecasts = pd.DataFrame(\n    {\n        \"ln PCE\": np.log(pce.PCE),\n        \"theta=1.2\": res.forecast(12, theta=1.2),\n        \"theta=2\": res.forecast(12),\n        \"theta=3\": res.forecast(12, theta=3),\n        \"No damping\": res.forecast(12, theta=np.inf),\n    }\n)\n_ = forecasts.tail(36).plot()\nplt.title(\"Forecasts of ln PCE\")\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_18_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_18_0.png\"/>",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights->glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\", data=data, family=sm.families.Poisson()\n)\nres_o2 = glm.fit()\n# print(res_f2.summary())\nres_o2.pearson_chi2 - res_o.pearson_chi2, res_o2.deviance - res_o.deviance, res_o2.llf - res_o.llf"
        ]
    },
    {
        "id": "1143077",
        "GT": "Trips over 3 hours->trips = trips[trips['trip_duration'] < 3*3600]",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA->sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions->sns.relplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\nsns.rugplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions->sns.relplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\nsns.rugplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->idx = idx[idx.get_level_values(0) &lt;= idx.get_level_values(1)]"
        ]
    },
    {
        "id": "381263",
        "GT": "Practical->Exercise 4 - Credit Card Default Data Set\n\nWe previously used logistic regression to predict the probability of `default` using `income` and `balance` on the `Default` data set. We will now estimate the test error of this logistic regression model using the validation set approach.->loan = preprocessing.LoanDefault()\nloan.logistic_bootstrap(3)",
        "pred": [
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines->sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines->sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples->tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples->tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots->tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots->tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n"
        ]
    },
    {
        "id": "359291",
        "GT": "Adding a Hover Tool\nBefore creating a tooltip for our shot chart, lets set things up so we can add some color to differentiate between missed and made FGAs.\n\nFirst we create two dictionaries. One dictionary is our colormap to differentiate between missed and made shots.  The other dictionary contains the text indicating a missed or made shot.->fig = nba.bokeh_shot_chart(curry_shots_df, fill_color=\"color\",\n                           hover_tool=True, tooltips=tooltips)\nshow(fig)",
        "pred": [
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, row=\"smoker\", col=\"time\", margin_titles=True)\ng.map(sns.regplot, \"size\", \"total_bill\", color=\".3\", fit_reg=False, x_jitter=.1)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, row=\"smoker\", col=\"time\", margin_titles=True)\ng.map(sns.regplot, \"size\", \"total_bill\", color=\".3\", fit_reg=False, x_jitter=.1)\n",
            "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions->Violinplots->sns.catplot(\n    data=tips, x=\"total_bill\", y=\"day\", hue=\"sex\",\n    kind=\"violin\", bw=.15, cut=0,\n)\n",
            "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions->Violinplots->sns.catplot(\n    data=tips, x=\"total_bill\", y=\"day\", hue=\"sex\",\n    kind=\"violin\", bw=.15, cut=0,\n)\n",
            "statsmodels->Examples->Time Series Analysis->Time Series Filters->Christiano-Fitzgerald approximate band-pass filter: Inflation and Unemployment->fig = plt.figure(figsize=(14, 10))\nax = fig.add_subplot(111)\ncf_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "statsmodels->Examples->Time Series Analysis->Time Series Filters->Baxter-King approximate band-pass filter: Inflation and Unemployment->Explore the hypothesis that inflation and unemployment are counter-cyclical.->fig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111)\nbk_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris, hue=\"species\")\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\ng.add_legend()\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris, hue=\"species\")\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\ng.add_legend()\n"
        ]
    },
    {
        "id": "1325342",
        "GT": "+ Requirements->import os\nfrom selenium import webdriver\nimport pandas as pd\nimport datetime, time, csv",
        "pred": [
            "scipy->Signal Processing (scipy.signal)->B-splines->import numpy as np\n from scipy import signal, datasets\n import matplotlib.pyplot as plt",
            "scipy->Signal Processing (scipy.signal)->Filtering->Convolution/Correlation->import numpy as np\n from scipy import signal, datasets\n import matplotlib.pyplot as plt",
            "scipy->Signal Processing (scipy.signal)->Filtering->Convolution/Correlation->import numpy as np\n from scipy import signal, datasets\n import matplotlib.pyplot as plt",
            "scipy->Spatial data structures and algorithms (scipy.spatial)->Voronoi diagrams->import numpy as np\n from scipy import spatial\n import matplotlib.pyplot as plt",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->import numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
            "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->FIR Filter->import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->FIR Filter->import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->IIR Filter->import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "scipy->Signal Processing (scipy.signal)->Filtering->Analog Filter Design->import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Periodogram Measurements->import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "scipy->Signal Processing (scipy.signal)->Spectral Analysis->Spectral Analysis using Welch\u00e2\u0080\u0099s Method->import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "scipy->Signal Processing (scipy.signal)->Detrend->import numpy as np\n import scipy.signal as signal\n import matplotlib.pyplot as plt",
            "statsmodels->Examples->State space models->VARMAX: Introduction->import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "statsmodels->Examples->State space models->Trends and cycles in unemployment->import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt"
        ]
    },
    {
        "id": "85441",
        "GT": "Index folders and files-># Initialize some variables / containers\nfolder_list = os.listdir('f_center') # folder list\n\n# create dictionary that maps key to number of files per name folder\nlen_dict = {}\n\nfor i, folder in enumerate(folder_list):\n    num_files = len(os.listdir('f_center\\%s' %folder)) \n    len_dict[i] = num_files\n    \n# Create hash that maps previous key to actual names\ndf_target_info = pd.DataFrame(index = list(len_dict.keys()), data = os.listdir('f_center'), columns = ['name'])\ndf_target_info['n_im'] = len_dict.values()",
        "pred": [
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Sentiment Analysis on the Speech Data-># To store predicted sentiments\npredictions = {}\n\n# define the length of a paragraph\npara_len = 100\n\n# Retrieve trained values of the parameters\nif os.path.isfile('tutorial-nlp-from-scratch/parameters.npy'):\n    parameters = np.load('tutorial-nlp-from-scratch/parameters.npy', allow_pickle=True).item()\n\n# This is the prediction loop.\nfor index, text in enumerate(X_pred):\n    # split each speech into paragraphs\n    paras = textproc.text_to_paras(text, para_len)\n    # To store the network outputs\n    preds = []\n\n    for para in paras:\n        # split text sample into words/tokens\n        para_tokens = textproc.word_tokeniser(para)\n        # Forward Propagation\n        caches = forward_prop(para_tokens,\n                              parameters,\n                              input_dim)\n\n        # Retrieve the output of the fully connected layer\n        sent_prob = caches['fc_values'][0]['a2'][0][0]\n        preds.append(sent_prob)\n\n    threshold = 0.5\n    preds = np.array(preds)\n    # Mark all predictions &gt; threshold as positive and &lt; threshold as negative\n    pos_indices = np.where(preds &gt; threshold)  # indices where output &gt; 0.5\n    neg_indices = np.where(preds &lt; threshold)  # indices where output &lt; 0.5\n    # Store predictions and corresponding piece of text\n    predictions[speakers[index]] = {'pos_paras': paras[pos_indices[0]],\n                                    'neg_paras': paras[neg_indices[0]]}",
            "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Comparing kernel functions->The available kernel functions-># Create a figure\nfig = plt.figure(figsize=(12, 5))\n\n# Enumerate every option for the kernel\nfor i, (ker_name, ker_class) in enumerate(kernel_switch.items()):\n\n    # Initialize the kernel object\n    kernel = ker_class()\n\n    # Sample from the domain\n    domain = kernel.domain or [-3, 3]\n    x_vals = np.linspace(*domain, num=2 ** 10)\n    y_vals = kernel(x_vals)\n\n    # Create a subplot, set the title\n    ax = fig.add_subplot(3, 3, i + 1)\n    ax.set_title('Kernel function \"{}\"'.format(ker_name))\n    ax.plot(x_vals, y_vals, lw=3, label=\"{}\".format(ker_name))\n    ax.scatter([0], [0], marker=\"x\", color=\"red\")\n    plt.grid(True, zorder=-5)\n    ax.set_xlim(domain)\n\nplt.tight_layout()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_21_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_21_0.png\"/>",
            "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results-># Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "sklearn->Examples->Examples based on real world datasets->Outlier detection on a real data set->Second example-># Get data\nX2 = ()[\"data\"][:, [6, 9]]  # \"banana\"-shaped\n\n# Learn a frontier for outlier detection with several classifiers\nxx2, yy2 = ((-1, 5.5, 500), (-2.5, 19, 500))\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    (2)\n    clf.fit(X2)\n    Z2 = clf.decision_function([xx2.ravel(), yy2.ravel()])\n    Z2 = Z2.reshape(xx2.shape)\n    legend2[clf_name] = (\n        xx2, yy2, Z2, levels=[0], linewidths=2, colors=colors[i]\n    )\n\nlegend2_values_list = list(legend2.values())\nlegend2_keys_list = list(legend2.keys())\n\n# Plot the results (= shape of the data points cloud)\n(2)  # \"banana\" shape\n(\"Outlier detection on a real data set (wine recognition)\")\n(X2[:, 0], X2[:, 1], color=\"black\")\n((xx2.min(), xx2.max()))\n((yy2.min(), yy2.max()))\n(\n    (\n        legend2_values_list[0].collections[0],\n        legend2_values_list[1].collections[0],\n        legend2_values_list[2].collections[0],\n    ),\n    (legend2_keys_list[0], legend2_keys_list[1], legend2_keys_list[2]),\n    loc=\"upper center\",\n    prop=(size=11),\n)\n(\"color_intensity\")\n(\"flavanoids\")\n\n()\n\n\n<img alt=\"Outlier detection on a real data set (wine recognition)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_outlier_detection_wine_002.png\" srcset=\"../../_images/sphx_glr_plot_outlier_detection_wine_002.png\"/>",
            "statsmodels->User Guide->Regression and Linear Models->Linear Regression->Examples-># Load modules and data\nIn [1]: import numpy as np\n\nIn [2]: import statsmodels.api as sm\n\nIn [3]: spector_data = sm.datasets.spector.load()\n\nIn [4]: spector_data.exog = sm.add_constant(spector_data.exog, prepend=False)\n\n# Fit and summarize OLS model\nIn [5]: mod = sm.OLS(spector_data.endog, spector_data.exog)\n\nIn [6]: res = mod.fit()\n\nIn [7]: print(res.summary())\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  GRADE   R-squared:                       0.416\nModel:                            OLS   Adj. R-squared:                  0.353\nMethod:                 Least Squares   F-statistic:                     6.646\nDate:                Wed, 02 Nov 2022   Prob (F-statistic):            0.00157\nTime:                        17:12:47   Log-Likelihood:                -12.978\nNo. Observations:                  32   AIC:                             33.96\nDf Residuals:                      28   BIC:                             39.82\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nGPA            0.4639      0.162      2.864      0.008       0.132       0.796\nTUCE           0.0105      0.019      0.539      0.594      -0.029       0.050\nPSI            0.3786      0.139      2.720      0.011       0.093       0.664\nconst         -1.4980      0.524     -2.859      0.008      -2.571      -0.425\n==============================================================================\nOmnibus:                        0.176   Durbin-Watson:                   2.346\nProb(Omnibus):                  0.916   Jarque-Bera (JB):                0.167\nSkew:                           0.141   Prob(JB):                        0.920\nKurtosis:                       2.786   Cond. No.                         176.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
        ]
    },
    {
        "id": "1081613",
        "GT": "4. Identify the 200 highest frequency words in this corpus.-># Build the frequency distribution table\nquotes_freq = nltk.FreqDist(quotes)\nquotes_freq = pd.DataFrame(quotes_freq.most_common(200), columns=['word','n'])\nquotes_freq['relative_frequency'] = (quotes_freq['n']/float(len(quotes)))\nquotes_freq.head(20)",
        "pred": [
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Plot the updated dataset\nfig, ax = plt.subplots(figsize=(15, 3))\ny_post.plot(ax=ax)\nax.hlines(0, '2009', '2017-06', linewidth=1.0)\nax.set_xlim('2009', '2017-06');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_news_43_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_news_43_0.png\"/>",
            "statsmodels->Examples->State space models->Unobserved Components: Application->Data-># Plot the data\nax = dta.plot(figsize=(13,3))\nylim = ax.get_ylim()\nax.xaxis.grid()\nax.fill_between(dates, ylim[0]+1e-5, ylim[1]-1e-5, recessions, facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\"/>",
            "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Appendix 1: Extending the dynamic factor model-># Create the model\nextended_mod = ExtendedDFM(endog)\ninitial_extended_res = extended_mod.fit(maxiter=1000, disp=False)\nextended_res = extended_mod.fit(initial_extended_res.params, method='nm', maxiter=1000)\nprint(extended_res.summary(separate_params=False))",
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->2. Download and plot the data on US CPI-># Plot the series\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\nax.plot(inf.index, inf, label=r\"$\\Delta \\log CPI$\", lw=2)\nax.legend(loc=\"lower left\")\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_6_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_6_0.png\"/>",
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression-># Plot the fit line\nfig, ax = pylab.subplots()\n\nax.scatter(x, y)\nax.plot(smoothed[:, 0], smoothed[:, 1], c=\"k\")\npylab.autoscale(enable=True, axis=\"x\", tight=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_lowess_4_0.png\" src=\"../../../_images/examples_notebooks_generated_lowess_4_0.png\"/>"
        ]
    },
    {
        "id": "1175637",
        "GT": "An Introductiony\n\n## Example: Tips for Service\nLet's assume that you are a small restaurant owner or a very business minded server at a nice restaurant. Here in the U.S. \"tips\" are a very important part of a waiter's pay. Most of the time the dollar amount of the tip is related to the dollar amount of the total bill.\n\nAs the waiter or owner, you would like to develop a model that will allow you to make a prediction about what amount of tip to expect for any given bill amount. Therefore one evening, you collect data for six meals.\n\nUnfortunatly when you begin to look at your data, you realize you only collected data for the tip amount and not the meal amount also\n\nHow might you predict the tip amount for future meals using only this data?->\"Goodness of Fit\" for the tips->tips_df.loc[4, 'Tip amount ($)']",
        "pred": [
            "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->sns.pairplot(penguins)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->sns.pairplot(penguins)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Conditioning on other variables->sns.displot(penguins, x=\"flipper_length_mm\", col=\"sex\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Conditioning on other variables->sns.displot(penguins, x=\"flipper_length_mm\", col=\"sex\")\n",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->sns.set_theme()\nsinplot()\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->sns.set_theme()\nsinplot()\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(tips, x=\"total_bill\", kind=\"kde\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(tips, x=\"total_bill\", kind=\"kde\")\n"
        ]
    },
    {
        "id": "797363",
        "GT": "Continental Drift - the movie\n\nThe idea of continental drift, controversial for the first half of the 20$^{th}$ century, was basically proven in the 50s by the concept of apparent polar wander.  So what is apparent polar wander? By assuming that the magnetic field is generated by a magnetic dipole at the center of the Earth and that it is constrained to be aligned with the spin axis, it is possible to determine the position of the spin axis through time.   In the 50s it was recognized that this pole seemed to move with respect to the spin axis, so \nthis  apparent wandering  could be interpreted in two ways:  wandering of continents whose paleomagnetic directions reflect the changing orientations and distances to the (fixed) pole (lefthand figure below), or alternatively, the pole itself could be wandering, as in righthand figure while the continent remains fixed.->display.Image(filename='Figures/plates.png')",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Stationarity and detrending (ADF/KPSS)->sunspots.plot(figsize=(12, 8))",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data->data.describe()",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(diamonds, x=\"carat\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(diamonds, x=\"carat\")\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(diamonds, x=\"color\", y=\"clarity\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(diamonds, x=\"color\", y=\"clarity\")\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->sns.pairplot(penguins)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->sns.pairplot(penguins)\n"
        ]
    },
    {
        "id": "1254723",
        "GT": "Support Vector Machines\n<br>\nIn this tutorial, we will explore the last supervised classification algorithm of the course - **Support Vector Machines**. The first half of the assignment will introduce the major components of SVMs whereas the second half of the assignment will an application of SVMs for spam email classification (covered in the next blog post). Let's get started!\n\nOur first dataset has two features and is linearly separatible, so let's start by visualizing it in a plot.->linear_svc_c100 = svm.SVC(C=100, kernel=\"linear\")\nlinear_svc_c100.fit(ex1_X, ex1_y)\n\nvisualizeBoundaryLinear(linear_svc_c100)",
        "pred": [
            "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->mod = TVRegression(y_t, x_t, w_t)\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points->rlm_mod = sm.RLM(y, X, sm.robust.norms.TrimmedMean(0.5)).fit()\nabline_plot(model_results=rlm_mod, ax=ax, color=\"red\")",
            "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Fit and summary->glm_binom = sm.GLM(data.endog, data.exog, family=sm.families.Binomial())\nres = glm_binom.fit()\nprint(res.summary())",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson->poisson_mod = sm.Poisson(rand_data.endog, rand_exog)\npoisson_res = poisson_mod.fit(method=\"newton\")\nprint(poisson_res.summary())",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor->mod_pre = sm.tsa.DynamicFactor(y_pre, exog=const_pre, k_factors=1, factor_order=6)\nres_pre = mod_pre.fit()\nprint(res_pre.summary())"
        ]
    },
    {
        "id": "630340",
        "GT": "4. create train&test dataset->from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_new_data, y_new_data, test_size=0.2,random_state=0)",
        "pred": [
            "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Frequency model \u2013 Poisson distribution->from sklearn.model_selection import \nfrom sklearn.linear_model import \n\n\ndf_train, df_test, X_train, X_test = (df, X, random_state=0)",
            "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Cross-validation of likelihood ratios->from sklearn.model_selection import \n\nestimator = ()\nextract_score((estimator, X, y, scoring=scoring, cv=10))\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "sklearn->Examples->Model Selection->Sample pipeline for text feature extraction and evaluation->Pipeline with hyperparameter tuning->from sklearn.feature_extraction.text import \nfrom sklearn.naive_bayes import \nfrom sklearn.pipeline import \n\npipeline = (\n    [\n        (\"vect\", ()),\n        (\"clf\", ()),\n    ]\n)\npipeline",
            "sklearn->Examples->Cross decomposition->Compare cross decomposition methods->CCA (PLS mode B with symmetric deflation)->from sklearn.cross_decomposition import \n\ncca = (n_components=2)\ncca.fit(X_train, Y_train)\nX_train_r, Y_train_r = cca.transform(X_train, Y_train)\nX_test_r, Y_test_r = cca.transform(X_test, Y_test)",
            "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 2. Finetuning the Quantizable Model->Finetuning the model->from torch.quantization import convert\nmodel_ft_tuned.cpu()\n\nmodel_quantized_and_trained = convert(model_ft_tuned, inplace=False)"
        ]
    },
    {
        "id": "244408",
        "GT": "Concatenate to \"wp_posts\"->wp_posts_with_eng = wp_posts_with_eng.sort_values('ID') ",
        "pred": [
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison->results_all = [res_o, res_f, res_e, res_a]\nnames = \"res_o res_f res_e res_a\".split()",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->train_df = textproc.txt_to_df(imdb_train)\ntest_df = textproc.txt_to_df(imdb_test)",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->pca_model = PCA(dta.T, standardize=False, demean=True)",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked"
        ]
    },
    {
        "id": "1019751",
        "GT": "Dependency2vec code word identification\n\nHere is where I detailed the main approach for for selecting hate speech code word candidates. The funtion logic is in the module `models.codeword_search`. It takes the frequency of a word under the biased and unbiased dataset and uses that as a metric for determining which words might be ideal candidates. \n\nThis notebook makes initial calls to an elasticsearch instance and assumes its existence.\n\n### Related issues \n- [#85](https://github.com/JherezTaylor/thesis-preprocessing/issues/85)\n- [#93](https://github.com/JherezTaylor/thesis-preprocessing/issues/93)\n- [#116](https://github.com/JherezTaylor/thesis-preprocessing/issues/116)->Initialize params and objects\n\nHere we define common functions for loading our embeddings and extracting the vocabulary and vocabulary counts. ft_word_embeddings and w2v_word_embeddings each store a list of references to embedding models that exist on disk.->Load dataframes and other objects->_es = elasticsearch_base.connect(settings.ES_URL)\npositive_hs_filter = \"_exists_:hs_keyword_matches\"\nnegative_hs_filter = \"!_exists_:hs_keyword_matches\"\n\nhs_keywords = set(file_ops.read_csv_file(\"refined_hs_keywords\", settings.TWITTER_SEARCH_PATH))",
        "pred": [
            "torch->Introduction to PyTorch->Build the Neural Network->Define the Class->model = ().to(device)\nprint(model)",
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Finetuning the convnet->model_ft = (pretrained=True)\nnum_ftrs = .in_features\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n = (num_ftrs, 2)\n\nmodel_ft = ()\n\n = ()\n\n# Observe that all parameters are being optimized\n = ((), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\n = (, step_size=7, gamma=0.1)",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data->flights = sns.load_dataset(\"flights\")\nflights.head()\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data->flights = sns.load_dataset(\"flights\")\nflights.head()\n",
            "sklearn->Examples->Miscellaneous->ROC Curve with Visualization API->Plotting the ROC Curve->svc_disp = (svc, X_test, y_test)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\"/>",
            "torch->Parallel and Distributed Training->Getting Started with Fully Sharded Data Parallel(FSDP)->How to use FSDP->model = Net().to(rank)\nmodel = DDP(model)"
        ]
    },
    {
        "id": "581774",
        "GT": "Step 2: explore the data and engineer features-># log transform target variable and recalculate skewness\n\ntarget = np.log(train.SalePrice)\nprint(\"Skew is:\", target.skew())\n\nplt.hist(target, color = 'blue')\nplt.show()",
        "pred": [
            "seaborn->User guide and tutorial->An introduction to seaborn-># Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n",
            "statsmodels->Examples->User Notes->Formulas->Using formulas with models that do not (yet) support them->f = \"Lottery ~ Literacy * Wealth\"\ny, X = patsy.dmatrices(f, df, return_type=\"dataframe\")\nprint(y[:5])\nprint(X[:5])",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack->accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()"
        ]
    },
    {
        "id": "143534",
        "GT": "Lab 5 : Young's modulus of steel\nFebruary 2nd, 2017\n\nProcedure:\ni) The micrometer was first set so that the level was even. The measurement was recorded as the origin for the measurements that followed. 100 grams were added into the pan and the micrometer was adjusted to re-centre the level. The reading was then recorded and this was repeated while adding 100g each time up to 1kg. After the measurements were taken the 50g weight was removed and the micrometer reading was recorded. This was repeated while removing 100g each time until 50g was reached. The length of the wire was then measured using a 2 metre stick and the thickness was measured by taking three separate measurements using a micrometer and the taking the average. \n\nii) The mass was first struck using the rubber malet. Once the osciliscope displayed a period shape the signal was frozen. The number of complete cycles on the screen was then recorded along with the total time duration of the cycles. The amplitude of the oscillations was also recorded (by looking at the largest amplitude on screen).\n\nZero: 0.170mm +/- 0.005 mm\n\nLength of wire for part 1: 1.74 +/- 0.005 m\nLength of Wire for part 2: 1.09 +/- 0.005 m\nTotal Length: 2.83 +/- 0.01 m->Static Method->import pandas as pd\nimport matplotlib.pyplot as plt \nimport numpy as np\nstatictable = pd.read_excel('Lab5/staticmethoddata.xlsx')\nstatictablereverse = pd.read_excel('Lab5/staticreverse.xlsx')\nstaticdata = statictable.as_matrix()\nstaticdatareverse = statictablereverse.as_matrix()\nzero = 0.170\nstaticdata[:, 1] -= zero\nstaticdatareverse[:,1] -= zero\nstatictable",
        "pred": [
            "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps->import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings->import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Training->Plotting the Results->import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nplt.figure()\nplt.plot(all_losses)\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_001.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_001.png\"/>",
            "matplotlib->Tutorials->Text->Text in Matplotlib Plots->Labels for x- and y-axis->import matplotlib.pyplot as plt\nimport numpy as np\n\nx1 = np.linspace(0.0, 5.0, 100)\ny1 = np.cos(2 * np.pi * x1) * np.exp(-x1)\n\nfig, ax = plt.subplots(figsize=(5, 3))\nfig.subplots_adjust(bottom=0.15, left=0.2)\nax.plot(x1, y1)\nax.set_xlabel('Time [s]')\nax.set_ylabel('Damped oscillation [V]')\n\nplt.show()\n\n\n<img alt=\"text intro\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_text_intro_002.png\" srcset=\"../../_images/sphx_glr_text_intro_002.png, ../../_images/sphx_glr_text_intro_002_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types->import pandas as pd\n\ncv_results = (search_cv.cv_results_)\ncv_results = cv_results.sort_values(\"mean_test_score\", ascending=False)\ncv_results[\n    [\n        \"mean_test_score\",\n        \"std_test_score\",\n        \"param_preprocessor__num__imputer__strategy\",\n        \"param_preprocessor__cat__selector__percentile\",\n        \"param_classifier__C\",\n    ]\n].head(5)\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ]
    },
    {
        "id": "1487228",
        "GT": "How do I read  tabular data file into pandas?->user_names = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\nusers = pd.read_table(url2, sep='|', header=None, names=user_names)\nusers.head()",
        "pred": [
            "pandas_toms_blog->Fast Pandas->Constructors->files = glob.glob('weather/*.csv')\nweather_dfs = [pd.read_csv(fp, names=columns) for fp in files]\nweather = pd.concat(weather_dfs)",
            "pandas_toms_blog->Scaling->Using Dask->df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "pandas_toms_blog->Time Series->Timeseries->gs = web.DataReader(\"GS\", data_source='yahoo', start='2006-01-01',\n                    end='2010-01-01')\ngs.head().round(2)",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()"
        ]
    },
    {
        "id": "772060",
        "GT": "2.1 Generate fake pictures->Combine train and valid set. make all the data useful.->X_train_valid = np.concatenate((X_train, X_valid), axis = 0)\ny_train_valid = np.concatenate((y_train, y_valid), axis = 0)",
        "pred": [
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding->training_labels = one_hot_encoding(y_train[:training_sample])\ntest_labels = one_hot_encoding(y_test[:test_sample])",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->train_df = textproc.txt_to_df(imdb_train)\ntest_df = textproc.txt_to_df(imdb_test)",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->train_importances = (\n    train_result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\ntest_importances = (\n    test_results.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the image data to the floating-point format->The data type of training images: float64\nThe data type of test images: float64\n\n\n\n\n<blockquote>"
        ]
    },
    {
        "id": "357432",
        "GT": "Introduction\n\nAs a professor, I am often called upon to advise students. I teach at a college with a petroleum engineering program, and in recent years (up until the price of oil took a nosedive last year, in fact) the program was very successful, both in terms of attracting students as well as getting them placed in good jobs. At this point, though (February of 2016, as of this writing), a barrel of oil costs less than the barrel to put it in, and that's having a negative impact on the job prospects of our graduates (to put it mildly). I've had a couple of conversations lately with students who have a very different perspective on the job market than I do, and so I became interested in seeing what data was available and what conclusions I could draw from it. The actual comment that made me want to do this analysis was, and I quote, \"I believe in oil.\" On the other hand, he was talking to me about other options, so his belief may not harm him too much.\n\n**TL;DR:** After looking at the data I personally think now is not a good time to get into the petroleum industry, and students who are going to graduate in the next few years with Petroleum Engineering degrees will have an extremely hard time getting jobs. Draw your own conclusions.\n\nThere are three pieces to the puzzle:\n\n1. The number of available jobs\n2. The price of oil.\n3. The number of people with petroleum engineering degrees (i.e., the competition)\n\nObviously, the price of oil has a causal effect; if the price is too low, then companies can't hire, or may go out of business. More on that later.  We'll begin with the number of jobs.\n\n### Disclaimers:\n\n- I'm not an economist, and so the economic part of the analysis is more suggestive than rigorous\n- While I do aspire to the Buddhist ideal of Right Livelihood, what you choose for your career isn't really any of my business. I'm not trying to convert anyone here; I just think people should make decisions with full information.\n- I don't take any responsibility for what you choose to do with your life. (This should be a given)\n- My analysis can only ever be as good as my data sources. All of the data are publicly available from US Government sources (details are below).\n\n\n## Jobs in the petroleum industry\n\nI'm only considering jobs in the US, here. I'm not sure (off the top of my head) where to get data on the international oil scene, and I think it's much less relevant to most of our students. We get statistics on jobs from the Employment, Hours, and Earnings portion of the Current Employment Statistics available from the [Bureau of Labor Statistics](http://www.bls.gov). (The specific survey data is available [here](http://download.bls.gov/pub/time.series/ce/) ).->prices = jd.PriceData()\nprices.plot()",
        "pred": [
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns->data = ()\nX = ().fit_transform(data[\"data\"])\nfeature_names = data[\"feature_names\"]",
            "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Fusing Convolution with Batch Norm->traced_model = (model)\nprint(traced_model.graph)",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data->flights = sns.load_dataset(\"flights\")\nflights.head()\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data->flights = sns.load_dataset(\"flights\")\nflights.head()\n",
            "torch->Parallel and Distributed Training->Getting Started with Fully Sharded Data Parallel(FSDP)->How to use FSDP->model = Net().to(rank)\nmodel = DDP(model)"
        ]
    },
    {
        "id": "1467939",
        "GT": "Handle missing data->minyr = data[data.YearBuilt==1196].index[0]\nmaxyr = data[data.YearBuilt==2106].index[0]",
        "pred": [
            "torch->PyTorch Recipes->PyTorch Benchmark->Steps->4. Benchmarking with Blocked Autorange->m0 = t0.blocked_autorange()\nm1 = t1.blocked_autorange()\n\nprint(m0)\nprint(m1)\n\n\n\nOutput",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA->sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()",
            "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->n_neighbors = 12  # neighborhood which is used to recover the locally linear structure\nn_components = 2  # number of coordinates for the manifold",
            "pandas_toms_blog->Scaling->Dask->most_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->delta_hat, rho_hat = sarima_res.params[:2]\ndelta_hat + rho_hat * y[0]"
        ]
    },
    {
        "id": "1026713",
        "GT": "2. Normal Distribution->md = {}\nmd['normal'] = np.random.normal(popMean, scale=100, size=100)",
        "pred": [
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Fitting with the default arguments->kde = sm.nonparametric.KDEUnivariate(obs_dist)\nkde.fit()  # Estimate the densities",
            "torch->Frontend APIs->Per-sample-gradients->Per-sample-grads, the efficient way, using function transforms->ft_compute_sample_grad = (ft_compute_grad, in_dims=(None, None, 0, 0))",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->pca_model = PCA(dta.T, standardize=False, demean=True)",
            "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Gaussian gradient magnitude method->x_ray_image_gaussian_gradient = ndimage.gaussian_gradient_magnitude(xray_image, sigma=2)"
        ]
    },
    {
        "id": "1280451",
        "GT": "Let us explore through visualization\n\nSince data contains geographical distribution of data over cantons, the best way for visualizing such data is to place such data on a map. The position of each canton and the geo(political) factors such as neighboring different coutries or certain geographical features may provide worthy in the analysis.\n\n#### GeoJSON vs TopoJSON\nWe are provided with TopoJSON files, which is a variant of GeoJSON. In GeoJSON each object to represent on map had separately listed coordinates which served as an input to the specified method of drawing (e.g. a line or polygon). TopoJSON utilizes the same paradigm, but it uses a compression strategy: instead of specifying coordinates for each object, we construct a dictionary of coordinates. This way we avoid repetition and reduce the total size of a GeoJSON file by accessing a single dictionary.\n\nNevertheless, *Folium* uses GeoJSON as a primary source for displaying data and certain preprocessing steps are necessary to achieve the same result!\n\nWe load the Swiss TopoJSON object, representing the outline of each canton:->Unifying TopoJSON and statistical data identifier\n\nBefore we show data on the map, we need to have corresponding identifiers both in statistical data and in the TopoJSON data. One logical option was to use the canton abbreviations. Since in our data both German and French names occur, depending on the downloaded data (the file combining nationality and age was available only in German), we need to establish a mapping between such names and the canton code.\n\nFirst, we extract the canton code (`id`) from the TopoJSON file:->cantons_pairs_de",
        "pred": [
            "numpy->NumPy Features->Masked Arrays->Missing data->nbcases_ma",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator->Squared error loss->beta_true",
            "pandas_toms_blog->Scaling->Dask->by_occupation",
            "pandas_toms_blog->Scaling->Dask->avg_transaction",
            "pandas_toms_blog->Scaling->Dask->total_by_employee"
        ]
    },
    {
        "id": "999816",
        "GT": "grouped = df.groupby('key1')",
        "pred": [
            "torch->Introduction to PyTorch->Build the Neural Network->Define the Class->model = ().to(device)\nprint(model)",
            "statsmodels->Examples->Generalized Estimating Equations->GEE Nested Covariance Structure->groups_var = 1\nlevel1_var = 2\nlevel2_var = 3\nresid_var = 4",
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis->df = generate_crossed()",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->delays.nsmallest(5).sort_values()",
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model->exog = sm.add_constant(exog, prepend=True)"
        ]
    },
    {
        "id": "989685",
        "GT": "Top 10 tracks for last 90 days->nbDays = '90'\ndf = lf.retrieve_top_tracks_as_dataframe(cursor, nbDays, 10)\n\niplot(lf.create_figure(df.Track, df.ArtistAlbumTrack, df.PlayCount, 'tracks', nbDays))",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables->nsample = 50\ngroups = np.zeros(nsample, int)\ngroups[20:40] = 1\ngroups[40:] = 2\n# dummy = (groups[:,None] == np.unique(groups)).astype(float)\n\ndummy = pd.get_dummies(groups).values\nx = np.linspace(0, 20, nsample)\n# drop reference category\nX = np.column_stack((x, dummy[:, 1:]))\nX = sm.add_constant(X, prepend=False)\n\nbeta = [1.0, 3, -3, 10]\ny_true = np.dot(X, beta)\ne = np.random.normal(size=nsample)\ny = y_true + e",
            "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs->n_components = 150\n\nprint(\n    \"Extracting the top %d eigenfaces from %d faces\" % (n_components, X_train.shape[0])\n)\nt0 = ()\npca = (n_components=n_components, svd_solver=\"randomized\", whiten=True).fit(X_train)\nprint(\"done in %0.3fs\" % (() - t0))\n\neigenfaces = pca.components_.reshape((n_components, h, w))\n\nprint(\"Projecting the input data on the eigenfaces orthonormal basis\")\nt0 = ()\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\nprint(\"done in %0.3fs\" % (() - t0))",
            "matplotlib->Tutorials->Colors->Colormap Normalization->Centered->delta = 0.1\nx = np.arange(-3.0, 4.001, delta)\ny = np.arange(-4.0, 3.001, delta)\nX, Y = np.meshgrid(x, y)\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (0.9*Z1 - 0.5*Z2) * 2\n\n# select a divergent colormap\ncmap = cm.coolwarm\n\nfig, (ax1, ax2) = plt.subplots(ncols=2)\npc = ax1.pcolormesh(Z, cmap=cmap)\nfig.colorbar(pc, ax=ax1)\nax1.set_title('Normalize()')\n\npc = ax2.pcolormesh(Z, norm=colors.CenteredNorm(), cmap=cmap)\nfig.colorbar(pc, ax=ax2)\nax2.set_title('CenteredNorm()')\n\nplt.show()\n\n\n<img alt=\"Normalize(), CenteredNorm()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_002.png\" srcset=\"../../_images/sphx_glr_colormapnorms_002.png, ../../_images/sphx_glr_colormapnorms_002_2_0x.png 2.0x\"/>",
            "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->Comparing OLS and RLM->nsample = 50\nx1 = np.linspace(0, 20, nsample)\nX = np.column_stack((x1, (x1 - 5) ** 2))\nX = sm.add_constant(X)\nsig = 0.3  # smaller error variance makes OLS&lt;-RLM contrast bigger\nbeta = [5, 0.5, -0.0]\ny_true2 = np.dot(X, beta)\ny2 = y_true2 + sig * 1.0 * np.random.normal(size=nsample)\ny2[[39, 41, 43, 45, 48]] -= 5  # add some outliers (10% of nsample)",
            "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->WLS Estimation->Artificial data: Heteroscedasticity 2 groups->nsample = 50\nx = np.linspace(0, 20, nsample)\nX = np.column_stack((x, (x - 5) ** 2))\nX = sm.add_constant(X)\nbeta = [5.0, 0.5, -0.01]\nsig = 0.5\nw = np.ones(nsample)\nw[nsample * 6 // 10 :] = 3\ny_true = np.dot(X, beta)\ne = np.random.normal(size=nsample)\ny = y_true + sig * w * e\nX = X[:, [0, 1]]"
        ]
    },
    {
        "id": "100476",
        "GT": "## With ensembling\nif toEnsemble:\n    pred_test_l = model_l.predict_proba(test_f.iloc[:, 1:])\n    pred_test_r = model_r.predict_proba(test_f.iloc[:, 1:])\n    pred_test_x = model_x.predict_proba(test_f.iloc[:, 1:])\n    x = 0.0*pred_test_l + 0.5*pred_test_r + 0.5*pred_test_x",
        "pred": [
            "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch->Steps->4. Save the general checkpoint-># Additional information\nEPOCH = 5\nPATH = \"model.pt\"\nLOSS = 0.4\n\n({\n            'epoch': EPOCH,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': LOSS,\n            }, PATH)",
            "torch->Introduction to PyTorch->Datasets & DataLoaders->Iterate through the DataLoader-># Display image and label.\n,  = next(iter())\nprint(f\"Feature batch shape: {.size()}\")\nprint(f\"Labels batch shape: {.size()}\")\n = [0].squeeze()\n = [0]\nplt.imshow(, cmap=\"gray\")\nplt.show()\nprint(f\"Label: {}\")\n\n\n<img alt=\"data tutorial\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_data_tutorial_002.png\" srcset=\"../../_images/sphx_glr_data_tutorial_002.png\"/>",
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models-># Graph\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n\n# Plot data points\ninf[\"CPIAUCNS\"].plot(ax=ax, style=\"-\", label=\"Observed data\")\n\n# Plot estimate of the level term\nres_uc_mle.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (MLE)\")\nres_uc_bayes.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (Bayesian)\")\n\nax.legend(loc=\"lower left\");\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\"/>",
            "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Transformer Block With a Novel Normalization-># Profile rms_norm\nfunc = functools.partial(\n    with_rms_norm,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"Eager Mode - RMS Norm\")",
            "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->nvFuser & Dynamic Shapes-># Perform warm-up iterations\nfor _ in range(3):\n     = inputs1[0]\n     = inputs2[0]\n     = grad_outputs[0]\n    # Run model, forward and backward\n     = (\n        ,\n        ,\n        ,\n        ,\n        ,\n        normalization_axis=2,\n        dropout_prob=0.1,\n    )\n    ()"
        ]
    },
    {
        "id": "545262",
        "GT": "Boston Housing\n\nGoals:    \n+  Measured the performance of the model using R2 MSE\n+  Learned R2, mean_squared_error\n+  Implemented a new model using L2 regularization using Ridge->from sklearn import datasets\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nimport numpy as np",
        "pred": [
            "sklearn->Examples->Generalized Linear Models->Curve Fitting with Bayesian Ridge Regression->Fit by cubic polynomial->from sklearn.linear_model import \n\nn_order = 3\nX_train = (x_train, n_order + 1, increasing=True)\nX_test = (x_test, n_order + 1, increasing=True)\nreg = (tol=1e-6, fit_intercept=False, compute_score=True)",
            "sklearn->Examples->Semi Supervised Classification->Label Propagation learning a complex structure->import numpy as np\nfrom sklearn.datasets import \n\nn_samples = 200\nX, y = (n_samples=n_samples, shuffle=False)\nouter, inner = 0, 1\nlabels = (n_samples, -1.0)\nlabels[0] = outer\nlabels[-1] = inner",
            "sklearn->Examples->Pipelines and composite estimators->Effect of transforming the targets in regression model->Synthetic example->import numpy as np\nfrom sklearn.datasets import \n\nX, y = (n_samples=10_000, noise=100, random_state=0)\ny = ((y + abs(y.min())) / 200)\ny_trans = (y)",
            "sklearn->Examples->Ensemble methods->Discrete versus Real AdaBoost->Adaboost with discrete SAMME and real SAMME.R->from sklearn.ensemble import \n\nada_discrete = (\n    estimator=dt_stump,\n    learning_rate=learning_rate,\n    n_estimators=n_estimators,\n    algorithm=\"SAMME\",\n)\nada_discrete.fit(X_train, y_train)",
            "sklearn->Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Evaluation->from sklearn import metrics\n\nY_pred = rbm_features_classifier.predict(X_test)\nprint(\n    \"Logistic regression using RBM features:\\n%s\\n\"\n    % ((Y_test, Y_pred))\n)"
        ]
    },
    {
        "id": "1507299",
        "GT": "Classification with Scikit-Learn->Classfication using Logistic Regression->## Split the input features and outcome variable\n\ngot_data_X = got_data.drop('dead',axis=1)\ngot_data_Y = got_data['dead']\ngot_data_X.head()",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()",
            "statsmodels->Examples->State space models->State space modeling: Local Linear Trends-># Perform prediction and forecasting\npredict = res.get_prediction()\nforecast = res.get_forecast('2014')",
            "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->Bayesian estimation-># Set sampling params\nndraws = 3000  #  3000 number of draws from the distribution\nnburn = 600  # 600 number of \"burn-in points\" (which will be discarded)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example-># Compute the root mean square error\nrmse = (flattened**2).mean(axis=1)**0.5\n\nprint(rmse)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend-># Compute the root mean square error\nrmse = (flattened**2).mean(axis=1)**0.5\n\nprint(rmse)",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')"
        ]
    },
    {
        "id": "755868",
        "GT": "Tropical Storm Dataset\n## Profile, Clean, ETL and Plot GIS Data->df = pd.read_csv('storm_data.csv', encoding = 'utf8')",
        "pred": [
            "pandas_toms_blog->Scaling->Dask->df.visualize(rankdir='LR')",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->df.info()",
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis->df = generate_nested()",
            "statsmodels->Examples->Statistics->ANOVA->df_infl = infl.summary_frame()",
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis->df = generate_crossed()"
        ]
    },
    {
        "id": "659175",
        "GT": "Data analysis\n- Step 0: Training-validation data split\n- **Step 1: Data analysis**\n\t- Handle missing values (NaNs)\n\t- Clip outliers\n\t- Analyze distribution of the features\n- Step 2: Classification\n- Step 3: Summary->Feature description\n- The features include continuous, binary, categorical and string variables.->display(pd.read_csv(os.path.join('data','description','features_description.csv'), encoding='iso-8859-1'))",
        "pred": [
            "torch->PyTorch Recipes->Timer quick start->6. A/B testing with Callgrind->print(delta.transform(extract_fn_name).filter(lambda fn: \"TensorIterator\" in fn))\n\n\n\n<strong>Instruction count delta (filter)</strong>",
            "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Investigating the Performance of ResNet18->interp = (rn18)\n(input)\nprint(interp.summary(True))",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "scipy->Compressed Sparse Graph Routines (scipy.sparse.csgraph)->Example: Word Ladders->word_list = open('/usr/share/dict/words').readlines()\n word_list = map(str.strip, word_list)",
            "statsmodels->Examples->Time Series Analysis->Time Series Filters->Christiano-Fitzgerald approximate band-pass filter: Inflation and Unemployment->print(sm.tsa.stattools.adfuller(dta[\"unemp\"])[:3])"
        ]
    },
    {
        "id": "1001963",
        "GT": "Appendix A -- Example transformation from .fits to pd.dataframe->Convert spectrum file to dataframe, header-># Grab the first file from the glob list.\ntest_spec = spec_files[0]\ntest_spec",
        "pred": [
            "seaborn->User guide and tutorial->An introduction to seaborn-># Import seaborn\nimport seaborn as sns\n",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Show the summary of the news results\nprint(news.summary())",
            "seaborn->User guide and tutorial->An introduction to seaborn-># Load an example dataset\ntips = sns.load_dataset(\"tips\")\n",
            "torch->Model Optimization->torch.compile Tutorial->Comparison to TorchScript and FX Tracing-># Reset since we are using a different mode.\n()\n\ncompile_f1 = (f1)\nprint(\"compile 1, 1:\", test_fns(f1, compile_f1, (, )))\nprint(\"compile 1, 2:\", test_fns(f1, compile_f1, (-, )))\nprint(\"~\" * 10)",
            "seaborn->User guide and tutorial->An introduction to seaborn-># Apply the default theme\nsns.set_theme()\n"
        ]
    },
    {
        "id": "597142",
        "GT": "Part one: getting general information about the movies with Selenium\n\nThe reason why I do this with Selenium and the further data scraping (also) with Scrapy instead of doing everything with one package is twofold:\n* I didn't manage to find the necessary urls with Scrapy, not even when copying the exact Xpath from the browser, so I wasn't able to do part one with scrapy. Part two takes forever with Selenium (driver.get(url) is not time efficient at all), so I decided to scrape the data for each movie with Scrapy.\n* To learn the basics about both Selenium and Scrapy. \n\nThe main difference in utility that I noticed is that when working with Selenium, you simulate a real person. For instance, you cannot click buttons that are scrolled out of view. I assume that this is because Selenium uses a webdriver. With Scrapy you can simulate clicking whatever there is on the page, independently of whether it is in view or not.\n\nThree things will be extracted from the initial page (all Sci-Fi and Fantasy movies): \n\n* title\n* url to movie page\n\n*__Important:__* the rest of the information will be extracted per url. In order to put all of the data together in a df, there has to be a variable on which to match the rating information with the general information, nl. url.  \nAs a first step, I will create two lists: one with urls, one with titles. Since lists are ordered, I assume that the elements at the same index in each of the three lists refers to the same movie. They can be added to a df, and afterwards, the url-column will be used as a reference to merge the rest of the data.->for i in range(49):\n    button = driver.find_element_by_css_selector('button.btn.btn-secondary-rt.mb-load-btn')\n    driver.execute_script(\"arguments[0].scrollIntoView();\", button)\n    button.click()",
        "pred": [
            "torch->Introduction to PyTorch->Quickstart->Optimizing the Model Parameters->epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->def sinplot(n=10, flip=1):\n    x = np.linspace(0, 14, 100)\n    for i in range(1, n + 1):\n        plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip)\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->def sinplot(n=10, flip=1):\n    x = np.linspace(0, 14, 100)\n    for i in range(1, n + 1):\n        plt.plot(x, np.sin(x + i * .5) * (n + 2 - i) * flip)\n",
            "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->4. Test dynamic quantization->def print_size_of_model(model):\n    ((), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\nprint_size_of_model(model)\nprint_size_of_model(quantized_model)",
            "numpy->NumPy Features->Masked Arrays->Exploring the data->import matplotlib.pyplot as plt\n\nselected_dates = [0, 3, 11, 13]\nplt.plot(dates, nbcases.T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\")",
            "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->Inference->Preparing the image->with open(\"../_static/img/sample_file.jpeg\", 'rb') as f:\n    image_bytes = f.read()\n    tensor = transform_image(image_bytes=image_bytes)\n    print(tensor)"
        ]
    },
    {
        "id": "207208",
        "GT": "avalon = h2s_avalon[np.logical_and(h2s_avalon != np.array(None), h2s_liberty != np.array(None))]\nliberty = h2s_liberty[np.logical_and(h2s_avalon != np.array(None), h2s_liberty != np.array(None))]",
        "pred": [
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->train_result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\ntest_results = (\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_importances_idx = train_result.importances_mean.argsort()",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->train_importances = (\n    train_result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\ntest_importances = (\n    test_results.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->fl_date         datetime64[ns]\norigin                category\ncrs_dep_time    datetime64[ns]\ndep_time        datetime64[ns]\ncrs_arr_time    datetime64[ns]\narr_time        datetime64[ns]\ndtype: object",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Binary Model compared to Logit->res_logit_hac = mod_logit.fit(method='bfgs', disp=False, cov_type=\"hac\", cov_kwds={\"maxlags\": 2})\nres_log_hac = mod_log.fit(method='bfgs', disp=False, cov_type=\"hac\", cov_kwds={\"maxlags\": 2})",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA->sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()"
        ]
    },
    {
        "id": "1511473",
        "GT": "Helper functions for K-means-># Assigns each data point to a centroid\ndef assignment(X, centroids):\n    C = dict.fromkeys(range(X.shape[0]), np.inf)\n    Z = {}\n    for i in centroids.keys():\n        for j in range(X.shape[0]):\n            \n            # Euclidean dist\n            dist = abs(np.linalg.norm(X[j] - centroids[i]))\n            \n            # Change assignment if dist is lesser than previous\n            if dist < C[j]:\n                C[j] = dist\n                Z[j] = i\n    return Z",
        "pred": [
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Updating the Parameters-># Update the parameters using Adam optimization\ndef update_parameters(parameters, gradients, v, s,\n                      learning_rate=0.01, beta1=0.9, beta2=0.999):\n    for key in parameters:\n        # Moving average of the gradients\n        v['d' + key] = (beta1 * v['d' + key]\n                        + (1 - beta1) * gradients['d' + key])\n\n        # Moving average of the squared gradients\n        s['d' + key] = (beta2 * s['d' + key]\n                        + (1 - beta2) * (gradients['d' + key] ** 2))\n\n        # Update parameters\n        parameters[key] = (parameters[key] - learning_rate\n                           * v['d' + key] / np.sqrt(s['d' + key] + 1e-8))\n    # Return updated parameters and moving averages\n    return parameters, v, s",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Updating the Parameters-># initialise the moving averages\ndef initialise_mav(hidden_dim, input_dim, params):\n    v = {}\n    s = {}\n    # Initialize dictionaries v, s\n    for key in params:\n        v['d' + key] = np.zeros(params[key].shape)\n        s['d' + key] = np.zeros(params[key].shape)\n    # Return initialised moving averages\n    return v, s",
            "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Setup and Performance Metrics-># Utility to profile the workload\ndef profile_workload(forward_func, , iteration_count=100, label=\"\"):\n    # Perform warm-up iterations\n    for _ in range(3):\n        # Run model, forward and backward\n         = forward_func()\n        ()\n        # delete gradiens to avoid profiling the gradient accumulation\n        for  in parameters:\n            .grad = None\n\n    # Synchronize the GPU before starting the timer\n    ()\n    start = time.perf_counter()\n    for _ in range(iteration_count):\n        # Run model, forward and backward\n         = forward_func()\n        ()\n        # delete gradiens to avoid profiling the gradient accumulation\n        for  in parameters:\n            .grad = None\n\n    # Synchronize the GPU before stopping the timer\n    ()\n    stop = time.perf_counter()\n    iters_per_second = iteration_count / (stop - start)\n    if label:\n        print(label)\n    print(\"Average iterations per second: {:.2f}\".format(iters_per_second))",
            "torch->Text->NLP From Scratch: Generating Names with a Character-Level RNN->Training->Preparing for Training-># Make category, input, and target tensors from a random category, line pair\ndef randomTrainingExample():\n    category, line = randomTrainingPair()\n    category_tensor = categoryTensor(category)\n    input_line_tensor = inputTensor(line)\n    target_line_tensor = targetTensor(line)\n    return category_tensor, input_line_tensor, target_line_tensor",
            "torch->Image and Video->Adversarial Example Generation->Implementation->FGSM Attack-># FGSM attack code\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = (perturbed_image, 0, 1)\n    # Return the perturbed image\n    return perturbed_image"
        ]
    },
    {
        "id": "365388",
        "GT": "df_round_trip = pd.DataFrame()\ndf_round_trip = df2016.loc[df2016['start station id'] == df2016['end station id'], :]\nlen(df_round_trip)/len(df2016)",
        "pred": [
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "statsmodels->User Guide->Statistics and Tools->Contingency tables->Symmetry and homogeneity->sqtab = sm.stats.SquareTable.from_data(data[['left', 'right']])\nprint(sqtab.summary())",
            "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures->summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "pandas_toms_blog->Time Series->Timeseries->Forecasting->pred = res_seasonal.get_prediction(start='2001-03-01')\npred_ci = pred.conf_int()",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']"
        ]
    },
    {
        "id": "1317623",
        "GT": "Avoid Variables\n\n### Avdsurg3\n\nWhat number would you use to rate how important is to you to avoid surgery?\n\n0=not at all important, 1=1, 2=2, 3=3, 4=4, 5=5, 6=6, 7=7, 8=8, 9=9, 10=extremely important->sns.countplot(x = 'Avdsurg3', data=va, hue = 'txgot_binary')",
        "pred": [
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations->sns.displot(data=tips, kind=\"ecdf\", x=\"total_bill\", col=\"time\", hue=\"smoker\", rug=True)\n",
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Plots for categorical data->sns.catplot(data=tips, kind=\"violin\", x=\"day\", y=\"total_bill\", hue=\"smoker\", split=True)\n",
            "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Combining multiple views on the data->sns.jointplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\")\n",
            "seaborn->API Overview->Overview of seaborn plotting functions->Combining multiple views on the data->sns.jointplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\")\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", thresh=.2, levels=4)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", thresh=.2, levels=4)\n",
            "seaborn->User guide and tutorial->Visualizing categorical data->Comparing distributions->Violinplots->sns.catplot(\n    data=tips, x=\"total_bill\", y=\"day\", hue=\"sex\", kind=\"violin\",\n)\n",
            "seaborn->Plotting functions->Visualizing categorical data->Comparing distributions->Violinplots->sns.catplot(\n    data=tips, x=\"total_bill\", y=\"day\", hue=\"sex\", kind=\"violin\",\n)\n"
        ]
    },
    {
        "id": "942816",
        "GT": "Text classification using TfidVectorization and Multinomial Gaussian Naive Bayes\nUsing `20_newsgroups` dataset->Import and inspect data->We have 5000 samples in our subset containing 5 text categories->subset.shape",
        "pred": [
            "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Products with n-dimensional arrays->reconstructed.shape",
            "numpy->NumPy Features->Saving and sharing your NumPy arrays->Our arrays as a csv file->load_xy.shape",
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time",
            "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution->kde.entropy",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.dtypes"
        ]
    },
    {
        "id": "1023140",
        "GT": "A bigger example->1000 randomly selected words->matplotlib.rcParams['font.size'] = 7\nplot(space, sample(list(space.index.values), 1000))",
        "pred": [
            "matplotlib->Tutorials->Text->Text rendering with XeLaTeX/LuaLaTeX via the ``pgf`` backend->matplotlib.use('pgf')",
            "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Using style sheets->Defining your own style->import matplotlib.pyplot as plt\n plt.style.use('./images/presentation.mplstyle')",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Scaling plot elements->sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\nsinplot()\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Scaling plot elements->sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\nsinplot()\n",
            "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Using style sheets->Composing styles->import matplotlib.pyplot as plt\n plt.style.use(['dark_background', 'presentation'])",
            "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Using style sheets->Distributing styles->import matplotlib.pyplot as plt\n plt.style.use(&lt;style-name)"
        ]
    },
    {
        "id": "1017405",
        "GT": "one_mode_multilevel = RCT2.networkMultiLevel('keywords', 'authorsFull')\nmk.graphStats(one_mode_multilevel)",
        "pred": [
            "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach->mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "statsmodels->Examples->User Notes->Prediction->Estimation->olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())"
        ]
    },
    {
        "id": "810669",
        "GT": "let's look at some data !!\n<img src=\"data.jpg\">->Airline code to name\n\nSince the airline is denoted as the code in the on-time performance data, we created the following look-up table for airline code to full name. The data is from [IATA webpage](http://www.iata.org/publications/Pages/code-search.aspx).->airline_dictionary = {'9E': 'Pinnacle Airlines', \n              'AA': 'American Airlines', \n              'AS': 'Alaska Airlines',\n              'B6': 'JetBlue',\n              'DL': 'Delta Air Lines',\n              'EV': 'Atlantic Southeast Airlines',\n              'F9': 'Frontier Airlines',\n              'G4': 'Allegiant Air',\n              'HA': 'Hawaiian Airlines',\n              'MQ': 'Envoy Air',\n              'NK': 'Spirit Airlines',\n              'OH': 'Comair',\n              'OO': 'SkyWest  Airlines',\n              'UA': 'United Airlines',\n              'VX': 'Virgin America',\n              'WN': 'Southwest Airlines',\n              'YV': 'Mesa Airlines',\n              'YX': 'Midwest Airlines'\n             }",
        "pred": [
            "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method->austourists_data = [\n    30.05251300,\n    19.14849600,\n    25.31769200,\n    27.59143700,\n    32.07645600,\n    23.48796100,\n    28.47594000,\n    35.12375300,\n    36.83848500,\n    25.00701700,\n    30.72223000,\n    28.69375900,\n    36.64098600,\n    23.82460900,\n    29.31168300,\n    31.77030900,\n    35.17787700,\n    19.77524400,\n    29.60175000,\n    34.53884200,\n    41.27359900,\n    26.65586200,\n    28.27985900,\n    35.19115300,\n    42.20566386,\n    24.64917133,\n    32.66733514,\n    37.25735401,\n    45.24246027,\n    29.35048127,\n    36.34420728,\n    41.78208136,\n    49.27659843,\n    31.27540139,\n    37.85062549,\n    38.83704413,\n    51.23690034,\n    31.83855162,\n    41.32342126,\n    42.79900337,\n    55.70835836,\n    33.40714492,\n    42.31663797,\n    45.15712257,\n    59.57607996,\n    34.83733016,\n    44.84168072,\n    46.97124960,\n    60.01903094,\n    38.37117851,\n    46.97586413,\n    50.73379646,\n    61.64687319,\n    39.29956937,\n    52.67120908,\n    54.33231689,\n    66.83435838,\n    40.87118847,\n    51.82853579,\n    57.49190993,\n    65.25146985,\n    43.06120822,\n    54.76075713,\n    59.83447494,\n    73.25702747,\n    47.69662373,\n    61.09776802,\n    66.05576122,\n]\nindex = pd.date_range(\"1999-03-01\", \"2015-12-01\", freq=\"3MS\")\naustourists = pd.Series(austourists_data, index=index)\naustourists.plot()\nplt.ylabel(\"Australian Tourists\")",
            "torch->Introduction to PyTorch->Quickstart->Loading Models->classes = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\n()\n, y = [0][0], [0][1]\nwith ():\n     = model()\n    predicted, actual = classes[[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')",
            "torch->Introduction to PyTorch->Datasets & DataLoaders->Iterating and Visualizing the Dataset->labels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = (len(), size=(1,)).item()\n    ,  = [sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[])\n    plt.axis(\"off\")\n    plt.imshow(.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n<img alt=\"Pullover, Sandal, Bag, Bag, Sneaker, Bag, Pullover, Sandal, Sandal\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_data_tutorial_001.png\" srcset=\"../../_images/sphx_glr_data_tutorial_001.png\"/>",
            "torch->Model Optimization->Multi-Objective NAS with Ax->Choosing the GenerationStrategy->total_trials = 48  # total evaluation budget\n\nfrom ax.modelbridge.dispatch_utils import choose_generation_strategy\n\ngs = choose_generation_strategy(\n    search_space=experiment.search_space,\n    optimization_config=experiment.optimization_config,\n    num_trials=total_trials,\n  )",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_mask = (\n    (locations[:, 1] == \"China\")\n    &amp; (locations[:, 0] != \"Hong Kong\")\n    &amp; (locations[:, 0] != \"Taiwan\")\n    &amp; (locations[:, 0] != \"Macau\")\n    &amp; (locations[:, 0] != \"Unspecified*\")\n)"
        ]
    },
    {
        "id": "1163019",
        "GT": "Modern Portfolio Theory: Creating The Most Efficient Portfolio->clean_table = data.set_index('date').pivot(columns = 'ticker')\n\n# Check the data now\nclean_table.head()",
        "pred": [
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()",
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->flights_wide = flights.pivot(index=\"year\", columns=\"month\", values=\"passengers\")\nflights_wide.head()\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->flights_wide = flights.pivot(index=\"year\", columns=\"month\", values=\"passengers\")\nflights_wide.head()\n",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()"
        ]
    },
    {
        "id": "715874",
        "GT": "Visualization tools->Nicely formatted heatmap->allfeatures = list(df.columns.copy())\nallfeatures.remove('customer_id')\ndf_mat = df.head(20).as_matrix(columns=allfeatures)\ndst.heatmap(df_mat, rownames=df.head(20).index, colnames=allfeatures, xlabel='feature', ylabel='customer ID', figsize=(10,5))",
        "pred": [
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, small regularization, normalized variables\")\n(\"Raw coefficient values\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, small regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_010.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_010.png\"/>",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->coefs = (\n    model[-1].regressor_.coef_ * X_train_preprocessed.std(axis=0),\n    columns=[\"Coefficient importance\"],\n    index=feature_names,\n)\ncoefs.plot(kind=\"barh\", figsize=(9, 7))\n(\"Coefficient values corrected by the feature's std. dev.\")\n(\"Ridge model, small regularization\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_005.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_005.png\"/>",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->delta = (by_game.home_rest - by_game.away_rest).dropna().astype(int)\nax = (delta.value_counts()\n    .reindex(np.arange(delta.min(), delta.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind='bar', color='k', width=.9, rot=0, figsize=(12, 6))\n)\nsns.despine()\nax.set(xlabel='Difference in Rest (Home - Away)', ylabel='Games');",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, with regularization, normalized variables\")\n(\"Raw coefficient values\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, with regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_013.png\"/>",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->speech_data_path = 'tutorial-nlp-from-scratch/speeches.csv'\nspeech_df = pd.read_csv(speech_data_path)\nX_pred = textproc.cleantext(speech_df,\n                            text_column='speech',\n                            remove_stopwords=True,\n                            remove_punc=False)\nspeakers = speech_df['speaker'].to_numpy()"
        ]
    },
    {
        "id": "1118804",
        "GT": "now we move towards outlier analysis and scaling and after this and check whole data how is distubuted around 0.05 and -0.05 of zscores then we will do Factor Analysis in PCA and LDA , then at last i will also make cluster of popularity of too , if possible->mpl.rcParams['font.size'] = 30.0\n#labels = ['business','technology','lifestyle','social media','world','entertainment']\nplt.figure(figsize = (30, 30))",
        "pred": [
            "matplotlib->Tutorials->Toolkits->The mplot3d toolkit->import matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')",
            "torch->PyTorch Recipes->Loading data in PyTorch->5. [Optional] Visualize the data->import matplotlib.pyplot as plt\n\nprint(data[0][0].numpy())\n\nplt.figure()\nplt.plot(waveform.t().numpy())",
            "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->plt.rc(\"figure\", figsize=(16, 12))\nplt.rc(\"font\", size=13)",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "matplotlib->Tutorials->Text->Writing mathematical expressions-># math text\nplt.title(r'$\\alpha  \\beta$')"
        ]
    },
    {
        "id": "850496",
        "GT": "5.Data Exploration->5.2 Heart disease types discovering-># print the types of heart disease\nset(heart.loc[:, \"diagnosis\"].values)",
        "pred": [
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Addendum: manually computing the news, weights, and impacts-># Print the news, computed by the `news` method\nprint(news.news)\n\n# Manually compute the news\nprint()\nprint((y_update.iloc[0] - phi_hat * y_pre.iloc[-1]).round(6))",
            "seaborn->User guide and tutorial->An introduction to seaborn-># Apply the default theme\nsns.set_theme()\n",
            "matplotlib->Tutorials->Text->Writing mathematical expressions-># plain text\nplt.title('alpha  beta')",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization-># Select the 5 largest delays\ndelays.nlargest(5).sort_values()",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Show the summary of the news results\nprint(news.summary())"
        ]
    },
    {
        "id": "508146",
        "GT": "new_merged = merged.merge(climate_new,how='inner',on = ['Country Name','years'])",
        "pred": [
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog->data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)",
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Sampling->rng = default_rng()\n\nbefore_sample = rng.choice(before_lock, size=30, replace=False)\nafter_sample = rng.choice(after_lock, size=30, replace=False)",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->feature_names = model[:-1].get_feature_names_out()\n\ncoefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients\"],\n    index=feature_names,\n)\n\ncoefs\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ]
    },
    {
        "id": "643703",
        "GT": "Subset higher order stats by our bounding boxes->#This is our known metadata for the outputs\nmetadata = pd.DataFrame({\n         'orgName': [\n                   #these don't have depth\n                   'icephl_latlon','ben_latlon',\n                   'aice_latlon',\n                   #these do\n                 'phs_latlon','phl_latlon','mzl_latlon','cop_latlon','ncao_latlon','ncas_latlon','eup_latlon','det_latlon',\n                 'temp_latlon',\n                 'u_latlon',\n                 'v_latlon',],\n         'name': ['Ice Phytoplankton Concentration',\n                  'Benthos Concentration',\n                  'Sea Ice Area Fraction',\n                  'Small Phytoplankton Concentration',\n                  'Large Phytoplankton Concentration',\n                  'Large Microzooplankton Concentration',\n                  'Small Coastal Copepod Concentration',\n                  'Offshore Neocalanus Concentration',\n                  'Neocalanus Concentration',\n                  'Euphausiids Concentration',\n                  'Detritus Concentration',\n                  'Sea Water Temperature',\n                  'Zonal (U) Current',\n                  'Meridional (V) Current'],\n         'units': ['mgC/m2','mgC/m2',\n                   'Fraction',\n                   'mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3','mgC/m3',\n                   'degrees C',\n                   'm/s',\n                   'm/s']\n       })",
        "pred": [
            "statsmodels->Examples->State space models->Unobserved Components: Application->Model-># Create Table I\ntable_i = np.zeros((5,6))\n\nstart = dta.index[0]\nend = dta.index[-1]\ntime_range = '%d:%d-%d:%d' % (start.year, start.quarter, end.year, end.quarter)\nmodels = [\n    ('US GNP', time_range, 'None'),\n    ('US Prices', time_range, 'None'),\n    ('US Prices', time_range, r'$\\sigma_\\eta^2 = 0$'),\n    ('US monetary base', time_range, 'None'),\n    ('US monetary base', time_range, r'$\\sigma_\\eta^2 = 0$'),\n]\nindex = pd.MultiIndex.from_tuples(models, names=['Series', 'Time range', 'Restrictions'])\nparameter_symbols = [\n    r'$\\sigma_\\zeta^2$', r'$\\sigma_\\eta^2$', r'$\\sigma_\\kappa^2$', r'$\\rho$',\n    r'$2 \\pi / \\lambda_c$', r'$\\sigma_\\varepsilon^2$',\n]\n\ni = 0\nfor res in (output_res, prices_res, prices_restricted_res, money_res, money_restricted_res):\n    if res.model.stochastic_level:\n        (sigma_irregular, sigma_level, sigma_trend,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n    else:\n        (sigma_irregular, sigma_level,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n        sigma_trend = np.nan\n    period_cycle = 2 * np.pi / frequency_cycle\n\n    table_i[i, :] = [\n        sigma_level*1e7, sigma_trend*1e7,\n        sigma_cycle*1e7, damping_cycle, period_cycle,\n        sigma_irregular*1e7\n    ]\n    i += 1\n\npd.set_option('float_format', lambda x: '%.4g' % np.round(x, 2) if not np.isnan(x) else '-')\ntable_i = pd.DataFrame(table_i, index=index, columns=parameter_symbols)\ntable_i",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Logarithmic and other nonlinear axes-># Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n# make up some data in the open interval (0, 1)\ny = np.random.normal(loc=0.5, scale=0.4, size=1000)\ny = y[(y  0) & (y &lt; 1)]\ny.sort()\nx = np.arange(len(y))\n\n# plot with various axes scales\nplt.figure()\n\n# linear\nplt.subplot(221)\nplt.plot(x, y)\nplt.yscale('linear')\nplt.title('linear')\nplt.grid(True)\n\n# log\nplt.subplot(222)\nplt.plot(x, y)\nplt.yscale('log')\nplt.title('log')\nplt.grid(True)\n\n# symmetric log\nplt.subplot(223)\nplt.plot(x, y - y.mean())\nplt.yscale('symlog', linthresh=0.01)\nplt.title('symlog')\nplt.grid(True)\n\n# logit\nplt.subplot(224)\nplt.plot(x, y)\nplt.yscale('logit')\nplt.title('logit')\nplt.grid(True)\n# Adjust the subplot layout, because the logit one may take more space\n# than usual, due to y-tick labels like \"1 - 10^{-3}\"\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n\nplt.show()\n\n\n<img alt=\"linear, log, symlog, logit\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_010.png\" srcset=\"../../_images/sphx_glr_pyplot_010.png, ../../_images/sphx_glr_pyplot_010_2_0x.png 2.0x\"/>",
            "torch->Image and Video->Adversarial Example Generation->Results->Sample Adversarial Examples-># Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()\n\n\n<img alt=\"2 -&gt; 2, 6 -&gt; 6, 4 -&gt; 4, 6 -&gt; 6, 2 -&gt; 2, 8 -&gt; 3, 4 -&gt; 2, 9 -&gt; 5, 7 -&gt; 9, 8 -&gt; 9, 6 -&gt; 0, 1 -&gt; 8, 6 -&gt; 8, 9 -&gt; 8, 8 -&gt; 6, 5 -&gt; 8, 5 -&gt; 2, 4 -&gt; 8, 4 -&gt; 8, 5 -&gt; 8, 3 -&gt; 5, 4 -&gt; 8, 8 -&gt; 2, 8 -&gt; 6, 3 -&gt; 8, 6 -&gt; 1, 1 -&gt; 3, 1 -&gt; 8, 4 -&gt; 8, 8 -&gt; 2, 7 -&gt; 8, 1 -&gt; 2, 0 -&gt; 2, 7 -&gt; 8, 7 -&gt; 8\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_002.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_002.png\"/>",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Dataset\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\ndata['ln_wpi'] = np.log(data['wpi'])\ndata['D.ln_wpi'] = data['ln_wpi'].diff()\n\n\n\n\n\n\n\nIn\u00a0[5]:",
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data-># Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")"
        ]
    },
    {
        "id": "1159324",
        "GT": "Iris Dataset->The Logistic Model->from sklearn.linear_model import LogisticRegression\nlogr = LogisticRegression(C = 10000, random_state = 0)\nlogr.fit(X_train_std, y_train)\nimport VisualFuncs as vf\n\n\nvf.VDR(X_test_std, y_test, classifier = logr)",
        "pred": [
            "sklearn->Examples->Decomposition->Kernel PCA->Projecting data: PCA vs. KernelPCA->from sklearn.datasets import \nfrom sklearn.model_selection import \n\nX, y = (n_samples=1_000, factor=0.3, noise=0.05, random_state=0)\nX_train, X_test, y_train, y_test = (X, y, stratify=y, random_state=0)",
            "sklearn->Examples->Generalized Linear Models->Sparsity Example: Fitting only features 1  and 2->from sklearn import datasets\nimport numpy as np\n\nX, y = (return_X_y=True)\nindices = (0, 1)\n\nX_train = X[:-20, indices]\nX_test = X[-20:, indices]\ny_train = y[:-20]\ny_test = y[-20:]",
            "sklearn->Examples->Calibration->Probability Calibration for 3-class classification->Compare probabilities->from sklearn.metrics import \n\nscore = (y_test, clf_probs)\ncal_score = (y_test, cal_clf_probs)\n\nprint(\"Log-loss of\")\nprint(f\" * uncalibrated classifier: {score:.3f}\")\nprint(f\" * calibrated classifier: {cal_score:.3f}\")",
            "sklearn->Examples->Generalized Linear Models->Non-negative least squares->from sklearn.model_selection import \n\nX_train, X_test, y_train, y_test = (X, y, test_size=0.5)",
            "sklearn->Examples->Model Selection->Custom refit strategy of a grid search with cross-validation->The dataset->from sklearn.model_selection import \n\nX_train, X_test, y_train, y_test = (X, y, test_size=0.5, random_state=0)"
        ]
    },
    {
        "id": "331630",
        "GT": "Honourable Mentions->Most Profitable Games in each Genre->#There are games with duplicate names (For each platform for example), so let's deal with this\nx = vg_df.groupby(['Genre', 'Name']).sum().reset_index().groupby('Genre')\n\n#A dataframe that will hold rankings, for nice display\nbest_selling_titles_by_genre_df = pd.DataFrame()\n\nfor name, group in x:\n    temp_col = group.sort_values('Global_Sales', ascending=False).head(10).Name.reset_index(drop=True)\n    best_selling_titles_by_genre_df[name] = temp_col",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Hamilton (1989) switching model of GNP-># Get the RGNP data to replicate Hamilton\ndta = pd.read_stata(\"https://www.stata-press.com/data/r14/rgnp.dta\").iloc[1:]\ndta.index = pd.DatetimeIndex(dta.date, freq=\"QS\")\ndta_hamilton = dta.rgnp\n\n# Plot the data\ndta_hamilton.plot(title=\"Growth rate of Real GNP\", figsize=(12, 3))\n\n# Fit the model\nmod_hamilton = sm.tsa.MarkovAutoregression(\n    dta_hamilton, k_regimes=2, order=4, switching_ar=False\n)\nres_hamilton = mod_hamilton.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_4_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_4_0.png\"/>",
            "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Kim, Nelson, and Startz (1998) Three-state Variance Switching-># Get the dataset\new_excs = requests.get(\"http://econ.korea.ac.kr/~cjkim/MARKOV/data/ew_excs.prn\").content\nraw = pd.read_table(BytesIO(ew_excs), header=None, skipfooter=1, engine=\"python\")\nraw.index = pd.date_range(\"1926-01-01\", \"1995-12-01\", freq=\"MS\")\n\ndta_kns = raw.loc[:\"1986\"] - raw.loc[:\"1986\"].mean()\n\n# Plot the dataset\ndta_kns[0].plot(title=\"Excess returns\", figsize=(12, 3))\n\n# Fit the model\nmod_kns = sm.tsa.MarkovRegression(\n    dta_kns, k_regimes=3, trend=\"n\", switching_variance=True\n)\nres_kns = mod_kns.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\"/>",
            "statsmodels->Examples->State space models->State space modeling: Local Linear Trends-># Load Dataset\ndf.index = pd.date_range(start='%d-01-01' % df.date[0], end='%d-01-01' % df.iloc[-1, 0], freq='AS')\n\n# Log transform\ndf['lff'] = np.log(df['ff'])\n\n# Setup the model\nmod = LocalLinearTrend(df['lff'])\n\n# Fit it using MLE (recall that we are fitting the three variance parameters)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "sklearn->Examples->Examples based on real world datasets->Outlier detection on a real data set->Second example-># Get data\nX2 = ()[\"data\"][:, [6, 9]]  # \"banana\"-shaped\n\n# Learn a frontier for outlier detection with several classifiers\nxx2, yy2 = ((-1, 5.5, 500), (-2.5, 19, 500))\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    (2)\n    clf.fit(X2)\n    Z2 = clf.decision_function([xx2.ravel(), yy2.ravel()])\n    Z2 = Z2.reshape(xx2.shape)\n    legend2[clf_name] = (\n        xx2, yy2, Z2, levels=[0], linewidths=2, colors=colors[i]\n    )\n\nlegend2_values_list = list(legend2.values())\nlegend2_keys_list = list(legend2.keys())\n\n# Plot the results (= shape of the data points cloud)\n(2)  # \"banana\" shape\n(\"Outlier detection on a real data set (wine recognition)\")\n(X2[:, 0], X2[:, 1], color=\"black\")\n((xx2.min(), xx2.max()))\n((yy2.min(), yy2.max()))\n(\n    (\n        legend2_values_list[0].collections[0],\n        legend2_values_list[1].collections[0],\n        legend2_values_list[2].collections[0],\n    ),\n    (legend2_keys_list[0], legend2_keys_list[1], legend2_keys_list[2]),\n    loc=\"upper center\",\n    prop=(size=11),\n)\n(\"color_intensity\")\n(\"flavanoids\")\n\n()\n\n\n<img alt=\"Outlier detection on a real data set (wine recognition)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_outlier_detection_wine_002.png\" srcset=\"../../_images/sphx_glr_plot_outlier_detection_wine_002.png\"/>",
            "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Visualize our data-># Generate the class/group data\nn_points = 100\nX = rng.randn(100, 10)\n\npercentiles_classes = [0.1, 0.3, 0.6]\ny = ([[ii] * int(100 * perc) for ii, perc in enumerate(percentiles_classes)])\n\n# Generate uneven groups\ngroup_prior = rng.dirichlet([2] * 10)\ngroups = ((10), rng.multinomial(100, group_prior))\n\n\ndef visualize_groups(classes, groups, name):\n    # Visualize dataset groups\n    fig, ax = ()\n    ax.scatter(\n        range(len(groups)),\n        [0.5] * len(groups),\n        c=groups,\n        marker=\"_\",\n        lw=50,\n        cmap=cmap_data,\n    )\n    ax.scatter(\n        range(len(groups)),\n        [3.5] * len(groups),\n        c=classes,\n        marker=\"_\",\n        lw=50,\n        cmap=cmap_data,\n    )\n    ax.set(\n        ylim=[-1, 5],\n        yticks=[0.5, 3.5],\n        yticklabels=[\"Data\\ngroup\", \"Data\\nclass\"],\n        xlabel=\"Sample index\",\n    )\n\n\nvisualize_groups(y, groups, \"no groups\")\n\n\n<img alt=\"plot cv indices\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cv_indices_001.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_001.png\"/>"
        ]
    },
    {
        "id": "1026300",
        "GT": "Lab3 - Automated Variable Selection Regression\n## Regression Prediction Competition->1. Fitting the model\n\n#### Please write a function that takes the training data and the outputs an lm (smf) object. It should be of the form,->def group_A_fit(training_data):\n    crimedata = pd.read_csv(training_data)\n    m1 = smf.ols(formula=\"ViolentCrimesPerPop ~ NumUnderPov + MalePctDivorce + PctPersDenseHous + pctUrban + racepctblack + \\\n        np.power(racepctblack,2) + PctKids2Par\", data=crimedata).fit()\n    return m1\n    \nm1 = group_A_fit(\"crime-train.csv\")\nprint(m1.summary())",
        "pred": [
            "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Iterative imputation of the missing values->def get_impute_iterative(X_missing, y_missing):\n    imputer = (\n        missing_values=,\n        add_indicator=True,\n        random_state=0,\n        n_nearest_features=3,\n        max_iter=1,\n        sample_posterior=True,\n    )\n    iterative_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return iterative_impute_scores.mean(), iterative_impute_scores.std()\n\n\nmses_california[4], stds_california[4] = get_impute_iterative(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[4], stds_diabetes[4] = get_impute_iterative(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Iterative Imputation\")\n\nmses_diabetes = mses_diabetes * -1\nmses_california = mses_california * -1",
            "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Estimate the score->def get_full_score(X_full, y_full):\n    full_scores = (\n        regressor, X_full, y_full, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    )\n    return full_scores.mean(), full_scores.std()\n\n\nmses_california[0], stds_california[0] = get_full_score(X_california, y_california)\nmses_diabetes[0], stds_diabetes[0] = get_full_score(X_diabetes, y_diabetes)\nx_labels.append(\"Full data\")",
            "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Impute missing values with mean->def get_impute_mean(X_missing, y_missing):\n    imputer = (missing_values=, strategy=\"mean\", add_indicator=True)\n    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return mean_impute_scores.mean(), mean_impute_scores.std()\n\n\nmses_california[3], stds_california[3] = get_impute_mean(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes, y_miss_diabetes)\nx_labels.append(\"Mean Imputation\")",
            "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->kNN-imputation of the missing values->def get_impute_knn_score(X_missing, y_missing):\n    imputer = (missing_values=, add_indicator=True)\n    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return knn_impute_scores.mean(), knn_impute_scores.std()\n\n\nmses_california[2], stds_california[2] = get_impute_knn_score(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[2], stds_diabetes[2] = get_impute_knn_score(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"KNN Imputation\")",
            "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Robust Fitting->def add_stl_plot(fig, res, legend):\n    \"\"\"Add 3 plots from a second STL fit\"\"\"\n    axs = fig.get_axes()\n    comps = [\"trend\", \"seasonal\", \"resid\"]\n    for ax, comp in zip(axs[1:], comps):\n        series = getattr(res, comp)\n        if comp == \"resid\":\n            ax.plot(series, marker=\"o\", linestyle=\"none\")\n        else:\n            ax.plot(series)\n            if comp == \"trend\":\n                ax.legend(legend, frameon=False)\n\n\nstl = STL(elec_equip, period=12, robust=True)\nres_robust = stl.fit()\nfig = res_robust.plot()\nres_non_robust = STL(elec_equip, period=12, robust=False).fit()\nadd_stl_plot(fig, res_non_robust, [\"Robust\", \"Non-robust\"])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\"/>"
        ]
    },
    {
        "id": "997656",
        "GT": "5. Model Building\n## Picking Target Feature\n\nSince we don't have any class already labelled and any continuous target variable, we need to pick a feature for our target feature to predict skipping action of listener. According to the features we created, `per_listen` will be more suitable for that problem since it obviously gives idea about skipping action. If we pick it as a target feature, this problem will turn out a scoring/probability problem because of having ratio of listening time, which tends between 0 to 1.\n\nIf we want to convert that problem to a classfication problem, we can determine a treshold for skipping aciton as a rule of thump. `per_listen` denotes how much percentage of the track that were listened by listener. So, our threshold could be 25%, 50% even 51% and so on. However, before making a decision, we can check out Complementary Cumulative Distribution Function (CCDF) of `per_listen`. It would be give an idea about our reasonanle threshold. According the following plot, we have 65% of instances, whose `per_listen` value is greater than 0.5. Therefore, 0.5 is reasonable, however, when we think about it more realistic, less than 0.5 around 0.25 would be more suitable determine any skipping action.->Cleaning Irrelevant Features\n\nWe created many different features using `listen_duration` including `per_listen`. `per_listen` is our target feature at this point. Since it has high correlation with `listen_duration`, `n_loops`, `is_loop`. To overcome side effects (dataleakage) in model training, we need to get rid of those features.\n\nIn addition, we need to also remove the features in unix time format since we already decompose those time features in the previous stages.->features = ['per_listen', 'listen_duration', 'n_loops', 'is_loop']\n\ng = sns.clustermap(df_data[features].corr(), \n                   linewidths=1, \n                   cmap=\"YlGnBu\", \n                   square=True, \n                   annot=True, \n                   fmt='.3f', \n                   figsize = (5, 5))\nplt.setp(g.ax_heatmap.get_xticklabels(), rotation=90); ",
        "pred": [
            "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with partial observations->features_names = [\"experience\", \"parent hourly wage\", \"college degree\"]\n\nregressor_without_ability = ()\nregressor_without_ability.fit(X_train[features_names], y_train)\ny_pred_without_ability = regressor_without_ability.predict(X_test[features_names])\nR2_without_ability = (y_test, y_pred_without_ability)\n\nprint(f\"R2 score without ability: {R2_without_ability:.3f}\")",
            "pandas_toms_blog->Indexes->airports = ['DSM', 'ORD', 'JFK', 'PDX']\n\ng = sns.FacetGrid(weather.sort_index().loc[airports].reset_index(),\n                  col='station', hue='station', col_wrap=2, size=4)\ng.map(sns.regplot, 'sped', 'gust_mph')\nplt.savefig('../content/images/indexes_wind_gust_facet.svg', transparent=True);",
            "pandas_toms_blog->Indexes->airports = ['DSM', 'ORD', 'JFK', 'PDX']\n\ng = sns.FacetGrid(weather.sort_index().loc[airports].reset_index(),\n                  col='station', hue='station', col_wrap=2, size=4)\ng.map(sns.regplot, 'sped', 'gust_mph')\nplt.savefig('../content/images/indexes_wind_gust_facet.png');",
            "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates->time_s = np.s_[:100]\n\nfig3 = plt.figure()\nax3 = fig3.add_subplot(111)\nh31, = ax3.plot(idx[time_s], res_f.freq_seasonal[1].filtered[time_s] + res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh32, = ax3.plot(idx[time_s], res_tf.freq_seasonal[0].filtered[time_s] + res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh33, = ax3.plot(idx[time_s], true_sum[time_s], label='True Seasonal 100(2)')\nh34, = ax3.plot(idx[time_s], res_lf.freq_seasonal[0].filtered[time_s], label='Lazy Freq. Seas')\nh35, = ax3.plot(idx[time_s], res_lt.seasonal.filtered[time_s], label='Lazy Time Seas')\n\nplt.legend([h31, h32, h33, h34, h35], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth', 'Lazy Freq. Seas', 'Lazy Time Seas'], loc=1)\nplt.title('Seasonal components combined')\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_23_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_23_0.png\"/>",
            "pandas_toms_blog->Fast Pandas->Constructors->files = glob.glob('weather/*.csv')\ncolumns = ['station', 'date', 'tmpf', 'relh', 'sped', 'mslp',\n           'p01i', 'vsby', 'gust_mph', 'skyc1', 'skyc2', 'skyc3']\n\n# init empty DataFrame, like you might for a list\nweather = pd.DataFrame(columns=columns)\n\nfor fp in files:\n    city = pd.read_csv(fp, columns=columns)\n    weather.append(city)"
        ]
    },
    {
        "id": "1448218",
        "GT": "3. Attach additional features to the data set (without writing SQL yourself)\n\nBased on what we already have, automatically attach additional features to the data set, for exmaple, adding new _problem difficutly_ information to the data set becuase we have _problem id_.->sql_with_features = attach_query_result_by_header(sql_data_path)",
        "pred": [
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->data = sm.datasets.fertility.load_pandas().data\ndata.head()",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison->results_all = [res_o, res_f, res_e, res_a]\nnames = \"res_o res_f res_e res_a\".split()",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog->data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)"
        ]
    },
    {
        "id": "1076097",
        "GT": "Correlation of features with target variable:->sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_df[cols], size = 2.5)\nplt.show()",
        "pred": [
            "seaborn->User guide and tutorial->An introduction to seaborn->Opinionated defaults and flexible customization->sns.set_theme(style=\"ticks\", font_scale=1.25)\ng = sns.relplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"body_mass_g\",\n    palette=\"crest\", marker=\"x\", s=100,\n)\ng.set_axis_labels(\"Bill length (mm)\", \"Bill depth (mm)\", labelpad=10)\ng.legend.set_title(\"Body mass (g)\")\ng.figure.set_size_inches(6.5, 4.5)\ng.ax.margins(.15)\ng.despine(trim=True)\n",
            "sklearn->Examples->Model Selection->Comparison between grid search and successive halving->rng = (0)\nX, y = (n_samples=1000, random_state=rng)\n\ngammas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\nCs = [1, 10, 100, 1e3, 1e4, 1e5]\nparam_grid = {\"gamma\": gammas, \"C\": Cs}\n\nclf = (random_state=rng)\n\ntic = ()\ngsh = (\n    estimator=clf, param_grid=param_grid, factor=2, random_state=rng\n)\ngsh.fit(X, y)\ngsh_time = () - tic\n\ntic = ()\ngs = (estimator=clf, param_grid=param_grid)\ngs.fit(X, y)\ngs_time = () - tic",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(tips, x=\"total_bill\", kind=\"kde\", cut=0)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(tips, x=\"total_bill\", kind=\"kde\", cut=0)\n",
            "sklearn->Examples->Nearest Neighbors->Neighborhood Components Analysis Illustration->Learning an embedding->nca = (max_iter=30, random_state=0)\nnca = nca.fit(X, y)\n\n(2)\nax2 = ()\nX_embedded = nca.transform(X)\nrelate_point(X_embedded, i, ax2)\n\nfor i in range(len(X)):\n    ax2.text(X_embedded[i, 0], X_embedded[i, 1], str(i), va=\"center\", ha=\"center\")\n    ax2.scatter(X_embedded[i, 0], X_embedded[i, 1], s=300, c=cm.Set1(y[[i]]), alpha=0.4)\n\nax2.set_title(\"NCA embedding\")\nax2.axes.get_xaxis().set_visible(False)\nax2.axes.get_yaxis().set_visible(False)\nax2.axis(\"equal\")\n()\n\n\n<img alt=\"NCA embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_nca_illustration_002.png\" srcset=\"../../_images/sphx_glr_plot_nca_illustration_002.png\"/>",
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n"
        ]
    },
    {
        "id": "180012",
        "GT": "sns.boxplot(y=\"tenure\", data=churn, fliersize=5)\nplt.show()\n\nsns.boxplot(y=\"MonthlyCharges\", data=churn, fliersize=5)\nplt.show()\n\nsns.boxplot(y=\"TotalCharges\", data=churn, fliersize=10)\nplt.show()",
        "pred": [
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Histogram of standardized deviance residuals with Kernel Density Estimate overlaid->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, title=\"Standardized Deviance Residuals\")\nax.hist(resid_std, bins=25, density=True)\nax.plot(kde_resid.support, kde_resid.density, \"r\")",
            "torch->Introduction to PyTorch->Quickstart->Optimizing the Model Parameters->epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "seaborn->User guide and tutorial->An introduction to seaborn-># Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n"
        ]
    },
    {
        "id": "319062",
        "GT": "Tf-idf\nAnother popular technique when dealing with text is to use the term frequency - inverse document frequency (tf-idf) measure.->for fpr, tpr, roc_label in zip(fprs, tprs, roc_labels):\n    plt.plot(fpr, tpr, label=roc_label)\n\nplt.xlabel(\"fpr\")\nplt.ylabel(\"tpr\")\nplt.title(\"ROC Curves\")\nplt.legend()\nplt.xlim([0, .07])\nplt.ylim([.98, 1])\nplt.show()",
        "pred": [
            "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits->for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>",
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data->for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->for name, importances in zip([\"train\", \"test\"], [train_importances, test_importances]):\n    ax = importances.plot.box(vert=False, whis=10)\n    ax.set_title(f\"Permutation Importances ({name} set)\")\n    ax.set_xlabel(\"Decrease in accuracy score\")\n    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n    ax.figure.tight_layout()\n\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_004.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_004.png\"/>\n<img alt=\"Permutation Importances (test set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_005.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_005.png\"/>",
            "statsmodels->Examples->State space models->ETS models->Predictions->for i in range(simulated.shape[1]):\n    simulated.iloc[:, i].plot(label=\"_\", color=\"gray\", alpha=0.1)\ndf[\"mean\"].plot(label=\"mean prediction\")\ndf[\"pi_lower\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"95% interval\")\ndf[\"pi_upper\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"_\")\npred.endog.plot(label=\"data\")\nplt.legend()",
            "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-Rest multiclass ROC->ROC curve using the OvR macro-average->for i in range(n_classes):\n    fpr[i], tpr[i], _ = (y_onehot_test[:, i], y_score[:, i])\n    roc_auc[i] = (fpr[i], tpr[i])\n\nfpr_grid = (0.0, 1.0, 1000)\n\n# Interpolate all ROC curves at these points\nmean_tpr = (fpr_grid)\n\nfor i in range(n_classes):\n    mean_tpr += (fpr_grid, fpr[i], tpr[i])  # linear interpolation\n\n# Average it and compute AUC\nmean_tpr /= n_classes\n\nfpr[\"macro\"] = fpr_grid\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = (fpr[\"macro\"], tpr[\"macro\"])\n\nprint(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")"
        ]
    },
    {
        "id": "1091790",
        "GT": "Average Graduation Rates by State ###-># import csv and create dataframe\ngrad_rates = os.path.join('Resources','gradrates.csv')\ngrad_df = pd.read_csv(grad_rates)\ngrad_df = grad_df.rename(columns={\"State\":\"ST\"})\ngrad_df.head()",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->glove = data.fetch('glove.6B.50d.zip')\nemb_path = textproc.unzipper(glove, 'glove.6B.300d.txt')\nemb_matrix = textproc.loadGloveModel(emb_path)",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Get the Data->zf = zipfile.ZipFile(\"data/flights.csv.zip\")\nfp = zf.extract(zf.filelist[0].filename, path='data/')\ndf = pd.read_csv(fp, parse_dates=[\"FL_DATE\"]).rename(columns=str.lower)\n\ndf.info()",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson->rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)"
        ]
    },
    {
        "id": "1424865",
        "GT": "3.5. Adapted Algorithm-># http://scikit.ml/api/api/skmultilearn.adapt.html#skmultilearn.adapt.MLkNN\n\nfrom skmultilearn.adapt import MLkNN\nfrom scipy.sparse import csr_matrix, lil_matrix",
        "pred": [
            "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime-># Some standard imports\nimport io\nimport numpy as np\n\nfrom torch import nn\nimport torch.utils.model_zoo as model_zoo\nimport torch.onnx",
            "statsmodels->Examples->User Notes->Least squares fitting of models to data->Non-linear models-># TODO: we could use the examples from here:\n# http://probfit.readthedocs.org/en/latest/api.html#probfit.costfunc.Chi2Regression",
            "sklearn->Examples->Ensemble methods->Combine predictors using stacking->Stack of predictors on a single data set->from sklearn.ensemble import \n\nrf_pipeline = (tree_preprocessor, (random_state=42))\nrf_pipeline",
            "sklearn->Examples->Model Selection->Sample pipeline for text feature extraction and evaluation->Pipeline with hyperparameter tuning->from sklearn.feature_extraction.text import \nfrom sklearn.naive_bayes import \nfrom sklearn.pipeline import \n\npipeline = (\n    [\n        (\"vect\", ()),\n        (\"clf\", ()),\n    ]\n)\npipeline",
            "sklearn->Examples->Clustering->Hierarchical clustering: structured vs unstructured ward->We are defining k-Nearest Neighbors with 10 neighbors->from sklearn.neighbors import \n\nconnectivity = (X, n_neighbors=10, include_self=False)"
        ]
    },
    {
        "id": "161674",
        "GT": "#merging dataframes order_prior and products\norder_prior = pd.merge(order_prior, products, on='product_id', how='left')",
        "pred": [
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach-># initialize random variable\nt_post = (\n    df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n)",
            "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept and lagged dependent variable-># Fit the model\nmod_fedfunds2 = sm.tsa.MarkovRegression(\n    dta_fedfunds.iloc[1:], k_regimes=2, exog=dta_fedfunds.iloc[:-1]\n)\nres_fedfunds2 = mod_fedfunds2.fit()",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->tidy = pd.melt(games.reset_index(),\n               id_vars=['game_id', 'date'], value_vars=['away_team', 'home_team'],\n               value_name='team')\ntidy.head()",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit->mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)"
        ]
    },
    {
        "id": "187377",
        "GT": "Recommending Restaurants->Compute Business ids with Distance Constraint->from geopy.distance import great_circle\n\n#Specify mile limit and default location\nwithin_x_miles = 1\nlatitude_default, longitude_default = 43.6544, -79.3807 # Toronto Eaton Centre\nmy_location = (latitude_default, longitude_default)\n\n#Filter out business_id within the mile limit\nrestaurants_info_my_dataset = pd.read_csv('restaurants_info_my_dataset.csv')\ndistances_to_my_location = restaurants_info_my_dataset.apply(lambda x: great_circle(np.array(x[['latitude','longitude']]), my_location).miles, axis=1) \nrestaurants_info_my_dataset['distances_to_my_location'] = distances_to_my_location\nrestaurants_info_my_dataset = restaurants_info_my_dataset.set_index('business_id')\nbusiness_id_within_x_miles = list(restaurants_info_my_dataset[np.array(restaurants_info_my_dataset['distances_to_my_location'])<=within_x_miles].index)",
        "pred": [
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points->from matplotlib.patches import Ellipse\n\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(\n    111,\n    xlabel=\"log(Temp)\",\n    ylabel=\"log(Light)\",\n    title=\"Hertzsprung-Russell Diagram of Star Cluster CYG OB1\",\n)\nax.scatter(*dta.values.T)\n# highlight outliers\ne = Ellipse((3.5, 6), 0.2, 1, alpha=0.25, color=\"r\")\nax.add_patch(e)\nax.annotate(\n    \"Red giants\",\n    xy=(3.6, 6),\n    xytext=(3.8, 6),\n    arrowprops=dict(facecolor=\"black\", shrink=0.05, width=2),\n    horizontalalignment=\"left\",\n    verticalalignment=\"bottom\",\n    clip_on=True,  # clip to the axes bounding box\n    fontsize=16,\n)\n# annotate these with their index\nfor i, row in dta.loc[dta[\"log.Te\"] &lt; 3.8].iterrows():\n    ax.annotate(i, row, row + 0.01, fontsize=14)\nxlim, ylim = ax.get_xlim(), ax.get_ylim()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_robust_models_1_75_0.png\" src=\"../../../_images/examples_notebooks_generated_robust_models_1_75_0.png\"/>",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->from statsmodels.graphics.api import abline_plot\n\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, ylabel=\"Observed Values\", xlabel=\"Fitted Values\")\nax.scatter(yhat, y)\ny_vs_yhat = sm.OLS(y, sm.add_constant(yhat, prepend=True)).fit()\nfig = abline_plot(model_results=y_vs_yhat, ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_discrete_choice_example_55_0.png\" src=\"../../../_images/examples_notebooks_generated_discrete_choice_example_55_0.png\"/>",
            "matplotlib->Tutorials->Text->Annotations->Coordinate systems for annotations->Using ConnectionPatch->from matplotlib.patches import ConnectionPatch\n\nfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(6, 3))\nxy = (0.3, 0.2)\ncon = ConnectionPatch(xyA=xy, coordsA=ax1.transData,\n                      xyB=xy, coordsB=ax2.transData)\n\nfig.add_artist(con)\n\n\n<img alt=\"annotations\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_annotations_022.png\" srcset=\"../../_images/sphx_glr_annotations_022.png, ../../_images/sphx_glr_annotations_022_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Training->from sklearn.base import clone\n\n# Hyper-parameters. These were set by cross-validation,\n# using a GridSearchCV. Here we are not performing cross-validation to\n# save time.\nrbm.learning_rate = 0.06\nrbm.n_iter = 10\n\n# More components tend to give better prediction performance, but larger\n# fitting time\nrbm.n_components = 100\nlogistic.C = 6000\n\n# Training RBM-Logistic Pipeline\nrbm_features_classifier.fit(X_train, Y_train)\n\n# Training the Logistic regression classifier directly on the pixel\nraw_pixel_classifier = clone(logistic)\nraw_pixel_classifier.C = 100.0\nraw_pixel_classifier.fit(X_train, Y_train)",
            "matplotlib->Tutorials->Text->Annotations->Advanced annotation->Placing Artist at anchored Axes locations->from matplotlib.patches import Circle\nfrom mpl_toolkits.axes_grid1.anchored_artists import AnchoredDrawingArea\n\nfig, ax = plt.subplots(figsize=(3, 3))\nada = AnchoredDrawingArea(40, 20, 0, 0,\n                          loc='upper right', pad=0., frameon=False)\np1 = Circle((10, 10), 10)\nada.drawing_area.add_artist(p1)\np2 = Circle((30, 10), 5, fc=\"r\")\nada.drawing_area.add_artist(p2)\nax.add_artist(ada)\n\n\n<img alt=\"annotations\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_annotations_012.png\" srcset=\"../../_images/sphx_glr_annotations_012.png, ../../_images/sphx_glr_annotations_012_2_0x.png 2.0x\"/>"
        ]
    },
    {
        "id": "437875",
        "GT": "Data Cleaning Pt.1->Remove Withdrawn Applications and Combine Expired with Certified->df=df[(df.case_status!='Withdrawn')]\ndf=df.replace({'Certified-Expired':'Certified'})",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->table = pd.DataFrame(data, columns=[\"lag\", \"AC\", \"Q\", \"Prob(Q)\"])\nprint(table.set_index(\"lag\"))",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "statsmodels->Examples->User Notes->Formulas->OLS regression using formulas->df = dta.data[[\"Lottery\", \"Literacy\", \"Wealth\", \"Region\"]].dropna()\ndf.head()",
            "pandas_toms_blog->Scaling->Using Dask->df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)"
        ]
    },
    {
        "id": "204993",
        "GT": "Exploring My Fitbit Data\n\nJanuary 21, 2018->1. Import or install necessary libraries->#!pip install fitbit\n#!pip install -r requirements/base.txt\n#!pip install -r requirements/dev.txt\n#!pip install -r requirements/test.txt\nfrom time import sleep\nimport fitbit\nimport cherrypy\nimport requests\nimport json\nimport datetime\nimport scipy.stats\nimport pandas as pd\nimport numpy as np\n\n# plotting\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport seaborn as sns",
        "pred": [
            "torch->PyTorch Recipes->PyTorch Profiler->Steps->3. Using profiler to analyze execution time-># (omitting some columns)\n# -------------------------------------------------------  ------------  ------------\n#                                                    Name     Self CUDA    CUDA total\n# -------------------------------------------------------  ------------  ------------\n#                                         model_inference       0.000us      11.666ms\n#                                            aten::conv2d       0.000us      10.484ms\n#                                       aten::convolution       0.000us      10.484ms\n#                                      aten::_convolution       0.000us      10.484ms\n#                              aten::_convolution_nogroup       0.000us      10.484ms\n#                                       aten::thnn_conv2d       0.000us      10.484ms\n#                               aten::thnn_conv2d_forward      10.484ms      10.484ms\n# void at::native::im2col_kernel&lt;float&gt;(long, float co...       3.844ms       3.844ms\n#                                       sgemm_32x32x32_NN       3.206ms       3.206ms\n#                                   sgemm_32x32x32_NN_vec       3.093ms       3.093ms\n# -------------------------------------------------------  ------------  ------------\n# Self CPU time total: 23.015ms\n# Self CUDA time total: 11.666ms",
            "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Transformer Block With a Novel Normalization-># Profile rms_norm\nfunc = functools.partial(\n    with_rms_norm,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"Eager Mode - RMS Norm\")",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Dataset\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\ndata['ln_wpi'] = np.log(data['wpi'])\ndata['D.ln_wpi'] = data['ln_wpi'].diff()\n\n\n\n\n\n\n\nIn\u00a0[5]:",
            "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation-># plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>"
        ]
    },
    {
        "id": "184313",
        "GT": "Summary->print(\"NO. of values in different varibles for train_df\")\nprint(train_df.count())\nprint(\"NO. of values in different varibles for test_df\")\nprint(test_df.count())",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)",
            "statsmodels->Examples->Statistics->ANOVA->Minority Employment Data->min_lm = ols(\"JPERF ~ TEST\", data=jobtest_table).fit()\nprint(min_lm.summary())",
            "numpy->NumPy Features->Saving and sharing your NumPy arrays->Rearrange the data into a single 2D array->array_out = np.block([x[:, np.newaxis], y[:, np.newaxis]])\nprint(\"the output array has shape \", array_out.shape, \" with values:\")\nprint(array_out)",
            "statsmodels->Examples->Statistics->ANOVA->Minority Employment Data->min_lm3 = ols(\"JPERF ~ TEST + MINORITY\", data=jobtest_table).fit()\nprint(min_lm3.summary())",
            "statsmodels->Examples->Statistics->ANOVA->Minority Employment Data->min_lm4 = ols(\"JPERF ~ TEST * MINORITY\", data=jobtest_table).fit()\nprint(min_lm4.summary())"
        ]
    },
    {
        "id": "995106",
        "GT": "Using FIPS code as the Index-># take a look at the current structure of counties_df\n\ncounties_df.head()",
        "pred": [
            "seaborn->User guide and tutorial->An introduction to seaborn-># Apply the default theme\nsns.set_theme()\n",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Show the summary of the news results\nprint(news.summary())",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization-># Select the 5 largest delays\ndelays.nlargest(5).sort_values()",
            "matplotlib->Tutorials->Text->Writing mathematical expressions-># plain text\nplt.title('alpha  beta')",
            "torch->Deploying PyTorch Models in Production->Introduction to TorchScript->Using Scripting to Convert Modules-># New inputs\n,  = (3, 4), (3, 4)\ntraced_cell(, )"
        ]
    },
    {
        "id": "1129237",
        "GT": "print(departments_df[departments_df.department == 'missing'])\nmissing_prod_df = products_df[products_df.department_id == 21].reset_index()\nprint (len(missing_prod_df), \"products items are associated with department='missing'\")\n\n#checking for any missing labled aisles\nprint(aisles_df[aisles_df.aisle == 'missing'])\n\nmissing_prod_df.head()",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production->res_ar5 = AutoReg(ind_prod, 5, old_names=False).fit()\npredictions = pd.DataFrame(\n    {\n        \"AR(5)\": res_ar5.predict(start=714, end=726),\n        \"AR(13)\": res.predict(start=714, end=726),\n        \"Restr. AR(13)\": res_glob.predict(start=714, end=726),\n    }\n)\n_, ax = plt.subplots()\nax = predictions.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_42_0.png\"/>",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions->last_hours = slice(-96, None)\nfig, ax = (figsize=(12, 4))\nfig.suptitle(\"Predictions by linear models\")\nax.plot(\n    y.iloc[test_0].values[last_hours],\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(naive_linear_predictions[last_hours], \"x-\", label=\"Ordinal time features\")\nax.plot(\n    cyclic_cossin_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Trigonometric time features\",\n)\nax.plot(\n    cyclic_spline_linear_predictions[last_hours],\n    \"x-\",\n    label=\"Spline-based time features\",\n)\nax.plot(\n    one_hot_linear_predictions[last_hours],\n    \"x-\",\n    label=\"One-hot time features\",\n)\n_ = ax.legend()\n\n\n<img alt=\"Predictions by linear models\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_006.png\"/>",
            "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot->x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Plotting partial dependence for one feature->tree_disp = (tree, X, [\"age\"])\nmlp_disp = (\n    mlp, X, [\"age\"], ax=tree_disp.axes_, line_kw={\"color\": \"red\"}\n)\n\n\n<img alt=\"plot partial dependence visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\" srcset=\"../../_images/sphx_glr_plot_partial_dependence_visualization_api_006.png\"/>",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights->glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf"
        ]
    },
    {
        "id": "510941",
        "GT": "idxstats_df = getTableFromDB('select * from idxstats_reads_per_chromosome;',database_path)\nidxstats_df.index = idxstats_df['region']\nreads_per_chr_df = reorderDFbyChrOrder(idxstats_df.drop('region', 1))\nprint ('this table shows million reads per chromosome')\nreads_per_chr_df.divide(1000000)",
        "pred": [
            "statsmodels->Examples->Statistics->ANOVA->lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot->x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "sklearn->Examples->Model Selection->Comparison between grid search and successive halving->rng = (0)\nX, y = (n_samples=1000, random_state=rng)\n\ngammas = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\nCs = [1, 10, 100, 1e3, 1e4, 1e5]\nparam_grid = {\"gamma\": gammas, \"C\": Cs}\n\nclf = (random_state=rng)\n\ntic = ()\ngsh = (\n    estimator=clf, param_grid=param_grid, factor=2, random_state=rng\n)\ngsh.fit(X, y)\ngsh_time = () - tic\n\ntic = ()\ngs = (estimator=clf, param_grid=param_grid)\ngs.fit(X, y)\ngs_time = () - tic",
            "sklearn->Examples->Model Selection->Successive Halving Iterations->rng = (0)\n\nX, y = (n_samples=400, n_features=12, random_state=rng)\n\nclf = (n_estimators=20, random_state=rng)\n\nparam_dist = {\n    \"max_depth\": [3, None],\n    \"max_features\": (1, 6),\n    \"min_samples_split\": (2, 11),\n    \"bootstrap\": [True, False],\n    \"criterion\": [\"gini\", \"entropy\"],\n}\n\nrsh = (\n    estimator=clf, param_distributions=param_dist, factor=2, random_state=rng\n)\nrsh.fit(X, y)",
            "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance->feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>"
        ]
    },
    {
        "id": "1021331",
        "GT": "3.4. Dropping Unnecessary Variables\n\nThe last step we need to take in this data preprocessing is to drop `ID` and `TargetD`. The reason is `ID` does not contribute anything towards the model building. While for `TargetD`, we will focus on classification task of predicting `TargetB`. `TargetD` can be used if you would want to experiment with regression.-># for gender, before one hot encoding. .head() is used to display first 5 records.\ndf['DemGender'].head(5)",
        "pred": [
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data-># For each team... get number of days between games\ntidy.groupby('team')['date'].diff().dt.days - 1",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)"
        ]
    },
    {
        "id": "68031",
        "GT": "def get_count_df( df ):\n    lead_digit_counts = { str(n): 0 for n in range(1,10)}\n    \n    for t in df.columns:\n        prices = df[t].dropna()\n        d = count_lead_digits( prices )\n        for digit, count in d.items():\n            lead_digit_counts[digit] += count\n    \n    df_counts = pd.DataFrame.from_dict(lead_digit_counts, orient='index')\n    df_counts.columns = ['frequency']\n    df_counts.sort_index(inplace=True)\n    return df_counts",
        "pred": [
            "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)->def frame_preprocessing(observation_frame):\n    # Crop the frame.\n    observation_frame = observation_frame[35:195]\n    # Downsample the frame by a factor of 2.\n    observation_frame = observation_frame[::2, ::2, 0]\n    # Remove the background and apply other enhancements.\n    observation_frame[observation_frame == 144] = 0  # Erase the background (type 1).\n    observation_frame[observation_frame == 109] = 0  # Erase the background (type 2).\n    observation_frame[observation_frame != 0] = 1  # Set the items (rackets, ball) to 1.\n    # Return the preprocessed frame as a 1D floating-point array.\n    return observation_frame.astype(float)",
            "statsmodels->Examples->Statistics->Mediation analysis with duration data->def build_df(ytime, ystatus, mtime0, mtime, mstatus):\n    df = pd.DataFrame(\n        {\n            \"ytime\": ytime,\n            \"ystatus\": ystatus,\n            \"mtime\": mtime,\n            \"mstatus\": mstatus,\n            \"exp\": exp,\n        }\n    )\n    return df",
            "sklearn->Examples->Model Selection->Comparison between grid search and successive halving->def make_heatmap(ax, gs, is_sh=False, make_cbar=False):\n    \"\"\"Helper to make a heatmap.\"\"\"\n    results = (gs.cv_results_)\n    results[[\"param_C\", \"param_gamma\"]] = results[[\"param_C\", \"param_gamma\"]].astype(\n        \n    )\n    if is_sh:\n        # SH dataframe: get mean_test_score values for the highest iter\n        scores_matrix = results.sort_values(\"iter\").pivot_table(\n            index=\"param_gamma\",\n            columns=\"param_C\",\n            values=\"mean_test_score\",\n            aggfunc=\"last\",\n        )\n    else:\n        scores_matrix = results.pivot(\n            index=\"param_gamma\", columns=\"param_C\", values=\"mean_test_score\"\n        )\n\n    im = ax.imshow(scores_matrix)\n\n    ax.set_xticks((len(Cs)))\n    ax.set_xticklabels([\"{:.0E}\".format(x) for x in Cs])\n    ax.set_xlabel(\"C\", fontsize=15)\n\n    ax.set_yticks((len(gammas)))\n    ax.set_yticklabels([\"{:.0E}\".format(x) for x in gammas])\n    ax.set_ylabel(\"gamma\", fontsize=15)\n\n    # Rotate the tick labels and set their alignment.\n    (ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\n    if is_sh:\n        iterations = results.pivot_table(\n            index=\"param_gamma\", columns=\"param_C\", values=\"iter\", aggfunc=\"max\"\n        ).values\n        for i in range(len(gammas)):\n            for j in range(len(Cs)):\n                ax.text(\n                    j,\n                    i,\n                    iterations[i, j],\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"w\",\n                    fontsize=20,\n                )\n\n    if make_cbar:\n        fig.subplots_adjust(right=0.8)\n        cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n        fig.colorbar(im, cax=cbar_ax)\n        cbar_ax.set_ylabel(\"mean_test_score\", rotation=-90, va=\"bottom\", fontsize=15)\n\n\nfig, axes = (ncols=2, sharey=True)\nax1, ax2 = axes\n\nmake_heatmap(ax1, gsh, is_sh=True)\nmake_heatmap(ax2, gs, make_cbar=True)\n\nax1.set_title(\"Successive Halving\\ntime = {:.3f}s\".format(gsh_time), fontsize=15)\nax2.set_title(\"GridSearch\\ntime = {:.3f}s\".format(gs_time), fontsize=15)\n\n()\n\n\n<img alt=\"Successive Halving time = 1.240s, GridSearch time = 5.943s\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_successive_halving_heatmap_001.png\" srcset=\"../../_images/sphx_glr_plot_successive_halving_heatmap_001.png\"/>",
            "sklearn->Examples->Examples based on real world datasets->Model Complexity Influence->Run the code and plot the results->def plot_influence(conf, mse_values, prediction_times, complexities):\n    \"\"\"\n    Plot influence of model complexity on both accuracy and latency.\n    \"\"\"\n\n    fig = ()\n    fig.subplots_adjust(right=0.75)\n\n    # first axes (prediction error)\n    ax1 = fig.add_subplot(111)\n    line1 = ax1.plot(complexities, mse_values, c=\"tab:blue\", ls=\"-\")[0]\n    ax1.set_xlabel(\"Model Complexity (%s)\" % conf[\"complexity_label\"])\n    y1_label = conf[\"prediction_performance_label\"]\n    ax1.set_ylabel(y1_label)\n\n    ax1.spines[\"left\"].set_color(line1.get_color())\n    ax1.yaxis.label.set_color(line1.get_color())\n    ax1.tick_params(axis=\"y\", colors=line1.get_color())\n\n    # second axes (latency)\n    ax2 = fig.add_subplot(111, sharex=ax1, frameon=False)\n    line2 = ax2.plot(complexities, prediction_times, c=\"tab:orange\", ls=\"-\")[0]\n    ax2.yaxis.tick_right()\n    ax2.yaxis.set_label_position(\"right\")\n    y2_label = \"Time (s)\"\n    ax2.set_ylabel(y2_label)\n    ax1.spines[\"right\"].set_color(line2.get_color())\n    ax2.yaxis.label.set_color(line2.get_color())\n    ax2.tick_params(axis=\"y\", colors=line2.get_color())\n\n    (\n        (line1, line2), (\"prediction error\", \"prediction latency\"), loc=\"upper center\"\n    )\n\n    (\n        \"Influence of varying '%s' on %s\"\n        % (conf[\"changing_param\"], conf[\"estimator\"].__name__)\n    )\n\n\nfor conf in configurations:\n    prediction_performances, prediction_times, complexities = benchmark_influence(conf)\n    plot_influence(conf, prediction_performances, prediction_times, complexities)\n()\n\n\n\n<img alt=\"Influence of varying 'l1_ratio' on SGDClassifier\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_model_complexity_influence_001.png\" srcset=\"../../_images/sphx_glr_plot_model_complexity_influence_001.png\"/>\n<img alt=\"Influence of varying 'nu' on NuSVR\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_model_complexity_influence_002.png\" srcset=\"../../_images/sphx_glr_plot_model_complexity_influence_002.png\"/>\n<img alt=\"Influence of varying 'n_estimators' on GradientBoostingRegressor\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_model_complexity_influence_003.png\" srcset=\"../../_images/sphx_glr_plot_model_complexity_influence_003.png\"/>",
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Preliminary investigation with ad-hoc parameters in H, Q->def plot_coefficients_by_equation(states):\n    fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n\n    # The way we defined Z_t implies that the first 5 elements of the\n    # state vector correspond to the first variable in y_t, which is GDP growth\n    ax = axes[0, 0]\n    states.iloc[:, :5].plot(ax=ax)\n    ax.set_title('GDP growth')\n    ax.legend()\n\n    # The next 5 elements correspond to inflation\n    ax = axes[0, 1]\n    states.iloc[:, 5:10].plot(ax=ax)\n    ax.set_title('Inflation rate')\n    ax.legend();\n\n    # The next 5 elements correspond to unemployment\n    ax = axes[1, 0]\n    states.iloc[:, 10:15].plot(ax=ax)\n    ax.set_title('Unemployment equation')\n    ax.legend()\n\n    # The last 5 elements correspond to the interest rate\n    ax = axes[1, 1]\n    states.iloc[:, 15:20].plot(ax=ax)\n    ax.set_title('Interest rate equation')\n    ax.legend();\n\n    return ax\n<br/>"
        ]
    },
    {
        "id": "1293630",
        "GT": "odds = np.exp(answer2.params[1:])\nodds",
        "pred": [
            "numpy->NumPy Features->Masked Arrays->Missing data->china_total = china_masked.data\nchina_total",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:->predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "statsmodels->Examples->State space models->ETS models->Predictions->df = pred.summary_frame(alpha=0.05)\ndf",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->norms = sm.robust.norms",
            "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation->y_train = (y)\ny_train[unlabeled_set] = -1"
        ]
    },
    {
        "id": "583059",
        "GT": "Data and sounds - between science and art->Creating sound-># Check out what the data looks like.\n\nplt.figure(figsize = (15,5))\nplt.hist(muons.M, bins = 500, range=(0,130))\nplt.xlabel(\"Invariant mass (GeV)\", fontsize = 15)\nplt.ylabel(\"Number of events \\n\", fontsize = 15)\nplt.show()",
        "pred": [
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Addendum: manually computing the news, weights, and impacts-># Print the news, computed by the `news` method\nprint(news.news)\n\n# Manually compute the news\nprint()\nprint((y_update.iloc[0] - phi_hat * y_pre.iloc[-1]).round(6))",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example-># Step 1: append a new observation to the sample and refit the parameters\nappend_res = training_res.append(endog[training_obs:training_obs + 1], refit=True)\n\n# Print the re-estimated parameters\nprint(append_res.params)",
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->TVP-VAR model in Statsmodels-># Create an instance of our TVPVAR class with our observed dataset y\nmod = TVPVAR(y)",
            "sklearn->Examples->Nearest Neighbors->Approximate nearest neighbors in TSNE->import sys\n\ntry:\n    import nmslib\nexcept ImportError:\n    print(\"The package 'nmslib' is required to run this example.\")\n    ()\n\ntry:\n    from pynndescent import PyNNDescentTransformer\nexcept ImportError:\n    print(\"The package 'pynndescent' is required to run this example.\")\n    ()",
            "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Steps->3. Save model A-># Specify a path to save to\nPATH = \"model.pt\"\n\n(netA.state_dict(), PATH)"
        ]
    },
    {
        "id": "598798",
        "GT": "YOATP = rATP/rO",
        "pred": [
            "sklearn->Examples->Generalized Linear Models->Non-negative least squares->OLS R2 score 0.7436926291700356",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points->%R print(mod)",
            "statsmodels->Examples->Linear Regression Models->Linear Mixed-Effects->%R library(lme4)",
            "sklearn->Examples->Cross decomposition->Principal Component Regression vs Partial Least Squares Regression->Projection on one component and predictive power->PCR r-squared -0.026\nPLS r-squared 0.658",
            "sklearn->Examples->Generalized Linear Models->Non-negative least squares->NNLS R2 score 0.8225220806196526"
        ]
    },
    {
        "id": "1432865",
        "GT": "Pandas introduction->Importing data->giss_temp = pd.read_table('data/temperatures/GLB.Ts+dSST.txt', sep='\\s+',\n                          skiprows=7, skip_footer=11, engine='python')\ngiss_temp",
        "pred": [
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Likelihood Ratio type tests->original observations and frequency weights->glm = smf.glm(\n    \"affairs ~ rate_marriage + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    freq_weights=np.asarray(dc[\"freq\"]),\n)\nres_f2 = glm.fit()\n# print(res_f2.summary())\nres_f2.pearson_chi2 - res_f.pearson_chi2, res_f2.deviance - res_f.deviance, res_f2.llf - res_f.llf",
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "numpy->NumPy Features->Masked Arrays->Exploring the data->totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "statsmodels->Examples->User Notes->Prediction->Create a new sample of explanatory variables Xnew, predict and plot->x1n = np.linspace(20.5, 25, 10)\nXnew = np.column_stack((x1n, np.sin(x1n), (x1n - 5) ** 2))\nXnew = sm.add_constant(Xnew)\nynewpred = olsres.predict(Xnew)  # predict out of sample\nprint(ynewpred)",
            "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->5. Quantization-aware training->qat_model = load_model(saved_model_dir + float_model_file)\nqat_model.fuse_model()\n\noptimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nqat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')"
        ]
    },
    {
        "id": "1045719",
        "GT": "Export model for production->import dill\nf = open('twitter_sentiment_model.pkl','wb')\nr = RegexPreprocess()\ndill.dump(sentiment_lr, f)\nf.close()",
        "pred": [
            "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)->import matplotlib.pyplot as plt\n\nenv.seed(42)\nenv.reset()\nrandom_frame = env.render(mode=\"rgb_array\")\nprint(random_frame.shape)\nplt.imshow(random_frame)",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Get the Data->zf = zipfile.ZipFile(\"data/flights.csv.zip\")\nfp = zf.extract(zf.filelist[0].filename, path='data/')\ndf = pd.read_csv(fp, parse_dates=[\"FL_DATE\"]).rename(columns=str.lower)\n\ndf.info()",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->glove = data.fetch('glove.6B.50d.zip')\nemb_path = textproc.unzipper(glove, 'glove.6B.300d.txt')\nemb_matrix = textproc.loadGloveModel(emb_path)",
            "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Gamma for proportional count response->Load Scottish Parliament Voting data->data2 = sm.datasets.scotland.load()\ndata2.exog = sm.add_constant(data2.exog, prepend=False)\nprint(data2.exog.head())\nprint(data2.endog.head())",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson->rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)"
        ]
    },
    {
        "id": "1247369",
        "GT": "Feature engineering:->Genres corellation->data['genre_game-show'] = data[['genre_game-show', 'genre_reality-tv']].max(axis=1)\ndata['genre_music'] = data[['genre_musical', 'genre_music']].max(axis=1)\ndata['genre_thriller'] = 1-data[['genre_comedy']]",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "pandas_toms_blog->Scaling->Dask->subset = daily.loc['2011':'2016']\nax = subset.div(1000).plot(figsize=(12, 6))\nax.set(ylim=0, title=\"Daily Donations\", ylabel=\"$ (thousands)\",)\nsns.despine();",
            "statsmodels->Examples->State space models->VARMAX: Introduction->Caution: VARMA(p,q) specifications->mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(1,1))\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->df['home_win'] = df['home_points'] &gt; df['away_points']\ndf['rest_spread'] = df['home_rest'] - df['away_rest']\ndf.dropna().head()"
        ]
    },
    {
        "id": "191170",
        "GT": "3. Load the data->Customer features->info=pd.read_csv(\"customer_info_numeric.csv\")\nrfm_train=pd.read_csv(\"rfm train seq.csv\")",
        "pred": [
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots->tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots->tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog->data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "statsmodels->Examples->User Notes->Dates in Time-Series Models->Using Pandas->fig = pandas_ar_res.plot_predict(start=\"2005\", end=\"2027\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\" src=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\"/>",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->dowjones = sns.load_dataset(\"dowjones\")\nsns.relplot(data=dowjones, x=\"Date\", y=\"Price\", kind=\"line\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->dowjones = sns.load_dataset(\"dowjones\")\nsns.relplot(data=dowjones, x=\"Date\", y=\"Price\", kind=\"line\")\n"
        ]
    },
    {
        "id": "625632",
        "GT": "Sector->Banks->banks_data = labels[labels.Bank1.notnull()]\nbanks_data.Bank1 = banks_data.Bank1.apply(clean)\nbanks_data.Bank2 = banks_data.Bank2.apply(clean)",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit->anes_data = sm.datasets.anes96.load()\nanes_exog = anes_data.exog\nanes_exog = sm.add_constant(anes_exog)",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson->rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n"
        ]
    },
    {
        "id": "1220759",
        "GT": "Which programming language do you recommend?->sns.countplot(y='LanguageRecommendationSelect', data=mc, palette='Spectral')",
        "pred": [
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->sns.catplot(data=anagrams_long, x=\"solutions\", y=\"score\", hue=\"attnr\", kind=\"point\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->sns.catplot(data=anagrams_long, x=\"solutions\", y=\"score\", hue=\"attnr\", kind=\"point\")\n",
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations->sns.displot(data=tips, x=\"total_bill\", col=\"time\", kde=True)\n",
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations->sns.displot(data=tips, kind=\"ecdf\", x=\"total_bill\", col=\"time\", hue=\"smoker\", rug=True)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5), cbar=True)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5), cbar=True)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(diamonds, x=\"price\", y=\"clarity\", log_scale=(True, False))\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(diamonds, x=\"price\", y=\"clarity\", log_scale=(True, False))\n"
        ]
    },
    {
        "id": "279837",
        "GT": "Building the model\n\nBuilding an autoencoder with 100 hidden layer neurons.->input_dim = X_train.shape[1]\nencoding_dim = 15",
        "pred": [
            "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation->y_train = (y)\ny_train[unlabeled_set] = -1",
            "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Model Support->Simulate Some Data->auto_reg_forecast = res.predict(200, 211)\nauto_reg_forecast",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_total = china_masked.data\nchina_total",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:->predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total"
        ]
    },
    {
        "id": "954218",
        "GT": "Feature Exploration 2\n## Contextual: neighboring words\n\nSometimes neighboring words could change the meaning of the question. We want to explore whether word in a context would be an useful feature->def context_tag(comb_q1):\n    context_tags = []\n    for i in range(len(comb_q1)):\n        context_tag = []\n        context = context_a_sent(comb_q1[i])\n        \n        for item in context:\n            x = list(zip(item.keys(), item.values()))  \n            context_tag.append(x[0][0] + x[0][1]+\" \"+x[1][0] + x[1][1])\n        context_tags.append(context_tag)\n    return context_tags",
        "pred": [
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->def plot_weights(support, weights_func, xlabels, xticks):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(support, weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_ylim(-0.1, 1.1)\n    return ax",
            "sklearn->Examples->Clustering->Adjustment for chance in clustering performance evaluation->Second experiment: varying number of classes and clusters->def uniform_labelings_scores(score_func, n_samples, n_clusters_range, n_runs=5):\n    scores = ((len(n_clusters_range), n_runs))\n\n    for i, n_clusters in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            labels_a = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            labels_b = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores",
            "numpy->NumPy Applications->Plotting Fractals->Julia set->def julia(mesh, c=-1, num_iter=10, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; radius\n        z[conv_mask] = np.square(z[conv_mask]) + c\n        diverge_len[conv_mask] += 1\n\n    return diverge_len",
            "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Customizing the plot->def currency(x, pos):\n    \"\"\"The two arguments are the value and tick position\"\"\"\n    if x = 1e6:\n        s = '${:1.1f}M'.format(x*1e-6)\n    else:\n        s = '${:1.0f}K'.format(x*1e-3)\n    return s",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Forward Propagation->def fp_input_gate(concat, parameters):\n    it = sigmoid(np.dot(parameters['Wi'], concat)\n                 + parameters['bi'])\n    cmt = np.tanh(np.dot(parameters['Wcm'], concat)\n                  + parameters['bcm'])\n    return it, cmt"
        ]
    },
    {
        "id": "554471",
        "GT": "Chicago Crime Dataset Analysis->Data Sources\nBrought in data sets all from the City of Chicago.  The main dataset is has incidents of crime since 2001 to present.  The second one has  socioeconomic indicators such as unemployment rate, poverty rate, hardship index, etc., all by neighborhood.  The third dataset has population by neighborhood in 2010.  We needed this to establish a crime rate for each neighborhood.  The population data was only available as a pdf which was converted to a csv.  The fourth was weather information, and we selected the Illinois file (state11_IL.txt.gz).  The fifth is a linke shape files of Chicago community areas used during the mapping portion.  These files are included in the repository and do not need to be downloaded.  The locations of the data are:\n\nhttps://data.cityofchicago.org/Public-Safety/Crimes-2001-to-present/ijzp-q8t2 (click export at the top, save as CSV)\n\nhttps://data.cityofchicago.org/Health-Human-Services/Census-Data-Selected-socioeconomic-indicators-in-C/kn9c-c2s2\n\nhttp://www.cityofchicago.org/city/en/depts/dcd/supp_info/community_area_2000and2010censuspopulationcomparisons.html\n\nhttp://cdiac.ornl.gov/ftp/ushcn_daily/ (state11_IL.txt.gz)\n\nhttps://data.cityofchicago.org/Facilities-Geographic-Boundaries/Boundaries-Community-Areas-current-/cauq-8yn6 (needed for geopandas, files are already in the repository)->df = pd.read_csv(\"Crimes_-_2001_to_present.csv\")",
        "pred": [
            "pandas_toms_blog->Indexes->weather = pd.read_hdf(\"weather.h5\", \"weather\").sort_index()\n\nweather.head()",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "statsmodels->Examples->User Notes->Dates in Time-Series Models->Using Pandas->fig = pandas_ar_res.plot_predict(start=\"2005\", end=\"2027\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\" src=\"../../../_images/examples_notebooks_generated_tsa_dates_12_0.png\"/>",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples->tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples->tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n"
        ]
    },
    {
        "id": "1049448",
        "GT": "Using PyLDAVis to visualize topic modeling->sw = stopwords.words('english')\n\ndef tokenizer_for_LDAvis(sentence):\n    tokens = ret.tokenize(sentence)\n    sentence = [lem.lemmatize(t.lower()) for t in tokens if t.lower() not in sw and len(t) >1]\n    string_tokenize_title = ' '.join(sentence)#map(str, sentence))\n    return string_tokenize_title",
        "pred": [
            "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->3. Load the pre-trained model->ntokens = len(corpus.dictionary)\n\nmodel = (\n    ntoken = ntokens,\n    ninp = 512,\n    nhid = 256,\n    nlayers = 5,\n)\n\n(\n    (\n        model_data_filepath + 'word_language_model_quantize.pth',\n        map_location=('cpu')\n        )\n    )\n\n()\nprint(model)",
            "scipy->Compressed Sparse Graph Routines (scipy.sparse.csgraph)->Example: Word Ladders->word_bytes = np.ndarray((word_list.size, word_list.itemsize),\n...                         dtype='uint8',\n...                         buffer=word_list.data)\n # each unicode character is four bytes long. We only need first byte\n # we know that there are three characters in each word\n word_bytes = word_bytes[:, ::word_list.itemsize//3]\n word_bytes.shape\n(586, 3)    # may vary",
            "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates->time_s = np.s_[:50]  # After this they basically agree\nfig1 = plt.figure()\nax1 = fig1.add_subplot(111)\nidx = np.asarray(series.index)\nh1, = ax1.plot(idx[time_s], res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh2, = ax1.plot(idx[time_s], res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh3, = ax1.plot(idx[time_s], true_seasonal_10_3[time_s], label='True Seasonal 10(3)')\nplt.legend([h1, h2, h3], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 10(3) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\"/>",
            "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns->n_comps = 2\n\nmethods = [\n    (\"PCA\", ()),\n    (\"Unrotated FA\", ()),\n    (\"Varimax FA\", (rotation=\"varimax\")),\n]\nfig, axes = (ncols=len(methods), figsize=(10, 8), sharey=True)\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks((len(feature_names)))\n    ax.set_yticklabels(feature_names)\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\nfig.suptitle(\"Factors\")\n()\n()\n\n\n<img alt=\"Factors, PCA, Unrotated FA, Varimax FA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_002.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_002.png\"/>",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->speech_data_path = 'tutorial-nlp-from-scratch/speeches.csv'\nspeech_df = pd.read_csv(speech_data_path)\nX_pred = textproc.cleantext(speech_df,\n                            text_column='speech',\n                            remove_stopwords=True,\n                            remove_punc=False)\nspeakers = speech_df['speaker'].to_numpy()"
        ]
    },
    {
        "id": "321538",
        "GT": "Number of Users and items->users = merged['personId'].unique() ; print(\"# of users\" , len(users))",
        "pred": [
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->respondent1000 = dta.iloc[1000]\nprint(respondent1000)",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->respondent1000 = dta.iloc[[1000]]\naffair_mod.predict(respondent1000)",
            "scipy->Compressed Sparse Graph Routines (scipy.sparse.csgraph)->Example: Word Ladders->word_list = open('/usr/share/dict/words').readlines()\n word_list = map(str.strip, word_list)",
            "scipy->Spatial data structures and algorithms (scipy.spatial)->Delaunay triangulations->Coplanar points->points = np.array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 1]])\n tri = Delaunay(points)\n np.unique(tri.simplices.ravel())\narray([0, 1, 2, 3], dtype=int32)"
        ]
    },
    {
        "id": "810835",
        "GT": "Vehicle registration in Belgium (2014)->Rechtspersoon (M) of Natuurlijke persoon (P)->Enkel voor Vlaanderen->VL_registered = registered[registered['Region Gewest'] == 'VLA']\n(100*VL_registered.groupby('Titular Type')['Titular Type'].count()/(len(VL_registered))).plot(kind='barh', )\nplt.xlabel('% aantal wagens')\nplt.ylabel(' ')\nplt.gca().set_yticklabels(['Rechtsperson', 'Natuurlijk persoon'])",
        "pred": [
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Optimisation of kernel hyperparameters in GPR->vmin, vmax = (-log_marginal_likelihood).min(), 50\nlevel = (((vmin), (vmax), num=50), decimals=1)\n(\n    length_scale_grid,\n    noise_level_grid,\n    -log_marginal_likelihood,\n    levels=level,\n    norm=(vmin=vmin, vmax=vmax),\n)\n()\n(\"log\")\n(\"log\")\n(\"Length-scale\")\n(\"Noise-level\")\n(\"Log-marginal-likelihood\")\n()\n\n\n<img alt=\"Log-marginal-likelihood\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\"/>",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->X_train_preprocessed = (\n    model[:-1].transform(X_train), columns=feature_names\n)\n\nX_train_preprocessed.std(axis=0).plot.barh(figsize=(9, 7))\n(\"Feature ranges\")\n(\"Std. dev. of feature values\")\n(left=0.3)\n\n\n<img alt=\"Feature ranges\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_004.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_004.png\"/>",
            "sklearn->Examples->Support Vector Machines->RBF SVM parameters->Train classifiers->C_2d_range = [1e-2, 1, 1e2]\ngamma_2d_range = [1e-1, 1, 1e1]\nclassifiers = []\nfor C in C_2d_range:\n    for gamma in gamma_2d_range:\n        clf = (C=C, gamma=gamma)\n        clf.fit(X_2d, y_2d)\n        classifiers.append((C, gamma, clf))",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings->data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "statsmodels->Examples->Statistics->ANOVA->infl = interM_lm.get_influence()\nresid = infl.resid_studentized_internal\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        resid[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X\")\nplt.ylabel(\"standardized resids\")"
        ]
    },
    {
        "id": "161998",
        "GT": "Exercise 2 : Indexing and time series. \n###a. Display\nPrint the first and last 5 elements of the series `s`.->b. Resampling\n- Using the resample method, upsample the daily data to monthly frequency. Use the median method so that each monthly value is the median price of all the days in that month.\n- Take the daily data and fill in every day, including weekends and holidays, using forward-fills.->symbol = \"CMG\"\nstart = \"2012-01-01\"\nend = \"2016-01-01\"\nprices = get_pricing(symbol, start_date=start, end_date=end, fields=\"price\")\n\n# Resample daily prices to get monthly prices using median. \nmonthly_prices = prices.resample('M').median()\nmonthly_prices.head(24)",
        "pred": [
            "numpy->NumPy Features->Masked Arrays->Exploring the data->totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings->data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors->alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "statsmodels->Examples->Statistics->ANOVA->Two-way ANOVA->kidney_lm = ols(\"np.log(Days+1) ~ C(Duration) * C(Weight)\", data=kt).fit()\n\ntable10 = anova_lm(kidney_lm)\n\nprint(\n    anova_lm(ols(\"np.log(Days+1) ~ C(Duration) + C(Weight)\", data=kt).fit(), kidney_lm)\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Duration)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)\nprint(\n    anova_lm(\n        ols(\"np.log(Days+1) ~ C(Weight)\", data=kt).fit(),\n        ols(\"np.log(Days+1) ~ C(Duration) + C(Weight, Sum)\", data=kt).fit(),\n    )\n)"
        ]
    },
    {
        "id": "507192",
        "GT": "df2['intercept']=1\nmodel=sm.Logit(df2['converted'],df2[['intercept','ab_page']])\nresult=model.fit()",
        "pred": [
            "statsmodels->User Guide->Statistics and Tools->Contingency tables->Symmetry and homogeneity->sqtab = sm.stats.SquareTable.from_data(data[['left', 'right']])\nprint(sqtab.summary())",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit->mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)",
            "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper->print(res.recursive_coefficients.filtered[0])\nres.plot_recursive_coefficient(range(mod.k_exog), alpha=None, figsize=(10, 6))",
            "statsmodels->User Guide->Background->Pitfalls->Repeated calls to fit with different parameters->mod = AR(endog)\naic = []\nfor lag in range(1,11):\n    res = mod.fit(maxlag=lag)\n    aic.append(res.aic)",
            "statsmodels->Examples->State space models->VARMAX: Introduction->Caution: VARMA(p,q) specifications->mod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(1,1))\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())"
        ]
    },
    {
        "id": "918063",
        "GT": "7. Subtract latitude and altitude relation->try:\n    df = df.reset_index()\nexcept:\n    pass\n    \n# copy specific columns\ndf1 = df[['date','alt','lat','count']]\ndf1['date'] = pd.to_datetime(df1['date'])\n\ndf1['count'] = df1['count'] + pp(40000) - pp(df1['alt']) # subtract altitude relation\ndf1['count'] = df1['count'] + p(60) - p(df1['lat']) # subtract latitude relation\n\ndf1 = df1.set_index('date')\ndf1 = df1.sort_index()\n\ndf1.to_csv('./radiation.csv') # save data ",
        "pred": [
            "statsmodels->Examples->Statistics->ANOVA->One-way ANOVA->try:\n    rehab_table = pd.read_csv(\"rehab.table\")\nexcept:\n    url = \"http://stats191.stanford.edu/data/rehab.csv\"\n    rehab_table = pd.read_table(url, delimiter=\",\")\n    rehab_table.to_csv(\"rehab.table\")\n\nfig, ax = plt.subplots(figsize=(8, 6))\nfig = rehab_table.boxplot(\"Time\", \"Fitness\", ax=ax, grid=False)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_53_0.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_53_0.png\"/>",
            "statsmodels->Examples->Statistics->ANOVA->Minority Employment Data->try:\n    jobtest_table = pd.read_table(\"jobtest.table\")\nexcept:  # do not have data already\n    url = \"http://stats191.stanford.edu/data/jobtest.table\"\n    jobtest_table = pd.read_table(url)\n\nfactor_group = jobtest_table.groupby([\"MINORITY\"])\n\nfig, ax = plt.subplots(figsize=(6, 6))\ncolors = [\"purple\", \"green\"]\nmarkers = [\"o\", \"v\"]\nfor factor, group in factor_group:\n    ax.scatter(\n        group[\"TEST\"],\n        group[\"JPERF\"],\n        color=colors[factor],\n        marker=markers[factor],\n        s=12 ** 2,\n    )\nax.set_xlabel(\"TEST\")\nax.set_ylabel(\"JPERF\")",
            "sklearn->Examples->Clustering->Vector Quantization Example->Original image->try:  # Scipy = 1.10\n    from scipy.datasets import \nexcept ImportError:\n    from scipy.misc import \n\nraccoon_face = (gray=True)\n\nprint(f\"The dimension of the image is {raccoon_face.shape}\")\nprint(f\"The data used to encode the image is of type {raccoon_face.dtype}\")\nprint(f\"The number of bytes taken in RAM is {raccoon_face.nbytes}\")",
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing->fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "torch->Image and Video->Optimizing Vision Transformer Model for Deployment->Comparing Inference Speed->with (use_cuda=False) as :\n     = model()\nwith (use_cuda=False) as :\n     = scripted_model()\nwith (use_cuda=False) as :\n     = scripted_quantized_model()\nwith (use_cuda=False) as :\n     = optimized_scripted_quantized_model()\nwith (use_cuda=False) as :\n     = ptl()\n\nprint(\"original model: {:.2f}ms\".format(/1000))\nprint(\"scripted model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized &amp; optimized model: {:.2f}ms\".format(/1000))\nprint(\"lite model: {:.2f}ms\".format(/1000))",
            "torch->Model Optimization->Optimizing Vision Transformer Model for Deployment->Comparing Inference Speed->with (use_cuda=False) as :\n     = model()\nwith (use_cuda=False) as :\n     = scripted_model()\nwith (use_cuda=False) as :\n     = scripted_quantized_model()\nwith (use_cuda=False) as :\n     = optimized_scripted_quantized_model()\nwith (use_cuda=False) as :\n     = ptl()\n\nprint(\"original model: {:.2f}ms\".format(/1000))\nprint(\"scripted model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized &amp; optimized model: {:.2f}ms\".format(/1000))\nprint(\"lite model: {:.2f}ms\".format(/1000))"
        ]
    },
    {
        "id": "78081",
        "GT": "folium.Marker([43.6426, -79.3871], popup='<i>CN Tower</i>').add_to(m)\nfolium.Marker([43.6677, -79.3948], popup='<b>Royal Ontario Museum</b>').add_to(m)\nm",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dynamics->sel = ar_select_order(yoy_housing, 13, old_names=False)\nsel.ar_lags",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "sklearn->Examples->Gaussian Process for Machine Learning->Illustration of prior and posterior Gaussian process for different kernels->Dataset and Gaussian process generation->rng = (4)\nX_train = rng.uniform(0, 5, 10).reshape(-1, 1)\ny_train = ((X_train[:, 0] - 2.5) ** 2)\nn_samples = 5",
            "torch->Model Optimization->torch.compile Tutorial->Comparison to TorchScript and FX Tracing->try:\n    (f3)\nexcept:\n    tb.print_exc()\n\ntry:\n    (f3)\nexcept:\n    tb.print_exc()",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines->sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines->sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n"
        ]
    },
    {
        "id": "136245",
        "GT": "Removing odor to determine whether or not it can predict edibility:->X = mushroom_col.iloc[:, 0:9].values\nY = mushroom_col.iloc[:, 1].values\n\nX_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, random_state=1)\nlinreg.fit(X_train, Y_train)\nY_pred = linreg.predict(X_test)\n\nprint(np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))",
        "pred": [
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian processes on discrete data structures->Regression->X = ([\"AGCT\", \"AGC\", \"AACT\", \"TAA\", \"AAA\", \"GAACA\"])\nY = ([1.0, 1.0, 2.0, 2.0, 3.0, 3.0])\n\ntraining_idx = [0, 1, 3, 4]\ngp = (kernel=kernel)\ngp.fit(X[training_idx], Y[training_idx])\n\n(figsize=(8, 5))\n((len(X)), gp.predict(X), color=\"b\", label=\"prediction\")\n(training_idx, Y[training_idx], width=0.2, color=\"r\", alpha=1, label=\"training\")\n((len(X)), X)\n(\"Regression on sequences\")\n()\n()\n\n\n<img alt=\"Regression on sequences\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_on_structured_data_002.png\" srcset=\"../../_images/sphx_glr_plot_gpr_on_structured_data_002.png\"/>",
            "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation->X = digits.data[indices[:340]]\ny = digits.target[indices[:340]]\nimages = digits.images[indices[:340]]\n\nn_total_samples = len(y)\nn_labeled_points = 40\n\nindices = (n_total_samples)\n\nunlabeled_set = indices[n_labeled_points:]",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->X_train = textproc.cleantext(train_df,\n                       text_column='review',\n                       remove_stopwords=True,\n                       remove_punc=True)[0:2000]\n\nX_test = textproc.cleantext(test_df,\n                       text_column='review',\n                       remove_stopwords=True,\n                       remove_punc=True)[0:1000]\n\ny_train = train_df['sentiment'].to_numpy()[0:2000]\ny_test = test_df['sentiment'].to_numpy()[0:1000]",
            "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Log scalars->x = (-5, 5, 0.1).view(-1, 1)\ny = -5 * x + 0.1 * (x.size())\n\nmodel = (1, 1)\ncriterion = ()\noptimizer = (model.parameters(), lr = 0.1)\n\ndef train_model(iter):\n    for epoch in range(iter):\n        y1 = model(x)\n        loss = criterion(y1, y)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ntrain_model(10)\nwriter.flush()",
            "sklearn->Examples->Model Selection->Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV->Running GridSearchCV using multiple evaluation metrics->X, y = (n_samples=8000, random_state=42)\n\n# The scorers can be either one of the predefined metric strings or a scorer\n# callable, like the one returned by make_scorer\nscoring = {\"AUC\": \"roc_auc\", \"Accuracy\": ()}\n\n# Setting refit='AUC', refits an estimator on the whole dataset with the\n# parameter setting that has the best cross-validated AUC score.\n# That estimator is made available at ``gs.best_estimator_`` along with\n# parameters like ``gs.best_score_``, ``gs.best_params_`` and\n# ``gs.best_index_``\ngs = (\n    (random_state=42),\n    param_grid={\"min_samples_split\": range(2, 403, 20)},\n    scoring=scoring,\n    refit=\"AUC\",\n    n_jobs=2,\n    return_train_score=True,\n)\ngs.fit(X, y)\nresults = gs.cv_results_"
        ]
    },
    {
        "id": "1306957",
        "GT": "estimate roster model->y = dm['HomeWin']  \nX = dm[['x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10']] \nX = sm.add_constant(X)  ",
        "pred": [
            "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Creating the Network->input = lineToTensor('Albert')\n = (1, n_hidden)\n\n,  = rnn(input[0], )\nprint()",
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Usage Example->y = medpar.los\nX = medpar[[\"type2\", \"type3\", \"hmo\", \"white\"]].copy()\nX[\"constant\"] = 1",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS non-linear curve but linear in parameters->res = sm.OLS(y, X).fit()\nprint(res.summary())",
            "statsmodels->Examples->State space models->Statespace: Custom Models->Model 3: multiple observation and state equations->What do we need to modify?->2) The update() function->mod = MultipleYsModel(i_hat, s_t, m_hat)\nres = mod.fit()\n\nprint(res.summary())"
        ]
    },
    {
        "id": "346112",
        "GT": "pn_output_path = os.path.join(DTPM_TRXDir,'3_DAILY/daily_pn.csv')\nzp_output_path = os.path.join(DTPM_TRXDir,'3_DAILY/daily_zp.csv')",
        "pred": [
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->train_df = textproc.txt_to_df(imdb_train)\ntest_df = textproc.txt_to_df(imdb_test)",
            "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->FIR Filter->b1 = signal.firwin(40, 0.5)\n b2 = signal.firwin(41, [0.3, 0.8])\n w1, h1 = signal.freqz(b1)\n w2, h2 = signal.freqz(b2)",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->fl_date         datetime64[ns]\norigin                category\ncrs_dep_time    datetime64[ns]\ndep_time        datetime64[ns]\ncrs_arr_time    datetime64[ns]\narr_time        datetime64[ns]\ndtype: object",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "torch->Image and Video->Adversarial Example Generation->Implementation->Inputs->epsilons = [0, .05, .1, .15, .2, .25, .3]\npretrained_model = \"data/lenet_mnist_model.pth\"\nuse_cuda=True"
        ]
    },
    {
        "id": "1030915",
        "GT": "Train a NN to predict the numbers (as simple as it gets)\n## This also demonstrates the time problem of adjusting hyper-parameters\n\n---->lr = MLPClassifier(hidden_layer_sizes=(10, 10, 10,), batch_size=100,\n                   solver='sgd', learning_rate_init=0.01, early_stopping=True)\n\nstart = time.time()\nscores = cross_val_score(estimator=lr,\n                         X=X, \n                         y=y,\n                         cv=5)\n\nprint(\"\\nAccuracy: {}% (+/- {})\".format(round(scores.mean() * 100, 2), round(scores.std(), 3) * 2))\nprint('Finished in {}sec\\n'.format(round(time.time() - start, 2)))",
        "pred": [
            "sklearn->Examples->Calibration->Probability Calibration curves->Calibration curves->Linear support vector classifier->lr = (C=1.0)\nsvc = NaivelyCalibratedLinearSVC(max_iter=10_000)\nsvc_isotonic = (svc, cv=2, method=\"isotonic\")\nsvc_sigmoid = (svc, cv=2, method=\"sigmoid\")\n\nclf_list = [\n    (lr, \"Logistic\"),\n    (svc, \"SVC\"),\n    (svc_isotonic, \"SVC + Isotonic\"),\n    (svc_sigmoid, \"SVC + Sigmoid\"),\n]",
            "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs->lfw_people = (min_faces_per_person=70, resize=0.4)\n\n# introspect the images arrays to find the shapes (for plotting)\nn_samples, h, w = lfw_people.images.shape\n\n# for machine learning we use the 2 data directly (as relative pixel\n# positions info is ignored by this model)\nX = lfw_people.data\nn_features = X.shape[1]\n\n# the label to predict is the id of the person\ny = lfw_people.target\ntarget_names = lfw_people.target_names\nn_classes = target_names.shape[0]\n\nprint(\"Total dataset size:\")\nprint(\"n_samples: %d\" % n_samples)\nprint(\"n_features: %d\" % n_features)\nprint(\"n_classes: %d\" % n_classes)",
            "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types->clf = (\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", ())]\n)\n\nX_train, X_test, y_train, y_test = (X, y, test_size=0.2, random_state=0)\n\nclf.fit(X_train, y_train)\nprint(\"model score: %.3f\" % clf.score(X_test, y_test))",
            "sklearn->Examples->Support Vector Machines->Support Vector Regression (SVR) using linear and non-linear kernels->Look at the results->lw = 2\n\nsvrs = [svr_rbf, svr_lin, svr_poly]\nkernel_label = [\"RBF\", \"Linear\", \"Polynomial\"]\nmodel_color = [\"m\", \"c\", \"g\"]\n\nfig, axes = (nrows=1, ncols=3, figsize=(15, 10), sharey=True)\nfor ix, svr in enumerate(svrs):\n    axes[ix].plot(\n        X,\n        svr.fit(X, y).predict(X),\n        color=model_color[ix],\n        lw=lw,\n        label=\"{} model\".format(kernel_label[ix]),\n    )\n    axes[ix].scatter(\n        X[svr.support_],\n        y[svr.support_],\n        facecolor=\"none\",\n        edgecolor=model_color[ix],\n        s=50,\n        label=\"{} support vectors\".format(kernel_label[ix]),\n    )\n    axes[ix].scatter(\n        X[((len(X)), svr.support_)],\n        y[((len(X)), svr.support_)],\n        facecolor=\"none\",\n        edgecolor=\"k\",\n        s=50,\n        label=\"other training data\",\n    )\n    axes[ix].legend(\n        loc=\"upper center\",\n        bbox_to_anchor=(0.5, 1.1),\n        ncol=1,\n        fancybox=True,\n        shadow=True,\n    )\n\nfig.text(0.5, 0.04, \"data\", ha=\"center\", va=\"center\")\nfig.text(0.06, 0.5, \"target\", ha=\"center\", va=\"center\", rotation=\"vertical\")\nfig.suptitle(\"Support Vector Regression\", fontsize=14)\n()\n\n\n<img alt=\"Support Vector Regression\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_svm_regression_001.png\" srcset=\"../../_images/sphx_glr_plot_svm_regression_001.png\"/>",
            "sklearn->Examples->Generalized Linear Models->Lasso model selection via information criteria->lasso_lars_ic.set_params(lassolarsic__criterion=\"bic\").fit(X, y)\n\nbic_criterion = zou_et_al_criterion_rescaling(\n    lasso_lars_ic[-1].criterion_,\n    n_samples,\n    lasso_lars_ic[-1].noise_variance_,\n)\n\nindex_alpha_path_bic = (\n    lasso_lars_ic[-1].alphas_ == lasso_lars_ic[-1].alpha_\n)[0]"
        ]
    },
    {
        "id": "1341423",
        "GT": "**CHOOSING COLUMNS OF INTEREST**->nba['mademissnum'] = num",
        "pred": [
            "numpy->NumPy Features->Masked Arrays->Missing data->nbcases_ma",
            "sklearn->Examples->Ensemble methods->Pixel importances with a parallel forest of trees->Loading the data and model fitting->n_jobs = -1",
            "statsmodels->Examples->Generalized Estimating Equations->GEE Nested Covariance Structure->n = n_groups * group_size * level1_size * level2_size",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->modfd_logit.k_constant",
            "statsmodels->Examples->Generalized Estimating Equations->GEE Nested Covariance Structure->n_groups = 100"
        ]
    },
    {
        "id": "951841",
        "GT": "Import required packages->import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#%matplotlib inline\nimport warnings\nwarnings.simplefilter('ignore',DeprecationWarning)\nimport seaborn as sns\nimport time\nimport copy\n\nfrom pylab import rcParams\n#import hdbscan\n\n\nfrom tabulate import tabulate\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nfrom __future__ import print_function",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Rolling Least Squares->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn\n\nimport statsmodels.api as sm\nfrom statsmodels.regression.rolling import RollingOLS\n\nseaborn.set_style(\"darkgrid\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline",
            "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.23->Scalability and stability improvements to KMeans->import scipy\nimport numpy as np\nfrom sklearn.model_selection import \nfrom sklearn.cluster import \nfrom sklearn.datasets import \nfrom sklearn.metrics import \n\nrng = (0)\nX, y = (random_state=rng)\nX = (X)\nX_train, X_test, _, y_test = (X, y, random_state=rng)\nkmeans = (n_init=\"auto\").fit(X_train)\nprint((kmeans.predict(X_test), y_test))",
            "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings->import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "torch->Multimodality->TorchMultimodal Tutorial: Finetuning FLAVA->Steps->import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
            "torch->Reinforcement Learning->Reinforcement Learning (DQN) Tutorial->import gymnasium as gym\nimport math\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple, deque\nfrom itertools import count\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nenv = gym.make(\"CartPole-v1\")\n\n# set up matplotlib\nis_ipython = 'inline' in matplotlib.get_backend()\nif is_ipython:\n    from IPython import display\n\nplt.ion()\n\n# if gpu is to be used\n = (\"cuda\" if () else \"cpu\")"
        ]
    },
    {
        "id": "605365",
        "GT": "Topic: Classification\n\n**Settings: Where applicable, use test_size=.30, random_state=4444. This will permit comparison of results across users.\n\nData:\n\nChallenges 1-10: congressional votes Congressional Voting Records Dataset\n\nChallenge 11: movie data\n\nChallenge 12: breast cancer surgery Haberman Survival Dataset\n\nData \u2013 Congressional Votes\n\nDownload the congressional votes data from here:Congressional Voting Records Dataset\n\nThese are votes of U.S. House of Representatives Congressmen on 16 key issues in 1984.\n\nRead the description of the fields and download the data: house-votes-84.data\n\nWe will try to see if we can predict the house members' party based on their votes.\n\nWe will also use some of the general machine learning tools we learned (a bit more efficiently this time).->from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\ny_pred",
        "pred": [
            "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->Load and prepare data->from sklearn.linear_model import \n\nclassifier = ()\ny_score = classifier.fit(X_train, y_train).predict_proba(X_test)",
            "sklearn->Examples->Generalized Linear Models->Sparsity Example: Fitting only features 1  and 2->from sklearn import linear_model\n\nols = ()\n_ = ols.fit(X_train, y_train)",
            "sklearn->Examples->Model Selection->Custom refit strategy of a grid search with cross-validation->Tuning hyper-parameters->from sklearn.metrics import \n\ny_pred = grid_search.predict(X_test)\nprint((y_test, y_pred))",
            "sklearn->Examples->Feature Selection->Pipeline ANOVA SVM->from sklearn.metrics import \n\ny_pred = anova_svm.predict(X_test)\nprint((y_test, y_pred))",
            "sklearn->Examples->Ensemble methods->IsolationForest example->Training of the model->from sklearn.ensemble import \n\nclf = (max_samples=100, random_state=0)\nclf.fit(X_train)"
        ]
    },
    {
        "id": "1137947",
        "GT": "Case Study on Network Intrusion Detection\nThe dataset to be audited was provided which consists of a wide variety of intrusions simulated in a military network environment. It created an environment to acquire raw TCP/IP dump data for a network by simulating a typical US Air Force LAN. The LAN was focused like a real environment and blasted with multiple attacks. A connection is a sequence of TCP packets starting and ending at some time duration between which data flows to and from a source IP address to a target IP address under some well-defined protocol. Also, each connection is labelled as either normal or as an attack with exactly one specific attack type. Each connection record consists of about 100 bytes.\n\nFor each TCP/IP connection, 41 quantitative and qualitative features are obtained from normal and attack data (3 qualitative and 38 quantitative features) .The class variable has two categories:\n- Normal \n- Anomalous->Step 3: Descriptive Statistics of Train Dataset-># Descriptive statistics\nnwid_train_org.describe()",
        "pred": [
            "seaborn->User guide and tutorial->An introduction to seaborn-># Apply the default theme\nsns.set_theme()\n",
            "seaborn->User guide and tutorial->An introduction to seaborn-># Import seaborn\nimport seaborn as sns\n",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Show the summary of the news results\nprint(news.summary())",
            "seaborn->User guide and tutorial->An introduction to seaborn-># Load an example dataset\ntips = sns.load_dataset(\"tips\")\n",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization-># Select the 5 largest delays\ndelays.nlargest(5).sort_values()"
        ]
    },
    {
        "id": "777040",
        "GT": "Processing\nFor preprocessing, we are going to fix a few issues in the dataset. \n\n- This first includes the use of \"50K.\" instead of \"50K\" in the test set. \n- Next, we will encode the categorical features as integers (later on we will encode one hot)\n- Finally, we will make certain all the continuous data is scaled properly-># we need to tell tensorflow how many inputs to expect and what the data types will be\n# for this early example, everything is just numeric, real valued\nfeatures_tf = [layers.real_valued_column('', dimension=X_train.shape[1])]\nclf = SKCompat(# wrap with SKCompat for easy usage like sklearn\n            learn.DNNClassifier(hidden_units=[50], feature_columns=features_tf)\n        )\n\nclf.fit(X_train,y_train,steps=100)",
        "pred": [
            "torch->PyTorch Recipes->Dynamic Quantization->Steps->1: Set Up-># import the modules used here in this recipe\nimport torch\nimport torch.quantization\nimport torch.nn as nn\nimport copy\nimport os\nimport time\n\n# define a very, very simple LSTM for demonstration purposes\n# in this case, we are wrapping nn.LSTM, one layer, no pre or post processing\n# inspired by\n# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html, by Robert Guthrie\n# and https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html\nclass lstm_for_demonstration():\n  \"\"\"Elementary Long Short Term Memory style model which simply wraps nn.LSTM\n     Not to be used for anything other than demonstration.\n  \"\"\"\n  def __init__(self,in_dim,out_dim,depth):\n     super(lstm_for_demonstration,self).__init__()\n     self.lstm = (in_dim,out_dim,depth)\n\n  def forward(self,inputs,hidden):\n     out,hidden = self.lstm(inputs,hidden)\n     return out, hidden\n\n\n(29592)  # set the seed for reproducibility\n\n#shape parameters\nmodel_dimension=8\nsequence_length=20\nbatch_size=1\nlstm_depth=1\n\n# random data for input\ninputs = (sequence_length,batch_size,model_dimension)\n# hidden is actually is a tuple of the initial hidden state and the initial cell state\nhidden = ((lstm_depth,batch_size,model_dimension), (lstm_depth,batch_size,model_dimension))",
            "statsmodels->Examples->State space models->Fixed / constrained parameters in state space models->Adding a seasonal component-># Here we restrict the first three parameters to specific values\nwith mod.fix_params({'sigma2.irregular': 1, 'sigma2.level': 0, 'sigma2.trend': 1. / 129600}):\n    # Now we fit any remaining parameters, which in this case\n    # is just `sigma2.seasonal`\n    res_restricted = mod.fit()",
            "sklearn->Examples->Clustering->Segmenting the picture of greek coins in regions-># Computing a few extra eigenvectors may speed up the eigen_solver.\n# The spectral clustering quality may also benetif from requesting\n# extra regions for segmentation.\nn_regions_plus = 3\n\n# Apply spectral clustering using the default eigen_solver='arpack'.\n# Any implemented solver can be used: eigen_solver='arpack', 'lobpcg', or 'amg'.\n# Choosing eigen_solver='amg' requires an extra package called 'pyamg'.\n# The quality of segmentation and the speed of calculations is mostly determined\n# by the choice of the solver and the value of the tolerance 'eigen_tol'.\n# TODO: varying eigen_tol seems to have no effect for 'lobpcg' and 'amg' #21243.\nfor assign_labels in (\"kmeans\", \"discretize\", \"cluster_qr\"):\n    t0 = ()\n    labels = (\n        graph,\n        n_clusters=(n_regions + n_regions_plus),\n        eigen_tol=1e-7,\n        assign_labels=assign_labels,\n        random_state=42,\n    )\n\n    t1 = ()\n    labels = labels.reshape(rescaled_coins.shape)\n    (figsize=(5, 5))\n    (rescaled_coins, cmap=plt.cm.gray)\n\n    (())\n    (())\n    title = \"Spectral clustering: %s, %.2fs\" % (assign_labels, (t1 - t0))\n    print(title)\n    (title)\n    for l in range(n_regions):\n        colors = [plt.cm.nipy_spectral((l + 4) / float(n_regions + 4))]\n        (labels == l, colors=colors)\n        # To view individual segments as appear comment in plt.pause(0.5)\n()\n\n# TODO: After #21194 is merged and #21243 is fixed, check which eigen_solver\n# is the best and set eigen_solver='arpack', 'lobpcg', or 'amg' and eigen_tol\n# explicitly in this example.\n\n\n\n<img alt=\"Spectral clustering: kmeans, 2.04s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_001.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_001.png\"/>\n<img alt=\"Spectral clustering: discretize, 1.83s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_002.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_002.png\"/>\n<img alt=\"Spectral clustering: cluster_qr, 1.82s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_003.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_003.png\"/>",
            "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Gradient boosting estimator with native categorical support-># The ordinal encoder will first output the categorical features, and then the\n# continuous (passed-through) features\n\nhist_native = (\n    ordinal_encoder,\n    (\n        random_state=42,\n        categorical_features=categorical_columns,\n    ),\n).set_output(transform=\"pandas\")",
            "torch->PyTorch Recipes->Performance Tuning Guide->CPU specific optimizations->Use oneDNN Graph with TorchScript for inference-># AMP for JIT mode is enabled by default, and is divergent with its eager mode counterpart\ntorch._C._jit_set_autocast_mode(False)\n\nwith torch.no_grad(), torch.cpu.amp.autocast(cache_enabled=False, dtype=torch.bfloat16):\n    model = torch.jit.trace(model, (example_input))\n    model = torch.jit.freeze(model)\n    # a couple of warmup runs\n    model(example_input)\n    model(example_input)\n    # speedup would be observed in subsequent runs.\n    model(example_input)"
        ]
    },
    {
        "id": "1028081",
        "GT": "byyear = complaintdates.groupby(by='Year').count()\nbyyear",
        "pred": [
            "pandas_toms_blog->Scaling->Dask->most_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
            "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs->Total dataset size:\nn_samples: 1288\nn_features: 1850\nn_classes: 7",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:->predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted"
        ]
    },
    {
        "id": "1434696",
        "GT": "Kannada POS Tagger using Linear Chain CRF\n\nWe will now train and tag the words using the features extracted from Word2Vec in the previous step.->Splitting the data into Training and Test sets->split = 100\nx_test = X[-split:,:,:]\ny_test = Y[-split:,:]\ns_test = sequence_lengths[-split:]\n\nx = X[0:-split,:,:]\ny = Y[0:-split,:]\nsequence_lengths = sequence_lengths[0:-split]",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS estimation->nsample = 100\nx = np.linspace(0, 10, 100)\nX = np.column_stack((x, x ** 2))\nbeta = np.array([1, 0.1, 10])\ne = np.random.normal(size=nsample)",
            "sklearn->Examples->Generalized Linear Models->Plot Ridge coefficients as a function of the regularization->Compute paths->n_alphas = 200\nalphas = (-10, -2, n_alphas)\n\ncoefs = []\nfor a in alphas:\n    ridge = (alpha=a, fit_intercept=False)\n    ridge.fit(X, y)\n    coefs.append(ridge.coef_)",
            "sklearn->Examples->Ensemble methods->Plot individual and voting regression predictions->Making predictions->xt = X[:20]\n\npred1 = reg1.predict(xt)\npred2 = reg2.predict(xt)\npred3 = reg3.predict(xt)\npred4 = ereg.predict(xt)",
            "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack->accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Previous dataset runs through 2017-02\ny_pre = infl.loc[:'2017-01'].copy()\nconst_pre = np.ones(len(y_pre))\nprint(y_pre.tail())"
        ]
    },
    {
        "id": "899572",
        "GT": "Group factor exposures->factors.corrwith(port)",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Basic Use->det_proc.out_of_sample(15)",
            "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Custom Deterministic Terms->det_proc.out_of_sample(10)",
            "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Using a Date-like Index->det_proc.out_of_sample(12)",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->glm_mod.model.data.orig_endog.sum(1)",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resf_logit.predict()"
        ]
    },
    {
        "id": "452802",
        "GT": "Sample solutions->import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import cross_validation",
        "pred": [
            "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.neural_network import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.tree import \nfrom sklearn.inspection import PartialDependenceDisplay",
            "statsmodels->Examples->Time Series Analysis->ARMA: Artificial Data->import numpy as np\nimport pandas as pd\n\nfrom statsmodels.graphics.tsaplots import plot_predict\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.arima.model import ARIMA\n\nnp.random.seed(12345)",
            "statsmodels->Examples->User Notes->Dates in Time-Series Models->import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.ar_model import AutoReg, ar_select_order\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model->import numpy as np\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom statsmodels.base.model import GenericLikelihoodModel",
            "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation->import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)"
        ]
    },
    {
        "id": "1016193",
        "GT": "10C. Prepare Test Data for prediction.->test_X['churn'] = test_y['churn']\ntest_X.head(2)",
        "pred": [
            "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Capturing the Model with Symbolic Tracing->traced_rn18 = (rn18)\nprint()",
            "numpy->NumPy Features->Saving and sharing your NumPy arrays->Our arrays as a csv file->x = load_xy[:, 0]\ny = load_xy[:, 1]\nprint(x)\nprint(y)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n"
        ]
    },
    {
        "id": "124416",
        "GT": "Poverty->pov_wide_n24['ubi_reduction'] = 1 - (\n    pov_wide_n24.afti_ubi / pov_wide_n24.afti_keep)\npov_wide_n24['tubi_reduction'] = 1 - (\n    pov_wide_n24.afti_tubi / pov_wide_n24.afti_keep)\npov_wide_n24 * 100",
        "pred": [
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_012.png\"/>",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(6, 6))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Lasso model, optimum regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Lasso model, optimum regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_015.png\"/>",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->mae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}\n\n_, ax = (figsize=(5, 5))\ndisplay = (\n    y_test, y_pred, kind=\"actual_vs_predicted\", ax=ax, scatter_kwargs={\"alpha\": 0.5}\n)\nax.set_title(\"Ridge model, small regularization\")\nfor name, score in scores.items():\n    ax.plot([], [], \" \", label=f\"{name}: {score}\")\nax.legend(loc=\"upper left\")\n()\n\n\n<img alt=\"Ridge model, small regularization\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_009.png\"/>",
            "numpy->NumPy Features->Masked Arrays->Exploring the data->totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->delta_hat, rho_hat = sarima_res.params[:2]\ndelta_hat + rho_hat * y[0]"
        ]
    },
    {
        "id": "741644",
        "GT": "Step 4 - Evaluate Our Model-># get residuals\nresids = arma.resid.squeeze()\n\nplt.title('Residuals')\nplt.plot(resids)",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit->mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit->anes_data = sm.datasets.anes96.load()\nanes_exog = anes_data.exog\nanes_exog = sm.add_constant(anes_exog)",
            "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept and lagged dependent variable-># Fit the model\nmod_fedfunds2 = sm.tsa.MarkovRegression(\n    dta_fedfunds.iloc[1:], k_regimes=2, exog=dta_fedfunds.iloc[:-1]\n)\nres_fedfunds2 = mod_fedfunds2.fit()"
        ]
    },
    {
        "id": "400358",
        "GT": "Higher-resolution geographical data->g = sns.PairGrid(data = sfcrimes, x_vars = ['X'], y_vars = ['Y'], size = 5.5*1.3)\ng = g.map(plt.scatter)",
        "pred": [
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.kdeplot, lw=3, legend=False)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.kdeplot, lw=3, legend=False)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n",
            "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->mod = TVRegression(y_t, x_t, w_t)\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Partial Regression Plots (Duncan)->fig = sm.graphics.plot_partregress(\n    \"prestige\", \"income\", [\"income\", \"education\"], data=prestige\n)\nfig.tight_layout(pad=1.0)"
        ]
    },
    {
        "id": "871432",
        "GT": "Approach 3: Assign the average/median/mode value->df.loc[df['Fare'].isnull()].shape",
        "pred": [
            "statsmodels->Examples->Statistics->ANOVA->df_infl[:5]",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions->naive_linear_pipeline[:-1].transform(X).shape",
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset->co2_data.index.min(), co2_data.index.max()",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 1: Create an outcome variable->df['home_win'] = df.home_points &gt; df.away_points",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Qualitative analysis of the impact of features on linear model predictions->one_hot_linear_pipeline[:-1].transform(X).shape"
        ]
    },
    {
        "id": "1214459",
        "GT": "III. Network Visualization\n\n* in Python: Matplotlib or Graphviz with pydot (import and export NetworkX graphs in Graphviz dot format using pydot)\n* Gephi\n* Graphviz\n* Neo4j\n* etc.-># simplest way to draw a graph\nnx.draw(G)",
        "pred": [
            "seaborn->User guide and tutorial->An introduction to seaborn-># Apply the default theme\nsns.set_theme()\n",
            "matplotlib->Tutorials->Text->Writing mathematical expressions-># plain text\nplt.title('alpha  beta')",
            "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime-># Some standard imports\nimport io\nimport numpy as np\n\nfrom torch import nn\nimport torch.utils.model_zoo as model_zoo\nimport torch.onnx",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Show the summary of the news results\nprint(news.summary())",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization-># Select the 5 largest delays\ndelays.nlargest(5).sort_values()"
        ]
    },
    {
        "id": "603279",
        "GT": "XML example and exercise\n****\n+ study examples of accessing nodes in XML tree structure  \n+ work on exercise to be completed and submitted\n****\n+ reference: https://docs.python.org/2.7/library/xml.etree.elementtree.html\n+ data source: http://www.dbis.informatik.uni-goettingen.de/Mondial\n****->XML example\n\n+ for details about tree traversal and iterators, see https://docs.python.org/2.7/library/xml.etree.elementtree.html->#find all infant moratlity rates\ninfmort = []\nfor child in document.getroot():\n    try:\n        infmort.append([child.find('name').text, float(child.find('infant_mortality').text)])\n    except AttributeError:\n        continue\n\n#create dataframe for infant mortality rate then sort to get lowest 10\ndf_infmort = pd.DataFrame(infmort, columns=['country', 'infant_mortality'])\ndf_infmort.sort('infant_mortality', ascending=True).head(10)",
        "pred": [
            "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation-># plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>",
            "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds-># range of admissible distortions\neps_range = (0.1, 0.99, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = (1, 9, 9)\n\n()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = (n_samples_range, eps=eps)\n    (n_samples_range, min_n_components, color=color)\n\n([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n(\"Number of observations to eps-embed\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_samples vs n_components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\"/>",
            "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method-># fit in statsmodels\nmodel = ETSModel(\n    austourists,\n    error=\"add\",\n    trend=\"add\",\n    seasonal=\"add\",\n    damped_trend=True,\n    seasonal_periods=4,\n)\nfit = model.fit()\n\n# fit with R params\nparams_R = [\n    0.35445427,\n    0.03200749,\n    0.39993387,\n    0.97999997,\n    24.01278357,\n    0.97770147,\n    1.76951063,\n    -0.50735902,\n    -6.61171798,\n    5.34956637,\n]\nfit_R = model.smooth(params_R)\n\naustourists.plot(label=\"data\")\nplt.ylabel(\"Australian Tourists\")\n\nfit.fittedvalues.plot(label=\"statsmodels fit\")\nfit_R.fittedvalues.plot(label=\"R fit\", linestyle=\"--\")\nplt.legend()",
            "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->2. Writing to TensorBoard-># get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# create grid of images\nimg_grid = torchvision.utils.make_grid(images)\n\n# show images\nmatplotlib_imshow(img_grid, one_channel=True)\n\n# write to tensorboard\nwriter.add_image('four_fashion_mnist_images', img_grid)",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Sentiment Analysis on the Speech Data-># To store predicted sentiments\npredictions = {}\n\n# define the length of a paragraph\npara_len = 100\n\n# Retrieve trained values of the parameters\nif os.path.isfile('tutorial-nlp-from-scratch/parameters.npy'):\n    parameters = np.load('tutorial-nlp-from-scratch/parameters.npy', allow_pickle=True).item()\n\n# This is the prediction loop.\nfor index, text in enumerate(X_pred):\n    # split each speech into paragraphs\n    paras = textproc.text_to_paras(text, para_len)\n    # To store the network outputs\n    preds = []\n\n    for para in paras:\n        # split text sample into words/tokens\n        para_tokens = textproc.word_tokeniser(para)\n        # Forward Propagation\n        caches = forward_prop(para_tokens,\n                              parameters,\n                              input_dim)\n\n        # Retrieve the output of the fully connected layer\n        sent_prob = caches['fc_values'][0]['a2'][0][0]\n        preds.append(sent_prob)\n\n    threshold = 0.5\n    preds = np.array(preds)\n    # Mark all predictions &gt; threshold as positive and &lt; threshold as negative\n    pos_indices = np.where(preds &gt; threshold)  # indices where output &gt; 0.5\n    neg_indices = np.where(preds &lt; threshold)  # indices where output &lt; 0.5\n    # Store predictions and corresponding piece of text\n    predictions[speakers[index]] = {'pos_paras': paras[pos_indices[0]],\n                                    'neg_paras': paras[neg_indices[0]]}"
        ]
    },
    {
        "id": "1118023",
        "GT": "TEXT ANALYSIS AND MACHINE LEARNING  Part 2->Bag_of_words with TF-IDF (Term Frequency- Inverse Document Frequency) weights \nAccording to Gavin Hackeling(Mastering ML with Scikit-Learn), many words might appear with the same frequency in two documents but the documents could still be disimilar if one document is many time larger than the other. Scikit-Learn's TfidfTransformer object can mitigate this problem by transforming a matrix of term frequency vectors into a matrix of normalized term frequency weights.  \n\nThe smoothed, **normalized term frequencies** are by the following equations:->from IPython.display import Latex\nLatex(r\"\"\"\\begin{eqnarray}\n{\\mathbf{t}}\\,  {\\mathbf{f}}{\\mathbf{(t,d)}} & = \\frac{{\\mathbf{f}}{\\mathbf{(t,d)}}\\, {\\mathbf{+}}\\, \n{\\mathbf{1}} }{\\mathbf{||x||}}\\\\\n\\end{eqnarray}\"\"\")",
        "pred": [
            "torch->Learning PyTorch->What is torch.nn really?->MNIST data setup->from matplotlib import pyplot\nimport numpy as np\n\npyplot.imshow([0].reshape((28, 28)), cmap=\"gray\")\nprint(.shape)\n\n\n<img alt=\"nn tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_nn_tutorial_001.png\" srcset=\"../_images/sphx_glr_nn_tutorial_001.png\"/>",
            "statsmodels->Examples->Statistics->ANOVA->from statsmodels.stats.api import anova_lm\n\ntable1 = anova_lm(lm, interX_lm)\nprint(table1)\n\ninterM_lm = ols(\"S ~ X + C(E)*C(M)\", data=salary_table).fit()\nprint(interM_lm.summary())\n\ntable2 = anova_lm(lm, interM_lm)\nprint(table2)",
            "matplotlib->Tutorials->Text->Text in Matplotlib Plots->Labels for x- and y-axis->from matplotlib.font_manager import FontProperties\n\nfont = FontProperties()\nfont.set_family('serif')\nfont.set_name('Times New Roman')\nfont.set_style('italic')\n\nfig, ax = plt.subplots(figsize=(5, 3))\nfig.subplots_adjust(bottom=0.15, left=0.2)\nax.plot(x1, y1)\nax.set_xlabel('Time [s]', fontsize='large', fontweight='bold')\nax.set_ylabel('Damped oscillation [V]', fontproperties=font)\n\nplt.show()\n\n\n<img alt=\"text intro\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_text_intro_006.png\" srcset=\"../../_images/sphx_glr_text_intro_006.png, ../../_images/sphx_glr_text_intro_006_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Limitations of a simple linear model->from sklearn.linear_model import \n\nridge = ().fit(training_data, training_noisy_target)\n\n(data, target, label=\"True signal\", linewidth=2)\n(\n    training_data,\n    training_noisy_target,\n    color=\"black\",\n    label=\"Noisy measurements\",\n)\n(data, ridge.predict(data), label=\"Ridge regression\")\n()\n(\"data\")\n(\"target\")\n_ = (\"Limitation of a linear model such as ridge\")\n\n\n<img alt=\"Limitation of a linear model such as ridge\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_gpr_krr_002.png\" srcset=\"../../_images/sphx_glr_plot_compare_gpr_krr_002.png\"/>",
            "sklearn->Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Visualize the learning curves->from sklearn.model_selection import LearningCurveDisplay\n\n_, ax = ()\n\nsvr = (kernel=\"rbf\", C=1e1, gamma=0.1)\nkr = (kernel=\"rbf\", alpha=0.1, gamma=0.1)\n\ncommon_params = {\n    \"X\": X[:100],\n    \"y\": y[:100],\n    \"train_sizes\": (0.1, 1, 10),\n    \"scoring\": \"neg_mean_squared_error\",\n    \"negate_score\": True,\n    \"score_name\": \"Mean Squared Error\",\n    \"std_display_style\": None,\n    \"ax\": ax,\n}\n\n(svr, **common_params)\n(kr, **common_params)\nax.set_title(\"Learning curves\")\nax.legend(handles=ax.get_legend_handles_labels()[0], labels=[\"SVR\", \"KRR\"])\n\n()\n\n\n<img alt=\"Learning curves\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\" srcset=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\"/>"
        ]
    },
    {
        "id": "1098909",
        "GT": "The wine dataset\n\nThe wine dataset has 13 features and one identifier column. \nIn this dataset there are no missing values. Because there already is an identifier there is no noeed to add a label column.->df_wine.columns = ['Identifier','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']\ndf_wine.head(5)",
        "pred": [
            "statsmodels->Examples->Statistics->ANOVA->df_infl = infl.summary_frame()",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->df = sns.load_dataset('titanic')\n\nclf = RandomForestClassifier()\nparam_grid = dict(max_depth=[1, 2, 5, 10, 20, 30, 40],\n                  min_samples_split=[2, 5, 10],\n                  min_samples_leaf=[2, 3, 5])\nest = GridSearchCV(clf, param_grid=param_grid, n_jobs=4)\n\ny = df['survived']\nX = df.drop(['survived', 'who', 'alive'], axis=1)\n\nX = pd.get_dummies(X, drop_first=True)\nX = X.fillna(value=X.median())\nest.fit(X, y);",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 1: fitting the model on the available dataset->y_pre = y.iloc[:-5]\ny_pre.plot(figsize=(15, 3), title='Inflation');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\"/>",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Periodic spline features->hour_df = (\n    (0, 26, 1000).reshape(-1, 1),\n    columns=[\"hour\"],\n)\nsplines = periodic_spline_transformer(24, n_splines=12).fit_transform(hour_df)\nsplines_df = (\n    splines,\n    columns=[f\"spline_{i}\" for i in range(splines.shape[1])],\n)\n([hour_df, splines_df], axis=\"columns\").plot(x=\"hour\", cmap=plt.cm.tab20b)\n_ = (\"Periodic spline-based encoding for the 'hour' feature\")\n\n\n<img alt=\"Periodic spline-based encoding for the 'hour' feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_005.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_005.png\"/>",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team->df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()"
        ]
    },
    {
        "id": "422152",
        "GT": "Load Spatial Data Frame Tutorial\n\nThis is the completed sample for the [Load spatial data frame](https://developers.arcgis.com/labs/python/load-spatial-data-frame/) ArcGIS tutorial. \n\n[ArcGIS tutorials](https://developers.arcgis.com/labs/) are short guides demonstrating the three phases of building geospatial apps: Data, Design, Develop.\n\n__Note__: Please complete the [Import Data](https://developers.arcgis.com/labs/python/import-data/) tutorial if you have not done so already. You will use the output feature layer from this lab to learn how to find and share an item.->from arcgis.gis import GIS\nimport pandas as pd",
        "pred": [
            "scipy->Statistics (scipy.stats)->Kernel density estimation->Univariate estimation->from scipy import stats\n import matplotlib.pyplot as plt",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->from utils import download_airports\nimport zipfile",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers->from statsmodels.graphics.api import abline_plot\nfrom statsmodels.formula.api import ols, rlm",
            "statsmodels->Examples->Generalized Linear Models->Generalized Linear Models Overview->GLM: Binomial response data->Plots->from statsmodels.graphics.api import abline_plot",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->from statsmodels.graphics.api import qqplot",
            "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->from statsmodels.graphics.api import qqplot"
        ]
    },
    {
        "id": "396707",
        "GT": "Xgboost Demo->\nfrom sklearn.datasets import make_classification\nimport numpy as np\nimport pandas as pd \nimport xgboost as xgb \nfrom matplotlib import pylab as plt \nimport seaborn as sns",
        "pred": [
            "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->import matplotlib.pyplot as plt\nfrom sklearn.model_selection import \nfrom sklearn.datasets import \nfrom sklearn.tree import",
            "sklearn->Examples->Ensemble methods->Plot individual and voting regression predictions->import matplotlib.pyplot as plt\n\nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.ensemble import \nfrom sklearn.linear_model import \nfrom sklearn.ensemble import",
            "sklearn->Examples->Semi Supervised Classification->Label Propagation learning a complex structure->import numpy as np\nfrom sklearn.datasets import \n\nn_samples = 200\nX, y = (n_samples=n_samples, shuffle=False)\nouter, inner = 0, 1\nlabels = (n_samples, -1.0)\nlabels[0] = outer\nlabels[-1] = inner",
            "sklearn->Examples->Pipelines and composite estimators->Effect of transforming the targets in regression model->Synthetic example->import numpy as np\nfrom sklearn.datasets import \n\nX, y = (n_samples=10_000, noise=100, random_state=0)\ny = ((y + abs(y.min())) / 200)\ny_trans = (y)",
            "sklearn->Examples->Generalized Linear Models->Non-negative least squares->import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import"
        ]
    },
    {
        "id": "717129",
        "GT": "Replacing zero with NAN in runtime column.->ax = df.groupby('runtime').popularity.mean().plot(kind='line', title =\"Runtime over Film Popularity\", figsize=(15, 10), legend=True, fontsize=12)\nax.set_xlabel(\"Runtime\", fontsize=12)\nax.set_ylabel(\"Popularity\", fontsize=12)\nplt.show()\n\n",
        "pred": [
            "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data->fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(resid, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(resid, lags=40, ax=ax2)",
            "pandas_toms_blog->Scaling->Dask->ax = occupation_avg.sort_values(ascending=False).plot.barh(color='k', width=0.9);\nlim = ax.get_ylim()\nax.vlines(total_avg, *lim, color='C1', linewidth=3)\nax.legend(['Average donation'])\nax.set(xlabel=\"Donation Amount\", title=\"Average Dontation by Occupation\")\nsns.despine()",
            "pandas_toms_blog->Time Series->Timeseries->Forecasting->ax = y.plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='Forecast', alpha=.7)\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_ylabel(\"Monthly Flights\")\nplt.legend()\nsns.despine()",
            "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Component-Component plus Residual (CCPR) Plots->fig = sm.graphics.plot_ccpr_grid(prestige_model)\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_26_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_26_0.png\"/>",
            "statsmodels->Examples->State space models->VARMAX: Introduction->Example 1: VAR->ax = res.impulse_responses(10, orthogonalized=True, impulse=[1, 0]).plot(figsize=(13,3))\nax.set(xlabel='t', title='Responses to a shock to `dln_inv`');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_varmax_8_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_varmax_8_0.png\"/>"
        ]
    },
    {
        "id": "611386",
        "GT": "#set the ending date = 17/05/2015\ndata_2009.loc[lambda data_2009: data_2009.iloc[:,1] == '17/05/2015', :]\ndata_2009 = data_2009.iloc[87:,:].reset_index(drop=True)\ndata_2009.head()\n#save the data for further use\ndata_2009.to_hdf('result/n4.h5','obamaapproval',table=True,mode='a')",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Ordinal regression with a custom cumulative cLogLog distribution:-># using a SciPy distribution\nres_exp = OrderedModel(data_student['apply'],\n                           data_student[['pared', 'public', 'gpa']],\n                           distr=stats.expon).fit(method='bfgs', disp=False)\nres_exp.summary()",
            "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method-># fit in statsmodels\nmodel = ETSModel(\n    austourists,\n    error=\"add\",\n    trend=\"add\",\n    seasonal=\"add\",\n    damped_trend=True,\n    seasonal_periods=4,\n)\nfit = model.fit()\n\n# fit with R params\nparams_R = [\n    0.35445427,\n    0.03200749,\n    0.39993387,\n    0.97999997,\n    24.01278357,\n    0.97770147,\n    1.76951063,\n    -0.50735902,\n    -6.61171798,\n    5.34956637,\n]\nfit_R = model.smooth(params_R)\n\naustourists.plot(label=\"data\")\nplt.ylabel(\"Australian Tourists\")\n\nfit.fittedvalues.plot(label=\"statsmodels fit\")\nfit_R.fittedvalues.plot(label=\"R fit\", linestyle=\"--\")\nplt.legend()",
            "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation-># plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>",
            "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard-># 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)",
            "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results-># Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>"
        ]
    },
    {
        "id": "951620",
        "GT": "# Show the equivalent dataframe (i.e., dense matrix version).\npd.DataFrame(dtm_A_train.toarray(), columns=vect_A.get_feature_names())",
        "pred": [
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->TVP-VAR model in Statsmodels-># Create an instance of our TVPVAR class with our observed dataset y\nmod = TVPVAR(y)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example-># Compute the root mean square error\nrmse = (flattened**2).mean(axis=1)**0.5\n\nprint(rmse)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend-># Compute the root mean square error\nrmse = (flattened**2).mean(axis=1)**0.5\n\nprint(rmse)",
            "statsmodels->Examples->State space models->State space modeling: Local Linear Trends-># Perform prediction and forecasting\npredict = res.get_prediction()\nforecast = res.get_forecast('2014')",
            "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Steps->3. Save model A-># Specify a path to save to\nPATH = \"model.pt\"\n\n(netA.state_dict(), PATH)",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()"
        ]
    },
    {
        "id": "117203",
        "GT": "1. 10 countries with the lowest infant mortality rates->mr_df.sort_values().head(10)",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->glm_mod.model.data.orig_endog.sum(1)",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.head(5)",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->delays.nsmallest(5).sort_values()",
            "statsmodels->Examples->Linear Regression Models->Rolling Least Squares->Expanding Sample->res.nobs[10:15]",
            "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data->res.forecast_components(12)"
        ]
    },
    {
        "id": "86231",
        "GT": "payoff = s[:,-1]-k",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Generalized Least Squares->rho = resid_fit.params[1]",
            "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution->kde.evaluate(-1)",
            "sklearn->Tutorials->Working With Text Data->Extracting features from text files->Tokenizing text with scikit-learn->count_vect.vocabulary_.get(u'algorithm')\n4690",
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables->dta_c.T[0]",
            "torch->Frontend APIs->Jacobians, Hessians, hvp, vhp, and more: composing function transforms->Computing the Jacobian->get_perf(, \"without vmap\",  , \"vmap\")"
        ]
    },
    {
        "id": "1077041",
        "GT": "Summarizing Data->weather.corr() # pairwise correlation",
        "pred": [
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather.loc['DSM'].head()",
            "pandas_toms_blog->Indexes->Merging->The merge version->weather.head()",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->sns.pairplot(penguins)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->sns.pairplot(penguins)\n",
            "pandas_toms_blog->Indexes->Flavors->Indexes for Alignment->temp.head().to_frame()",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->sns.set_theme()\nsinplot()\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->sns.set_theme()\nsinplot()\n"
        ]
    },
    {
        "id": "148908",
        "GT": "lab = df_dbscan['clusterLabel']\nlab.SVD = df_dbscan['clusterLabel.SVD']\nlab.raw = df_dbscan['clusterLabel.raw']",
        "pred": [
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->OLS vs.\u00a0WLS->pred_ols = res_ols.get_prediction()\niv_l_ols = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u_ols = pred_ols.summary_frame()[\"obs_ci_upper\"]",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Data->spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n"
        ]
    },
    {
        "id": "1095879",
        "GT": "Thinkful Data Science Event:\n## Predicting the Oscars- \n### 5/29/2018\n\nIn this practical workshop you'll use a dataset that contains previous Oscar winners to build a prediction model to guess the winner for Best Picture Award. In the process, you'll get an introduction to the data scientist's tools and methods. This will include an overview of basic machine learning concepts.->Getting Started-\n\nBefore we can actually start making predictions, there are several steps that we must first take to load and pre-process the data.-># File location\nurl = 'https://www.dropbox.com/s/shg31hm4voydqnl/Thinkful%20Workshops%20-%20Predicting%20the%20Oscars.zip?dl=1'\n\nr = requests.get(url)",
        "pred": [
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "torch->PyTorch Recipes->PyTorch Profiler->Steps->7. Visualizing data as a flamegraph-># git clone https://github.com/brendangregg/FlameGraph\n# cd FlameGraph\n# ./flamegraph.pl --title \"CUDA time\" --countname \"us.\" /tmp/profiler_stacks.txt &gt; perf_viz.svg",
            "matplotlib->Tutorials->Introductory->Image tutorial->Importing image data into Numpy arrays->img = np.asarray(Image.open('../../doc/_static/stinkbug.png'))\nprint(repr(img))",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->mfx = affair_mod.get_margeff()\nprint(mfx.summary())",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->%%time\nr = gcd_vec(pairs['LATITUDE_1'], pairs['LONGITUDE_1'],\n            pairs['LATITUDE_2'], pairs['LONGITUDE_2'])"
        ]
    },
    {
        "id": "362761",
        "GT": "Slicing & selecting\n\nThe preferred way of slicing are:\n* Using <code>DataFrame.iloc</code> (by position)\n* Using <code>DataFrame.loc</code> (slice by index/column label).\n\nSlicing rows with <code>DataFrame.iloc</code> (by position):->df.loc[1:5, 'Country':'HasTraffic']",
        "pred": [
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->df.ix[10:15, ['fl_date', 'tail_num']]",
            "pandas_toms_blog->Scaling->Dask->df.visualize(rankdir='LR')",
            "statsmodels->Examples->Statistics->ANOVA->df_infl[:5]",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf.loc[pd.IndexSlice[:, ['ORD', 'DSM']], ['dep_time', 'dep_delay']]",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf.loc[['AA', 'DL', 'US'], ['dep_time', 'dep_delay']]"
        ]
    },
    {
        "id": "838692",
        "GT": "Model Evaluation Using a Test Set\nTo evaluate the model we now split the data between train and test using 2/3 of the data for training and 1/3 for testing. Then, we compute the model using the training data and evaluate it using the test data.->max_polynomial = 10\nmse = np.zeros(max_polynomial)\nr2 = np.zeros(max_polynomial)\nmse_train = np.zeros(max_polynomial)\nr2_train = np.zeros(max_polynomial)\n\nfor i in range(max_polynomial):\n    mse[i], r2[i],mse_train[i], r2_train[i] = compute_polynomial_regression_holdout(df,'LSTAT','MEDV',i+1)",
        "pred": [
            "numpy->NumPy Features->Masked Arrays->Exploring the data->totals_row = 35\nlocations = np.delete(locations, (totals_row), axis=0)\nnbcases = np.delete(nbcases, (totals_row), axis=0)\n\nchina_total = nbcases[locations[:, 1] == \"China\"].sum(axis=0)\nchina_total",
            "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation->n_features = 100\n# simulation covariance matrix (AR(1) process)\nr = 0.1\nreal_cov = (r ** (n_features))\ncoloring_matrix = (real_cov)\n\nn_samples_range = (6, 31, 1)\nrepeat = 100\nlw_mse = ((n_samples_range.size, repeat))\noa_mse = ((n_samples_range.size, repeat))\nlw_shrinkage = ((n_samples_range.size, repeat))\noa_shrinkage = ((n_samples_range.size, repeat))\nfor i, n_samples in enumerate(n_samples_range):\n    for j in range(repeat):\n        X = ((size=(n_samples, n_features)), coloring_matrix.T)\n\n        lw = (store_precision=False, assume_centered=True)\n        lw.fit(X)\n        lw_mse[i, j] = lw.error_norm(real_cov, scaling=False)\n        lw_shrinkage[i, j] = lw.shrinkage_\n\n        oa = (store_precision=False, assume_centered=True)\n        oa.fit(X)\n        oa_mse[i, j] = oa.error_norm(real_cov, scaling=False)\n        oa_shrinkage[i, j] = oa.shrinkage_\n\n# plot MSE\n(2, 1, 1)\n(\n    n_samples_range,\n    lw_mse.mean(1),\n    yerr=lw_mse.std(1),\n    label=\"Ledoit-Wolf\",\n    color=\"navy\",\n    lw=2,\n)\n(\n    n_samples_range,\n    oa_mse.mean(1),\n    yerr=oa_mse.std(1),\n    label=\"OAS\",\n    color=\"darkorange\",\n    lw=2,\n)\n(\"Squared error\")\n(loc=\"upper right\")\n(\"Comparison of covariance estimators\")\n(5, 31)\n\n# plot shrinkage coefficient\n(2, 1, 2)\n(\n    n_samples_range,\n    lw_shrinkage.mean(1),\n    yerr=lw_shrinkage.std(1),\n    label=\"Ledoit-Wolf\",\n    color=\"navy\",\n    lw=2,\n)\n(\n    n_samples_range,\n    oa_shrinkage.mean(1),\n    yerr=oa_shrinkage.std(1),\n    label=\"OAS\",\n    color=\"darkorange\",\n    lw=2,\n)\n(\"n_samples\")\n(\"Shrinkage\")\n(loc=\"lower right\")\n(()[0], 1.0 + (()[1] - ()[0]) / 10.0)\n(5, 31)\n\n()\n\n\n<img alt=\"Comparison of covariance estimators\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lw_vs_oas_001.png\" srcset=\"../../_images/sphx_glr_plot_lw_vs_oas_001.png\"/>",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables->nsample = 50\ngroups = np.zeros(nsample, int)\ngroups[20:40] = 1\ngroups[40:] = 2\n# dummy = (groups[:,None] == np.unique(groups)).astype(float)\n\ndummy = pd.get_dummies(groups).values\nx = np.linspace(0, 20, nsample)\n# drop reference category\nX = np.column_stack((x, dummy[:, 1:]))\nX = sm.add_constant(X, prepend=False)\n\nbeta = [1.0, 3, -3, 10]\ny_true = np.dot(X, beta)\ne = np.random.normal(size=nsample)\ny = y_true + e",
            "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors->alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns->n_comps = 2\n\nmethods = [\n    (\"PCA\", ()),\n    (\"Unrotated FA\", ()),\n    (\"Varimax FA\", (rotation=\"varimax\")),\n]\nfig, axes = (ncols=len(methods), figsize=(10, 8), sharey=True)\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks((len(feature_names)))\n    ax.set_yticklabels(feature_names)\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\nfig.suptitle(\"Factors\")\n()\n()\n\n\n<img alt=\"Factors, PCA, Unrotated FA, Varimax FA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_002.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_002.png\"/>"
        ]
    },
    {
        "id": "394300",
        "GT": "Two Wavelength Degradations on mCP\n\nThis will seperate what is happening to Ir(ppy)3 itself (when pumped at 405, that is all that is being excited) from the host to Ir(ppy)3 transfer.\n\n## 350 pump, 405 probe->325-400 Pump, 405 Probe->plt.plot(mcp325405['On Time'],mcp325405['350Signal'],c='blue',linestyle='-',label='mcp 325')\nplt.plot(mcp325405['On Time'],mcp325405['405Signal'],c='red',linestyle='-',label='mcp 405')\nplt.xlabel('Time (Hours)')\nplt.ylabel('Signal')\nplt.legend()\nplt.show()",
        "pred": [
            "statsmodels->Examples->Plotting->Box Plots->Bean Plots->plt.rcParams[\"figure.subplot.bottom\"] = 0.23  # keep labels visible\nplt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # make plot larger in notebook\nage = [data.exog[\"age\"][data.endog == id] for id in party_ID]\nfig = plt.figure()\nax = fig.add_subplot(111)\nplot_opts = {\n    \"cutoff_val\": 5,\n    \"cutoff_type\": \"abs\",\n    \"label_fontsize\": \"small\",\n    \"label_rotation\": 30,\n}\nsm.graphics.beanplot(age, ax=ax, labels=labels, plot_opts=plot_opts)\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\n# plt.show()",
            "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->IIR Filter->plt.title('Digital filter frequency response')\n plt.plot(w, 20*np.log10(np.abs(h)))\n plt.title('Digital filter frequency response')\n plt.ylabel('Amplitude Response [dB]')\n plt.xlabel('Frequency (rad/sample)')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with amplitude response on the Y axis vs Frequency on the X axis. A single trace shows a smooth low-pass filter with the left third passband near 0 dB. The right two-thirds are about 60 dB down with two sharp narrow valleys dipping down to -100 dB.\"' class=\"plot-directive\" src=\"../_images/signal-6.png\"/>\n</figure>",
            "scipy->Signal Processing (scipy.signal)->Detrend->plt.plot(t, y, '-rx')\n plt.plot(t, yconst, '-bo')\n plt.plot(t, ylin, '-k+')\n plt.grid()\n plt.legend(['signal', 'const. detrend', 'linear detrend'])\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with no units. A red trace corresponding to the original signal curves from the bottom left to the top right. A blue trace has the constant detrend applied and is below the red trace with zero Y offset. The last black trace has the linear detrend applied and is almost flat from left to right highlighting the curve of the original signal. This last trace has an average slope of zero and looks very different.\"' class=\"plot-directive\" src=\"../_images/signal-10.png\"/>\n</figure>",
            "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->FIR Filter->plt.title('Digital filter frequency response')\n plt.plot(w, np.abs(h))\n plt.title('Digital filter frequency response')\n plt.ylabel('Amplitude Response')\n plt.xlabel('Frequency (rad/sample)')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays an X-Y plot with amplitude response on the Y axis vs frequency on the X axis. A single trace forms a shape similar to a heartbeat signal.\"' class=\"plot-directive\" src=\"../_images/signal-5.png\"/>\n</figure>",
            "numpy->NumPy Features->Masked Arrays->Fitting Data->plt.plot(t, china_total)\nplt.plot(t[china_total.mask], model(t)[china_total.mask], \"--\", color=\"orange\")\nplt.plot(7, model(7), \"r*\")\nplt.xticks([0, 7, 13], dates[[0, 7, 13]])\nplt.yticks([0, model(7), 10000, 17500])\nplt.legend([\"Mainland China\", \"Cubic estimate\", \"7 days after start\"])\nplt.title(\n    \"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\n\"\n    \"Cubic estimate for 7 days after start\"\n)"
        ]
    },
    {
        "id": "1025041",
        "GT": "working on markdown 1->#2010 records\nusers_2010 = data_features[data_features['Date'] >= '2010-01-01']\nusers_2010 = users_2010[users_2010['Date'] <= '2010-12-31']\nusers_2010\n#2011 records\nusers_2011 = data_features[data_features['Date'] >= '2011-01-01']\nusers_2011 = users_2011[users_2011['Date'] <= '2011-12-31']\nusers_2011\n#2012 records\nusers_2012 = data_features[data_features['Date'] >= '2012-01-01']\nusers_2012 = users_2012[users_2012['Date'] <= '2012-12-31']\nusers_2012\n#2013 records\nusers_2013 = data_features[data_features['Date'] >= '2013-01-01']\nusers_2013 = users_2013[users_2013['Date'] <= '2013-12-31']\nusers_2013",
        "pred": [
            "sklearn->Examples->Generalized Linear Models->Lasso on dense and sparse data->Comparing the two Lasso implementations on Sparse data-># make a copy of the previous data\nXs = X.copy()\n# make Xs sparse by replacing the values lower than 2.5 with 0s\nXs[Xs &lt; 2.5] = 0.0\n# create a copy of Xs in sparse format\nXs_sp = (Xs)\nXs_sp = Xs_sp.tocsc()\n\n# compute the proportion of non-zero coefficient in the data matrix\nprint(f\"Matrix density : {(Xs_sp.nnz / float(X.size) * 100):.3f}%\")\n\nalpha = 0.1\nsparse_lasso = (alpha=alpha, fit_intercept=False, max_iter=10000)\ndense_lasso = (alpha=alpha, fit_intercept=False, max_iter=10000)\n\nt0 = ()\nsparse_lasso.fit(Xs_sp, y)\nprint(f\"Sparse Lasso done in {(() - t0):.3f}s\")\n\nt0 = ()\ndense_lasso.fit(Xs, y)\nprint(f\"Dense Lasso done in  {(() - t0):.3f}s\")\n\n# compare the regression coefficients\ncoeff_diff = (sparse_lasso.coef_ - dense_lasso.coef_)\nprint(f\"Distance between coefficients : {coeff_diff:.2e}\")",
            "statsmodels->Examples->State space models->Unobserved Components: Application->Model-># Output\noutput_mod = sm.tsa.UnobservedComponents(dta['US GNP'], **unrestricted_model)\noutput_res = output_mod.fit(method='powell', disp=False)\n\n# Prices\nprices_mod = sm.tsa.UnobservedComponents(dta['US Prices'], **unrestricted_model)\nprices_res = prices_mod.fit(method='powell', disp=False)\n\nprices_restricted_mod = sm.tsa.UnobservedComponents(dta['US Prices'], **restricted_model)\nprices_restricted_res = prices_restricted_mod.fit(method='powell', disp=False)\n\n# Money\nmoney_mod = sm.tsa.UnobservedComponents(dta['US monetary base'], **unrestricted_model)\nmoney_res = money_mod.fit(method='powell', disp=False)\n\nmoney_restricted_mod = sm.tsa.UnobservedComponents(dta['US monetary base'], **restricted_model)\nmoney_restricted_res = money_restricted_mod.fit(method='powell', disp=False)",
            "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard-># 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)",
            "statsmodels->User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->VAR(p) processes->Model fitting-># some example data\nIn [1]: import numpy as np\n\nIn [2]: import pandas\n\nIn [3]: import statsmodels.api as sm\n\nIn [4]: from statsmodels.tsa.api import VAR\n\nIn [5]: mdata = sm.datasets.macrodata.load_pandas().data\n\n# prepare the dates index\nIn [6]: dates = mdata[['year', 'quarter']].astype(int).astype(str)\n\nIn [7]: quarterly = dates[\"year\"] + \"Q\" + dates[\"quarter\"]\n\nIn [8]: from statsmodels.tsa.base.datetools import dates_from_str\n\nIn [9]: quarterly = dates_from_str(quarterly)\n\nIn [10]: mdata = mdata[['realgdp','realcons','realinv']]\n\nIn [11]: mdata.index = pandas.DatetimeIndex(quarterly)\n\nIn [12]: data = np.log(mdata).diff().dropna()\n\n# make a VAR model\nIn [13]: model = VAR(data)",
            "numpy->NumPy Applications->Deep learning on MNIST->3. Build and train a small neural network from scratch->Compose the model and begin training and testing it-># The training set metrics.\ny_training_error = [\n    store_training_loss[i] / float(len(training_images))\n    for i in range(len(store_training_loss))\n]\nx_training_error = range(1, len(store_training_loss) + 1)\ny_training_accuracy = [\n    store_training_accurate_pred[i] / float(len(training_images))\n    for i in range(len(store_training_accurate_pred))\n]\nx_training_accuracy = range(1, len(store_training_accurate_pred) + 1)\n\n# The test set metrics.\ny_test_error = [\n    store_test_loss[i] / float(len(test_images)) for i in range(len(store_test_loss))\n]\nx_test_error = range(1, len(store_test_loss) + 1)\ny_test_accuracy = [\n    store_training_accurate_pred[i] / float(len(training_images))\n    for i in range(len(store_training_accurate_pred))\n]\nx_test_accuracy = range(1, len(store_test_accurate_pred) + 1)\n\n# Display the plots.\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\naxes[0].set_title(\"Training set error, accuracy\")\naxes[0].plot(x_training_accuracy, y_training_accuracy, label=\"Training set accuracy\")\naxes[0].plot(x_training_error, y_training_error, label=\"Training set error\")\naxes[0].set_xlabel(\"Epochs\")\naxes[1].set_title(\"Test set error, accuracy\")\naxes[1].plot(x_test_accuracy, y_test_accuracy, label=\"Test set accuracy\")\naxes[1].plot(x_test_error, y_test_error, label=\"Test set error\")\naxes[1].set_xlabel(\"Epochs\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/a155f32ad6a3518292224002121b7167dd36f5c2c99a5ac7ac1d53357125a043.png\" src=\"../_images/a155f32ad6a3518292224002121b7167dd36f5c2c99a5ac7ac1d53357125a043.png\"/>"
        ]
    },
    {
        "id": "1405201",
        "GT": "This is an accuracy score to compare the training target variables to the labels we generated->from scipy.stats import mode\n\nlabels = np.zeros_like(clusters)\nfor i in range(4):\n    mask = (clusters == i)\n    labels[mask] = mode(training_target[mask])[0]\n#np.dtype(labels)\n# for x in range(0,len(training_target)):\n#      print(np.dtype(labels[x,]))\nfrom sklearn.metrics import accuracy_score\naccuracy_score(training_target.astype('int'), labels.astype('int'),normalize=False)",
        "pred": [
            "sklearn->Examples->Clustering->Spectral clustering for image segmentation->Plotting four circles->from sklearn.cluster import \nimport matplotlib.pyplot as plt\n\nlabels = (graph, n_clusters=4, eigen_solver=\"arpack\")\nlabel_im = (mask.shape, -1.0)\nlabel_im[mask] = labels\n\nfig, axs = (nrows=1, ncols=2, figsize=(10, 5))\naxs[0].matshow(img)\naxs[1].matshow(label_im)\n\n()\n\n\n<img alt=\"plot segmentation toy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\" srcset=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\"/>",
            "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Fitting non-linear quantile and least squares regressors->from sklearn.ensemble import \nfrom sklearn.metrics import , \n\n\nall_models = {}\ncommon_params = dict(\n    learning_rate=0.05,\n    n_estimators=200,\n    max_depth=2,\n    min_samples_leaf=9,\n    min_samples_split=9,\n)\nfor alpha in [0.05, 0.5, 0.95]:\n    gbr = (loss=\"quantile\", alpha=alpha, **common_params)\n    all_models[\"q %1.2f\" % alpha] = gbr.fit(X_train, y_train)",
            "sklearn->Examples->Ensemble methods->Monotonic Constraints->from sklearn.ensemble import \nfrom sklearn.inspection import PartialDependenceDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nrng = (0)\n\nn_samples = 1000\nf_0 = rng.rand(n_samples)\nf_1 = rng.rand(n_samples)\nX = [f_0, f_1]\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n\n# y is positively correlated with f_0, and negatively correlated with f_1\ny = 5 * f_0 + (10 *  * f_0) - 5 * f_1 - (10 *  * f_1) + noise",
            "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor->from sklearn.linear_model import \n\nquantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_normal).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_normal\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_normal\n        )",
            "sklearn->Examples->Clustering->Comparison of the K-Means and MiniBatchKMeans clustering algorithms->Compute clustering with MiniBatchKMeans->from sklearn.cluster import \n\nmbk = (\n    init=\"k-means++\",\n    n_clusters=3,\n    batch_size=batch_size,\n    n_init=10,\n    max_no_improvement=10,\n    verbose=0,\n)\nt0 = ()\nmbk.fit(X)\nt_mini_batch = () - t0"
        ]
    },
    {
        "id": "196336",
        "GT": "Salaries\n- Chicago, IL: \"Current Employee Names, Salaries, and Position Titles\"\n- Views: 803,342->#Chicago Salaries Dataset\n#Resource https://data.cityofchicago.org/Administration-Finance/Current-Employee-Names-Salaries-and-Position-Title/xzkq-xp2w\ndf_salaries = pd.read_json('https://data.cityofchicago.org/resource/tt4n-kn4t.json')\ndf_salaries.info()",
        "pred": [
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Addendum: manually computing the news, weights, and impacts-># Print the news, computed by the `news` method\nprint(news.news)\n\n# Manually compute the news\nprint()\nprint((y_update.iloc[0] - phi_hat * y_pre.iloc[-1]).round(6))",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example-># Step 1: append a new observation to the sample and refit the parameters\nappend_res = training_res.append(endog[training_obs:training_obs + 1], refit=True)\n\n# Print the re-estimated parameters\nprint(append_res.params)",
            "seaborn->User guide and tutorial->An introduction to seaborn-># Apply the default theme\nsns.set_theme()\n",
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing->Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_4_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_4_1.png\"/>",
            "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution->Mean AvgClaim Amount per policy:              71.78\nMean AvgClaim Amount | NbClaim  0:           1951.21\nPredicted Mean AvgClaim Amount | NbClaim  0: 1940.95\nPredicted Mean AvgClaim Amount (dummy) | NbClaim  0: 1978.59"
        ]
    },
    {
        "id": "601298",
        "GT": "McCulloch and Pitts Neuron\n\nIn 1943, McCulloch and Pitts introduced a mathematical model of a neuron. It consisted of three components:\n\n1. A set of **weights** $w_i$ corresponding to synapses (inputs)\n2. An **adder** for summing input signals; analogous to cell membrane that collects charge\n3. An **activation function** for determining when the neuron fires, based on accumulated input\n\nThe neuron model is shown schematically below. On the left are input nodes $\\{x_i\\}$, usually expressed as a vector. The strength with which the inputs are able to deliver the signal along the synapse is determined by their corresponding weights $\\{w_i\\}$. The adder then sums the inputs from all the synapses:\n\n$$h = \\sum_i w_i x_i$$\n\nThe parameter $\\theta$ determines whether or not the neuron fires given a weighted input of $h$. If it fires, it returns a value $y=1$, otherwise $y=0$. For example, a simple **activation function** is using $\\theta$ as a simple fixed threshold:\n\n$$y = g(h) = \\left\\{ \\begin{array}{l}\n1, \\text{if } h \\gt \\theta \\\\\n0, \\text{if } h \\le \\theta\n\\end{array} \\right.$$\n\nthis activation function may take any of several forms, such as a logistic function.\n\n![neuron](http://d.pr/i/9AMK+)->inputs = AND[['x1','x2']]\ntarget = AND['y']\n\nw = train(inputs, target, w, 0.25, 10)",
        "pred": [
            "torch->PyTorch Recipes->PyTorch Benchmark->Steps->4. Benchmarking with Blocked Autorange->m0 = t0.blocked_autorange()\nm1 = t1.blocked_autorange()\n\nprint(m0)\nprint(m1)\n\n\n\nOutput",
            "torch->Model Optimization->Parametrizations Tutorial->Implementing parametrizations by hand->layer = LinearSymmetric(3)\nout = layer((8, 3))",
            "numpy->NumPy Applications->Plotting Fractals->Warmup->z = [4, 1-0.2j, 1.6]\nf(z)",
            "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Fusing Convolution with Batch Norm->traced_model = (model)\nprint(traced_model.graph)",
            "scipy->Compressed Sparse Graph Routines (scipy.sparse.csgraph)->Example: Word Ladders->i1 = word_list.searchsorted('ape')\n i2 = word_list.searchsorted('man')\n word_list[i1]\n'ape'\n word_list[i2]\n'man'"
        ]
    },
    {
        "id": "1047227",
        "GT": "smuts = [i['selected'] for i in gametes[0]]",
        "pred": [
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Overriding elements of the seaborn styles->sns.axes_style()\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Overriding elements of the seaborn styles->sns.axes_style()\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->sns.pairplot(penguins)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->sns.pairplot(penguins)\n",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->dta.plot(figsize=(12, 8))",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->delays.nsmallest(5).sort_values()"
        ]
    },
    {
        "id": "197762",
        "GT": "Challenge->df3= df[(df['type_1']=='Water') | (df['type_1']=='Normal') \n                                | (df['type_1']=='Grass')]\ndf3= df3[['type_1','attack','defense','hp','speed']]",
        "pred": [
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset->co2_data = co2_data.resample(\"M\").mean().dropna(axis=\"index\", how=\"any\")\nco2_data.plot()\n(\"Monthly average of CO$_2$ concentration (ppm)\")\n_ = (\n    \"Monthly average of air samples measurements\\nfrom the Mauna Loa Observatory\"\n)\n\n\n<img alt=\"Monthly average of air samples measurements from the Mauna Loa Observatory\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_co2_002.png\" srcset=\"../../_images/sphx_glr_plot_gpr_co2_002.png\"/>",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->delta = (by_game.home_rest - by_game.away_rest).dropna().astype(int)\nax = (delta.value_counts()\n    .reindex(np.arange(delta.min(), delta.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind='bar', color='k', width=.9, rot=0, figsize=(12, 6))\n)\nsns.despine()\nax.set(xlabel='Difference in Rest (Home - Away)', ylabel='Games');",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->modfd_logit = OrderedModel.from_formula(\"apply ~ 1 + pared + public + gpa + C(dummy)\", data_student,\n                                      distr='logit')\nresfd_logit = modfd_logit.fit(method='bfgs')\nprint(resfd_logit.summary())",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team->df = df.assign(away_strength=df['away_team'].map(win_percent),\n               home_strength=df['home_team'].map(win_percent),\n               point_diff=df['home_points'] - df['away_points'],\n               rest_diff=df['home_rest'] - df['away_rest'])\ndf.head()",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 1: fitting the model on the available dataset->y_pre = y.iloc[:-5]\ny_pre.plot(figsize=(15, 3), title='Inflation');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\"/>"
        ]
    },
    {
        "id": "267469",
        "GT": "This file will have more deadheading analysis\nAt least at the beginning I'll be looking at busruns that are 50% plus deadheading->Subset data and attempt to fix the run type issue (still doesn't work)->fiftiers = dh_data[dh_data.PctDeadhead > .5]\nfiftiers['Run'] = fiftiers['Run'].astype('str')",
        "pred": [
            "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach->mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()"
        ]
    },
    {
        "id": "509262",
        "GT": "RENTAL LISTINGS - EDA \n\n----------------------------------------------------\n\n** by Ignacio Carracedo **\n\n*February 2017 *\n\n-------------------------------------------------->Imports and versions\n\nThe next two cells import all necessary libraries and shows the versions used->train_df = pd.read_json(\"./input/train.json\")\nprint(train_df.shape)\ntrain_df.info()",
        "pred": [
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "matplotlib->Tutorials->Introductory->Image tutorial->Importing image data into Numpy arrays->img = np.asarray(Image.open('../../doc/_static/stinkbug.png'))\nprint(repr(img))",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->dta = sm.datasets.star98.load_pandas().data\nprint(dta.columns)",
            "pandas_toms_blog->Scaling->Using Dask->df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)"
        ]
    },
    {
        "id": "1164439",
        "GT": "Class structure->print(\"Train accuracy: {:.1f}%\".format(model_mlp.score(X_train, y_train)[\"acc\"] * 100))\nprint(\"Valid accuracy: {:.1f}%\".format(model_mlp.score(X_valid, y_valid)[\"acc\"] * 100))",
        "pred": [
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding->print(\"The data type of training labels: {}\".format(y_train.dtype))\nprint(\"The data type of test labels: {}\".format(y_test.dtype))",
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding->print(\"The data type of training labels: {}\".format(training_labels.dtype))\nprint(\"The data type of test labels: {}\".format(test_labels.dtype))",
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the image data to the floating-point format->print(\"The data type of training images: {}\".format(x_train.dtype))\nprint(\"The data type of test images: {}\".format(x_test.dtype))",
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the image data to the floating-point format->print(\"The data type of training images: {}\".format(training_images.dtype))\nprint(\"The data type of test images: {}\".format(test_images.dtype))",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Accuracy of the Model->print(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->print(f\"RF train accuracy: {rf.score(X_train, y_train):.3f}\")\nprint(f\"RF test accuracy: {rf.score(X_test, y_test):.3f}\")"
        ]
    },
    {
        "id": "833395",
        "GT": "Logistic Regression->logreg_hitters_params = optimal_parameters.lr_hitters_params(to_predict_hitters,x_hitters, hitter_predictions)",
        "pred": [
            "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types->search_cv.fit(X_train, y_train)\n\nprint(\"Best params:\")\nprint(search_cv.best_params_)",
            "torch->PyTorch Recipes->Zeroing out gradients in PyTorch->Steps->4. Define a Loss function and optimizer->net = Net()\ncriterion = ()\noptimizer = (net.parameters(), lr=0.001, momentum=0.9)",
            "statsmodels->Examples->User Notes->Prediction->In-sample prediction->ypred = olsres.predict(X)\nprint(ypred)",
            "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)",
            "scipy->Optimization (scipy.optimize)->Constrained minimization of multivariate scalar functions (minimize)->Trust-Region Constrained Algorithm (method='trust-constr')->Defining Nonlinear Constraints:->nonlinear_constraint = NonlinearConstraint(cons_f, -np.inf, 1, jac=cons_J, hess='2-point')"
        ]
    },
    {
        "id": "1189520",
        "GT": "Clustering Lyrics Using Kmeans->Term Frequency-Inverse Document Frequency->english = stopwords.words('english')\nfrench = stopwords.words('french')\nspanish = stopwords.words('spanish')\nportuguese = stopwords.words('portuguese')\nmine = ['yeah', 'get', 'got', 'would', 'nan', 'ca']\nstop = english + french + spanish + portuguese + mine\n#stop",
        "pred": [
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Training the Network->hidden_dim = 64\ninput_dim = emb_matrix['memory'].shape[0]\nlearning_rate = 0.001\nepochs = 10\nparameters = initialise_params(hidden_dim,\n                               input_dim)\nv, s = initialise_mav(hidden_dim,\n                      input_dim,\n                      parameters)",
            "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor->quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "matplotlib->Tutorials->Intermediate->Arranging multiple Axes in a Figure->High-level methods for making grids->Nested Axes layouts->inner = [['innerA'],\n         ['innerB']]\nouter = [['upper left',  inner],\n          ['lower left', 'lower right']]\n\nfig, axd = plt.subplot_mosaic(outer, layout=\"constrained\")\nfor k in axd:\n    annotate_axes(axd[k], f'axd[\"{k}\"]')\n\n\n<img alt=\"arranging axes\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_arranging_axes_008.png\" srcset=\"../../_images/sphx_glr_arranging_axes_008.png, ../../_images/sphx_glr_arranging_axes_008_2_0x.png 2.0x\"/>",
            "torch->PyTorch Recipes->Automatic Mixed Precision->All together: \u201cAutomatic Mixed Precision\u201d->use_amp = True\n\nnet = make_model(in_size, out_size, num_layers)\nopt = (net.parameters(), lr=0.001)\nscaler = (enabled=use_amp)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        with (device_type='cuda', dtype=torch.float16, enabled=use_amp):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\nend_timer_and_print(\"Mixed precision:\")",
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance->res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)"
        ]
    },
    {
        "id": "1213431",
        "GT": "Setting up Data (label subscriber as class=0)->tl = TomekLinks(n_jobs=4, ratio='majority')\nX_res, y_res = tl.fit_sample(X, y)",
        "pred": [
            "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->mod = TVRegression(y_t, x_t, w_t)\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())",
            "statsmodels->Examples->State space models->Statespace: Custom Models->Model 2: time-varying parameters with non identity transition matrix->1) Change the starting parameters function->mod = TVRegressionExtended(y_t, x_t, w_t)\nres = mod.fit(maxiter=2000)  # it doesn't converge with 50 iters\nprint(res.summary())",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "statsmodels->Examples->User Notes->Prediction->Estimation->olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())"
        ]
    },
    {
        "id": "334256",
        "GT": "Build Date Array->def daterange(start_date, end_date):\n    for n in range(int ((end_date - start_date).days)):\n        yield start_date + timedelta(n)\n\n# Initialize date range start at 6/30 to compensate for UTC\nstart_date = date(2017, 6, 30)\nend_date = date(2017, 11, 1)\n\ndates = []\n\nfor single_date in daterange(start_date, end_date):\n        dates.append(single_date.strftime(\"%Y-%m-%d\"))",
        "pred": [
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Calculating the Air Quality Index->Moving averages->def moving_mean(a, n):\n    ret = np.cumsum(a, dtype=float, axis=0)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\npollutants_A_24hr_avg = moving_mean(pollutants_A, 24)\npollutants_B_8hr_avg = moving_mean(pollutants_B, 8)[-(pollutants_A_24hr_avg.shape[0]):]",
            "statsmodels->Examples->Statistics->Mediation analysis with duration data->def gen_outcome(otype, mtime0):\n    if otype == \"full\":\n        lp = 0.5 * mtime0\n    elif otype == \"no\":\n        lp = exp\n    else:\n        lp = exp + mtime0\n    mn = np.exp(-lp)\n    ytime0 = -mn * np.log(np.random.uniform(size=n))\n    ctime = -2 * mn * np.log(np.random.uniform(size=n))\n    ystatus = (ctime = ytime0).astype(int)\n    ytime = np.where(ytime0 &lt;= ctime, ytime0, ctime)\n    return ytime, ystatus",
            "torch->Model Optimization->(beta) Dynamic Quantization on BERT->3. Apply the dynamic quantization->3.2 Evaluate the inference accuracy and time->def time_model_evaluation(model, configs, tokenizer):\n    eval_start_time = time.time()\n    result = evaluate(configs, model, tokenizer, prefix=\"\")\n    eval_end_time = time.time()\n    eval_duration_time = eval_end_time - eval_start_time\n    print(result)\n    print(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time))\n\n# Evaluate the original FP32 BERT model\ntime_model_evaluation(model, configs, tokenizer)\n\n# Evaluate the INT8 BERT model after the dynamic quantization\ntime_model_evaluation(quantized_model, configs, tokenizer)",
            "matplotlib->Tutorials->Text->Text in Matplotlib Plots->Ticks and ticklabels->Tick Locators and Formatters->def formatoddticks(x, pos):\n    \"\"\"Format odd tick positions.\"\"\"\n    if x % 2:\n        return f'{x:1.2f}'\n    else:\n        return ''\n\n\nfig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\nax.plot(x1, y1)\nlocator = matplotlib.ticker.MaxNLocator(nbins=6)\nax.xaxis.set_major_formatter(formatoddticks)\nax.xaxis.set_major_locator(locator)\n\nplt.show()\n\n\n<img alt=\"text intro\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_text_intro_015.png\" srcset=\"../../_images/sphx_glr_text_intro_015.png, ../../_images/sphx_glr_text_intro_015_2_0x.png 2.0x\"/>",
            "statsmodels->Examples->Statistics->Mediation analysis with duration data->def build_df(ytime, ystatus, mtime0, mtime, mstatus):\n    df = pd.DataFrame(\n        {\n            \"ytime\": ytime,\n            \"ystatus\": ystatus,\n            \"mtime\": mtime,\n            \"mstatus\": mstatus,\n            \"exp\": exp,\n        }\n    )\n    return df"
        ]
    },
    {
        "id": "913352",
        "GT": "<a id='2.4' href='#section2'>2.4 bureau_balance.csv</a>-># aggregated stats of numerica variables\ntemp_agg = agg_numeric(bureau_balance, parent_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\n\n# value count categorical variables\ntemp_count = count_categorical(bureau_balance, group_var = 'SK_ID_BUREAU', df_name = 'bureau_balance')\n\n# merge two together\ntemp_merged = temp_agg.merge(temp_count, on = 'SK_ID_BUREAU', how ='left')",
        "pred": [
            "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds-># range of admissible distortions\neps_range = (0.1, 0.99, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = (1, 9, 9)\n\n()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = (n_samples_range, eps=eps)\n    (n_samples_range, min_n_components, color=color)\n\n([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n(\"Number of observations to eps-embed\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_samples vs n_components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\"/>",
            "sklearn->Examples->Model Selection->Statistical comparison of models using grid search-># create df of model scores ordered by performance\nmodel_scores = results_df.filter(regex=r\"split\\d*_test_score\")\n\n# plot 30 examples of dependency between cv fold and AUC scores\nfig, ax = ()\n(\n    data=model_scores.transpose().iloc[:30],\n    dashes=False,\n    palette=\"Set1\",\n    marker=\"o\",\n    alpha=0.5,\n    ax=ax,\n)\nax.set_xlabel(\"CV test fold\", size=12, labelpad=10)\nax.set_ylabel(\"Model AUC\", size=12)\nax.tick_params(bottom=True, labelbottom=False)\n()\n\n# print correlation of AUC scores across folds\nprint(f\"Correlation of models:\\n {model_scores.transpose().corr()}\")\n\n\n<img alt=\"plot grid search stats\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_grid_search_stats_002.png\" srcset=\"../../_images/sphx_glr_plot_grid_search_stats_002.png\"/>",
            "statsmodels->User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->VAR(p) processes->Model fitting-># some example data\nIn [1]: import numpy as np\n\nIn [2]: import pandas\n\nIn [3]: import statsmodels.api as sm\n\nIn [4]: from statsmodels.tsa.api import VAR\n\nIn [5]: mdata = sm.datasets.macrodata.load_pandas().data\n\n# prepare the dates index\nIn [6]: dates = mdata[['year', 'quarter']].astype(int).astype(str)\n\nIn [7]: quarterly = dates[\"year\"] + \"Q\" + dates[\"quarter\"]\n\nIn [8]: from statsmodels.tsa.base.datetools import dates_from_str\n\nIn [9]: quarterly = dates_from_str(quarterly)\n\nIn [10]: mdata = mdata[['realgdp','realcons','realinv']]\n\nIn [11]: mdata.index = pandas.DatetimeIndex(quarterly)\n\nIn [12]: data = np.log(mdata).diff().dropna()\n\n# make a VAR model\nIn [13]: model = VAR(data)",
            "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->2. Writing to TensorBoard-># get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# create grid of images\nimg_grid = torchvision.utils.make_grid(images)\n\n# show images\nmatplotlib_imshow(img_grid, one_channel=True)\n\n# write to tensorboard\nwriter.add_image('four_fashion_mnist_images', img_grid)",
            "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method-># fit in statsmodels\nmodel = ETSModel(\n    austourists,\n    error=\"add\",\n    trend=\"add\",\n    seasonal=\"add\",\n    damped_trend=True,\n    seasonal_periods=4,\n)\nfit = model.fit()\n\n# fit with R params\nparams_R = [\n    0.35445427,\n    0.03200749,\n    0.39993387,\n    0.97999997,\n    24.01278357,\n    0.97770147,\n    1.76951063,\n    -0.50735902,\n    -6.61171798,\n    5.34956637,\n]\nfit_R = model.smooth(params_R)\n\naustourists.plot(label=\"data\")\nplt.ylabel(\"Australian Tourists\")\n\nfit.fittedvalues.plot(label=\"statsmodels fit\")\nfit_R.fittedvalues.plot(label=\"R fit\", linestyle=\"--\")\nplt.legend()"
        ]
    },
    {
        "id": "627501",
        "GT": "#Use seaborn to create graphs that use logistic regression to predict the survival % at different ages\n#seperated by class\n               \ng = sns.lmplot(x='age', y='survived', hue='pclass', data=titanic_df, y_jitter=.02, logistic=True, size=6, aspect=4)\n\ng.set(xlim=(0,80),title='Survival Rate of All Passengers\\nSeperated by Class\\nUsing Logistic Regression')",
        "pred": [
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data-># Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
            "numpy->NumPy Applications->Deep learning on MNIST->1. Load the MNIST dataset-># Display 5 random images from the training set.\nnum_examples = 5\nseed = 147197952744\nrng = np.random.default_rng(seed)\n\nfig, axes = plt.subplots(1, num_examples)\nfor sample, ax in zip(rng.choice(x_train, size=num_examples, replace=False), axes):\n    ax.imshow(sample.reshape(28, 28), cmap=\"gray\")\n\n\n\n\n<img alt=\"../_images/8484ad8185328c820925db98e28702162d6c6d279eb7cd636dfc9e2d94b68215.png\" src=\"../_images/8484ad8185328c820925db98e28702162d6c6d279eb7cd636dfc9e2d94b68215.png\"/>",
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Preliminary investigation with ad-hoc parameters in H, Q-># Here, for illustration purposes only, we plot the time-varying\n# coefficients conditional on an ad-hoc parameterization\n\n# Recall that `initial_res` contains the Kalman filtering and smoothing,\n# and the `states.smoothed` attribute contains the smoothed states\nplot_coefficients_by_equation(initial_res.states.smoothed);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_27_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_27_0.png\"/>",
            "statsmodels->User Guide->Regression and Linear Models->Regression with Discrete Dependent Variable->Examples-># Load the data from Spector and Mazzeo (1980)\nIn [1]: import statsmodels.api as sm\n\nIn [2]: spector_data = sm.datasets.spector.load_pandas()\n\nIn [3]: spector_data.exog = sm.add_constant(spector_data.exog)\n\n# Logit Model\nIn [4]: logit_mod = sm.Logit(spector_data.endog, spector_data.exog)\n\nIn [5]: logit_res = logit_mod.fit()\nOptimization terminated successfully.\n         Current function value: 0.402801\n         Iterations 7\n\nIn [6]: print(logit_res.summary())\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                  GRADE   No. Observations:                   32\nModel:                          Logit   Df Residuals:                       28\nMethod:                           MLE   Df Model:                            3\nDate:                Wed, 02 Nov 2022   Pseudo R-squ.:                  0.3740\nTime:                        17:12:32   Log-Likelihood:                -12.890\nconverged:                       True   LL-Null:                       -20.592\nCovariance Type:            nonrobust   LLR p-value:                  0.001502\n==============================================================================\n                 coef    std err          z      P|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -13.0213      4.931     -2.641      0.008     -22.687      -3.356\nGPA            2.8261      1.263      2.238      0.025       0.351       5.301\nTUCE           0.0952      0.142      0.672      0.501      -0.182       0.373\nPSI            2.3787      1.065      2.234      0.025       0.292       4.465\n==============================================================================",
            "torch->Image and Video->Adversarial Example Generation->Results->Sample Adversarial Examples-># Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()\n\n\n<img alt=\"2 -&gt; 2, 6 -&gt; 6, 4 -&gt; 4, 6 -&gt; 6, 2 -&gt; 2, 8 -&gt; 3, 4 -&gt; 2, 9 -&gt; 5, 7 -&gt; 9, 8 -&gt; 9, 6 -&gt; 0, 1 -&gt; 8, 6 -&gt; 8, 9 -&gt; 8, 8 -&gt; 6, 5 -&gt; 8, 5 -&gt; 2, 4 -&gt; 8, 4 -&gt; 8, 5 -&gt; 8, 3 -&gt; 5, 4 -&gt; 8, 8 -&gt; 2, 8 -&gt; 6, 3 -&gt; 8, 6 -&gt; 1, 1 -&gt; 3, 1 -&gt; 8, 4 -&gt; 8, 8 -&gt; 2, 7 -&gt; 8, 1 -&gt; 2, 0 -&gt; 2, 7 -&gt; 8, 7 -&gt; 8\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_002.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_002.png\"/>"
        ]
    },
    {
        "id": "597055",
        "GT": "Data Preprocessing and Feature Engineering->embarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ndf_data[\"Embarked\"] = df_data[\"Embarked\"].map(embarked_mapping)\n# split Embanked into df_train and df_test:\ndf_train[\"Embarked\"] = df_data[\"Embarked\"][:891]\ndf_test[\"Embarked\"] = df_data[\"Embarked\"][891:]",
        "pred": [
            "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()->pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "statsmodels->Examples->Linear Regression Models->Rolling Least Squares->exog_vars = [\"Mkt-RF\", \"SMB\", \"HML\"]\nexog = sm.add_constant(factors[exog_vars])\nrols = RollingOLS(endog, exog, window=60)\nrres = rols.fit()\nfig = rres.plot_recursive_coefficient(variables=exog_vars, figsize=(14, 18))\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\" src=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\"/>",
            "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs->eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\nplot_gallery(eigenfaces, eigenface_titles, h, w)\n\n()\n\n\n<img alt=\"eigenface 0, eigenface 1, eigenface 2, eigenface 3, eigenface 4, eigenface 5, eigenface 6, eigenface 7, eigenface 8, eigenface 9, eigenface 10, eigenface 11\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_face_recognition_003.png\" srcset=\"../../_images/sphx_glr_plot_face_recognition_003.png\"/>",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Trigonometric features->cyclic_cossin_transformer = (\n    transformers=[\n        (\"categorical\", one_hot_encoder, categorical_columns),\n        (\"month_sin\", sin_transformer(12), [\"month\"]),\n        (\"month_cos\", cos_transformer(12), [\"month\"]),\n        (\"weekday_sin\", sin_transformer(7), [\"weekday\"]),\n        (\"weekday_cos\", cos_transformer(7), [\"weekday\"]),\n        (\"hour_sin\", sin_transformer(24), [\"hour\"]),\n        (\"hour_cos\", cos_transformer(24), [\"hour\"]),\n    ],\n    remainder=(),\n)\ncyclic_cossin_linear_pipeline = (\n    cyclic_cossin_transformer,\n    (alphas=alphas),\n)\nevaluate(cyclic_cossin_linear_pipeline, X, y, cv=ts_cv)",
            "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Pairwise comparison of all models: Bayesian approach->pairwise_bayesian = []\n\nfor model_i, model_k in (range(len(model_scores)), 2):\n    model_i_scores = model_scores.iloc[model_i].values\n    model_k_scores = model_scores.iloc[model_k].values\n    differences = model_i_scores - model_k_scores\n    t_post = (\n        df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n    )\n    worse_prob = t_post.cdf(rope_interval[0])\n    better_prob = 1 - t_post.cdf(rope_interval[1])\n    rope_prob = t_post.cdf(rope_interval[1]) - t_post.cdf(rope_interval[0])\n\n    pairwise_bayesian.append([worse_prob, better_prob, rope_prob])\n\npairwise_bayesian_df = (\n    pairwise_bayesian, columns=[\"worse_prob\", \"better_prob\", \"rope_prob\"]\n).round(3)\n\npairwise_comp_df = pairwise_comp_df.join(pairwise_bayesian_df)\npairwise_comp_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ]
    },
    {
        "id": "821955",
        "GT": "DataFrame\n\nPandas DataFrame can be created in many ways:\n- [from lists and dictionaries](http://pbpython.com/pandas-list-dict.html)\n- from Series\n- from other DataFrames\n\nFrom dicts:->sales = [{'account': 'Jones LLC', 'Jan': 150, 'Feb': 200, 'Mar': 140},\n         {'account': 'Alpha Co',  'Jan': 200, 'Feb': 210, 'Mar': 215},\n         {'account': 'Blue Inc',  'Jan': 50,  'Feb': 90,  'Mar': 95 }]\npd.DataFrame(sales)",
        "pred": [
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames",
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Building the dataset->pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)",
            "statsmodels->Examples->State space models->ETS models->Simple exponential smoothing->oildata = [\n    111.0091,\n    130.8284,\n    141.2871,\n    154.2278,\n    162.7409,\n    192.1665,\n    240.7997,\n    304.2174,\n    384.0046,\n    429.6622,\n    359.3169,\n    437.2519,\n    468.4008,\n    424.4353,\n    487.9794,\n    509.8284,\n    506.3473,\n    340.1842,\n    240.2589,\n    219.0328,\n    172.0747,\n    252.5901,\n    221.0711,\n    276.5188,\n    271.1480,\n    342.6186,\n    428.3558,\n    442.3946,\n    432.7851,\n    437.2497,\n    437.2092,\n    445.3641,\n    453.1950,\n    454.4096,\n    422.3789,\n    456.0371,\n    440.3866,\n    425.1944,\n    486.2052,\n    500.4291,\n    521.2759,\n    508.9476,\n    488.8889,\n    509.8706,\n    456.7229,\n    473.8166,\n    525.9509,\n    549.8338,\n    542.3405,\n]\noil = pd.Series(oildata, index=pd.date_range(\"1965\", \"2013\", freq=\"AS\"))\noil.plot()\nplt.ylabel(\"Annual oil production in Saudi Arabia (Mt)\")",
            "statsmodels->Examples->Statistics->ANOVA->lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->delta = (by_game.home_rest - by_game.away_rest).dropna().astype(int)\nax = (delta.value_counts()\n    .reindex(np.arange(delta.min(), delta.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind='bar', color='k', width=.9, rot=0, figsize=(12, 6))\n)\nsns.despine()\nax.set(xlabel='Difference in Rest (Home - Away)', ylabel='Games');"
        ]
    },
    {
        "id": "1242776",
        "GT": "Test Model->def get_street_images_url():\n    GENDER_I = 6\n    PRODUCT_ID = 8\n    all_street_urls = []\n    # unique products \n    unique_street_urls = [] #expect: 4181\n    products_added = []\n\n    \n    for key in test_embeddings: \n        if key[GENDER_I] == \"others\": continue \n        all_street_images_urls.append(key)\n        product = key[PRODUCT_ID]\n        if product not in products_added:\n            products_added.append(product)\n            unique_street_urls.append(product)\n            \n    return all_street_urls, unique_street_urls",
        "pred": [
            "pandas_toms_blog->Indexes->def get_ids(network):\n    url = \"http://mesonet.agron.iastate.edu/geojson/network.php?network={}\"\n    r = requests.get(url.format(network))\n    md = pd.io.json.json_normalize(r.json()['features'])\n    md['network'] = network\n    return md",
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Calculating the Air Quality Index->Sub-indices->def compute_indices(pol, con):\n    bp = breakpoints[pol]\n    \n    if pol == 'CO':\n        inc = 0.1\n    else:\n        inc = 1\n    \n    if bp[0] &lt;= con &lt; bp[1]:\n        Bl = bp[0]\n        Bh = bp[1] - inc\n        Ih = AQI[1] - inc\n        Il = AQI[0]\n\n    elif bp[1] &lt;= con &lt; bp[2]:\n        Bl = bp[1]\n        Bh = bp[2] - inc\n        Ih = AQI[2] - inc\n        Il = AQI[1]\n\n    elif bp[2] &lt;= con &lt; bp[3]:\n        Bl = bp[2]\n        Bh = bp[3] - inc\n        Ih = AQI[3] - inc\n        Il = AQI[2]\n\n    elif bp[3] &lt;= con &lt; bp[4]:\n        Bl = bp[3]\n        Bh = bp[4] - inc\n        Ih = AQI[4] - inc\n        Il = AQI[3]\n\n    elif bp[4] &lt;= con &lt; bp[5]:\n        Bl = bp[4]\n        Bh = bp[5] - inc\n        Ih = AQI[5] - inc\n        Il = AQI[4]\n\n    elif bp[5] &lt;= con:\n        Bl = bp[5]\n        Bh = bp[5] + bp[4] - (2 * inc)\n        Ih = AQI[6]\n        Il = AQI[5]\n\n    else:\n        print(\"Concentration out of range!\")\n        \n    return ((Ih - Il) / (Bh - Bl)) * (con - Bl) + Il",
            "statsmodels->Examples->Statistics->Mediation analysis with duration data->def gen_outcome(otype, mtime0):\n    if otype == \"full\":\n        lp = 0.5 * mtime0\n    elif otype == \"no\":\n        lp = exp\n    else:\n        lp = exp + mtime0\n    mn = np.exp(-lp)\n    ytime0 = -mn * np.log(np.random.uniform(size=n))\n    ctime = -2 * mn * np.log(np.random.uniform(size=n))\n    ystatus = (ctime = ytime0).astype(int)\n    ytime = np.where(ytime0 &lt;= ctime, ytime0, ctime)\n    return ytime, ystatus",
            "statsmodels->Examples->State space models->Statespace: Custom Models->Model 1: time-varying coefficients->def gen_data_for_model1():\n    nobs = 1000\n\n    rs = np.random.RandomState(seed=93572)\n\n    d = 5\n    var_y = 5\n    var_coeff_x = 0.01\n    var_coeff_w = 0.5\n\n    x_t = rs.uniform(size=nobs)\n    w_t = rs.uniform(size=nobs)\n    eps = rs.normal(scale=var_y ** 0.5, size=nobs)\n\n    beta_x = np.cumsum(rs.normal(size=nobs, scale=var_coeff_x ** 0.5))\n    beta_w = np.cumsum(rs.normal(size=nobs, scale=var_coeff_w ** 0.5))\n\n    y_t = d + beta_x * x_t + beta_w * w_t + eps\n    return y_t, x_t, w_t, beta_x, beta_w\n\n\ny_t, x_t, w_t, beta_x, beta_w = gen_data_for_model1()\n_ = plt.plot(y_t)",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Overview of the Model Architecture->def initialise_params(hidden_dim, input_dim):\n    # forget gate\n    Wf = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bf = rng.standard_normal(size=(hidden_dim, 1))\n    # input gate\n    Wi = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bi = rng.standard_normal(size=(hidden_dim, 1))\n    # candidate memory gate\n    Wcm = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bcm = rng.standard_normal(size=(hidden_dim, 1))\n    # output gate\n    Wo = rng.standard_normal(size=(hidden_dim, hidden_dim + input_dim))\n    bo = rng.standard_normal(size=(hidden_dim, 1))\n\n    # fully connected layer for classification\n    W2 = rng.standard_normal(size=(1, hidden_dim))\n    b2 = np.zeros((1, 1))\n\n    parameters = {\n        \"Wf\": Wf,\n        \"bf\": bf,\n        \"Wi\": Wi,\n        \"bi\": bi,\n        \"Wcm\": Wcm,\n        \"bcm\": bcm,\n        \"Wo\": Wo,\n        \"bo\": bo,\n        \"W2\": W2,\n        \"b2\": b2\n    }\n    return parameters"
        ]
    },
    {
        "id": "1349938",
        "GT": "Selecting the best features\n\nWe select the four best features using the k-best method. After analysing the importance scores, we decide a new features_list. The selected features list will be best suited for our predictions.->data = featureFormat(my_dataset, features_list, sort_keys = True)\nlabels, features = targetFeatureSplit(data)\n\nk=4\nk_best = SelectKBest(k=k)\nk_best.fit(features, labels)\nscores = k_best.scores_\nprint(scores)\n\nfeatures_list = ['poi','total_stock_value','fraction_to_poi_email','expenses','shared_receipt_with_poi']",
        "pred": [
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings->data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->3. Define dataset and data loaders->ImageNet Data->data_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\n\n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n# while also improving numerical accuracy. While this can be used with any model, this is\n# especially common with quantized models.\n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)",
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->data = [\n    [\"Carroll\", 94, 22, 60, 92, 20, 60],\n    [\"Grant\", 98, 21, 65, 92, 22, 65],\n    [\"Peck\", 98, 28, 40, 88, 26, 40],\n    [\"Donat\", 94, 19, 200, 82, 17, 200],\n    [\"Stewart\", 98, 21, 50, 88, 22, 45],\n    [\"Young\", 96, 21, 85, 92, 22, 85],\n]\ncolnames = [\"study\", \"mean_t\", \"sd_t\", \"n_t\", \"mean_c\", \"sd_c\", \"n_c\"]\nrownames = [i[0] for i in data]\ndframe1 = pd.DataFrame(data, columns=colnames)\nrownames",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->data = pdr.get_data_fred(\"HOUSTNSA\", \"1959-01-01\", \"2019-06-01\")\nhousing = data.HOUSTNSA.pct_change().dropna()\n# Scale by 100 to get percentages\nhousing = 100 * housing.asfreq(\"MS\")\nfig, ax = plt.subplots()\nax = housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\"/>",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = (\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (train set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_003.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_003.png\"/>"
        ]
    },
    {
        "id": "774917",
        "GT": "Part 2: Logistic Regression->Model\nWe want to build a model that would classify airline review as positive or negative based only on its content.\n\nTo do that, we need to extract feature data and class labels from dataset.->from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(binary=True)\nX_train_transformed = vectorizer.fit_transform(X_train)",
        "pred": [
            "sklearn->Tutorials->Working With Text Data->Extracting features from text files->From occurrences to frequencies->from sklearn.feature_extraction.text import TfidfTransformer\n tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n X_train_tf = tf_transformer.transform(X_train_counts)\n X_train_tf.shape\n(2257, 35788)",
            "sklearn->Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Evaluation->from sklearn import metrics\n\nY_pred = rbm_features_classifier.predict(X_test)\nprint(\n    \"Logistic regression using RBM features:\\n%s\\n\"\n    % ((Y_test, Y_pred))\n)",
            "sklearn->Tutorials->Working With Text Data->Extracting features from text files->Tokenizing text with scikit-learn->from sklearn.feature_extraction.text import CountVectorizer\n count_vect = CountVectorizer()\n X_train_counts = count_vect.fit_transform(twenty_train.data)\n X_train_counts.shape\n(2257, 35788)",
            "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data->from statsmodels.tsa.forecasting.theta import ThetaModel\n\ntm = ThetaModel(housing)\nres = tm.fit()\nprint(res.summary())",
            "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation->from sklearn import datasets\nimport numpy as np\n\ndigits = ()\nrng = (2)\nindices = (len(digits.data))\nrng.shuffle(indices)"
        ]
    },
    {
        "id": "1382150",
        "GT": "Part III  Indeed Data->IEOR 19 TOP JOB TITLES & SKILLS->skill_list = ['excel','vba','python','r','matlab','c#','c++','sas','stata','sql','mysql','php','html','java','javascript',\n    'ios','perl','hadoop','nosql','hive','mapreduce','pip','mongodb','hbase','tableau','spark','scala','d3']",
        "pred": [
            "sklearn->Tutorials->Working With Text Data->Loading the 20 newsgroups dataset->categories = ['alt.atheism', 'soc.religion.christian',\n...               'comp.graphics', 'sci.med']",
            "statsmodels->Examples->Linear Regression Models->Linear Mixed-Effects->array(['lme4', 'Matrix', 'tools', 'stats', 'graphics', 'grDevices',\n       'utils', 'datasets', 'methods', 'base'], dtype='&lt;U9')",
            "sklearn->Examples->Feature Selection->Model-based and sequential feature selection->Loading the data->from sklearn.datasets import \n\ndiabetes = ()\nX, y = diabetes.data, diabetes.target\nprint(diabetes.DESCR)",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "scipy->Spatial data structures and algorithms (scipy.spatial)->Voronoi diagrams->from scipy.spatial import Voronoi\n vor = Voronoi(points)\n vor.vertices\narray([[0.5, 0.5],\n       [0.5, 1.5],\n       [1.5, 0.5],\n       [1.5, 1.5]])"
        ]
    },
    {
        "id": "1141092",
        "GT": "After incorporating the historical data with the weather data,  its possible to produce more accurate prediction->data_combine.columns",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.dtypes",
            "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Products with n-dimensional arrays->reconstructed.shape",
            "numpy->NumPy Features->Saving and sharing your NumPy arrays->Our arrays as a csv file->load_xy.shape",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.head(5)",
            "statsmodels->Examples->Linear Regression Models->Rolling Least Squares->params.iloc[57:62]"
        ]
    },
    {
        "id": "1225844",
        "GT": "Schema for the tables in MySQL\n\nDROP DATABASE IF EXISTS Python_Project;\nCREATE DATABASE IF NOT EXISTS Python_Project;\nUSE Python_Project;\nCREATE TABLE IF NOT EXISTS CITIBIKE_TRIPDATA(\n    tripduration MEDIUMINT,\n    starttime datetime,\n    stoptime datetime,\n    start_station_id MEDIUMINT,\n    start_station_name VARCHAR(30),\n    start_station_latitude FLOAT,\n    start_station_longitude FLOAT,\n    end_station_id MEDIUMINT,\n    end_station_name VARCHAR(30),\n    end_station_latitude FLOAT,\n    end_station_longitude FLOAT,\n    bikeid MEDIUMINT,\n    usertype VARCHAR(20),\n    birth_year VARCHAR(6),\n    gender TINYINT\n  );\n \n\nLOAD DATA LOCAL INFILE 'C:/Users/Vineel/Documents/UB/Python Project/Data/newJC-201703-citibike-tripdata.csv' INTO TABLE  CITIBIKE_TRIPDATA FIELDS TERMINATED BY ','  ESCAPED BY '\"' LINES TERMINATED BY '\\n' IGNORE 1 LINES;->Around 70 % of the user base are Male->d_male=df1.loc[df1['gender'] == 1]\nd_female=df1.loc[df1['gender'] == 2]\ndf_pie1 = pd.DataFrame([d_male.shape[0], d_female.shape[0]], index=['MALE', 'FEMALE'], columns=['Frequency'])\ndf_pie1.plot(kind='pie', subplots=True, figsize=(10, 10))",
        "pred": [
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->delta = (by_game.home_rest - by_game.away_rest).dropna().astype(int)\nax = (delta.value_counts()\n    .reindex(np.arange(delta.min(), delta.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind='bar', color='k', width=.9, rot=0, figsize=(12, 6))\n)\nsns.despine()\nax.set(xlabel='Difference in Rest (Home - Away)', ylabel='Games');",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->modfd_logit = OrderedModel.from_formula(\"apply ~ 1 + pared + public + gpa + C(dummy)\", data_student,\n                                      distr='logit')\nresfd_logit = modfd_logit.fit(method='bfgs')\nprint(resfd_logit.summary())",
            "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()->pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Building the dataset->pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)",
            "statsmodels->Examples->State space models - Technical notes->State space models: Chandrasekhar recursions->Practical example->cpi_apparel = DataReader('CPIAPPNS', 'fred', start='1986')\ncpi_apparel.index = pd.DatetimeIndex(cpi_apparel.index, freq='MS')\ninf_apparel = np.log(cpi_apparel).diff().iloc[1:] * 1200\ninf_apparel.plot(figsize=(15, 5));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_chandrasekhar_10_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_chandrasekhar_10_0.png\"/>"
        ]
    },
    {
        "id": "1058006",
        "GT": "4 Feature scaling->4.1 Rescaling->def rescale_columns(data, colnames, total_samples_preview):\n    \n    scaler = pp.MinMaxScaler()\n    \n    rescaled_data = data[colnames]\n    rescaled_data = scaler.fit_transform(rescaled_data)\n    rescaled_data = pd.DataFrame(rescaled_data, columns=colnames)\n    \n    rescaled_data_sample = rescaled_data.sample(n=5)\n    \n    scatter_plot_two_features(rescaled_data, colnames, total_samples_preview)\n    \n    return rescaled_data_sample\n\n\nrescale_columns(data, cols, 500)",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Robust Fitting->def add_stl_plot(fig, res, legend):\n    \"\"\"Add 3 plots from a second STL fit\"\"\"\n    axs = fig.get_axes()\n    comps = [\"trend\", \"seasonal\", \"resid\"]\n    for ax, comp in zip(axs[1:], comps):\n        series = getattr(res, comp)\n        if comp == \"resid\":\n            ax.plot(series, marker=\"o\", linestyle=\"none\")\n        else:\n            ax.plot(series)\n            if comp == \"trend\":\n                ax.legend(legend, frameon=False)\n\n\nstl = STL(elec_equip, period=12, robust=True)\nres_robust = stl.fit()\nfig = res_robust.plot()\nres_non_robust = STL(elec_equip, period=12, robust=False).fit()\nadd_stl_plot(fig, res_non_robust, [\"Robust\", \"Non-robust\"])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\"/>",
            "sklearn->Examples->Examples based on real world datasets->Out-of-core classification of text documents->Plot results->def plot_accuracy(x, y, x_legend):\n    \"\"\"Plot accuracy as a function of x.\"\"\"\n    x = (x)\n    y = (y)\n    (\"Classification accuracy as a function of %s\" % x_legend)\n    (\"%s\" % x_legend)\n    (\"Accuracy\")\n    (True)\n    (x, y)\n\n\n[\"legend.fontsize\"] = 10\ncls_names = list(sorted(cls_stats.keys()))\n\n# Plot accuracy evolution\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with #examples\n    accuracy, n_examples = zip(*stats[\"accuracy_history\"])\n    plot_accuracy(n_examples, accuracy, \"training examples (#)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with runtime\n    accuracy, runtime = zip(*stats[\"runtime_history\"])\n    plot_accuracy(runtime, accuracy, \"runtime (s)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n# Plot fitting times\n()\nfig = ()\ncls_runtime = [stats[\"total_fit_time\"] for cls_name, stats in sorted(cls_stats.items())]\n\ncls_runtime.append(total_vect_time)\ncls_names.append(\"Vectorization\")\nbar_colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"]\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=10)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Training Times\")\n\n\ndef autolabel(rectangles):\n    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n    for rect in rectangles:\n        height = rect.get_height()\n        ax.text(\n            rect.get_x() + rect.get_width() / 2.0,\n            1.05 * height,\n            \"%.4f\" % height,\n            ha=\"center\",\n            va=\"bottom\",\n        )\n        (()[1], rotation=30)\n\n\nautolabel(rectangles)\n()\n()\n\n# Plot prediction times\n()\ncls_runtime = []\ncls_names = list(sorted(cls_stats.keys()))\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats[\"prediction_time\"])\ncls_runtime.append(parsing_time)\ncls_names.append(\"Read/Parse\\n+Feat.Extr.\")\ncls_runtime.append(vectorizing_time)\ncls_names.append(\"Hashing\\n+Vect.\")\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=8)\n(()[1], rotation=30)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Prediction Times (%d instances)\" % n_test_documents)\nautolabel(rectangles)\n()\n()\n\n\n\n<img alt=\"Classification accuracy as a function of training examples (#)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\"/>\n<img alt=\"Classification accuracy as a function of runtime (s)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\"/>\n<img alt=\"Training Times\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\"/>\n<img alt=\"Prediction Times (1000 instances)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\"/>",
            "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Replace missing values by 0->def get_impute_zero_score(X_missing, y_missing):\n\n    imputer = (\n        missing_values=, add_indicator=True, strategy=\"constant\", fill_value=0\n    )\n    zero_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return zero_impute_scores.mean(), zero_impute_scores.std()\n\n\nmses_california[1], stds_california[1] = get_impute_zero_score(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[1], stds_diabetes[1] = get_impute_zero_score(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Zero imputation\")",
            "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Iterative imputation of the missing values->def get_impute_iterative(X_missing, y_missing):\n    imputer = (\n        missing_values=,\n        add_indicator=True,\n        random_state=0,\n        n_nearest_features=3,\n        max_iter=1,\n        sample_posterior=True,\n    )\n    iterative_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return iterative_impute_scores.mean(), iterative_impute_scores.std()\n\n\nmses_california[4], stds_california[4] = get_impute_iterative(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[4], stds_diabetes[4] = get_impute_iterative(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Iterative Imputation\")\n\nmses_diabetes = mses_diabetes * -1\nmses_california = mses_california * -1",
            "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Impute missing values with mean->def get_impute_mean(X_missing, y_missing):\n    imputer = (missing_values=, strategy=\"mean\", add_indicator=True)\n    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return mean_impute_scores.mean(), mean_impute_scores.std()\n\n\nmses_california[3], stds_california[3] = get_impute_mean(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes, y_miss_diabetes)\nx_labels.append(\"Mean Imputation\")"
        ]
    },
    {
        "id": "192534",
        "GT": "PoisonousMushrooms->Data Visualization->mpl.rcParams['figure.figsize'] = (8, 6)\nsns.countplot(x=\"edibility\", data=md);",
        "pred": [
            "seaborn->User guide and tutorial->Estimating regression fits->Conditioning on other variables->sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", col=\"time\", data=tips);\n",
            "seaborn->Statistical operations->Estimating regression fits->Conditioning on other variables->sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", col=\"time\", data=tips);\n",
            "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data->dta.plot(figsize=(12,4));\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_arma_0_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_arma_0_9_0.png\"/>",
            "seaborn->User guide and tutorial->Estimating regression fits->Functions for drawing linear regression models->sns.lmplot(x=\"size\", y=\"tip\", data=tips, x_estimator=np.mean);\n",
            "seaborn->Statistical operations->Estimating regression fits->Functions for drawing linear regression models->sns.lmplot(x=\"size\", y=\"tip\", data=tips, x_estimator=np.mean);\n",
            "seaborn->User guide and tutorial->Estimating regression fits->Functions for drawing linear regression models->sns.lmplot(x=\"size\", y=\"tip\", data=tips, x_jitter=.05);\n",
            "seaborn->Statistical operations->Estimating regression fits->Functions for drawing linear regression models->sns.lmplot(x=\"size\", y=\"tip\", data=tips, x_jitter=.05);\n",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Seaborn figure styles->sns.set_style(\"whitegrid\")\ndata = np.random.normal(size=(20, 6)) + np.arange(6) / 2\nsns.boxplot(data=data);\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Seaborn figure styles->sns.set_style(\"whitegrid\")\ndata = np.random.normal(size=(20, 6)) + np.arange(6) / 2\nsns.boxplot(data=data);\n"
        ]
    },
    {
        "id": "431172",
        "GT": "Creating Orders->response = oanda.create_order(account_id,\n                              instrument = \"AUD_USD\",\n                              units=1000,\n                              side=\"buy\",\n                              type=\"limit\",\n                              price=0.7420,\n                              expiry=trade_expire)\nprint(response)",
        "pred": [
            "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Pure Premium Modeling via a Product Model vs single TweedieRegressor->res = []\nfor subset_label, X, df in [\n    (\"train\", X_train, df_train),\n    (\"test\", X_test, df_test),\n]:\n    exposure = df[\"Exposure\"].values\n    res.append(\n        {\n            \"subset\": subset_label,\n            \"observed\": df[\"ClaimAmount\"].values.sum(),\n            \"predicted, frequency*severity model\": (\n                exposure * glm_freq.predict(X) * glm_sev.predict(X)\n            ),\n            \"predicted, tweedie, power=%.2f\"\n            % glm_pure_premium.power: (exposure * glm_pure_premium.predict(X)),\n        }\n    )\n\nprint((res).set_index(\"subset\").T)",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->resp = dict(\n    zip(\n        range(1, 9),\n        respondent1000[\n            [\n                \"occupation\",\n                \"educ\",\n                \"occupation_husb\",\n                \"rate_marriage\",\n                \"age\",\n                \"yrs_married\",\n                \"children\",\n                \"religious\",\n            ]\n        ].tolist(),\n    )\n)\nresp.update({0: 1})\nprint(resp)",
            "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->How does the posterior distribution compare with the MLE estimation?->results_dict = {\n    \"intercept\": res_mle.params[0],\n    \"var.e\": res_mle.params[1],\n    \"var.x.coeff\": res_mle.params[2],\n    \"var.w.coeff\": res_mle.params[3],\n}\nplt.tight_layout()\n_ = pm.plot_trace(\n    trace,\n    lines=[(k, {}, [v]) for k, v in dict(results_dict).items()],\n    combined=True,\n    figsize=(12, 12),\n)",
            "statsmodels->Examples->Statistics->ANOVA->resid = lm.resid\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    group_num = i * 2 + j - 1  # for plotting purposes\n    x = [group_num] * len(group)\n    plt.scatter(\n        x,\n        resid[group.index],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"Group\")\nplt.ylabel(\"Residuals\")",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\n\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = (\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\nax = importances.plot.box(vert=False, whis=10)\nax.set_title(\"Permutation Importances (train set)\")\nax.axvline(x=0, color=\"k\", linestyle=\"--\")\nax.set_xlabel(\"Decrease in accuracy score\")\nax.figure.tight_layout()\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_003.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_003.png\"/>"
        ]
    },
    {
        "id": "1362139",
        "GT": "from datetime import datetime\n\ndates_by_model = []\n\nfor i in range(len(dates_model)):\n    dates_points = datetime.strptime(solve(dates_model[i]), '%B %d %Y')\n    dates_by_model.append(dates_points)\n\ndates_by_points = []\n\nfor i in range(len(maturities)):\n    dates_points = datetime.strptime(solve(maturities[i]), '%B %d %Y')\n    dates_by_points.append(dates_points)\n    \ndates_by_model_graph = matplotlib.dates.date2num(dates_by_model)\ndates_by_points_graph = matplotlib.dates.date2num(dates_by_points)    ",
        "pred": [
            "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Pairwise comparison of all models: frequentist approach->from itertools import \nfrom math import \n\nn_comparisons = (len(model_scores)) / (\n    (2) * (len(model_scores) - 2)\n)\npairwise_t_test = []\n\nfor model_i, model_k in (range(len(model_scores)), 2):\n    model_i_scores = model_scores.iloc[model_i].values\n    model_k_scores = model_scores.iloc[model_k].values\n    differences = model_i_scores - model_k_scores\n    t_stat, p_val = compute_corrected_ttest(differences, df, n_train, n_test)\n    p_val *= n_comparisons  # implement Bonferroni correction\n    # Bonferroni can output p-values higher than 1\n    p_val = 1 if p_val  1 else p_val\n    pairwise_t_test.append(\n        [model_scores.index[model_i], model_scores.index[model_k], t_stat, p_val]\n    )\n\npairwise_comp_df = (\n    pairwise_t_test, columns=[\"model_1\", \"model_2\", \"t_stat\", \"p_val\"]\n).round(3)\npairwise_comp_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "torch->Learning PyTorch->What is torch.nn really?->MNIST data setup->from pathlib import Path\nimport requests\n\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)",
            "torch->Learning PyTorch->What is torch.nn really?->Neural net from scratch (no torch.nn)->from IPython.core.debugger import set_trace\n\nlr = 0.5  # learning rate\nepochs = 2  # how many epochs to train for\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        #         set_trace()\n        start_i = i * bs\n        end_i = start_i + bs\n         = [start_i:end_i]\n         = [start_i:end_i]\n         = ()\n         = loss_func(, )\n\n        ()\n        with ():\n             -=  * lr\n             -=  * lr\n            .zero_()\n            .zero_()",
            "statsmodels->Examples->Statistics->ANOVA->from urllib.request import urlopen\nimport numpy as np\n\nnp.set_printoptions(precision=4, suppress=True)\n\nimport pandas as pd\n\npd.set_option(\"display.width\", 100)\nimport matplotlib.pyplot as plt\nfrom statsmodels.formula.api import ols\nfrom statsmodels.graphics.api import interaction_plot, abline_plot\nfrom statsmodels.stats.anova import anova_lm\n\ntry:\n    salary_table = pd.read_csv(\"salary.table\")\nexcept:  # recent pandas can read URL without urlopen\n    url = \"http://stats191.stanford.edu/data/salary.table\"\n    fh = urlopen(url)\n    salary_table = pd.read_table(fh)\n    salary_table.to_csv(\"salary.table\")\n\nE = salary_table.E\nM = salary_table.M\nX = salary_table.X\nS = salary_table.S",
            "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Atmospheric CO2->from statsmodels.tsa.seasonal import STL\n\nstl = STL(co2, seasonal=13)\nres = stl.fit()\nfig = res.plot()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_6_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_6_0.png\"/>"
        ]
    },
    {
        "id": "1006350",
        "GT": "Model 1->Precision, Recall and F1 Scores for all classes->report_model1 = classification_report(y_true_test, y_pred_test)\nprint (report_model1)",
        "pred": [
            "sklearn->Examples->Generalized Linear Models->Non-negative least squares->reg_ols = ()\ny_pred_ols = reg_ols.fit(X_train, y_train).predict(X_test)\nr2_score_ols = (y_test, y_pred_ols)\nprint(\"OLS R2 score\", r2_score_ols)",
            "statsmodels->Examples->Statistics->ANOVA->Minority Employment Data->min_lm = ols(\"JPERF ~ TEST\", data=jobtest_table).fit()\nprint(min_lm.summary())",
            "statsmodels->Examples->State space models->Statespace: Custom Models->Model 1: time-varying coefficients->And then estimate it with our custom model class->mod = TVRegression(y_t, x_t, w_t)\nres = mod.fit()\n\nprint(res.summary())",
            "statsmodels->Examples->Statistics->ANOVA->Minority Employment Data->min_lm2 = ols(\"JPERF ~ TEST + TEST:MINORITY\", data=jobtest_table).fit()\n\nprint(min_lm2.summary())",
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model->sm_probit_manual = MyProbit(endog, exog).fit()\nprint(sm_probit_manual.summary())"
        ]
    },
    {
        "id": "285581",
        "GT": "Task: Exploration and Regression->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew, kurtosis\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor,RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\nfrom collections import OrderedDict",
        "pred": [
            "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings->import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence->import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom collections import \n\npopulations = (list)\ncommon_params = {\n    \"n_samples\": 10_000,\n    \"n_features\": 2,\n    \"n_informative\": 2,\n    \"n_redundant\": 0,\n    \"random_state\": 0,\n}\nweights = (0.1, 0.8, 6)\nweights = weights[::-1]\n\n# fit and evaluate base model on balanced classes\nX, y = (**common_params, weights=[0.5, 0.5])\nestimator = ().fit(X, y)\nlr_base = extract_score((estimator, X, y, scoring=scoring, cv=10))\npos_lr_base, pos_lr_base_std = lr_base[\"positive\"].values\nneg_lr_base, neg_lr_base_std = lr_base[\"negative\"].values",
            "sklearn->Examples->Pipelines and composite estimators->Selecting dimensionality reduction with Pipeline and GridSearchCV->Illustration of Pipeline and GridSearchCV->import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.model_selection import \nfrom sklearn.pipeline import \nfrom sklearn.svm import \nfrom sklearn.decomposition import , \nfrom sklearn.feature_selection import , \nfrom sklearn.preprocessing import \n\nX, y = (return_X_y=True)\n\npipe = (\n    [\n        (\"scaling\", ()),\n        # the reduce_dim stage is populated by the param_grid\n        (\"reduce_dim\", \"passthrough\"),\n        (\"classify\", (dual=False, max_iter=10000)),\n    ]\n)\n\nN_FEATURES_OPTIONS = [2, 4, 8]\nC_OPTIONS = [1, 10, 100, 1000]\nparam_grid = [\n    {\n        \"reduce_dim\": [(iterated_power=7), (max_iter=1_000)],\n        \"reduce_dim__n_components\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n    {\n        \"reduce_dim\": [()],\n        \"reduce_dim__k\": N_FEATURES_OPTIONS,\n        \"classify__C\": C_OPTIONS,\n    },\n]\nreducer_labels = [\"PCA\", \"NMF\", \"KBest(mutual_info_classif)\"]\n\ngrid = (pipe, n_jobs=1, param_grid=param_grid)\ngrid.fit(X, y)",
            "sklearn->Examples->Missing Value Imputation->Imputing missing values with variants of IterativeImputer->import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# To use this experimental feature, we need to explicitly ask for it:\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.datasets import \nfrom sklearn.impute import \nfrom sklearn.impute import \nfrom sklearn.linear_model import , \nfrom sklearn.kernel_approximation import \nfrom sklearn.ensemble import \nfrom sklearn.neighbors import \nfrom sklearn.pipeline import \nfrom sklearn.model_selection import \n\nN_SPLITS = 5\n\nrng = (0)\n\nX_full, y_full = (return_X_y=True)\n# ~2k samples is enough for the purpose of the example.\n# Remove the following two lines for a slower run with different error bars.\nX_full = X_full[::10]\ny_full = y_full[::10]\nn_samples, n_features = X_full.shape\n\n# Estimate the score on the entire dataset, with no missing values\nbr_estimator = ()\nscore_full_data = (\n    (\n        br_estimator, X_full, y_full, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    ),\n    columns=[\"Full Data\"],\n)\n\n# Add a single missing value to each row\nX_missing = X_full.copy()\ny_missing = y_full\nmissing_samples = (n_samples)\nmissing_features = rng.choice(n_features, n_samples, replace=True)\nX_missing[missing_samples, missing_features] = \n\n# Estimate the score after imputation (mean and median strategies)\nscore_simple_imputer = ()\nfor strategy in (\"mean\", \"median\"):\n    estimator = (\n        (missing_values=, strategy=strategy), br_estimator\n    )\n    score_simple_imputer[strategy] = (\n        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    )\n\n# Estimate the score after iterative imputation of the missing values\n# with different estimators\nestimators = [\n    (),\n    (\n        # We tuned the hyperparameters of the RandomForestRegressor to get a good\n        # enough predictive performance for a restricted execution time.\n        n_estimators=4,\n        max_depth=10,\n        bootstrap=True,\n        max_samples=0.5,\n        n_jobs=2,\n        random_state=0,\n    ),\n    (\n        (kernel=\"polynomial\", degree=2, random_state=0), (alpha=1e3)\n    ),\n    (n_neighbors=15),\n]\nscore_iterative_imputer = ()\n# iterative imputer is sensible to the tolerance and\n# dependent on the estimator used internally.\n# we tuned the tolerance to keep this example run with limited computational\n# resources while not changing the results too much compared to keeping the\n# stricter default value for the tolerance parameter.\ntolerances = (1e-3, 1e-1, 1e-1, 1e-2)\nfor impute_estimator, tol in zip(estimators, tolerances):\n    estimator = (\n        (\n            random_state=0, estimator=impute_estimator, max_iter=25, tol=tol\n        ),\n        br_estimator,\n    )\n    score_iterative_imputer[impute_estimator.__class__.__name__] = (\n        estimator, X_missing, y_missing, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n    )\n\nscores = (\n    [score_full_data, score_simple_imputer, score_iterative_imputer],\n    keys=[\"Original\", \"SimpleImputer\", \"IterativeImputer\"],\n    axis=1,\n)\n\n# plot california housing results\nfig, ax = (figsize=(13, 6))\nmeans = -scores.mean()\nerrors = scores.std()\nmeans.plot.barh(xerr=errors, ax=ax)\nax.set_title(\"California Housing Regression with Different Imputation Methods\")\nax.set_xlabel(\"MSE (smaller is better)\")\nax.set_yticks((means.shape[0]))\nax.set_yticklabels([\" w/ \".join(label) for label in means.index.tolist()])\n(pad=1)\n()",
            "sklearn->Examples->Nearest Neighbors->Nearest Centroid Classification->import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import \nfrom sklearn import datasets\nfrom sklearn.neighbors import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nn_neighbors = 15\n\n# import some data to play with\niris = ()\n# we only take the first two features. We could avoid this ugly\n# slicing by using a two-dim dataset\nX = iris.data[:, :2]\ny = iris.target\n\n# Create color maps\ncmap_light = ([\"orange\", \"cyan\", \"cornflowerblue\"])\ncmap_bold = ([\"darkorange\", \"c\", \"darkblue\"])\n\nfor shrinkage in [None, 0.2]:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = (shrink_threshold=shrinkage)\n    clf.fit(X, y)\n    y_pred = clf.predict(X)\n    print(shrinkage, (y == y_pred))\n\n    _, ax = ()\n    (\n        clf, X, cmap=cmap_light, ax=ax, response_method=\"predict\"\n    )\n\n    # Plot also the training points\n    (X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor=\"k\", s=20)\n    (\"3-Class classification (shrink_threshold=%r)\" % shrinkage)\n    (\"tight\")\n\n()"
        ]
    },
    {
        "id": "1415485",
        "GT": "Task 3. Unemployment rates in Switzerland for Swiss and foreign workers according to amstat->max_foreing = complete_fo_sw['foreign_unemployment_rate'].max()\nmin_foreign = complete_fo_sw['foreign_unemployment_rate'].min()\nmax_swiss = complete_fo_sw['swiss_unemployment_rate'].max()\nmin_swiss = complete_fo_sw['swiss_unemployment_rate'].min()",
        "pred": [
            "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack->accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "pandas_toms_blog->Time Series->Timeseries->Forecasting->pred = res_seasonal.get_prediction(start='2001-03-01')\npred_ci = pred.conf_int()",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson->poisson_mod = sm.Poisson(rand_data.endog, rand_exog)\npoisson_res = poisson_mod.fit(method=\"newton\")\nprint(poisson_res.summary())",
            "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach->mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit->mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)"
        ]
    },
    {
        "id": "775310",
        "GT": "import math\nimport numpy as np\nfrom numpy import mean\nimport pandas\nimport csv\nimport seaborn\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import linear_model\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import classification_report\n",
        "pred": [
            "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Fine-tuning HF T5->import os\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom transformers import AutoTokenizer, GPT2TokenizerFast\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport functools\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom transformers.models.t5.modeling_t5 import T5Block\n\nfrom torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n checkpoint_wrapper,\n CheckpointImpl,\n apply_activation_checkpointing_wrapper)\n\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    MixedPrecision,\n    BackwardPrefetch,\n    ShardingStrategy,\n    FullStateDictConfig,\n    StateDictType,\n)\nfrom torch.distributed.fsdp.wrap import (\n    transformer_auto_wrap_policy,\n    enable_wrap,\n    wrap,\n)\nfrom functools import partial\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\nfrom summarization_dataset import *\nfrom transformers.models.t5.modeling_t5 import T5Block\nfrom typing import Type\nimport time\nimport tqdm\nfrom datetime import datetime",
            "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism->Define the model->import sys\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport tempfile\nfrom torch.nn import TransformerEncoder, \n\nclass PositionalEncoding():\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = (p=dropout)\n\n        pe = (max_len, d_model)\n        position = (0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = ((0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = (position * div_term)\n        pe[:, 1::2] = (position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.pe = nn.Parameter(pe, requires_grad=False)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)",
            "torch->Parallel and Distributed Training->Getting Started with Distributed Data Parallel->Basic Use Case->import os\nimport sys\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# On Windows platform, the torch.distributed package only\n# supports Gloo backend, FileStore and TcpStore.\n# For FileStore, set init_method parameter in init_process_group\n# to a local file. Example as follow:\n# init_method=\"file:///f:/libtmp/some_file\"\n# dist.init_process_group(\n#    \"gloo\",\n#    rank=rank,\n#    init_method=init_method,\n#    world_size=world_size)\n# For TcpStore, same way as on Linux.\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()",
            "statsmodels->Examples->Linear Regression Models->Rolling Least Squares->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn\n\nimport statsmodels.api as sm\nfrom statsmodels.regression.rolling import RollingOLS\n\nseaborn.set_style(\"darkgrid\")\npd.plotting.register_matplotlib_converters()\n%matplotlib inline",
            "sklearn->Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Kernel methods: kernel ridge and Gaussian process->Kernel ridge->import time\nfrom sklearn.gaussian_process.kernels import \nfrom sklearn.kernel_ridge import \n\nkernel_ridge = (kernel=())\n\nstart_time = ()\nkernel_ridge.fit(training_data, training_noisy_target)\nprint(\n    f\"Fitting KernelRidge with default kernel: {() - start_time:.3f} seconds\"\n)"
        ]
    },
    {
        "id": "245702",
        "GT": "sns.pairplot(features[hum_col])",
        "pred": [
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->sns.catplot(data=flights_wide, kind=\"box\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->sns.catplot(data=flights_wide, kind=\"box\")\n",
            "seaborn->User guide and tutorial->An introduction to seaborn->Multivariate views on complex datasets->sns.pairplot(data=penguins, hue=\"species\")\n",
            "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Combining multiple views on the data->sns.pairplot(data=penguins, hue=\"species\")\n",
            "seaborn->API Overview->Overview of seaborn plotting functions->Combining multiple views on the data->sns.pairplot(data=penguins, hue=\"species\")\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->sns.pairplot(penguins)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->sns.pairplot(penguins)\n",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->sns.pairplot(iris, hue=\"species\", height=2.5)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->sns.pairplot(iris, hue=\"species\", height=2.5)\n",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->sns.relplot(data=flights_wide, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->sns.relplot(data=flights_wide, kind=\"line\")\n"
        ]
    },
    {
        "id": "1308287",
        "GT": "Step D: Collaborative Filtering Recommender\n\nAs the most popular model in recommender system, Collaborative Filtering Recommender will make recommendation based on similarities between users or items. There are several approaches to it. Here I am goint to do two major approaches `Similarity Based` Collaborative Filtering and `Matrix Factorization` Collaborative Filtering. Both methods are pretty similar, we will see in more detail soon.\n\n### D-1: Item-item Based Collaborative Filtering\n\nThe intuitive idea behind `Item-Item Based Collaborative Filtering` is to recommend Song_B to user if user like Song_A and Song_A is similar to Song_B.\n\n#### D-1-I: Re-index \n\nRe-index `song_id` and `msno` so that less space will be used when constructing pivot table.-># Function convert long id to integer\n# Read \n#    ds - a data series with long index\n#    dic - empty dictionary for store pair \n#    max_val - counter for updating integer\ndef dummy_encoding(ds, dic, max_val):\n    for i, count in zip(ds, range(len(ds))):\n        if dic.has_key(i): # if exist, use integer\n            ds.iloc[count] = dic.get(i)\n        else: # otherwise, add new integer pairs\n            max_val += 1\n            dic[i] = max_val\n            ds.iloc[count] = max_val\n    return ds, dic, max_val",
        "pred": [
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Updating the Parameters-># initialise the moving averages\ndef initialise_mav(hidden_dim, input_dim, params):\n    v = {}\n    s = {}\n    # Initialize dictionaries v, s\n    for key in params:\n        v['d' + key] = np.zeros(params[key].shape)\n        s['d' + key] = np.zeros(params[key].shape)\n    # Return initialised moving averages\n    return v, s",
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data-># Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
            "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Setup and Performance Metrics-># Utility to profile the workload\ndef profile_workload(forward_func, , iteration_count=100, label=\"\"):\n    # Perform warm-up iterations\n    for _ in range(3):\n        # Run model, forward and backward\n         = forward_func()\n        ()\n        # delete gradiens to avoid profiling the gradient accumulation\n        for  in parameters:\n            .grad = None\n\n    # Synchronize the GPU before starting the timer\n    ()\n    start = time.perf_counter()\n    for _ in range(iteration_count):\n        # Run model, forward and backward\n         = forward_func()\n        ()\n        # delete gradiens to avoid profiling the gradient accumulation\n        for  in parameters:\n            .grad = None\n\n    # Synchronize the GPU before stopping the timer\n    ()\n    stop = time.perf_counter()\n    iters_per_second = iteration_count / (stop - start)\n    if label:\n        print(label)\n    print(\"Average iterations per second: {:.2f}\".format(iters_per_second))",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Updating the Parameters-># Update the parameters using Adam optimization\ndef update_parameters(parameters, gradients, v, s,\n                      learning_rate=0.01, beta1=0.9, beta2=0.999):\n    for key in parameters:\n        # Moving average of the gradients\n        v['d' + key] = (beta1 * v['d' + key]\n                        + (1 - beta1) * gradients['d' + key])\n\n        # Moving average of the squared gradients\n        s['d' + key] = (beta2 * s['d' + key]\n                        + (1 - beta2) * (gradients['d' + key] ** 2))\n\n        # Update parameters\n        parameters[key] = (parameters[key] - learning_rate\n                           * v['d' + key] / np.sqrt(s['d' + key] + 1e-8))\n    # Return updated parameters and moving averages\n    return parameters, v, s",
            "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests-># hyp = 0 is the null hypothesis, hyp = 1 is the alternative hypothesis.\n# cov_struct is a statsmodels covariance structure\ndef dosim(hyp, cov_struct=None, mcrep=500):\n\n    # Storage for the simulation results\n    scales = [[], []]\n\n    # P-values from the score test\n    pv = []\n\n    # Monte Carlo loop\n    for k in range(mcrep):\n\n        # Generate random \"probability points\" u  that are uniformly\n        # distributed, and correlated within clusters\n        z = np.random.normal(size=n)\n        u = np.random.normal(size=n//m)\n        u = np.kron(u, np.ones(m))\n        z = r*z +np.sqrt(1-r**2)*u\n        u = norm.cdf(z)\n\n        # Generate the observed responses\n        y = negbinom(u, mu=mu[hyp], scale=scale)\n\n        # Fit the null model\n        m0 = sm.GEE(y, x0, groups=grp, cov_struct=cov_struct, family=sm.families.Poisson())\n        r0 = m0.fit(scale='X2')\n        scales[0].append(r0.scale)\n\n        # Fit the alternative model\n        m1 = sm.GEE(y, x, groups=grp, cov_struct=cov_struct, family=sm.families.Poisson())\n        r1 = m1.fit(scale='X2')\n        scales[1].append(r1.scale)\n\n        # Carry out the score test\n        st = m1.compare_score_test(r0)\n        pv.append(st[\"p-value\"])\n\n    pv = np.asarray(pv)\n    rslt = [np.mean(pv), np.mean(pv &lt; 0.1)]\n\n    return rslt, scales"
        ]
    },
    {
        "id": "233409",
        "GT": "Evaluate and predict customer churn\nThis notebook is an adaptation from the work done by [Sidney Phoon and Eleva Lowery](https://github.com/IBMDataScience/DSX-DemoCenter/tree/master/DSX-Local-Telco-Churn-master) with the following modifications:\n* Use datasets persisted in DB2 Warehouse running on ICP\n* Deploy and run the notebook on DSX enterprise running on IBM Cloud Private (ICP)\n* Run spark Machine learning job on ICP as part of the worker nodes.\n* Document some actions for a beginner data scienctist / developer who wants to understand what's going on.\n* The web application was separated in another git project\n\nThe goal is still to demonstrate how to build a predictive model with Spark machine learning API (SparkML) to predict customer churn, and deploy it for scoring in Machine Learning (ML) running on ICP or within IBM public Cloud, Watson Machine Learning service.\n\n## Scope\nA lot of industries have the issue of customers moving to competitors when the product differentiation is not that important, or there is some customer support issues. One industry illustrating this problem is the telecom industry with mobile, internet and IP TV product offerings. \n\n\n## Note book explanations\nThe notebook aims to follow the classical data science modeling steps:\n1. load the data\n1. prepare the data\n1. analyze the data (iterate on those two activities)\n1. build a model\n1. validate the accuracy of the model\n1. deploy the model\n1. consume the model as a service\n\nThis jupyter notebook uses Apache Spark to run the machine learning jobs to build decision trees and random forest classifier to assess when a customer is at risk to move to competitor. Apache Spark offers a Python module called pyspark to operate on data and use ML constructs.\n\n### Start by all imports\nAs a best practices for notebook implementation is to do the import at the top of the notebook. \n* [Spark SQLContext](https://spark.apache.org/docs/latest/sql-programming-guide.html) a spark module to process structured data\n* [spark conf]() to access Spark cluster configuration and then be able to execute queries\n* [jaydebeapi](https://pypi.python.org/pypi/JayDeBeApi) is used to connect to the DB 2 warehouse where customer data are persisted. We assume they are loaded.\n* [ibmdbpy](https://pypi.python.org/pypi/ibmdbpy) interface for data manipulation and access to in-database algorithms in IBM dashDB and IBM DB2.\n* [pandas](https://pandas.pydata.org) Python super library for data analysis\n* [brunel](https://github.com/Brunel-Visualization/Brunel/wiki) API and tool to visualize data quickly. \n* [pixiedust](www.ibm.com/PixieDust) Visualize data inside Jupyter notebooks\n\nThe first cell below is to execute some system commands to update the kernel with updated dependant library.->Load data\nWe suppose the churn data were built by a marketing department who used the customer id and flag them as potential churn or not. The data are delivered as csv file to the data scientist.\n\nIn this  notebook the data are loaded to the internal DSX storage using the following steps:\n* Use the `+` icon on right side of the DSX menu bar to access to `Add Dataset` and then load the customer.csv and churn.csv files from the folder `refarch-analytics/jupyter-notebooks/TelcoChurn/data_assets`. The churn attribute is just a boolean. \n* Add a `code` cell in the netbook, select `1001` icon and then using `Insert to code > Insert spark DataFrame in python` to get a code snippet to load the data. \n* rename the auto generated data frame name\n\nIn the code below the `sc` variable is the Spark Context, and it should be initialized by the execution of the notebook and the DSX spark kernel.->data=customers.join(churns,customers['ID']==churns['ID']).select(customers['*'],churns['CHURN'])\ndata.show(5)",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Autoregressions->sel = ar_select_order(housing, 13, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "pandas_toms_blog->Scaling->Using Dask->df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dynamics->sel = ar_select_order(yoy_housing, 13, glob=True, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dummies->sel = ar_select_order(housing, 13, seasonal=True, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers->sidak = ols_model.outlier_test(\"sidak\")\nsidak.sort_values(\"unadj_p\", inplace=True)\nprint(sidak)"
        ]
    },
    {
        "id": "1487660",
        "GT": "Exploratory graphing->Histogram of number of Rebounds vs. number of days since shot was posted-># Rebounds over time\nrebound_time_since_post = []\nfor rebound in all_rebounds:\n    shot_id_for_rebound = int(rebound['rebound_source_url'].replace('https://api.dribbble.com/v1/shots/', ''))\n    rebound_time = pd.to_datetime(rebound['created_at'])\n    \n    for shot in all_shots: \n        if shot['id'] == shot_id_for_rebound:\n            time_diff = get_days_between(pd.to_datetime(shot['created_at']), rebound_time)\n            rebound_time_since_post.append(time_diff)\n            \n# simple stats\npd.Series(rebound_time_since_post).describe()",
        "pred": [
            "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results-># Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>",
            "matplotlib->Tutorials->Intermediate->Artist tutorial->Object containers->Axis containers-># plt.figure creates a matplotlib.figure.Figure instance\nfig = plt.figure()\nrect = fig.patch  # a rectangle instance\nrect.set_facecolor('lightgoldenrodyellow')\n\nax1 = fig.add_axes([0.1, 0.3, 0.4, 0.4])\nrect = ax1.patch\nrect.set_facecolor('lightslategray')\n\n\nfor label in ax1.xaxis.get_ticklabels():\n    # label is a Text instance\n    label.set_color('red')\n    label.set_rotation(45)\n    label.set_fontsize(16)\n\nfor line in ax1.yaxis.get_ticklines():\n    # line is a Line2D instance\n    line.set_color('green')\n    line.set_markersize(25)\n    line.set_markeredgewidth(3)\n\nplt.show()\n\n\n<img alt=\"artists\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_artists_004.png\" srcset=\"../../_images/sphx_glr_artists_004.png, ../../_images/sphx_glr_artists_004_2_0x.png 2.0x\"/>",
            "torch->Image and Video->Adversarial Example Generation->Results->Sample Adversarial Examples-># Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()\n\n\n<img alt=\"2 -&gt; 2, 6 -&gt; 6, 4 -&gt; 4, 6 -&gt; 6, 2 -&gt; 2, 8 -&gt; 3, 4 -&gt; 2, 9 -&gt; 5, 7 -&gt; 9, 8 -&gt; 9, 6 -&gt; 0, 1 -&gt; 8, 6 -&gt; 8, 9 -&gt; 8, 8 -&gt; 6, 5 -&gt; 8, 5 -&gt; 2, 4 -&gt; 8, 4 -&gt; 8, 5 -&gt; 8, 3 -&gt; 5, 4 -&gt; 8, 8 -&gt; 2, 8 -&gt; 6, 3 -&gt; 8, 6 -&gt; 1, 1 -&gt; 3, 1 -&gt; 8, 4 -&gt; 8, 8 -&gt; 2, 7 -&gt; 8, 1 -&gt; 2, 0 -&gt; 2, 7 -&gt; 8, 7 -&gt; 8\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_002.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_002.png\"/>",
            "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard-># 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)"
        ]
    },
    {
        "id": "1188623",
        "GT": "Lists and `for` loops->primes = [2, 3, 5, 7, 11, 13]     # A list of primes\nmore_primes = primes + [17, 19]   # List concatentation\nprint('First few primes are: {primes}'.format(primes=primes))\nprint('Here are the primes up to the number 20: {}'.format(more_primes))",
        "pred": [
            "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()->pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "statsmodels->Examples->State space models->Statespace: Custom Models->Model 3: multiple observation and state equations->Matrix notation for the state space model->true_values = {\n    \"var_e1\": 0.01,\n    \"var_e2\": 0.01,\n    \"var_w1\": 0.01,\n    \"var_w2\": 0.01,\n    \"delta1\": 0.8,\n    \"delta2\": 0.5,\n    \"delta3\": 0.7,\n}\n\n\ndef gen_data_for_model3():\n    # Starting values\n    alpha1_0 = 2.1\n    alpha2_0 = 1.1\n\n    t_max = 500\n\n    def gen_i(alpha1, s):\n        return alpha1 * s + np.sqrt(true_values[\"var_e1\"]) * np.random.randn()\n\n    def gen_m_hat(alpha2):\n        return 1 * alpha2 + np.sqrt(true_values[\"var_e2\"]) * np.random.randn()\n\n    def gen_alpha1(alpha1, alpha2):\n        w1 = np.sqrt(true_values[\"var_w1\"]) * np.random.randn()\n        return true_values[\"delta1\"] * alpha1 + true_values[\"delta2\"] * alpha2 + w1\n\n    def gen_alpha2(alpha2):\n        w2 = np.sqrt(true_values[\"var_w2\"]) * np.random.randn()\n        return true_values[\"delta3\"] * alpha2 + w2\n\n    s_t = 0.3 + np.sqrt(1.4) * np.random.randn(t_max)\n    i_hat = np.empty(t_max)\n    m_hat = np.empty(t_max)\n\n    current_alpha1 = alpha1_0\n    current_alpha2 = alpha2_0\n    for t in range(t_max):\n        # Obs eqns\n        i_hat[t] = gen_i(current_alpha1, s_t[t])\n        m_hat[t] = gen_m_hat(current_alpha2)\n\n        # state eqns\n        new_alpha1 = gen_alpha1(current_alpha1, current_alpha2)\n        new_alpha2 = gen_alpha2(current_alpha2)\n\n        # Update states for next period\n        current_alpha1 = new_alpha1\n        current_alpha2 = new_alpha2\n\n    return i_hat, m_hat, s_t\n\n\ni_hat, m_hat, s_t = gen_data_for_model3()",
            "torch->Model Optimization->Hyperparameter tuning with Ray Tune->Configuring the search space->gpus_per_trial = 2\n# ...\nresult = tune.run(\n    partial(train_cifar, data_dir=data_dir),\n    resources_per_trial={\"cpu\": 8, \"gpu\": gpus_per_trial},\n    config=config,\n    num_samples=num_samples,\n    scheduler=scheduler,\n    progress_reporter=reporter,\n    checkpoint_at_end=True)",
            "scipy->Multidimensional image processing (scipy.ndimage)->Filter functions->Generic filter functions->a = np.arange(12).reshape(3,4)\n\n class fnc1d_class:\n...     def __init__(self, shape, axis = -1):\n...         # store the filter axis:\n...         self.axis = axis\n...         # store the shape:\n...         self.shape = shape\n...         # initialize the coordinates:\n...         self.coordinates = [0] * len(shape)\n...\n...     def filter(self, iline, oline):\n...         oline[...] = iline[:-2] + 2 * iline[1:-1] + 3 * iline[2:]\n...         print(self.coordinates)\n...         # calculate the next coordinates:\n...         axes = list(range(len(self.shape)))\n...         # skip the filter axis:\n...         del axes[self.axis]\n...         axes.reverse()\n...         for jj in axes:\n...             if self.coordinates[jj] &lt; self.shape[jj] - 1:\n...                 self.coordinates[jj] += 1\n...                 break\n...             else:\n...                 self.coordinates[jj] = 0\n...\n fnc = fnc1d_class(shape = (3,4))\n generic_filter1d(a, fnc.filter, 3)\n[0, 0]\n[1, 0]\n[2, 0]\narray([[ 3,  8, 14, 17],\n       [27, 32, 38, 41],\n       [51, 56, 62, 65]])",
            "sklearn->Examples->Decomposition->Faces dataset decompositions->Dataset preparation->n_row, n_col = 2, 3\nn_components = n_row * n_col\nimage_shape = (64, 64)\n\n\ndef plot_gallery(title, images, n_col=n_col, n_row=n_row, cmap=plt.cm.gray):\n    fig, axs = (\n        nrows=n_row,\n        ncols=n_col,\n        figsize=(2.0 * n_col, 2.3 * n_row),\n        facecolor=\"white\",\n        constrained_layout=True,\n    )\n    fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, hspace=0, wspace=0)\n    fig.set_edgecolor(\"black\")\n    fig.suptitle(title, size=16)\n    for ax, vec in zip(axs.flat, images):\n        vmax = max(vec.max(), -vec.min())\n        im = ax.imshow(\n            vec.reshape(image_shape),\n            cmap=cmap,\n            interpolation=\"nearest\",\n            vmin=-vmax,\n            vmax=vmax,\n        )\n        ax.axis(\"off\")\n\n    fig.colorbar(im, ax=axs, orientation=\"horizontal\", shrink=0.99, aspect=40, pad=0.01)\n    ()"
        ]
    },
    {
        "id": "1068412",
        "GT": "Text Classification Exercise Answers\n\nWe will be using the SMS spam data from the previous set of exercises.->from accessory_functions import preprocess_series_text, nltk_path\n\ndata = pd.read_csv('../data/spam.csv', sep='\\t')\ndata['text'] = preprocess_series_text(data.text, nltk_path=nltk_path)\n\ndata.head()",
        "pred": [
            "torch->Model Optimization->Multi-Objective NAS with Ax->Evaluating the results->from ax.service.utils.report_utils import exp_to_df\n\ndf = exp_to_df(experiment)\ndf.head(10)",
            "sklearn->Examples->Kernel Approximation->Scalable learning with polynomial kernel approximation->Preparing the data->from sklearn.datasets import \n\nX, y = (return_X_y=True)\n\ny[y != 2] = 0\ny[y == 2] = 1  # We will try to separate class 2 from the other 6 classes.",
            "matplotlib->Tutorials->Toolkits->The axisartist toolkit->axisartist->axisartist with ParasiteAxes->import mpl_toolkits.axisartist as AA\nfrom mpl_toolkits.axes_grid1 import host_subplot\n\nhost = host_subplot(111, axes_class=AA.Axes)",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->from statsmodels.datasets.longley import load_pandas\n\ny = load_pandas().endog\nX = load_pandas().exog\nX = sm.add_constant(X)",
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset->from sklearn.datasets import \n\nco2 = (data_id=41187, as_frame=True, parser=\"pandas\")\nco2.frame.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ]
    },
    {
        "id": "915120",
        "GT": "Population Data->household_population = df2.groupby(['County_name'])[['Households_estimate_total', 'Families_estimate_total', \n                                                     'Married_couple_families_estimate_total', \n                                                     'Nonfamily_households_estimate_total']].sum()\nhousehold_population",
        "pred": [
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Building the dataset->pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->aggregated or averaged data (unique values of explanatory variables)->using exposure->glm = smf.glm(\n    \"affairs_sum ~ rate_marriage + age + yrs_married\",\n    data=df_a,\n    family=sm.families.Poisson(),\n    exposure=np.asarray(df_a[\"affairs_count\"]),\n)\nres_e = glm.fit()\nprint(res_e.summary())",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->condensed using var_weights instead of freq_weights->glm = smf.glm(\n    \"affairs ~ rate_marriage + age + yrs_married\",\n    data=dc,\n    family=sm.families.Poisson(),\n    var_weights=np.asarray(dc[\"freq\"]),\n)\nres_fv = glm.fit()\nprint(res_fv.summary())",
            "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->Handling Multicollinear Features->cluster_ids = (dist_linkage, 1, criterion=\"distance\")\ncluster_id_to_feature_ids = (list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nX_train_sel = X_train[:, selected_features]\nX_test_sel = X_test[:, selected_features]\n\nclf_sel = (n_estimators=100, random_state=42)\nclf_sel.fit(X_train_sel, y_train)\nprint(\n    \"Accuracy on test data with features removed: {:.2f}\".format(\n        clf_sel.score(X_test_sel, y_test)\n    )\n)",
            "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure->forecasts = pd.DataFrame(\n    {\n        \"ln PCE\": np.log(pce.PCE),\n        \"theta=1.2\": res.forecast(12, theta=1.2),\n        \"theta=2\": res.forecast(12),\n        \"theta=3\": res.forecast(12, theta=3),\n        \"No damping\": res.forecast(12, theta=np.inf),\n    }\n)\n_ = forecasts.tail(36).plot()\nplt.title(\"Forecasts of ln PCE\")\nplt.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_theta-model_18_0.png\" src=\"../../../_images/examples_notebooks_generated_theta-model_18_0.png\"/>"
        ]
    },
    {
        "id": "1449845",
        "GT": "Data Standardization->plt.scatter(y, x_RM, s=5, label = 'RM')\nplt.scatter(y, x_LSTAT, s=5, label = 'LSTAT')\nplt.legend(fontsize=15)\nplt.xlabel('Average number of rooms & Low status population', fontsize=15)\nplt.ylabel('Price', fontsize=15)\nplt.legend()\nplt.show()",
        "pred": [
            "scipy->Signal Processing (scipy.signal)->Detrend->plt.plot(t, y, '-rx')\n plt.plot(t, yconst, '-bo')\n plt.plot(t, ylin, '-k+')\n plt.grid()\n plt.legend(['signal', 'const. detrend', 'linear detrend'])\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with no units. A red trace corresponding to the original signal curves from the bottom left to the top right. A blue trace has the constant detrend applied and is below the red trace with zero Y offset. The last black trace has the linear detrend applied and is almost flat from left to right highlighting the curve of the original signal. This last trace has an average slope of zero and looks very different.\"' class=\"plot-directive\" src=\"../_images/signal-10.png\"/>\n</figure>",
            "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->IIR Filter->plt.title('Digital filter frequency response')\n plt.plot(w, 20*np.log10(np.abs(h)))\n plt.title('Digital filter frequency response')\n plt.ylabel('Amplitude Response [dB]')\n plt.xlabel('Frequency (rad/sample)')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code generates an X-Y plot with amplitude response on the Y axis vs Frequency on the X axis. A single trace shows a smooth low-pass filter with the left third passband near 0 dB. The right two-thirds are about 60 dB down with two sharp narrow valleys dipping down to -100 dB.\"' class=\"plot-directive\" src=\"../_images/signal-6.png\"/>\n</figure>",
            "scipy->Signal Processing (scipy.signal)->Filtering->Analog Filter Design->plt.title('Pole / Zero Plot')\n plt.xlabel('Real')\n plt.ylabel('Imaginary')\n plt.grid()\n plt.show()\n\n\n<figure class=\"align-default\">\n<img alt='\"This code displays two plots. The first plot is an IIR filter response as an X-Y plot with amplitude response on the Y axis vs frequency on the X axis. The low-pass filter shown has a passband from 0 to 100 Hz with 0 dB response and a stop-band from about 175 Hz to 1 KHz about 40 dB down. There are two sharp discontinuities in the filter near 175 Hz and 300 Hz. The second plot is an X-Y showing the transfer function in the complex plane. The Y axis is real-valued an the X axis is complex-valued. The filter has four zeros near [300+0j, 175+0j, -175+0j, -300+0j] shown as blue X markers. The filter also has four poles near [50-30j, -50-30j, 100-8j, -100-8j] shown as red dots.\"' class=\"plot-directive\" src=\"../_images/signal-7_01_00.png\"/>\n</figure>",
            "numpy->NumPy Features->Masked Arrays->Fitting Data->plt.plot(t, china_total)\nplt.plot(t[china_total.mask], model(t)[china_total.mask], \"--\", color=\"orange\")\nplt.plot(7, model(7), \"r*\")\nplt.xticks([0, 7, 13], dates[[0, 7, 13]])\nplt.yticks([0, model(7), 10000, 17500])\nplt.legend([\"Mainland China\", \"Cubic estimate\", \"7 days after start\"])\nplt.title(\n    \"COVID-19 cumulative cases from Jan 21 to Feb 3 2020 - Mainland China\\n\"\n    \"Cubic estimate for 7 days after start\"\n)",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Introduction to pyplot->Formatting the style of your plot->plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')\nplt.axis([0, 6, 0, 20])\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_003.png\" srcset=\"../../_images/sphx_glr_pyplot_003.png, ../../_images/sphx_glr_pyplot_003_2_0x.png 2.0x\"/>"
        ]
    },
    {
        "id": "396334",
        "GT": "Total Mass Ratios: m1, m2, r1, r2, spin1, spin2, momentum(?)->Total_mass = m",
        "pred": [
            "torch->Parallel and Distributed Training->Training Transformer models using Pipeline Parallelism->Model scale and Pipe initialization->Total parameters in model: 1,444,261,998",
            "statsmodels->Examples->Generalized Estimating Equations->GEE Nested Covariance Structure->n_groups = 100",
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables->dta_c.T[0]",
            "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Products with n-dimensional arrays->reconstructed.shape",
            "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Frequency model \u2013 Poisson distribution->6237"
        ]
    },
    {
        "id": "48669",
        "GT": "Setup \nNote: Same as above, however, in order to pick up the file from here, the same settings are copied.->Train the top layers\nStep 1: train only the top layers (which were randomly initialized), i.e. freeze all convolutional InceptionV3 layers->for layer in model_tune.layers:\n    layer.trainable = False\n    \n# compile the model (should be done *after* setting layers to non-trainable)\nmodel_tune.compile(optimizer='rmsprop', loss='binary_crossentropy',  metrics=[eval_metric])\n\nmodel_tune.layers",
        "pred": [
            "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 2. Finetuning the Quantizable Model->Finetuning the model->for param in model_ft.parameters():\n  param.requires_grad = True\n\nmodel_ft.to(device)  # We can fine-tune on GPU if available\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are training everything, so the learning rate is lower\n# Notice the smaller learning rate\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)\n\n# Decay LR by a factor of 0.3 every several epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3)\n\nmodel_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                             num_epochs=25, device=device)",
            "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits->for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>",
            "torch->PyTorch Recipes->Automatic Mixed Precision->Inspecting/modifying gradients (e.g., clipping)->for epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        with (device_type='cuda', dtype=torch.float16):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n\n        # Unscales the gradients of optimizer's assigned params in-place\n        scaler.unscale_(opt)\n\n        # Since the gradients of optimizer's assigned params are now unscaled, clips as usual.\n        # You may use the same value for max_norm here as you would without gradient scaling.\n        (net.parameters(), max_norm=0.1)\n\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance",
            "torch->PyTorch Recipes->Automatic Mixed Precision->Adding autocast->for epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        # Runs the forward pass under autocast.\n        with (device_type='cuda', dtype=torch.float16):\n            output = net(input)\n            # output is float16 because linear layers autocast to float16.\n            assert output.dtype is torch.float16\n\n            loss = loss_fn(output, target)\n            # loss is float32 because mse_loss layers autocast to float32.\n            assert loss.dtype is torch.float32\n\n        # Exits autocast before backward().\n        # Backward passes under autocast are not recommended.\n        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->for name, importances in zip([\"train\", \"test\"], [train_importances, test_importances]):\n    ax = importances.plot.box(vert=False, whis=10)\n    ax.set_title(f\"Permutation Importances ({name} set)\")\n    ax.set_xlabel(\"Decrease in accuracy score\")\n    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n    ax.figure.tight_layout()\n\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_004.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_004.png\"/>\n<img alt=\"Permutation Importances (test set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_005.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_005.png\"/>"
        ]
    },
    {
        "id": "1194669",
        "GT": "Word2Vec on Instacart products\n### The goal of this kernel is to try a Word2Vec model on the sequential data of product orders\n### The sequences can act as sentences and product ids can act as words, in this kernel we will see if the model will learn any useful information about the products from the order history of all users, maybe in the future this can be used as input to a classifier that recommends products.\n## This gave me a slight increase in my LB score, so it's a useful feature\n### Please upvote if you like it and let me know in the discussion if you have any remarks/ideas->Load the needed libraries->import pandas as pd\nimport numpy as np\nimport gensim\nfrom matplotlib import pyplot as plt\nfrom sklearn.manifold import TSNE\n",
        "pred": [
            "seaborn->User guide and tutorial->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "statsmodels->Examples->Statistics->Mediation analysis with duration data->import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.stats.mediation import Mediation",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(9876789)",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.tsa.arima.model import ARIMA",
            "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Classes of colormaps->import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom colorspacious import cspace_converter"
        ]
    },
    {
        "id": "145564",
        "GT": "Data Cleaning->mpg_data.horsepower = mpg_data.horsepower.astype('float')\nmpg_data.dtypes",
        "pred": [
            "statsmodels->Examples->Statistics->ANOVA->interM_lm.model.exog\ninterM_lm.model.exog_names",
            "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Applying to all colors->img_array_transposed = np.transpose(img_array, (2, 0, 1))\nimg_array_transposed.shape",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:->predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "scipy->File IO (scipy.io)->MATLAB files->The basic functions->sio.loadmat\nsio.savemat\nsio.whosmat"
        ]
    },
    {
        "id": "297475",
        "GT": "Overview\nIn this notebook, I'll show you how to make a simple query on Craigslist using some nifty python modules. You can take advantage of all the structure data that exists on webpages to collect interesting datasets.-># BS4 can quickly parse our text, make sure to tell it that you're giving html\nhtml = bs4(rsp.text, 'html.parser')\n\n# BS makes it easy to look through a document\nprint(html.prettify()[:1000])",
        "pred": [
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Here we'll catch the exception to prevent printing too much of\n# the exception trace output in this notebook\ntry:\n    res.forecast('2000-01-03')\nexcept KeyError as e:\n    print(e)",
            "torch->PyTorch Recipes->PyTorch Benchmark->Steps->6. Saving/Loading benchmark results-># And just to show that we can round trip all of the results from earlier:\nround_tripped_results = pickle.loads(pickle.dumps(results))\nassert(str(benchmark.Compare(results)) == str(benchmark.Compare(round_tripped_results)))",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets-># Importing the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pooch\nimport string\nimport re\nimport zipfile\nimport os\n\n# Creating the random instance\nrng = np.random.default_rng()",
            "torch->Model Optimization->torch.compile Tutorial->TorchDynamo and FX Graphs->try:\n    torch._dynamo.export(bar, (10), (10))\nexcept:\n    tb.print_exc()\n\nmodel_exp = torch._dynamo.export(init_model(), generate_data(16)[0])\nprint(model_exp[0](generate_data(16)[0]))",
            "torch->PyTorch Recipes->Defining a Neural Network in PyTorch->Steps->4. [Optional] Pass data through your model to test-># Equates to one random 28x28 image\nrandom_data = ((1, 1, 28, 28))\n\nmy_nn = Net()\nresult = my_nn(random_data)\nprint (result)"
        ]
    },
    {
        "id": "1139357",
        "GT": "PREPARING  THE  FEATURES:\n\n\n\n### Handling  Missing  Values->loans_2007.isnull().sum()",
        "pred": [
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->delays.nsmallest(5).sort_values()",
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time.unique()",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_mask.nonzero()",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resf_logit.predict()",
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time.head()"
        ]
    },
    {
        "id": "661865",
        "GT": "Introduction to Pandas->Pandas Series->populations = pd.Series({\"China\":1357000000, \"India\":1252000000, \"United States\":321068000, \"Indonesia\":249900000, \n                         \"Brazil\":200400000, \"Pakistan\":191854000})\nprint(populations)",
        "pred": [
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_array = flights_wide.to_numpy()\nsns.relplot(data=flights_array, kind=\"line\")\n",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->dowjones = sns.load_dataset(\"dowjones\")\nsns.relplot(data=dowjones, x=\"Date\", y=\"Price\", kind=\"line\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->dowjones = sns.load_dataset(\"dowjones\")\nsns.relplot(data=dowjones, x=\"Date\", y=\"Price\", kind=\"line\")\n",
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models-># Retrieve the posterior means\nparams = pm.summary(trace_uc)[\"mean\"].values\n\n# Construct results using these posterior means as parameter values\nres_uc_bayes = mod_uc.smooth(params)",
            "sklearn->Examples->Classification->Plot classification probability->Accuracy (train) for L1 logistic: 83.3%\nAccuracy (train) for L2 logistic (Multinomial): 82.7%\nAccuracy (train) for L2 logistic (OvR): 79.3%\nAccuracy (train) for Linear SVC: 82.0%\nAccuracy (train) for GPC: 82.7%\n\n\n\n<br/>",
            "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Using robust regression to correct for outliers.-># rob_crime_model = rlm(\"murder ~ pctmetro + poverty + pcths + single\", data=dta, M=sm.robust.norms.TukeyBiweight()).fit(conv=\"weights\")\n# print(rob_crime_model.summary())"
        ]
    },
    {
        "id": "1164436",
        "GT": "# Number of Unique Items\n\ntotal_unique_items = heroes_df['Item Name'].nunique()\n\n# total puchases\n\ntotal_purchases = heroes_df['Price'].count()\n\n#total Revenue\n\ntotal_revenue = round(heroes_df['Price'].sum(),2)\n\n#avg price\n\navg_price = round(total_revenue /total_purchases, 2)\n\n#create dataframe for values\n\npur_analysis = pd.DataFrame([{\n    \n    \"Number of Unique Items\": total_unique_items,\n    'Average Purchase Price': avg_price,\n    'Total Purchases': total_purchases,\n    'Total Revenue': total_revenue\n}])\n\npur_analysis.style.format({'Average Purchase Price': '${:.2f}', 'Total Revenue': '${:,.2f}'})",
        "pred": [
            "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds-># range of admissible distortions\neps_range = (0.1, 0.99, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = (1, 9, 9)\n\n()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = (n_samples_range, eps=eps)\n    (n_samples_range, min_n_components, color=color)\n\n([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n(\"Number of observations to eps-embed\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_samples vs n_components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\"/>",
            "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results-># Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation-># plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>",
            "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard-># 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)",
            "statsmodels->User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->VAR(p) processes->Model fitting-># some example data\nIn [1]: import numpy as np\n\nIn [2]: import pandas\n\nIn [3]: import statsmodels.api as sm\n\nIn [4]: from statsmodels.tsa.api import VAR\n\nIn [5]: mdata = sm.datasets.macrodata.load_pandas().data\n\n# prepare the dates index\nIn [6]: dates = mdata[['year', 'quarter']].astype(int).astype(str)\n\nIn [7]: quarterly = dates[\"year\"] + \"Q\" + dates[\"quarter\"]\n\nIn [8]: from statsmodels.tsa.base.datetools import dates_from_str\n\nIn [9]: quarterly = dates_from_str(quarterly)\n\nIn [10]: mdata = mdata[['realgdp','realcons','realinv']]\n\nIn [11]: mdata.index = pandas.DatetimeIndex(quarterly)\n\nIn [12]: data = np.log(mdata).diff().dropna()\n\n# make a VAR model\nIn [13]: model = VAR(data)"
        ]
    },
    {
        "id": "1283162",
        "GT": "Analysis\n\nNow let's do some visual analysis to see what we can find out about this dataset. Particularly, we're interested in which breeds are most common in this dataset. Let's start by breaking down the counts of the categorical variable `breed`.->Insights\n - There are 110 different breeds in the dataset.\n - The 5 most frequent breeds are Golden Retriever, Labrador Retriever, Pembroke, Chihuahua and Pug.\n - The most frequent breed is Golden Retriever (7.5%).->twitter_archive_master.to_csv(\"/home/workspace/twitter_archive_master.csv\", encoding='utf-8')",
        "pred": [
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data->flights = sns.load_dataset(\"flights\")\nflights.head()\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data->flights = sns.load_dataset(\"flights\")\nflights.head()\n",
            "pandas_toms_blog->Indexes->weather = pd.read_hdf(\"weather.h5\", \"weather\").sort_index()\n\nweather.head()",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production->fig = res_glob.plot_predict(start=714, end=732)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\"/>",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n"
        ]
    },
    {
        "id": "546488",
        "GT": "Exploring Thanksgiving->Filter out non-celebraters->data[\"Do you celebrate Thanksgiving?\"].value_counts()",
        "pred": [
            "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns->data = ()\nX = ().fit_transform(data[\"data\"])\nfeature_names = data[\"feature_names\"]",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.head(5)",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data->data[:3]",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data->data = sm.datasets.fair.load_pandas().data",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.dtypes"
        ]
    },
    {
        "id": "968489",
        "GT": "6 - playtime \n\nAs an exercise lets try to plot both figures in one plot.  \nThe idea is that we want to check if blood sugar rises when we are gaining weight.->ggplot(data=sug, mapping=aes(x='datetime')) + \\\n    scale_x_date(breaks=['2014-05-20', '2014-06-01','2014-06-15','2014-07-01'],\n                 labels=['20 may','1 june','15 june','1 july']) + \\\n    geom_line(mapping=aes(y='weight'), color='red') + \\\n    geom_line(mapping=aes(y='sugar'), color='blue') + \\\n    ylab('-')",
        "pred": [
            "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Specifying figure sizes->g = sns.FacetGrid(penguins, col=\"sex\", height=3.5, aspect=.75)\n",
            "seaborn->API Overview->Overview of seaborn plotting functions->Figure-level vs. axes-level functions->Specifying figure sizes->g = sns.FacetGrid(penguins, col=\"sex\", height=3.5, aspect=.75)\n",
            "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Influence plots->fig = sm.graphics.influence_plot(prestige_model, criterion=\"cooks\")\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_12_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_12_0.png\"/>",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->g = sns.pairplot(df, hue='cut')",
            "scipy->Statistics (scipy.stats)->Kernel density estimation->Univariate estimation->ax.plot(x1, np.zeros(x1.shape), 'b+', ms=20)  # rug plot\n x_eval = np.linspace(-10, 10, num=200)\n ax.plot(x_eval, kde1(x_eval), 'k-', label=\"Scott's Rule\")\n ax.plot(x_eval, kde2(x_eval), 'r-', label=\"Silverman's Rule\")",
            "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Component-Component plus Residual (CCPR) Plots->fig = sm.graphics.plot_ccpr_grid(prestige_model)\nfig.tight_layout(pad=1.0)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_regression_plots_26_0.png\" src=\"../../../_images/examples_notebooks_generated_regression_plots_26_0.png\"/>"
        ]
    },
    {
        "id": "118313",
        "GT": "s.describe()   # descritptive statistics",
        "pred": [
            "seaborn->User guide and tutorial->Choosing color palettes->Qualitative color palettes->sns.color_palette()\n",
            "seaborn->Figure aesthetics->Choosing color palettes->Qualitative color palettes->sns.color_palette()\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->sns.pairplot(penguins)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->sns.pairplot(penguins)\n",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Scaling plot elements->sns.set_theme()\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Scaling plot elements->sns.set_theme()\n",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Overriding elements of the seaborn styles->sns.axes_style()\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Overriding elements of the seaborn styles->sns.axes_style()\n",
            "seaborn->User guide and tutorial->Choosing color palettes->Sequential color palettes->Discrete vs. continuous mapping->sns.color_palette(\"rocket\")\n",
            "seaborn->Figure aesthetics->Choosing color palettes->Sequential color palettes->Discrete vs. continuous mapping->sns.color_palette(\"rocket\")\n"
        ]
    },
    {
        "id": "1168946",
        "GT": "Check everything's fine->len(entries), len(cchmmpred_data)",
        "pred": [
            "sklearn->Examples->Working with text documents->FeatureHasher and DictVectorizer Comparison->DictVectorizer->len(vectorizer.vocabulary_)",
            "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Frequency model \u2013 Poisson distribution->len(df_test)",
            "torch->Frontend APIs->Jacobians, Hessians, hvp, vhp, and more: composing function transforms->reverse-mode Jacobian (jacrev) vs forward-mode Jacobian (jacfwd)->get_perf(, \"jacrev\", , \"jacfwd\")",
            "torch->Frontend APIs->Jacobians, Hessians, hvp, vhp, and more: composing function transforms->Computing the Jacobian->get_perf(, \"without vmap\",  , \"vmap\")",
            "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Frequency model \u2013 Poisson distribution->len(df_test[df_test[\"ClaimAmount\"]  0])"
        ]
    },
    {
        "id": "78187",
        "GT": "Data Adjustment: Algorithm # 1\n### Histogram Stretching\nA simple way of image enhancement that improves the contrast in an image by stretching the range of pixel intensity to span the entire range from minimum (0) to maximum (1). Over 8 bits, this algorithm expands the contrast in the image over the entire 0 to 255 range\n\nThe reference article is:\nhttp://cs229.stanford.edu/proj2014/Yue%20Wang,Yang%20Song,Facial%20Keypoints%20Detection.pdf\n#### Ref: Facial Keypoints Detection, by Yue Wang and Yang Song, Stanford University->Testing improvement with Histogram Shifting\n\nSee how the model improved by increasing brightness->print(len(train_shift_left))",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Time Series Filters->print(dta.head(10))",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Small group effects->print(res3.f_test(R))",
            "torch->Model Optimization->Parametrizations Tutorial->Inspecting a parametrized module->print(layer.parametrizations.weight[0])",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers->print(prestige.head(10))",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators->print(stats.norm.fit(fat_tails))"
        ]
    },
    {
        "id": "1097968",
        "GT": "Self-Driving Car Engineer Nanodegree\n\n## Deep Learning\n\n## Project: Build a Traffic Sign Recognition Classifier\n\nIn this project, I apply deep neural networks and convolutional neural networks to classify traffic signs. Specifically, I train a model to classify traffic signs from the German Traffic Sign Dataset.->Visualization of the dataset\n\nTo get an idea what the images in the German Traffic Signs Dataset look like, let's visualize one sign of each class.->import matplotlib.pyplot as plt\nimport random\n# Visualizations will be shown in the notebook.\n\n# Plot sign of each class\nplt.figure(figsize=(20, 10))\nfor i in range(n_classes):\n    plt.subplot(4, 11, i+1)\n    img = X_train[y_train == i][0]\n    plt.axis('off')\n    plt.title(\"ClassId: \" + str(i))\n    plt.imshow(img)\nplt.show()",
        "pred": [
            "matplotlib->Tutorials->Intermediate->Tight Layout guide->Simple Example->import matplotlib.pyplot as plt\nimport numpy as np\n\nplt.rcParams['savefig.facecolor'] = \"0.8\"\n\n\ndef example_plot(ax, fontsize=12):\n    ax.plot([1, 2])\n\n    ax.locator_params(nbins=3)\n    ax.set_xlabel('x-label', fontsize=fontsize)\n    ax.set_ylabel('y-label', fontsize=fontsize)\n    ax.set_title('Title', fontsize=fontsize)\n\nplt.close('all')\nfig, ax = plt.subplots()\nexample_plot(ax, fontsize=24)\n\n\n<img alt=\"Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_001.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_001.png, ../../_images/sphx_glr_tight_layout_guide_001_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Text->Text in Matplotlib Plots->Labels for x- and y-axis->import matplotlib.pyplot as plt\nimport numpy as np\n\nx1 = np.linspace(0.0, 5.0, 100)\ny1 = np.cos(2 * np.pi * x1) * np.exp(-x1)\n\nfig, ax = plt.subplots(figsize=(5, 3))\nfig.subplots_adjust(bottom=0.15, left=0.2)\nax.plot(x1, y1)\nax.set_xlabel('Time [s]')\nax.set_ylabel('Damped oscillation [V]')\n\nplt.show()\n\n\n<img alt=\"text intro\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_text_intro_002.png\" srcset=\"../../_images/sphx_glr_text_intro_002.png, ../../_images/sphx_glr_text_intro_002_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Runtime rc settings->import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom cycler import cycler\nmpl.rcParams['lines.linewidth'] = 2\nmpl.rcParams['lines.linestyle'] = '--'\ndata = np.random.randn(50)\nplt.plot(data)\n\n\n<img alt=\"customizing\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_customizing_001.png\" srcset=\"../../_images/sphx_glr_customizing_001.png, ../../_images/sphx_glr_customizing_001_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Tutorial exercises->Cross-validation on diabetes Dataset Exercise->Load dataset and apply GridSearchCV->import matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import datasets\nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \n\nX, y = (return_X_y=True)\nX = X[:150]\ny = y[:150]\n\nlasso = (random_state=0, max_iter=10000)\nalphas = (-4, -0.5, 30)\n\ntuned_parameters = [{\"alpha\": alphas}]\nn_folds = 5\n\nclf = (lasso, tuned_parameters, cv=n_folds, refit=False)\nclf.fit(X, y)\nscores = clf.cv_results_[\"mean_test_score\"]\nscores_std = clf.cv_results_[\"std_test_score\"]",
            "sklearn->Examples->Covariance estimation->Robust covariance estimation and Mahalanobis distances relevance->Comparison of results->import matplotlib.pyplot as plt\nfrom sklearn.covariance import , \n\n# fit a MCD robust estimator to data\nrobust_cov = ().fit(X)\n# fit a MLE estimator to data\nemp_cov = ().fit(X)\nprint(\n    \"Estimated covariance matrix:\\nMCD (Robust):\\n{}\\nMLE:\\n{}\".format(\n        robust_cov.covariance_, emp_cov.covariance_\n    )\n)"
        ]
    },
    {
        "id": "2582",
        "GT": "Girl=df[df['Gender']=='Girl']\nGirl_performance=Girl.groupby(['State'])['Total'].mean()\nprint(Girl_performance.plot(kind='bar',figsize=(20,7)))\nplt.xlabel(\"Girls performance across States\")",
        "pred": [
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Histogram of standardized deviance residuals with Kernel Density Estimate overlaid->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, title=\"Standardized Deviance Residuals\")\nax.hist(resid_std, bins=25, density=True)\nax.plot(kde_resid.support, kde_resid.density, \"r\")",
            "statsmodels->Examples->User Notes->Formulas->Using formulas with models that do not (yet) support them->f = \"Lottery ~ Literacy * Wealth\"\ny, X = patsy.dmatrices(f, df, return_type=\"dataframe\")\nprint(y[:5])\nprint(X[:5])"
        ]
    },
    {
        "id": "995746",
        "GT": "explore data->#correlation matrix\ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(10, 9))\nsns.heatmap(abs(corrmat), vmax=0.6, square=True,annot=True)\nplt.savefig('./figures/corrmat.png')",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Histogram of standardized deviance residuals with Kernel Density Estimate overlaid->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111, title=\"Standardized Deviance Residuals\")\nax.hist(resid_std, bins=25, density=True)\nax.plot(kde_resid.support, kde_resid.density, \"r\")",
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis->oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))"
        ]
    },
    {
        "id": "1292954",
        "GT": "NTDS demo 2: Twitter data aquisition\nMichael Defferrard and Effrosyni Simou->Web scraping\nThere exists a bunch of [Python-based clients](https://dev.twitter.com/overview/api/twitter-libraries#python) for Twitter. [Tweepy](http://tweepy.readthedocs.io) is a popular choice.\n\nTasks:\n1. Download the relevant information from Twitter. Try to minimize the quantity of collected data to the minimum required to answer the questions.\n2. Organize the collected data in a [panda dataframe](http://pandas.pydata.org/). Each row is a tweet, and the columns are at least: the tweet id, the text, the creation time, the number of likes (was called favorite before) and the number of retweets.-># Number of posts / tweets to retrieve.\n# Small value for development, then increase to collect final data.\nn = 20  # 4000",
        "pred": [
            "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->Bayesian estimation-># Set sampling params\nndraws = 3000  #  3000 number of draws from the distribution\nnburn = 600  # 600 number of \"burn-in points\" (which will be discarded)",
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->5. Bayesian estimation with NUTS-># Set sampling params\nndraws = 3000  # number of draws from the distribution\nnburn = 600  # number of \"burn-in points\" (which will be discarded)",
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models-># Set sampling params\nndraws = 3000  # number of draws from the distribution\nnburn = 600  # number of \"burn-in points\" (which will be discarded)",
            "statsmodels->Examples->State space models->State space modeling: Local Linear Trends-># Perform prediction and forecasting\npredict = res.get_prediction()\nforecast = res.get_forecast('2014')",
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression-># Seed for consistency\nnp.random.seed(1)",
            "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Define Hyperparameters->Data collection parameters->frame_skip = 1\nframes_per_batch = 1000 // frame_skip\n# For a complete training, bring the number of frames up to 1M\ntotal_frames = 50_000 // frame_skip"
        ]
    },
    {
        "id": "1188870",
        "GT": "Select Rows In That DataFrame\n\nSelecting rows is good in data exploration and familiarizing yourself with the values you could see. Check this out, I am about to select the first five rows of our DataFrame:->data[4995:]",
        "pred": [
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data->data[:3]",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.head(5)",
            "statsmodels->Examples->Linear Regression Models->Rolling Least Squares->params.iloc[57:62]",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.dtypes",
            "statsmodels->Examples->Linear Regression Models->Rolling Least Squares->Expanding Sample->res.nobs[10:15]"
        ]
    },
    {
        "id": "129968",
        "GT": "Explore\n\n### Grammar of graphics \n\n- *data*: The data that you want to visualise.\n- *aes*: A set of aesthetic mappings describing how variables in the data are mapped to aesthetic attributes that you can perceive.\n- *geom*: Geometric objects represent what you actually see on the plot: points, lines, polygons, etc.\n- *stat*: Statistical transformations summarise data in many useful ways. For example, binning and counting observations to create a histogram, or summarising a 2d relationship with a linear model.\n- *scales*: The scales map values in the data space to values in an aesthetic space, whether it be colour, or size, or shape. Scales draw a legend or axes, which provide an inverse mapping to make it possible to read the original data values from the graph.\n- *coord*: A coordinate system describes how data coordinates are mapped to the plane of the graphic. It also provides axes and gridlines to make it possible to read the graph. We normally use a Cartesian coordinate system, but a number of others are available, including polar coordinates and map projections.\n- *facet*: A faceting specification describes how to break up the data into subsets and how to display those subsets as small multiples. This is also known as conditioning or latticing/trellising.->1 Dimensional: Continuous - Bars->ggplot(airports) + aes(x = 'altitude') + geom_histogram(bins = 40)",
        "pred": [
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.pairplot(iris, hue=\"species\", palette=\"Set2\", diag_kind=\"kde\", height=2.5)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.pairplot(iris, hue=\"species\", palette=\"Set2\", diag_kind=\"kde\", height=2.5)\n",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->Industrial Production->fig = res_glob.plot_predict(start=714, end=732)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_40_0.png\"/>",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics->sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"density\", common_norm=False)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Normalized histogram statistics->sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", stat=\"density\", common_norm=False)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Conditioning on other variables->sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", kind=\"kde\", fill=True)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Conditioning on other variables->sns.displot(penguins, x=\"flipper_length_mm\", hue=\"species\", kind=\"kde\", fill=True)\n",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n"
        ]
    },
    {
        "id": "1515095",
        "GT": "Treatment of lawMinor\nTo treat missing values in <b>lawMinor</b> we will do the following:\n- Create new columns, lawMinorPresent, 0 if not present, 1 if present\n- Impute missing values in original column with 0->#dateArgument\ntemp_dataset['lawMinorPresent'] = numpy.where(temp_dataset.lawMinor.isnull(), 0, 1)\ntemp_dataset['lawMinor'] = temp_dataset['lawMinor'].fillna(0)",
        "pred": [
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)",
            "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach->mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Annual frequency, using a PeriodIndex\nindex = pd.period_range(start='2000', periods=4, freq='A')\nendog1 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog1.index)"
        ]
    },
    {
        "id": "50668",
        "GT": "SC3251 - Assignment 5  \n**Name:** Hussary, Elias   \n**Assignment:** SC3251 - Module 6  \n**Date:** 2018-07-02\n\n---->Assignment Instructions\nFor each data set in the attached Excel file (Module6_Exercise.xlsx). Check all spreadsheets, the workbook contains 6 datasets.\n\n1. Run linear regression fit and plot y vs x (scatter plot) and a linear fitted line.\n2. Answer the following:\n    - Is a linear model appropriate? Please explain.\n    - Are outliers present? If there are outliers, do you expect them to be influential? Why?\n3. Perform an analysis of a linear regression:\n    - Do a graphical analysis: a. plot histogram of residuals; b. plot residuals vs. predictor.\n    - are the requirements for linear regression met? (see Slide 11).\n___->import statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt",
        "pred": [
            "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression->import statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO",
            "statsmodels->Examples->State space models->VARMAX: Introduction->import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "statsmodels->Examples->State space models->Trends and cycles in unemployment->import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->import numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->What you\u2019ll need->import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm",
            "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->import matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.api as sm",
            "statsmodels->Examples->Generalized Estimating Equations->GEE Nested Covariance Structure->import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm",
            "statsmodels->Examples->User Notes->Least squares fitting of models to data->import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm"
        ]
    },
    {
        "id": "124443",
        "GT": "Review Summary Stats->biz_df.hist(column='stars')",
        "pred": [
            "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution->kde.evaluate(-1)",
            "statsmodels->User Guide->Other Models->Methods for Survival and Duration Analysis->Survival function estimation and inference->sf.plot()",
            "statsmodels->Examples->Statistics->ANOVA->Two-way ANOVA->kidney_table.head(10)",
            "statsmodels->User Guide->Other Models->Methods for Survival and Duration Analysis->Survival function estimation and inference->sf.summary().head()",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->glm_mod.model.data.orig_endog.sum(1)"
        ]
    },
    {
        "id": "570824",
        "GT": "Data cleaning->Dectect outliers\nWe first get a brief statistics of the dataframe of the feature.->feature_df.describe()",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resf_logit.predict()",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.head(5)",
            "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Custom Deterministic Terms->det_proc.in_sample().head()",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->arima_res.predict(0, 2)",
            "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Switching variances->res_areturns.summary()"
        ]
    },
    {
        "id": "273867",
        "GT": "Classifying Text Features by Class Within Each Axis\nThis approach attempts to classify the text features by training four separate binary classifiers; one for each axis. This was effective for reducing the noise in the data by allowing SGDClassifier to focus on only two hypothetically polar aspects of the data at a time, and also increased the support for each sample size by decreasing the number of classes it was split into from 16 down to 2 at a time. By checking the distribution of the classification probabilities, we can observe how confident the classifier is in its classifications. Ideally we would want to see a bimodal distribution, showing that the classifier is confident in its classifications. This would suggest that the idea of 4 axis to describe personality is supported by the features of the text being analyzed.\n## Import data set and clear out null values->Seperate labels into individual class labels per axis->#split training data labels into new labels for each axis\ntrain_labels_IE = [entry[0] for entry in train_labels]\ntrain_labels_NS = [entry[1] for entry in train_labels]\ntrain_labels_TF = [entry[2] for entry in train_labels]\ntrain_labels_JP = [entry[3] for entry in train_labels]",
        "pred": [
            "scipy->Sparse eigenvalue problems with ARPACK->Use of LinearOperator->N = 21\n D = np.diag(0.5*np.ones(N-1), k=1) - np.diag(0.5*np.ones(N-1), k=-1)\n D[0] = D[-1] = 0 # take away edge effects\n Dop = FirstDerivative(N, dtype=np.float64)",
            "torch->Text->Language Modeling with nn.Transformer and TorchText->Evaluate the best model on the test dataset->test_loss = evaluate(model, )\ntest_ppl = math.exp(test_loss)\nprint('=' * 89)\nprint(f'| End of training | test loss {test_loss:5.2f} | '\n      f'test ppl {test_ppl:8.2f}')\nprint('=' * 89)",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Probit Model->probit_mod = sm.Probit(spector_data.endog, spector_data.exog)\nprobit_res = probit_mod.fit()\nprobit_margeff = probit_res.get_margeff()\nprint(\"Parameters: \", probit_res.params)\nprint(\"Marginal effects: \")\nprint(probit_margeff.summary())",
            "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression->model1 = sm.GLM.from_formula(\n    \"blotch ~ 0 + C(variety) + C(site)\", family=sm.families.Binomial(), data=df\n)\nresult1 = model1.fit(scale=\"X2\")\nprint(result1.summary())",
            "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Train tree classifier->iris = ()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = (X, y, random_state=0)\n\nclf = (max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)"
        ]
    },
    {
        "id": "1438729",
        "GT": "Are there products we should be aware of?\n\nTo answer this question, I approached it two ways. One way is to tabulate the total number of producted queried by hospitals and another is to look at the top items reported by each item.\n\nThe top ten producted reported by hospitals are listed below. It appears that 1842 and 1807 are the top products that most hospital report.->data.plot_product('product_1842')",
        "pred": [
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data->data.describe()",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.head(5)",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.dtypes",
            "statsmodels->Examples->User Notes->Dates in Time-Series Models->Getting started->data = sm.datasets.sunspots.load()",
            "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper->res.plot_cusum_squares()",
            "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money->res.plot_cusum_squares()"
        ]
    },
    {
        "id": "1026224",
        "GT": "Function to plot histogram->DEFINE lats and lons->lats = np.array(lat)\nlons = np.array(lon)",
        "pred": [
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "torch->Model Optimization->Parametrizations Tutorial->Implementing parametrizations by hand->layer = LinearSymmetric(3)\nout = layer((8, 3))",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->delta_hat, rho_hat = sarima_res.params[:2]\ndelta_hat + rho_hat * y[0]",
            "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Capturing the Model with Symbolic Tracing->traced_rn18 = (rn18)\nprint()",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA->sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()"
        ]
    },
    {
        "id": "96976",
        "GT": "Example 3: Country dataset\nhttps://raw.githubusercontent.com/sjankin/PUBLG088/master/week5/WDI_Data.csv->import pandas as pd\n\ncountry_data = pd.read_csv('datasets/WDI_Data.csv', encoding = \"ISO-8859-1\")",
        "pred": [
            "seaborn->User guide and tutorial->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "numpy->NumPy Applications->X-ray image processing->Examine an X-ray with imageio->import os\nimport imageio\n\nDIR = \"tutorial-x-ray-image-processing\"\n\nxray_image = imageio.v3.imread(os.path.join(DIR, \"00000011_001.png\"))",
            "torch->Model Optimization->Multi-Objective NAS with Ax->Evaluating the results->from ax.service.utils.report_utils import exp_to_df\n\ndf = exp_to_df(experiment)\ndf.head(10)",
            "statsmodels->Examples->User Notes->Contrasts->Example Data->import pandas as pd\n\nurl = \"https://stats.idre.ucla.edu/stat/data/hsb2.csv\"\nhsb2 = pd.read_table(url, delimiter=\",\")",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->import pandas as pd\n\nfeature_names = rf[:-1].get_feature_names_out()\n\nmdi_importances = (\n    rf[-1].feature_importances_, index=feature_names\n).sort_values(ascending=True)"
        ]
    },
    {
        "id": "1111978",
        "GT": "Compartive network reconstruction scan over theta of simulated data \n\nSimulated data generated in the notebook: Model simulation.ipynb->Check fixed theta->ETA = 0.005",
        "pred": [
            "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New PolynomialCountSketch kernel approximation function->0.7137",
            "sklearn->Examples->Working with text documents->Clustering text documents using k-means->K-means clustering on text features->Feature Extraction using TfidfVectorizer->0.007",
            "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Tuning the hyper-parameters of the quantile regressors->0.796",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->0.001",
            "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New PolynomialCountSketch kernel approximation function->0.7344"
        ]
    },
    {
        "id": "523898",
        "GT": "Fitting normal distribution to sizedistribution->out = dist_TS.fit_normal()",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->Dropping an observation->infl = ols_results.get_influence()",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points->infl = ols_model.get_influence()",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resf_logit.predict()",
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Usage Example->mod = NBin(y, X)\nres = mod.fit()",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->pca_model = PCA(dta.T, standardize=False, demean=True)"
        ]
    },
    {
        "id": "1423447",
        "GT": "df = pd.read_csv('/home/nbangs/Notebooks/data/mcnulty/sahie_2008.csv')",
        "pred": [
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples->tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples->tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots->tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots->tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n"
        ]
    },
    {
        "id": "1496843",
        "GT": "y = df_25_70.target\n\nsubset_care_categories = ['physiotherapy', 'obstetrics', 'geriatrics', 'pharmaceuticals']\nX = df_25_70[subset_care_categories]",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Weighted Least Squares->OLS vs.\u00a0WLS->pred_ols = res_ols.get_prediction()\niv_l_ols = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u_ols = pred_ols.summary_frame()[\"obs_ci_upper\"]",
            "statsmodels->Examples->Linear Regression Models->Generalized Least Squares->sigma = rho ** order\ngls_model = sm.GLS(data.endog, data.exog, sigma=sigma)\ngls_results = gls_model.fit()",
            "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Calibration of the confidence interval->coverage_fraction(\n    y_test, all_models[\"q 0.05\"].predict(X_test), all_models[\"q 0.95\"].predict(X_test)\n)",
            "sklearn->Examples->Ensemble methods->Monotonic Constraints->gbdt_with_monotonic_cst = (monotonic_cst=[1, -1])\ngbdt_with_monotonic_cst.fit(X, y)",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog->data_student.apply_str = pd.Categorical(data_student.apply_str, ordered=True)\ndata_student.public = data_student.public.astype(float)\ndata_student.pared = data_student.pared.astype(float)"
        ]
    },
    {
        "id": "1047259",
        "GT": "4. Function for smartdownsampling the image by factor n*n  by binning n*n samples->def smartdownsample(img,n):\n    img_2D = img.reshape(28,28)\n    no_samples = len(img_2D)//n\n    img_binned = np.zeros([no_samples,no_samples])\n    for i in range(no_samples):\n        for j in range(no_samples):\n            img_binned[i,j] = img_2D[i*n:(i+1)*n, j*n:(j+1)*n].sum()\n    return img_binned            ",
        "pred": [
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data->Visualize a few images->def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\n,  = next(iter(dataloaders['train']))\n\n# Make a grid from batch\n = ()\n\nimshow(, title=[class_names[x] for x in ])\n\n\n<img alt=\"['bees', 'bees', 'ants', 'bees']\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\" srcset=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\"/>",
            "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)->def frame_preprocessing(observation_frame):\n    # Crop the frame.\n    observation_frame = observation_frame[35:195]\n    # Downsample the frame by a factor of 2.\n    observation_frame = observation_frame[::2, ::2, 0]\n    # Remove the background and apply other enhancements.\n    observation_frame[observation_frame == 144] = 0  # Erase the background (type 1).\n    observation_frame[observation_frame == 109] = 0  # Erase the background (type 2).\n    observation_frame[observation_frame != 0] = 1  # Set the items (rackets, ball) to 1.\n    # Return the preprocessed frame as a 1D floating-point array.\n    return observation_frame.astype(float)",
            "sklearn->Examples->Examples based on real world datasets->Faces recognition example using eigenfaces and SVMs->def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    (figsize=(1.8 * n_col, 2.4 * n_row))\n    (bottom=0, left=0.01, right=0.99, top=0.90, hspace=0.35)\n    for i in range(n_row * n_col):\n        (n_row, n_col, i + 1)\n        (images[i].reshape((h, w)), cmap=plt.cm.gray)\n        (titles[i], size=12)\n        (())\n        (())",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->def plot_weights(support, weights_func, xlabels, xticks):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(support, weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_ylim(-0.1, 1.1)\n    return ax",
            "numpy->NumPy Applications->Plotting Fractals->Julia set->def julia(mesh, c=-1, num_iter=10, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; radius\n        z[conv_mask] = np.square(z[conv_mask]) + c\n        diverge_len[conv_mask] += 1\n\n    return diverge_len"
        ]
    },
    {
        "id": "560286",
        "GT": "Corporate IT spending->Amount vs Category->heights = [] # barplot heights\nfor i in categories:\n    temp_df = corp_it.loc[corp_it['category'] == i]\n    heights.append((i,sum(temp_df['amount'])))\ntemp_df = None",
        "pred": [
            "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack->accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach-># initialize random variable\nt_post = (\n    df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n)",
            "matplotlib->Tutorials->Advanced->Path Tutorial->Compound paths-># histogram our data with numpy\ndata = np.random.randn(1000)\nn, bins = np.histogram(data, 100)",
            "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests->_ = plt.boxplot([scales[0][0], scales[0][1], scales[1][0], scales[1][1]])\nplt.ylabel(\"Estimated scale\")"
        ]
    },
    {
        "id": "1385211",
        "GT": "Random Forest with tf-idf Vectorization->rf = RandomForestClassifier(class_weight='balanced', random_state= 42)\nrf.fit(X_train_1, y_train)",
        "pred": [
            "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Train tree classifier->iris = ()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = (X, y, random_state=0)\n\nclf = (max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)",
            "statsmodels->Examples->State space models->Statespace: Custom Models->Model 1: time-varying coefficients->And then estimate it with our custom model class->mod = TVRegression(y_t, x_t, w_t)\nres = mod.fit()\n\nprint(res.summary())",
            "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper->mod = sm.RecursiveLS(endog, exog)\nres = mod.fit()\n\nprint(res.summary())",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS estimation->model = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())",
            "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)->fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)"
        ]
    },
    {
        "id": "1517204",
        "GT": "Predict Dog Breed with the Model-># also, in order to have exact bottleneck features to the pre-computed ones, I needed to remove dividing the images by 255 \ntrain_tensors_a = paths_to_tensor(train_files).astype('float32')\nvalid_tensors_a = paths_to_tensor(valid_files).astype('float32')\ntest_tensors_a = paths_to_tensor(test_files).astype('float32')",
        "pred": [
            "sklearn->Examples->Clustering->A demo of the mean-shift clustering algorithm->Compute clustering with MeanShift-># The following bandwidth can be automatically detected using\nbandwidth = (X, quantile=0.2, n_samples=500)\n\nms = (bandwidth=bandwidth, bin_seeding=True)\nms.fit(X)\nlabels = ms.labels_\ncluster_centers = ms.cluster_centers_\n\nlabels_unique = (labels)\nn_clusters_ = len(labels_unique)\n\nprint(\"number of estimated clusters : %d\" % n_clusters_)",
            "sklearn->Examples->Clustering->Segmenting the picture of greek coins in regions-># Computing a few extra eigenvectors may speed up the eigen_solver.\n# The spectral clustering quality may also benetif from requesting\n# extra regions for segmentation.\nn_regions_plus = 3\n\n# Apply spectral clustering using the default eigen_solver='arpack'.\n# Any implemented solver can be used: eigen_solver='arpack', 'lobpcg', or 'amg'.\n# Choosing eigen_solver='amg' requires an extra package called 'pyamg'.\n# The quality of segmentation and the speed of calculations is mostly determined\n# by the choice of the solver and the value of the tolerance 'eigen_tol'.\n# TODO: varying eigen_tol seems to have no effect for 'lobpcg' and 'amg' #21243.\nfor assign_labels in (\"kmeans\", \"discretize\", \"cluster_qr\"):\n    t0 = ()\n    labels = (\n        graph,\n        n_clusters=(n_regions + n_regions_plus),\n        eigen_tol=1e-7,\n        assign_labels=assign_labels,\n        random_state=42,\n    )\n\n    t1 = ()\n    labels = labels.reshape(rescaled_coins.shape)\n    (figsize=(5, 5))\n    (rescaled_coins, cmap=plt.cm.gray)\n\n    (())\n    (())\n    title = \"Spectral clustering: %s, %.2fs\" % (assign_labels, (t1 - t0))\n    print(title)\n    (title)\n    for l in range(n_regions):\n        colors = [plt.cm.nipy_spectral((l + 4) / float(n_regions + 4))]\n        (labels == l, colors=colors)\n        # To view individual segments as appear comment in plt.pause(0.5)\n()\n\n# TODO: After #21194 is merged and #21243 is fixed, check which eigen_solver\n# is the best and set eigen_solver='arpack', 'lobpcg', or 'amg' and eigen_tol\n# explicitly in this example.\n\n\n\n<img alt=\"Spectral clustering: kmeans, 2.04s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_001.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_001.png\"/>\n<img alt=\"Spectral clustering: discretize, 1.83s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_002.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_002.png\"/>\n<img alt=\"Spectral clustering: cluster_qr, 1.82s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_003.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_003.png\"/>",
            "torch->Deploying PyTorch Models in Production->Real Time Inference on Raspberry Pi 4 (30 fps!)->Raspberry Pi 4 Setup-># This enables the extended features such as the camera.\nstart_x=1\n\n# This needs to be at least 128M for the camera processing, if it's bigger you can just leave it as is.\ngpu_mem=128\n\n# You need to commment/remove the existing camera_auto_detect line since this causes issues with OpenCV/V4L2 capture.\n#camera_auto_detect=1",
            "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()-># The threshold is \"greater than 150\"\n# Return `1` if true, `0` otherwise\nxray_image_mask_less_noisy = np.where(xray_image &gt; 150, 1, 0)\n\nplt.imshow(xray_image_mask_less_noisy, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/2b2a2a73c09768a3e924a35f2e10a716c3171b763df137f3c1688aa222499701.png\" src=\"../_images/2b2a2a73c09768a3e924a35f2e10a716c3171b763df137f3c1688aa222499701.png\"/>",
            "torch->PyTorch Recipes->Dynamic Quantization->Steps->1: Set Up-># import the modules used here in this recipe\nimport torch\nimport torch.quantization\nimport torch.nn as nn\nimport copy\nimport os\nimport time\n\n# define a very, very simple LSTM for demonstration purposes\n# in this case, we are wrapping nn.LSTM, one layer, no pre or post processing\n# inspired by\n# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html, by Robert Guthrie\n# and https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html\nclass lstm_for_demonstration():\n  \"\"\"Elementary Long Short Term Memory style model which simply wraps nn.LSTM\n     Not to be used for anything other than demonstration.\n  \"\"\"\n  def __init__(self,in_dim,out_dim,depth):\n     super(lstm_for_demonstration,self).__init__()\n     self.lstm = (in_dim,out_dim,depth)\n\n  def forward(self,inputs,hidden):\n     out,hidden = self.lstm(inputs,hidden)\n     return out, hidden\n\n\n(29592)  # set the seed for reproducibility\n\n#shape parameters\nmodel_dimension=8\nsequence_length=20\nbatch_size=1\nlstm_depth=1\n\n# random data for input\ninputs = (sequence_length,batch_size,model_dimension)\n# hidden is actually is a tuple of the initial hidden state and the initial cell state\nhidden = ((lstm_depth,batch_size,model_dimension), (lstm_depth,batch_size,model_dimension))"
        ]
    },
    {
        "id": "122427",
        "GT": "DATA VALIDATION\n## Compare Machine Learning Algorithms Consistently->1.Checking wheather data set is sufficient->k.info()",
        "pred": [
            "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution->kde.evaluate(-1)",
            "statsmodels->Examples->Time Series Analysis->Stationarity and detrending (ADF/KPSS)->KPSS test->kpss_test(sunspots[\"SUNACTIVITY\"])",
            "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution->kde.entropy",
            "pandas_toms_blog->Indexes->Flavors->Indexes for Alignment->sped.head()",
            "statsmodels->Examples->Linear Regression Models->Rolling Least Squares->params.tail()"
        ]
    },
    {
        "id": "1149662",
        "GT": "Mass Transit Usage->Analysis\n\nThe first thing to notice here is that we have a feature called `city` that seems to simply be an index of which city the data came from. This really doesn't provide any information to help us with making predictions about how many people will ride per week, so it is probably safe to drop this feature.\n\nAnother point of interest is that a lot of the data has a very different scale. Now, this won't affect a linear regression, but it will affect your ability to compare these items on a graph. When graphing it is often useful to do some additional normalization of the data so that we can better understand how variables relate. The `monthly income`, `city population`, and `weekly riders` are much larger numbers when compared with the `weekly price` and `average parking rates`. Therefore, for graphing purposes we can normalize these values by dividing by some constant across the entire feature.\n\nLet's divide `monthly_income` by $1000$ to get `monthly_income_per_k`, `weekly_riders` by $10,000$ to get `weekly_riders_per_10k`, and `city_pop` by $100k$ to get `city_pop_per_100k`. In order to keep the graph simple, we will drop `city_pop_per_100k` while graphing.\n\nIn order to make it more clear how these values are related to weekly riders, we can round the `weekly_riders_per_10k` to the closest whole number. Keep in mind we only want to do this for graphing because otherwise we are losing information.\n\nNow we can do a pair plot to identify features that might be related.->Pair Plot in Plotly->from plotly.tools import FigureFactory as FF\niplot(FF.create_scatterplotmatrix(transit_graph,width=1200, size=10, height=1200, diag='box'));",
        "pred": [
            "matplotlib->Tutorials->Toolkits->The axes_grid1 toolkit->AxesDivider->from mpl_toolkits.axes_grid1.axes_size import Fixed, Scaled\nvert = [Fixed(2), Scaled(2), Scaled(3)]",
            "matplotlib->Tutorials->Text->Text rendering with XeLaTeX/LuaLaTeX via the ``pgf`` backend->from matplotlib.backends.backend_pgf import FigureCanvasPgf\nmatplotlib.backend_bases.register_backend('pdf', FigureCanvasPgf)",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team->g = sns.FacetGrid(wins.reset_index(), hue='team', size=7, aspect=.5, palette=['k'])\ng.map(sns.pointplot, 'is_home', 'win_pct').set(ylim=(0, 1));",
            "torch->Model Optimization->Multi-Objective NAS with Ax->Evaluating the results->from ax.plot.contour import interact_contour_plotly\n\ninteract_contour_plotly(model=gs.model, metric_name=\"val_acc\")",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Seaborn figure styles->sns.set_style(\"whitegrid\")\ndata = np.random.normal(size=(20, 6)) + np.arange(6) / 2\nsns.boxplot(data=data);\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Seaborn figure styles->sns.set_style(\"whitegrid\")\ndata = np.random.normal(size=(20, 6)) + np.arange(6) / 2\nsns.boxplot(data=data);\n"
        ]
    },
    {
        "id": "1125729",
        "GT": "Candy Trade\nThis notebook contains all data and code to replicate our candy trade analyses. Every participant of the tutorial received a handful of candy. They then conducted an experiment exploring the impact of candy trading on their candy selection happiness:\n\n1. **Pre-trade**: Participants were asked to rate the happiness of their candy selection on a scale from 1-10 (trade 0). \n\n2. **Trade 1**: Participants were then allowed to trade with one participant and rate the happiness with their selection following the trade on a scale from 1-10 (trade 1). \n\n3. **Trade 2**: Participants were then allowed to trade with the whole group and rate their happiness with their final selection on a scale from 1-10 (trade 2).->Changes in candy selection happiness of workshop group->data.boxplot('happiness_rating', by='trade_number', figsize=(12, 8))",
        "pred": [
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Distributional representations->sns.displot(data=tips, kind=\"ecdf\", x=\"total_bill\", col=\"time\", hue=\"smoker\", rug=True)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5), cbar=True)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5), cbar=True)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5))\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5))\n",
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Plots for categorical data->sns.catplot(data=tips, kind=\"violin\", x=\"day\", y=\"total_bill\", hue=\"smoker\", split=True)\n",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots->sns.relplot(\n    data=tips, x=\"total_bill\", y=\"tip\", hue=\"size\",\n)\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots->sns.relplot(\n    data=tips, x=\"total_bill\", y=\"tip\", hue=\"size\",\n)\n"
        ]
    },
    {
        "id": "103124",
        "GT": "Understand the sparse of each feature->df[(df == 0).astype(int).sum(axis=1)==10].groupby(['target']).agg(['count'])",
        "pred": [
            "pandas_toms_blog->Indexes->Merging->The merge version->m.groupby('skyc1').dep_delay.agg(['mean', 'count']).sort_values(by='mean')",
            "pandas_toms_blog->Indexes->Merging->The merge version->(weather.reset_index(level='station')\n .query('station in @locs')\n .groupby(['station', pd.TimeGrouper('H')])).mean()",
            "pandas_toms_blog->Indexes->Merging->Merge Version->pd.merge(temp.to_frame(), sped.to_frame(), left_index=True, right_index=True).head()",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->df.ix[10:15, ['fl_date', 'tail_num']]",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first.loc[['AA', 'AS', 'DL'], ['fl_date', 'tail_num']]"
        ]
    },
    {
        "id": "63594",
        "GT": "2. Pre-processing->2.2 Making Sequences->from keras.preprocessing import text as ktxt, sequence\n\nvocab_max = 100000\n\nhint(\"Fitting the tokenizer...\")\ntokenizer = ktxt.Tokenizer(num_words=vocab_max)\ntokenizer.fit_on_texts(X)\n\nhint(\"Tokenizing...\")\nX = tokenizer.texts_to_sequences(X)\nX_ = tokenizer.texts_to_sequences(X_)\n\nhint(\"Padding the sequences...\")\nmax_comment_length = 200  # padded/cropped comment length\nX = sequence.pad_sequences(X, maxlen=max_comment_length)\nX_ = sequence.pad_sequences(X_, maxlen=max_comment_length)\n\nhint(\"Done\")",
        "pred": [
            "sklearn->Examples->Tutorial exercises->Digits Classification Exercise->from sklearn import datasets, neighbors, linear_model\n\nX_digits, y_digits = (return_X_y=True)\nX_digits = X_digits / X_digits.max()\n\nn_samples = len(X_digits)\n\nX_train = X_digits[: int(0.9 * n_samples)]\ny_train = y_digits[: int(0.9 * n_samples)]\nX_test = X_digits[int(0.9 * n_samples) :]\ny_test = y_digits[int(0.9 * n_samples) :]\n\nknn = ()\nlogistic = (max_iter=1000)\n\nprint(\"KNN score: %f\" % knn.fit(X_train, y_train).score(X_test, y_test))\nprint(\n    \"LogisticRegression score: %f\"\n    % logistic.fit(X_train, y_train).score(X_test, y_test)\n)",
            "sklearn->Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Visualize the learning curves->from sklearn.model_selection import LearningCurveDisplay\n\n_, ax = ()\n\nsvr = (kernel=\"rbf\", C=1e1, gamma=0.1)\nkr = (kernel=\"rbf\", alpha=0.1, gamma=0.1)\n\ncommon_params = {\n    \"X\": X[:100],\n    \"y\": y[:100],\n    \"train_sizes\": (0.1, 1, 10),\n    \"scoring\": \"neg_mean_squared_error\",\n    \"negate_score\": True,\n    \"score_name\": \"Mean Squared Error\",\n    \"std_display_style\": None,\n    \"ax\": ax,\n}\n\n(svr, **common_params)\n(kr, **common_params)\nax.set_title(\"Learning curves\")\nax.legend(handles=ax.get_legend_handles_labels()[0], labels=[\"SVR\", \"KRR\"])\n\n()\n\n\n<img alt=\"Learning curves\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\" srcset=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\"/>",
            "sklearn->Examples->Cross decomposition->Compare cross decomposition methods->PLS regression, with multivariate response, a.k.a. PLS2->from sklearn.cross_decomposition import \n\nn = 1000\nq = 3\np = 10\nX = (size=n * p).reshape((n, p))\nB = ([[1, 2] + [0] * (p - 2)] * q).T\n# each Yj = 1*X1 + 2*X2 + noize\nY = (X, B) + (size=n * q).reshape((n, q)) + 5\n\npls2 = (n_components=3)\npls2.fit(X, Y)\nprint(\"True B (such that: Y = XB + Err)\")\nprint(B)\n# compare pls2.coef_ with B\nprint(\"Estimated B\")\nprint(np.round(pls2.coef_, 1))\npls2.predict(X)",
            "sklearn->Examples->Generalized Linear Models->Tweedie regression on insurance claims->Severity Model -  Gamma distribution->from sklearn.linear_model import \n\n\nmask_train = df_train[\"ClaimAmount\"]  0\nmask_test = df_test[\"ClaimAmount\"]  0\n\nglm_sev = (alpha=10.0, solver=\"newton-cholesky\")\n\nglm_sev.fit(\n    X_train[mask_train.values],\n    df_train.loc[mask_train, \"AvgClaimAmount\"],\n    sample_weight=df_train.loc[mask_train, \"ClaimNb\"],\n)\n\nscores = score_estimator(\n    glm_sev,\n    X_train[mask_train.values],\n    X_test[mask_test.values],\n    df_train[mask_train],\n    df_test[mask_test],\n    target=\"AvgClaimAmount\",\n    weights=\"ClaimNb\",\n)\nprint(\"Evaluation of GammaRegressor on target AvgClaimAmount\")\nprint(scores)",
            "sklearn->Examples->Decomposition->Image denoising using dictionary learning->Learn the dictionary from reference patches->from sklearn.decomposition import \n\nprint(\"Learning the dictionary...\")\nt0 = ()\ndico = (\n    # increase to 300 for higher quality results at the cost of slower\n    # training times.\n    n_components=50,\n    batch_size=200,\n    alpha=1.0,\n    max_iter=10,\n)\nV = dico.fit(data).components_\ndt = () - t0\nprint(f\"{dico.n_iter_} iterations / {dico.n_steps_} steps in {dt:.2f}.\")\n\n(figsize=(4.2, 4))\nfor i, comp in enumerate(V[:100]):\n    (10, 10, i + 1)\n    (comp.reshape(patch_size), cmap=plt.cm.gray_r, interpolation=\"nearest\")\n    (())\n    (())\n(\n    \"Dictionary learned from face patches\\n\"\n    + \"Train time %.1fs on %d patches\" % (dt, len(data)),\n    fontsize=16,\n)\n(0.08, 0.02, 0.92, 0.85, 0.08, 0.23)\n\n\n<img alt=\"Dictionary learned from face patches Train time 16.0s on 22692 patches\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_image_denoising_002.png\" srcset=\"../../_images/sphx_glr_plot_image_denoising_002.png\"/>"
        ]
    },
    {
        "id": "563488",
        "GT": "# To find individual cells\nl = 14\ntemp=markers.copy()\ntemp[markers!=l]=-1\nplt.imshow(temp)",
        "pred": [
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "torch->Introduction to PyTorch->Quickstart->Optimizing the Model Parameters->epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")",
            "pandas_toms_blog->Indexes->Flavors->Indexes for Easier Arithmetic, Analysis-># With indecies\ntemp = weather['tmpf']\n\nc = (temp - 32) * 5 / 9\nc.to_frame()",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())"
        ]
    },
    {
        "id": "128770",
        "GT": "Let's start tuning the SVM classifier and see if that outperforms GBM\n##### -> This time I will use RandomizedSearchCV since SVM is even more computationally expensive than GBM->Let's try it with GridSearchCV->param_grid = dict(gamma=gamma_range, C=C_range)\n\ngs_svc = GridSearchCV(SVC(), param_grid, scoring='accuracy', \n                      cv=5, n_jobs=-1).fit(X, y)\n\ngs_svc.best_params_",
        "pred": [
            "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types->param_grid = {\n    \"preprocessor__num__imputer__strategy\": [\"mean\", \"median\"],\n    \"preprocessor__cat__selector__percentile\": [10, 30, 50, 70],\n    \"classifier__C\": [0.1, 1.0, 10, 100],\n}\n\nsearch_cv = (clf, param_grid, n_iter=10, random_state=0)\nsearch_cv",
            "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Define a function to visualize cross-validation behavior->cvs = [, , ]\n\nfor cv in cvs:\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(cv(n_splits), X, y, groups, ax, n_splits)\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n\n\n\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_003.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_003.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_004.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_004.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_005.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_005.png\"/>",
            "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples->face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break",
            "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->One-vs-One multiclass ROC->ROC curve using the OvO macro-average->pair_scores = []\nmean_tpr = dict()\n\nfor ix, (label_a, label_b) in enumerate(pair_list):\n\n    a_mask = y_test == label_a\n    b_mask = y_test == label_b\n    ab_mask = (a_mask, b_mask)\n\n    a_true = a_mask[ab_mask]\n    b_true = b_mask[ab_mask]\n\n    idx_a = (label_binarizer.classes_ == label_a)[0]\n    idx_b = (label_binarizer.classes_ == label_b)[0]\n\n    fpr_a, tpr_a, _ = (a_true, y_score[ab_mask, idx_a])\n    fpr_b, tpr_b, _ = (b_true, y_score[ab_mask, idx_b])\n\n    mean_tpr[ix] = (fpr_grid)\n    mean_tpr[ix] += (fpr_grid, fpr_a, tpr_a)\n    mean_tpr[ix] += (fpr_grid, fpr_b, tpr_b)\n    mean_tpr[ix] /= 2\n    mean_score = (fpr_grid, mean_tpr[ix])\n    pair_scores.append(mean_score)\n\n    fig, ax = (figsize=(6, 6))\n    (\n        fpr_grid,\n        mean_tpr[ix],\n        label=f\"Mean {label_a} vs {label_b} (AUC = {mean_score :.2f})\",\n        linestyle=\":\",\n        linewidth=4,\n    )\n    (\n        a_true,\n        y_score[ab_mask, idx_a],\n        ax=ax,\n        name=f\"{label_a} as positive class\",\n    )\n    (\n        b_true,\n        y_score[ab_mask, idx_b],\n        ax=ax,\n        name=f\"{label_b} as positive class\",\n    )\n    ([0, 1], [0, 1], \"k--\", label=\"chance level (AUC = 0.5)\")\n    (\"square\")\n    (\"False Positive Rate\")\n    (\"True Positive Rate\")\n    (f\"{target_names[idx_a]} vs {label_b} ROC curves\")\n    ()\n    ()\n\nprint(f\"Macro-averaged One-vs-One ROC AUC score:\\n{(pair_scores):.2f}\")\n\n\n\n<img alt=\"setosa vs versicolor ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_004.png\" srcset=\"../../_images/sphx_glr_plot_roc_004.png\"/>\n<img alt=\"setosa vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_005.png\" srcset=\"../../_images/sphx_glr_plot_roc_005.png\"/>\n<img alt=\"versicolor vs virginica ROC curves\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_roc_006.png\" srcset=\"../../_images/sphx_glr_plot_roc_006.png\"/>",
            "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with partial observations->features_names = [\"experience\", \"parent hourly wage\", \"college degree\"]\n\nregressor_without_ability = ()\nregressor_without_ability.fit(X_train[features_names], y_train)\ny_pred_without_ability = regressor_without_ability.predict(X_test[features_names])\nR2_without_ability = (y_test, y_pred_without_ability)\n\nprint(f\"R2 score without ability: {R2_without_ability:.3f}\")"
        ]
    },
    {
        "id": "1358369",
        "GT": "Text Analysis\n## \n### Cristobal Donoso\n#### Mayo 2018->Reading the text file->#Leer un archivo de texto\nwith open('advs.txt', 'r') as a:\n    texto = a.read()",
        "pred": [
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime->import onnx\n\nonnx_model = onnx.load(\"super_resolution.onnx\")\nonnx.checker.check_model(onnx_model)",
            "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->Integrating the model in our API Server->import requests\n\nresp = requests.post(\"http://localhost:5000/predict\",\n                     files={\"file\": open('&lt;PATH/TO/.jpg/FILE&gt;/cat.jpg','rb')})",
            "numpy->NumPy Applications->X-ray image processing->Examine an X-ray with imageio->import os\nimport imageio\n\nDIR = \"tutorial-x-ray-image-processing\"\n\nxray_image = imageio.v3.imread(os.path.join(DIR, \"00000011_001.png\"))",
            "torch->Model Optimization->torch.compile Tutorial->Comparison to TorchScript and FX Tracing->import traceback as tb\ntry:\n    (f1)\nexcept:\n    tb.print_exc()"
        ]
    },
    {
        "id": "29495",
        "GT": "Exploratory Data Analysis\n\n### Listings\n\nPrice and Ratings are the two dependent variables we want to predict. We will explore these variables to provide a better understanding of the dataset, their distributions, and their relationship's with multiple varibales\n\n[Features](#features)\n* [Numerical](#numerical)\n* [Amenities](#amenities)\n\n\n[Price](#price)\n* [Bedrooms / Bathrooms & Accommodates / Beds](#bbab)\n* [Property Type / Room Type / Bed Type](#prb)\n* [Neighbourhood](#neighbourhood)\n* [Instant Bookable / Business Travel Ready](#ibbtr)\n\n\n[Ratings](#rating)\n* [Neighbourhood](#neighbourhood_r)\n* [Bedrooms / Bathrooms / Beds / Accommodates](#bbba_r)\n* [Property Type / Room Type / Bed Type](#prb_r)\n* [Instant Bookable / Business Travel Ready](#ibbtr_r)\n\n[Price & Rating](#priceandrating)->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.float_format', '{:.4f}'.format)\npd.options.display.max_columns = 999\npd.options.display.max_rows = 999\nsns.set()",
        "pred": [
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)",
            "statsmodels->Examples->Time Series Analysis->ARMA: Artificial Data->import numpy as np\nimport pandas as pd\n\nfrom statsmodels.graphics.tsaplots import plot_predict\nfrom statsmodels.tsa.arima_process import arma_generate_sample\nfrom statsmodels.tsa.arima.model import ARIMA\n\nnp.random.seed(12345)",
            "statsmodels->Examples->Statistics->Copulas->import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\n\nsns.set_style(\"darkgrid\")\nsns.mpl.rc(\"figure\", figsize=(8, 8))",
            "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Imports->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pandas_datareader as pdr\nimport seaborn as sns\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=15)\nplt.rc(\"lines\", linewidth=3)\nsns.set_style(\"darkgrid\")",
            "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")"
        ]
    },
    {
        "id": "1219618",
        "GT": "Data Cleaning\n\nAlthough the data isn't too messy, there are certain things that will need to be improved upon.\n\n- Remove Duplicate Rows: There is one fully duplicate record that will be deleted;\n- Rounding and Data Type Modifications: Popularity and Vote Average will be rounded to two decimal places, and Release Year will be converted to datetime;\n- Removal of Unnecessary Columns: We will be removing columns that are not needed in our analysis;\n- Populate missing keywords: We will be comparing keywords with words found in the overview column in order to fill in missing values. Specifically, we will first use the Porter Stemmer Algorithm to group keywords by their root word, and then group keywords based on common synonyms. From this, we will be compare words found in the overview section and populate the movie with the mapped keyword;->Removal of Unnecessary Columns\n\nTo make the dataset for manageable, I've decided to remove the id, imdb_id, homepage, tagline, budget_adj, and revenue_adj columns.->#Get a list of current columns\ncols_list = list(df.columns.values)\ncols_list",
        "pred": [
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing-># filter the warning for now on\nimport warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)",
            "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors->AB = results.params\nA = AB[0]\nB = AB[1]",
            "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 0. Prerequisites-># Imports\nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\n\nplt.ion()",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-based cross-validation->all_splits = list(ts_cv.split(X, y))\ntrain_0, test_0 = all_splits[0]"
        ]
    },
    {
        "id": "85609",
        "GT": "pvalue",
        "pred": [
            "pandas_toms_blog->Scaling->Dask->total_by_employee",
            "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types->clf",
            "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution->kde.entropy",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator->Squared error loss->beta_true",
            "numpy->NumPy Features->Saving and sharing your NumPy arrays->Remove the saved arrays and load them back with NumPy\u2019s->whos"
        ]
    },
    {
        "id": "694743",
        "GT": "example.loc['Name']",
        "pred": [
            "pandas_toms_blog->Indexes->Merging->The merge version->weather.loc['DSM']",
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables->dta_c.T[0]",
            "sklearn->Examples->Ensemble methods->Combine predictors using stacking->Make pipeline to preprocess the data->num_selector(X)",
            "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Load the data->diabetes = ()\nX, y = diabetes.data, diabetes.target",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator->Squared error loss->all_betas.mean(0)"
        ]
    },
    {
        "id": "1325610",
        "GT": "Churn Analysis->\nimport os\nimport re\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport datetime as dt\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble.partial_dependence import plot_partial_dependence\nfrom sklearn.ensemble.partial_dependence import partial_dependence\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import train_test_split\nimport datetime as dt\n\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc",
        "pred": [
            "scipy->Statistics (scipy.stats)->Kernel density estimation->Multiscale Graph Correlation (MGC)->import numpy as np\n import matplotlib.pyplot as plt; plt.style.use('classic')\n from scipy.stats import multiscale_graphcorr",
            "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->import statistics, tabulate, time\nfrom typing import Any, Dict, List\nfrom torch.fx import ",
            "sklearn->Examples->Nearest Neighbors->Approximate nearest neighbors in TSNE->import time\n\nfrom sklearn.manifold import \nfrom sklearn.neighbors import \nfrom sklearn.pipeline import \n\ndatasets = [\n    (\"MNIST_10000\", load_mnist(n_samples=10_000)),\n    (\"MNIST_20000\", load_mnist(n_samples=20_000)),\n]\n\nn_iter = 500\nperplexity = 30\nmetric = \"euclidean\"\n# TSNE requires a certain number of neighbors which depends on the\n# perplexity parameter.\n# Add one since we include each sample as its own neighbor.\nn_neighbors = int(3.0 * perplexity + 1) + 1\n\ntsne_params = dict(\n    init=\"random\",  # pca not supported for sparse matrices\n    perplexity=perplexity,\n    method=\"barnes_hut\",\n    random_state=42,\n    n_iter=n_iter,\n    learning_rate=\"auto\",\n)\n\ntransformers = [\n    (\n        \"KNeighborsTransformer\",\n        (n_neighbors=n_neighbors, mode=\"distance\", metric=metric),\n    ),\n    (\n        \"NMSlibTransformer\",\n        NMSlibTransformer(n_neighbors=n_neighbors, metric=metric),\n    ),\n    (\n        \"PyNNDescentTransformer\",\n        PyNNDescentTransformer(\n            n_neighbors=n_neighbors, metric=metric, parallel_batch_queries=True\n        ),\n    ),\n]\n\nfor dataset_name, (X, y) in datasets:\n\n    msg = f\"Benchmarking on {dataset_name}:\"\n    print(f\"\\n{msg}\\n\" + str(\"-\" * len(msg)))\n\n    for transformer_name, transformer in transformers:\n        longest = np.max([len(name) for name, model in transformers])\n        start = ()\n        transformer.fit(X)\n        fit_duration = () - start\n        print(f\"{transformer_name:&lt;{longest}} {fit_duration:.3f} sec (fit)\")\n        start = ()\n        Xt = transformer.transform(X)\n        transform_duration = () - start\n        print(f\"{transformer_name:&lt;{longest}} {transform_duration:.3f} sec (transform)\")\n        if transformer_name == \"PyNNDescentTransformer\":\n            start = ()\n            Xt = transformer.transform(X)\n            transform_duration = () - start\n            print(\n                f\"{transformer_name:&lt;{longest}} {transform_duration:.3f} sec\"\n                \" (transform)\"\n            )",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit",
            "numpy->NumPy Applications->X-ray image processing->Combine images into a multidimensional array to demonstrate progression->import numpy as np\nnum_imgs = 9\n\ncombined_xray_images_1 = np.array(\n    [imageio.v3.imread(os.path.join(DIR, f\"00000011_00{i}.png\")) for i in range(num_imgs)]\n)"
        ]
    },
    {
        "id": "941578",
        "GT": "Visualization->#Combining features and CGPA(Target)\nvis_dataset = pd.concat([features, target], axis = 1)",
        "pred": [
            "statsmodels->Examples->User Notes->Prediction->Estimation->olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach->mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Concentrating out the scale->mod_conc = LocalLevelConcentrated(dta.infl)\nres_conc = mod_conc.fit(disp=False)\nprint(res_conc.summary())",
            "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Fine-tuning HF T5->def setup_model(model_name):\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    tokenizer =  T5Tokenizer.from_pretrained(model_name)\n    return model, tokenizer"
        ]
    },
    {
        "id": "286998",
        "GT": "Sentiment Analysis Pipeline->Part 1: Subcategory Sentiment Analysis->df = pd.read_pickle(\"Electronics_meta.pickle\")\ndf.head(1)",
        "pred": [
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples->tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples->tips = sns.load_dataset(\"tips\")\ng = sns.FacetGrid(tips, col=\"time\")\n",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots->tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots->tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->tips = sns.load_dataset(\"tips\")\nsns.displot(tips, x=\"size\")\n"
        ]
    },
    {
        "id": "1242186",
        "GT": "Dimensionality Reduction->PCA - Optimal Dimensionality Reduction for Gaussians->full_PCA = sklearn.decomposition.PCA()",
        "pred": [
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->pca_model = PCA(dta.T, standardize=False, demean=True)",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resf_logit.predict()",
            "torch->Parallel and Distributed Training->Getting Started with Fully Sharded Data Parallel(FSDP)->How to use FSDP->model = Net().to(rank)\nmodel = DDP(model)",
            "torch->Learning PyTorch->What is torch.nn really?->Refactor using nn.Linear->fit()\n\nprint(loss_func((), ))",
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Usage Example->mod = NBin(y, X)\nres = mod.fit()"
        ]
    },
    {
        "id": "1241726",
        "GT": "mix = quandl.get(['NSE/OIL.1', 'WIKI/AAPL.4'])\nmix.plot(subplots=True)",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson->rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())",
            "statsmodels->Examples->User Notes->Prediction->Estimation->olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach->mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())"
        ]
    },
    {
        "id": "567587",
        "GT": "Check for Duplicate Index Values\nThis is a good time to check for duplicate index values since we have a DateTime index.\n\n`df.index.duplicated()` generates a True/False array, where True means that row has a duplicate index.->df[df.index.duplicated()]",
        "pred": [
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Data exploration on the Bike Sharing Demand dataset->df[\"count\"].max()",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->df.ix[10:15, ['fl_date', 'tail_num']]",
            "statsmodels->Examples->Statistics->ANOVA->df_infl[:5]",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first.ix[10:15, ['fl_date', 'tail_num']]",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first.iloc[[0, 1, 3], [0, 1]]"
        ]
    },
    {
        "id": "1225934",
        "GT": "Rest of Data\n\n<a id='Rest of Data'></a>\n\nThe remaining data from a separate sheet of the table is more categorical in nature. We'll try to one-hot encode it since the answers repeat themselves so often. We'll start by relabeling the columns to make the table easier to read.->xls2Hotcodeclean.columns",
        "pred": [
            "statsmodels->Examples->Statistics->ANOVA->lm.model.data.frame[:5]",
            "statsmodels->Examples->Statistics->ANOVA->lm.model.exog[:5]",
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables->dta_c.T[0]",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->glm_mod.model.data.orig_endog.sum(1)",
            "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data->res.forecast_components(12)"
        ]
    },
    {
        "id": "1179458",
        "GT": "What is the R-Squared of your model?->0.234",
        "pred": [
            "sklearn->Examples->Working with text documents->Clustering text documents using k-means->K-means clustering on text features->Feature Extraction using TfidfVectorizer->0.007",
            "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Tuning the hyper-parameters of the quantile regressors->0.796",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->0.001",
            "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New PolynomialCountSketch kernel approximation function->0.7137",
            "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Calibration of the confidence interval->0.868"
        ]
    },
    {
        "id": "1121415",
        "GT": "Run simulation->List outputs->print('The outputs are:')\nfor out in outputs.keys():\n    print('- {}'.format(out))\n\nif 'sub_volume_boundary' in outputs.keys():\n    print('There are {} sub_volume_boundary outputs'.format(outputs['sub_volume_boundary'].shape[0]))",
        "pred": [
            "torch->Text->Text classification with the torchtext library->Evaluate the model with test dataset->print('Checking the results of test dataset.')\naccu_test = evaluate()\nprint('test accuracy {:8.3f}'.format(accu_test))",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: SARIMAX->print('Optimizer iterations')\nprint('- Original model:     %d' % res_ar.mle_retvals['iterations'])\nprint('- Concentrated model: %d' % res_ar_conc.mle_retvals['iterations'])",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())",
            "sklearn->Examples->Model Selection->Custom refit strategy of a grid search with cross-validation->The dataset->n_samples = len(digits.images)\nX = digits.images.reshape((n_samples, -1))\ny = digits.target == 8\nprint(\n    f\"The number of images is {X.shape[0]} and each image contains {X.shape[1]} pixels\"\n)"
        ]
    },
    {
        "id": "170297",
        "GT": "Expected Adult Survivor Count->adult_dropna_df = dropna_titanic_df[(dropna_titanic_df.IsAdult == 'Adult')]\ntotal_adult = adult_dropna_df['PassengerId'].count()\ntotal_adult",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->table = pd.DataFrame(data, columns=[\"lag\", \"AC\", \"Q\", \"Prob(Q)\"])\nprint(table.set_index(\"lag\"))",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "statsmodels->Examples->User Notes->Prediction->Estimation->olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach->mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())"
        ]
    },
    {
        "id": "624962",
        "GT": "Now we want to know trends according to weekdays->Let's add some rows at midnight on each day->new_index = midnights.union(data.index)\nnew_index",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Model Support->Simulate Some Data->auto_reg_forecast = res.predict(200, 211)\nauto_reg_forecast",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total",
            "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation->y_train = (y)\ny_train[unlabeled_set] = -1",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_total = china_masked.data\nchina_total",
            "pandas_toms_blog->Scaling->Dask->most_common = df.occupation.value_counts().nlargest(100)\nmost_common"
        ]
    },
    {
        "id": "97209",
        "GT": "for fun\n\nNow let's look at consistency of the direct area estimation and the product of width*height->plt.scatter(area, width*height)\nplt.xlabel('area')\nplt.ylabel('width*height')\nplt.xlim(0.1,.25)\nplt.ylim(0.1,.25)",
        "pred": [
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))",
            "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression->plt.clf()\nplt.grid(True)\nplt.plot(result1.predict(linear=True), result1.resid_pearson, \"o\")\nplt.xlabel(\"Linear predictor\")\nplt.ylabel(\"Residual\")",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 2: Find the win percent for each team->win_percent.sort_values().plot.barh(figsize=(6, 12), width=.85, color='k')\nplt.tight_layout()\nsns.despine()\nplt.xlabel(\"Win Percent\")"
        ]
    },
    {
        "id": "906330",
        "GT": "import seaborn as sns\ncmap = sns.diverging_palette(220, 10, as_cmap=True)",
        "pred": [
            "matplotlib->Tutorials->Intermediate->Artist tutorial->import matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot(2, 1, 1) # two rows, one column, first plot",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "matplotlib->Tutorials->Toolkits->The mplot3d toolkit->import matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')",
            "matplotlib->Tutorials->Text->Text rendering with XeLaTeX/LuaLaTeX via the ``pgf`` backend->from matplotlib.backends.backend_pgf import FigureCanvasPgf\nmatplotlib.backend_bases.register_backend('pdf', FigureCanvasPgf)",
            "matplotlib->Tutorials->Toolkits->The axisartist toolkit->axisartist->axisartist with ParasiteAxes->import mpl_toolkits.axisartist as AA\nfrom mpl_toolkits.axes_grid1 import host_subplot\n\nhost = host_subplot(111, axes_class=AA.Axes)"
        ]
    },
    {
        "id": "158789",
        "GT": "Purpose in Life Pre and Post->ax = sns.barplot(x=\"WWID\", y=\"Pre_Q30\", hue=\"MatchSequence\",data=df,palette=\"husl\")\nsns.plt.ylim(0, 10)\nplt.legend(bbox_to_anchor=(1.04, 1),loc=1, borderaxespad=0.) ",
        "pred": [
            "sklearn->Examples->Generalized Linear Models->Comparing Linear Bayesian Regressors->Bayesian regressions with polynomial feature expansion->Plotting polynomial regressions with std errors of the scores->ax = (\n    data=full_data, x=\"input_feature\", y=\"target\", color=\"black\", alpha=0.75\n)\nax.plot(X_plot, y_plot, color=\"black\", label=\"Ground Truth\")\nax.plot(X_plot, y_brr, color=\"red\", label=\"BayesianRidge with polynomial features\")\nax.plot(X_plot, y_ard, color=\"navy\", label=\"ARD with polynomial features\")\nax.fill_between(\n    X_plot.ravel(),\n    y_ard - y_ard_std,\n    y_ard + y_ard_std,\n    color=\"navy\",\n    alpha=0.3,\n)\nax.fill_between(\n    X_plot.ravel(),\n    y_brr - y_brr_std,\n    y_brr + y_brr_std,\n    color=\"red\",\n    alpha=0.3,\n)\nax.legend()\n_ = ax.set_title(\"Polynomial fit of a non-linear feature\")\n\n\n<img alt=\"Polynomial fit of a non-linear feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_ard_003.png\" srcset=\"../../_images/sphx_glr_plot_ard_003.png\"/>",
            "statsmodels->Examples->State space models->VARMAX: Introduction->Example 1: VAR->ax = res.impulse_responses(10, orthogonalized=True, impulse=[1, 0]).plot(figsize=(13,3))\nax.set(xlabel='t', title='Responses to a shock to `dln_inv`');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_varmax_8_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_varmax_8_0.png\"/>",
            "statsmodels->Examples->Linear Regression Models->Rolling Least Squares->fig = rres.plot_recursive_coefficient(variables=[\"Mkt-RF\"], figsize=(14, 6))\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_rolling_ls_10_0.png\" src=\"../../../_images/examples_notebooks_generated_rolling_ls_10_0.png\"/>",
            "pandas_toms_blog->Time Series->Timeseries->Forecasting->ax = y.plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='Forecast', alpha=.7)\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_ylabel(\"Monthly Flights\")\nplt.legend()\nsns.despine()",
            "pandas_toms_blog->Time Series->Timeseries->Forecasting->ax = y.plot(label='observed')\npred_dy.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_dy_ci.index,\n                pred_dy_ci.iloc[:, 0],\n                pred_dy_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_ylabel(\"Monthly Flights\")\n\n# Highlight the forecast area\nax.fill_betweenx(ax.get_ylim(), pd.Timestamp('2013-01-01'), y.index[-1],\n                 alpha=.1, zorder=-1)\nax.annotate('Dynamic $\\\\longrightarrow$', (pd.Timestamp('2013-02-01'), 550))\n\nplt.legend()\nsns.despine()"
        ]
    },
    {
        "id": "1001878",
        "GT": "Scraping Notebook to Support Box Office Analysis->Scrape the movie gross Information from boxofficemojo.com->years = xrange(2009,2015)\npages = xrange(1,9)\nyear_pagetxt = {}\nfor year in years: \n    pagestext = {}\n    for page in pages: \n        r = requests.get(\"http://www.boxofficemojo.com/yearly/chart/?page=%s&view=releasedate&view2=domestic&yr=%s&p=.htm\"%(page, year))\n        pagestext[page] = r.text\n        time.sleep(1)\n    year_pagetxt[year] = pagestext\n    ",
        "pred": [
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Training the Network->hidden_dim = 64\ninput_dim = emb_matrix['memory'].shape[0]\nlearning_rate = 0.001\nepochs = 10\nparameters = initialise_params(hidden_dim,\n                               input_dim)\nv, s = initialise_mav(hidden_dim,\n                      input_dim,\n                      parameters)",
            "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor->quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "pandas_toms_blog->Fast Pandas->Constructors->files = glob.glob('weather/*.csv')\ncolumns = ['station', 'date', 'tmpf', 'relh', 'sped', 'mslp',\n           'p01i', 'vsby', 'gust_mph', 'skyc1', 'skyc2', 'skyc3']\n\n# init empty DataFrame, like you might for a list\nweather = pd.DataFrame(columns=columns)\n\nfor fp in files:\n    city = pd.read_csv(fp, columns=columns)\n    weather.append(city)",
            "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Log scalars->x = (-5, 5, 0.1).view(-1, 1)\ny = -5 * x + 0.1 * (x.size())\n\nmodel = (1, 1)\ncriterion = ()\noptimizer = (model.parameters(), lr = 0.1)\n\ndef train_model(iter):\n    for epoch in range(iter):\n        y1 = model(x)\n        loss = criterion(y1, y)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ntrain_model(10)\nwriter.flush()",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with categorical variables->names = ['group_a', 'group_b', 'group_c']\nvalues = [1, 10, 100]\n\nplt.figure(figsize=(9, 3))\n\nplt.subplot(131)\nplt.bar(names, values)\nplt.subplot(132)\nplt.scatter(names, values)\nplt.subplot(133)\nplt.plot(names, values)\nplt.suptitle('Categorical Plotting')\nplt.show()\n\n\n<img alt=\"Categorical Plotting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_006.png\" srcset=\"../../_images/sphx_glr_pyplot_006.png, ../../_images/sphx_glr_pyplot_006_2_0x.png 2.0x\"/>"
        ]
    },
    {
        "id": "985676",
        "GT": "kk.line_tidy(close_px['AAPL'])",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Stationarity and detrending (ADF/KPSS)->KPSS test->kpss_test(sunspots[\"SUNACTIVITY\"])",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Overriding elements of the seaborn styles->sns.axes_style()\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Overriding elements of the seaborn styles->sns.axes_style()\n",
            "seaborn->User guide and tutorial->Choosing color palettes->Qualitative color palettes->sns.color_palette()\n",
            "seaborn->Figure aesthetics->Choosing color palettes->Qualitative color palettes->sns.color_palette()\n",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->sns.set(style='ticks', context='paper')",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Conditioning on other variables->sns.displot(penguins, x=\"flipper_length_mm\", col=\"sex\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Conditioning on other variables->sns.displot(penguins, x=\"flipper_length_mm\", col=\"sex\")\n"
        ]
    },
    {
        "id": "15264",
        "GT": "1. Is the distribution of body temperatures normal?->sns.distplot(df['temperature'], bins=20, fit=norm)",
        "pred": [
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->sns.pairplot(iris, hue=\"species\", height=2.5)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->sns.pairplot(iris, hue=\"species\", height=2.5)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(tips, x=\"total_bill\", kind=\"kde\", cut=0)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(tips, x=\"total_bill\", kind=\"kde\", cut=0)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->sns.displot(tips, x=\"day\", shrink=.8)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Plotting univariate histograms->Choosing the bin size->sns.displot(tips, x=\"day\", shrink=.8)\n",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->sns.catplot(data=flights_wide, kind=\"box\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Wide-form data->sns.catplot(data=flights_wide, kind=\"box\")\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(diamonds, x=\"carat\", kde=True)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Kernel density estimation->Kernel density estimation pitfalls->sns.displot(diamonds, x=\"carat\", kde=True)\n"
        ]
    },
    {
        "id": "1425563",
        "GT": "*2. Was there a significant relationship between passenger class and survival?*-># Creating boxplots of fare paid by passenger survival status\n\nsns.boxplot(x='Survived',y='Fare',data=titanic_df_clean,fliersize=0,palette=['r','g'])\nplt.xticks([0,1],['Died','Survived'])\nplt.ylim(0,150)\nplt.xlabel('Survival status')\nplt.ylabel('Fare paid (USD)')\nplt.title(\"Chart 3: Boxplots of fare paid by survival status (excluding outliers)\")",
        "pred": [
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->Confidence interval-># Now create a bootstrap confidence interval around the a LOWESS fit\n\n\ndef lowess_with_confidence_bounds(\n    x, y, eval_x, N=200, conf_interval=0.95, lowess_kw=None\n):\n    \"\"\"\n    Perform Lowess regression and determine a confidence interval by bootstrap resampling\n    \"\"\"\n    # Lowess smoothing\n    smoothed = sm.nonparametric.lowess(exog=x, endog=y, xvals=eval_x, **lowess_kw)\n\n    # Perform bootstrap resamplings of the data\n    # and  evaluate the smoothing at a fixed set of points\n    smoothed_values = np.empty((N, len(eval_x)))\n    for i in range(N):\n        sample = np.random.choice(len(x), len(x), replace=True)\n        sampled_x = x[sample]\n        sampled_y = y[sample]\n\n        smoothed_values[i] = sm.nonparametric.lowess(\n            exog=sampled_x, endog=sampled_y, xvals=eval_x, **lowess_kw\n        )\n\n    # Get the confidence interval\n    sorted_values = np.sort(smoothed_values, axis=0)\n    bound = int(N * (1 - conf_interval) / 2)\n    bottom = sorted_values[bound - 1]\n    top = sorted_values[-bound]\n\n    return smoothed, bottom, top\n\n\n# Compute the 95% confidence interval\neval_x = np.linspace(0, 4 * np.pi, 31)\nsmoothed, bottom, top = lowess_with_confidence_bounds(\n    x, y, eval_x, lowess_kw={\"frac\": 0.1}\n)",
            "statsmodels->Examples->State space models->Unobserved Components: Application->Data-># Plot the data\nax = dta.plot(figsize=(13,3))\nylim = ax.get_ylim()\nax.xaxis.grid()\nax.fill_between(dates, ylim[0]+1e-5, ylim[1]-1e-5, recessions, facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\"/>",
            "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets-># Set up cluster parameters\n(figsize=(9 * 1.3 + 2, 14.5))\n(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n\n = [\n    (noisy_circles, {\"n_clusters\": 2}),\n    (noisy_moons, {\"n_clusters\": 2}),\n    (varied, {\"n_neighbors\": 2}),\n    (aniso, {\"n_neighbors\": 2}),\n    (blobs, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate():\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = ().fit_transform(X)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ward = (\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\n    )\n    complete = (\n        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\n    )\n    average = (\n        n_clusters=params[\"n_clusters\"], linkage=\"average\"\n    )\n    single = (\n        n_clusters=params[\"n_clusters\"], linkage=\"single\"\n    )\n\n    clustering_algorithms = (\n        (\"Single Linkage\", single),\n        (\"Average Linkage\", average),\n        (\"Complete Linkage\", complete),\n        (\"Ward Linkage\", ward),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = ()\n\n        # catch warnings related to kneighbors_graph\n        with ():\n            (\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \"  1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = ()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        (len(), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            (name, size=18)\n\n        colors = (\n            list(\n                (\n                    (\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        (X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        (-2.5, 2.5)\n        (-2.5, 2.5)\n        (())\n        (())\n        (\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\n()\n\n\n<img alt=\"Single Linkage, Average Linkage, Complete Linkage, Ward Linkage\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\"/>",
            "pandas_toms_blog->Indexes->Set Operations-># Bring in the flights data\n\nflights = pd.read_hdf('flights.h5', 'flights')\n\nweather_locs = weather.index.levels[0]\n# The `categories` attribute of a Categorical is an Index\norigin_locs = flights.origin.cat.categories\ndest_locs = flights.dest.cat.categories\n\nairports = weather_locs &amp; origin_locs &amp; dest_locs\nairports",
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data-># Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")"
        ]
    },
    {
        "id": "543699",
        "GT": "Predictions and Evaluations->pred = knn.predict(X_test)\nprint(confusion_matrix(y_test,pred))",
        "pred": [
            "sklearn->Examples->Ensemble methods->Monotonic Constraints->gbdt_with_monotonic_cst = (monotonic_cst=[1, -1])\ngbdt_with_monotonic_cst.fit(X, y)",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "sklearn->Examples->Clustering->Demo of affinity propagation clustering algorithm->Generate sample data->centers = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = (\n    n_samples=300, centers=centers, cluster_std=0.5, random_state=0\n)",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS non-linear curve but linear in parameters->res = sm.OLS(y, X).fit()\nprint(res.summary())",
            "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Calibration of the confidence interval->coverage_fraction(\n    y_test, all_models[\"q 0.05\"].predict(X_test), all_models[\"q 0.95\"].predict(X_test)\n)"
        ]
    },
    {
        "id": "1402926",
        "GT": "Let's plot interactive map visualitation inorder to see where the Hotels are located.\n* we are using the beautiful map visualization library called folium. (if you want to know more about folium library check out this link    https://media.readthedocs.org/pdf/folium/latest/folium.pdf )->#Loading the unique Hotel's information to plot them on the map\ntemp_df = df.drop_duplicates(['Hotel_Name'])\nlen(temp_df)",
        "pred": [
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)"
        ]
    },
    {
        "id": "1144631",
        "GT": "Calculating new columns->yahoo['day_range'] = yahoo['High'] - yahoo['Low']\nyahoo['intraday_return'] = (yahoo['Close'] - yahoo['Open'])/yahoo['Open']",
        "pred": [
            "pandas_toms_blog->Indexes->Flavors->Indexes for Alignment->dsm = weather.loc['DSM']\n\nhourly = dsm.resample('H').mean()\n\ntemp = hourly['tmpf'].sample(frac=.5, random_state=1).sort_index()\nsped = hourly['sped'].sample(frac=.5, random_state=2).sort_index()",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing long-form data->year = flights_avg.index\npassengers = flights_avg[\"passengers\"]\nsns.relplot(x=year, y=passengers, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing long-form data->year = flights_avg.index\npassengers = flights_avg[\"passengers\"]\nsns.relplot(x=year, y=passengers, kind=\"line\")\n",
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()",
            "statsmodels->Examples->State space models->Trends and cycles in unemployment->from pandas_datareader.data import DataReader\nendog = DataReader('UNRATE', 'fred', start='1954-01-01')\nendog.index.freq = endog.index.inferred_freq"
        ]
    },
    {
        "id": "313536",
        "GT": "Using Python-GMT->import gmt\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors as mcolors\n\nimport collections",
        "pred": [
            "seaborn->User guide and tutorial->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "matplotlib->Tutorials->Intermediate->Styling with cycler->from cycler import cycler\nimport numpy as np\nimport matplotlib.pyplot as plt",
            "statsmodels->Examples->Time Series Analysis->Deterministic Terms->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nplt.rc(\"figure\", figsize=(16, 9))\nplt.rc(\"font\", size=16)",
            "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Classes of colormaps->import numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom colorspacious import cspace_converter",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(9876789)"
        ]
    },
    {
        "id": "58274",
        "GT": "Your name:\n\n<pre> Joan Soo Li Lim </pre>\n\n### Collaborators:\n\n<pre> None </pre>->import matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport math\nimport operator",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(9876789)",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.tsa.arima.model import ARIMA",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "matplotlib->Tutorials->Introductory->Image tutorial->Startup commands->import matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image"
        ]
    },
    {
        "id": "50548",
        "GT": "import nltk\n#list to store the stop words\nstopwords_list_570 = []\n# all the stopwords are stored in a text file\nwith open('./stopwords_en.txt') as f:\n    stopwords_list_570 = f.read().splitlines()",
        "pred": [
            "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets->import time\nimport warnings\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster, \nfrom sklearn.preprocessing import \nfrom itertools import , \n\n(0)",
            "torch->PyTorch Recipes->Changing default device->import torch\n\nUSE_CUDA = False\n\n = (20, 30)\nif USE_CUDA:\n    ()\n\ndevice = 'cpu'\nif USE_CUDA:\n    device = 'cuda'\n = (128, 20, device=device)\nprint(().device)",
            "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->4. Test dynamic quantization->import torch.quantization\n\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {, }, dtype=\n)\nprint(quantized_model)",
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset->from sklearn.datasets import \n\nco2 = (data_id=41187, as_frame=True, parser=\"pandas\")\nco2.frame.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "sklearn->Examples->Feature Selection->Model-based and sequential feature selection->Loading the data->from sklearn.datasets import \n\ndiabetes = ()\nX, y = diabetes.data, diabetes.target\nprint(diabetes.DESCR)"
        ]
    },
    {
        "id": "967198",
        "GT": "Making predictions on the test set data->a_preds = model1.predict_proba(a_test)\n\nb_preds = model2.predict_proba(b_test)\n\nc_preds = model3.predict_proba(c_test)",
        "pred": [
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding->training_labels = one_hot_encoding(y_train[:training_sample])\ntest_labels = one_hot_encoding(y_test[:test_sample])",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA->sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()",
            "torch->PyTorch Recipes->Saving and loading multiple models in one file using PyTorch->Steps->3. Initialize the optimizer->optimizerA = (netA.parameters(), lr=0.001, momentum=0.9)\noptimizerB = (netB.parameters(), lr=0.001, momentum=0.9)",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Accuracy vs alpha for training and testing sets->train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = ()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\n()\n\n\n<img alt=\"Accuracy vs alpha for training and testing sets\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\" srcset=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\"/>"
        ]
    },
    {
        "id": "1438081",
        "GT": "Heatmap of revenue depending on application type and customer age->Select data from the customers table->df1 = pd.DataFrame ({\n        'Age' : customers['Field age'],\n        'Customer id' : customers['Field customer id']\n    })",
        "pred": [
            "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures->summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset->import pandas as pd\n\nco2_data = co2.frame\nco2_data[\"date\"] = (co2_data[[\"year\", \"month\", \"day\"]])\nco2_data = co2_data[[\"date\", \"co2\"]].set_index(\"date\")\nco2_data.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ]
    },
    {
        "id": "402972",
        "GT": "Check for Duplicate Index Values\nThis is a good time to check for duplicate index values since we have a DateTime index.\n\n`df.index.duplicated()` generates a True/False array, where True means that row has a duplicate index.->print(\"Size before dropping dubs =\", df.size)\ndf = df[~df.index.duplicated(keep='first')]\nprint(\"Size after dropping dubs =\", df.size)",
        "pred": [
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->means25[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.25)\nprint(means25)",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "pandas_toms_blog->Indexes->Merging->The merge version->m = pd.merge(flights, daily.reset_index().rename(columns={'date': 'fl_date', 'station': 'origin'}),\n             on=['fl_date', 'origin']).set_index(idx_cols).sort_index()\n\nm.head()"
        ]
    },
    {
        "id": "334960",
        "GT": "Building a model with statsmodel and sklearn [10 minutes]\n\nNow that we can concretely fit the training data from scratch, let's learn two Python packages to do it all for us: [statsmodels](http://www.statsmodels.org/stable/regression.html) and [scikit-learn (sklearn)](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).  Our goal  is to show how to implement simple linear regression with these packages.  For an important sanity check, we compare the $\\beta$ values from statsmodel and sklearn to the $\\beta$ values that we found from above from scratch.\n\nFor the purposes of this lab, statsmodels and sklearn do the same thing.  More generally though, statsmodels tends to be easier for inference, whereas sklearn has machine-learning algorithms and is better for prediction.->#build the least squares model\ntoyregr_skl = linear_model.LinearRegression()\n#save regression info (parameters, etc) in results_skl\nresults_skl = toyregr_skl.fit(x_train,y_train)\n#pull the beta parameters out from results_skl\nbeta0_skl = toyregr_skl.intercept_\nbeta1_skl = toyregr_skl.coef_[0]\n\nprint(\"(beta0, beta1) = (%f, %f)\" %(beta0_skl, beta1_skl))",
        "pred": [
            "sklearn->Examples->Generalized Linear Models->Lasso on dense and sparse data->Comparing the two Lasso implementations on Sparse data-># make a copy of the previous data\nXs = X.copy()\n# make Xs sparse by replacing the values lower than 2.5 with 0s\nXs[Xs &lt; 2.5] = 0.0\n# create a copy of Xs in sparse format\nXs_sp = (Xs)\nXs_sp = Xs_sp.tocsc()\n\n# compute the proportion of non-zero coefficient in the data matrix\nprint(f\"Matrix density : {(Xs_sp.nnz / float(X.size) * 100):.3f}%\")\n\nalpha = 0.1\nsparse_lasso = (alpha=alpha, fit_intercept=False, max_iter=10000)\ndense_lasso = (alpha=alpha, fit_intercept=False, max_iter=10000)\n\nt0 = ()\nsparse_lasso.fit(Xs_sp, y)\nprint(f\"Sparse Lasso done in {(() - t0):.3f}s\")\n\nt0 = ()\ndense_lasso.fit(Xs, y)\nprint(f\"Dense Lasso done in  {(() - t0):.3f}s\")\n\n# compare the regression coefficients\ncoeff_diff = (sparse_lasso.coef_ - dense_lasso.coef_)\nprint(f\"Distance between coefficients : {coeff_diff:.2e}\")",
            "matplotlib->Tutorials->Advanced->Faster rendering by using blitting->Class-based example-># make a new figure\nfig, ax = plt.subplots()\n# add a line\n(ln,) = ax.plot(x, np.sin(x), animated=True)\n# add a frame number\nfr_number = ax.annotate(\n    \"0\",\n    (0, 1),\n    xycoords=\"axes fraction\",\n    xytext=(10, -10),\n    textcoords=\"offset points\",\n    ha=\"left\",\n    va=\"top\",\n    animated=True,\n)\nbm = BlitManager(fig.canvas, [ln, fr_number])\n# make sure our window is on the screen and drawn\nplt.show(block=False)\nplt.pause(.1)\n\nfor j in range(100):\n    # update the artists\n    ln.set_ydata(np.sin(x + (j / 100) * np.pi))\n    fr_number.set_text(\"frame: {j}\".format(j=j))\n    # tell the blitting manager to do its thing\n    bm.update()\n\n\n<img alt=\"blitting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_blitting_002.png\" srcset=\"../../_images/sphx_glr_blitting_002.png, ../../_images/sphx_glr_blitting_002_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds-># range of admissible distortions\neps_range = (0.1, 0.99, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = (1, 9, 9)\n\n()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = (n_samples_range, eps=eps)\n    (n_samples_range, min_n_components, color=color)\n\n([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n(\"Number of observations to eps-embed\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_samples vs n_components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\"/>",
            "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation-># plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>",
            "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->2. Writing to TensorBoard-># get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# create grid of images\nimg_grid = torchvision.utils.make_grid(images)\n\n# show images\nmatplotlib_imshow(img_grid, one_channel=True)\n\n# write to tensorboard\nwriter.add_image('four_fashion_mnist_images', img_grid)"
        ]
    },
    {
        "id": "1421575",
        "GT": "Hospital data by day of week->dfweekdaytot = dftot\ndfweekdaytot['Day'] = dftot.index.weekday",
        "pred": [
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "statsmodels->Examples->User Notes->Dates in Time-Series Models->Using Pandas->data.endog.index = dates\nendog = data.endog\nendog",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->SettingWithCopy->f = pd.DataFrame({'a':[1,2,3,4,5], 'b':[10,20,30,40,50]})\nf",
            "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Load the data->diabetes = ()\nX, y = diabetes.data, diabetes.target",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->nobs = glm_mod.nobs\ny = glm_mod.model.endog\nyhat = glm_mod.mu"
        ]
    },
    {
        "id": "287224",
        "GT": "fig , ax = plt.subplots(figsize=(12,5))\nax.bar(range(x.shape[0]), absolute_deviation)\nax.set_title(\"Absolute deviation in trip time from tracks & trackspoints\")\n#ax.set_xlim((min(absolute_deviation), max(absolute_deviation)))\nplt.show();",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Exercise: Logit vs Probit->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nsupport = np.linspace(-6, 6, 1000)\nax.plot(support, stats.logistic.cdf(support), \"r-\", label=\"Logistic\")\nax.plot(support, stats.norm.cdf(support), label=\"Probit\")\nax.legend()",
            "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money->fig, ax = plt.subplots(figsize=(13, 3))\n\nax.plot(m2_ewma, label=\"M2 Growth (EWMA)\")\nax.plot(cpi_ewma, label=\"CPI Inflation (EWMA)\")\nax.legend()",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Exercise: Logit vs Probit->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nsupport = np.linspace(-6, 6, 1000)\nax.plot(support, stats.logistic.pdf(support), \"r-\", label=\"Logistic\")\nax.plot(support, stats.norm.pdf(support), label=\"Probit\")\nax.legend()",
            "matplotlib->Tutorials->Intermediate->Legend guide->Controlling the legend entries->fig, ax = plt.subplots()\nline_up, = ax.plot([1, 2, 3], label='Line 2')\nline_down, = ax.plot([3, 2, 1], label='Line 1')\nax.legend(handles=[line_up, line_down])",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()"
        ]
    },
    {
        "id": "389299",
        "GT": "Repeating these same plots and operations, for a DataFrame that shows the Month as the column.->dayMonth = df.groupby(by=['Dayofweek','Month']).count()['Reason'].unstack()\ndayMonth.head()",
        "pred": [
            "statsmodels->Examples->User Notes->Formulas->OLS regression using formulas->df = dta.data[[\"Lottery\", \"Literacy\", \"Wealth\", \"Region\"]].dropna()\ndf.head()",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first = df.groupby('airline_id')[['fl_date', 'unique_carrier']].first()\nfirst.head()"
        ]
    },
    {
        "id": "1226615",
        "GT": "Relating the subgoups using graph abstraction->theta = np.radians(150)\nc, s = np.cos(theta), np.sin(theta)\nr = np.array([[c, -s], [s, c]])\npos = r.dot(pos.T).T",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult->arparams = np.array([1, 0.35, -0.15, 0.55, 0.1])\nmaparams = np.array([1, 0.65])\narma_t = ArmaProcess(arparams, maparams)\narma_t.isstationary",
            "matplotlib->Tutorials->Intermediate->Styling with cycler->x = np.linspace(0, 2 * np.pi, 50)\noffsets = np.linspace(0, 2 * np.pi, 4, endpoint=False)\nyy = np.transpose([np.sin(x + phi) for phi in offsets])",
            "scipy->Signal Processing (scipy.signal)->Detrend->t = np.linspace(-10, 10, 20)\n y = 1 + t + 0.01*t**2\n yconst = signal.detrend(y, type='constant')\n ylin = signal.detrend(y, type='linear')",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Huber\u2019s t->t = 1.345\nsupport = np.linspace(-3 * t, 3 * t, 1000)\nhuber = norms.HuberT(t=t)\nplot_weights(support, huber.weights, [\"-3*t\", \"0\", \"3*t\"], [-3 * t, 0, 3 * t])",
            "scipy->Statistics (scipy.stats)->Kernel density estimation->Univariate estimation->x1 = np.array([-7, -5, 1, 4, 5], dtype=np.float64)\n kde1 = stats.gaussian_kde(x1)\n kde2 = stats.gaussian_kde(x1, bw_method='silverman')"
        ]
    },
    {
        "id": "548289",
        "GT": "Is the true population mean really  $98.6^{\\circ} F$?->scipy.stats.ttest_1samp(temp, 98.6, axis=0)",
        "pred": [
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators->sm.robust.mad(fat_tails, c=stats.t(6).ppf(0.75))",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->affair_mod.model.cdf(affair_mod.fittedvalues[1000])",
            "numpy->NumPy Features->Saving and sharing your NumPy arrays->Save your arrays with NumPy\u2019s->np.savez(\"x_y-squared.npz\", x_axis=x, y_axis=y)",
            "seaborn->User guide and tutorial->Choosing color palettes->Sequential color palettes->Sequential \u201ccubehelix\u201d palettes->sns.cubehelix_palette(start=.5, rot=-.75, as_cmap=True)\n",
            "seaborn->Figure aesthetics->Choosing color palettes->Sequential color palettes->Sequential \u201ccubehelix\u201d palettes->sns.cubehelix_palette(start=.5, rot=-.75, as_cmap=True)\n",
            "seaborn->User guide and tutorial->Choosing color palettes->Sequential color palettes->Sequential \u201ccubehelix\u201d palettes->sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True)\n",
            "seaborn->Figure aesthetics->Choosing color palettes->Sequential color palettes->Sequential \u201ccubehelix\u201d palettes->sns.cubehelix_palette(start=.5, rot=-.5, as_cmap=True)\n"
        ]
    },
    {
        "id": "991195",
        "GT": "In which months do films with Tom Cruise tend to be released in the USA?->release_dates.head()\npd.Series(pd.DatetimeIndex(pd.merge(\\\n    cast.loc[cast.name == 'Tom Cruise'].loc[:,['title']],\\\n    release_dates.loc[release_dates.country == 'USA'],\\\n    on='title').date).month).value_counts().sort_index().plot.bar()",
        "pred": [
            "torch->Extending PyTorch->Extending TorchScript with Custom C++ Operators->Using the TorchScript Custom Operator in C++->find_package(OpenCV REQUIRED)\nadd_library(warp_perspective SHARED op.cpp)\ntarget_compile_features(warp_perspective PRIVATE cxx_range_for)\ntarget_link_libraries(warp_perspective PRIVATE \"${TORCH_LIBRARIES}\")\ntarget_link_libraries(warp_perspective PRIVATE opencv_core opencv_photo)",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->make_plot(dta.index[idx[:5]])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\" src=\"../../../_images/examples_notebooks_generated_pca_fertility_factors_24_0.png\"/>",
            "sklearn->Tutorials->An introduction to machine learning with scikit-learn->Loading an example dataset->digits.images[0]\narray([[  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.],\n       [  0.,   0.,  13.,  15.,  10.,  15.,   5.,   0.],\n       [  0.,   3.,  15.,   2.,   0.,  11.,   8.,   0.],\n       [  0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.],\n       [  0.,   5.,   8.,   0.,   0.,   9.,   8.,   0.],\n       [  0.,   4.,  11.,   0.,   1.,  12.,   7.,   0.],\n       [  0.,   2.,  14.,   5.,  10.,  12.,   0.,   0.],\n       [  0.,   0.,   6.,  13.,  10.,   0.,   0.,   0.]])",
            "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Decision path->node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )",
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n"
        ]
    },
    {
        "id": "225031",
        "GT": "Project 5: NLP on Financial Statements\n## Instructions\nEach problem consists of a function to implement and instructions on how to implement the function.  The parts of the function that need to be implemented are marked with a `# TODO` comment. After implementing the function, run the cell to test it against the unit tests we've provided. For each problem, we provide one or more unit tests from our `project_tests` package. These unit tests won't tell you if your answer is correct, but will warn you of any major errors. Your code will be checked for the correct solution when you submit it to Udacity.\n\n## Packages\nWhen you implement the functions, you'll only need to you use the packages you've used in the classroom, like [Pandas](https://pandas.pydata.org/) and [Numpy](http://www.numpy.org/). These packages will be imported for you. We recommend you don't add any import statements, otherwise the grader might not be able to run your code.\n\nThe other packages that we're importing are `project_helper` and `project_tests`. These are custom packages built to help you solve the problems.  The `project_helper` module contains utility functions and graph functions. The `project_tests` contains the unit tests for all the problems.\n\n### Install Packages->Get 10ks\nWe'll be running NLP analysis on 10-k documents. To do that, we first need to download the documents. For this project, we'll download 10-ks for a few companies. To lookup documents for these companies, we'll use their CIK. If you would like to run this against other stocks, we've provided the dict `additional_cik` for more stocks. However, the more stocks you try, the long it will take to run.->Get list of 10-ks\nThe SEC has a limit on the number of calls you can make to the website per second. In order to avoid hiding that limit, we've created the `SecAPI` class. This will cache data from the SEC and prevent you from going over the limit.->sec_api = project_helper.SecAPI()",
        "pred": [
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->textproc = TextPreprocess()",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Scaling plot elements->sns.set_theme()\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Scaling plot elements->sns.set_theme()\n",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->pca_model = PCA(dta.T, standardize=False, demean=True)",
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time.head()"
        ]
    },
    {
        "id": "836642",
        "GT": "Installation\nFor those of you still here, lets start:\n\nMatplotlib relies on a library called Basemap to plot geographic data on a map. you can see its documentation here:\nhttps://matplotlib.org/basemap/users/intro.html\n\nBasemap is quite difficult to install. Fortunately I've scoured the internet and found this command:\n\n__mac & linux__\n\n`conda install basemap`\n\nIf the above fails:\n\n__mac:__\n\n`brew install geos`\n\n`sudo -H pip3 install https://github.com/matplotlib/basemap/archive/v1.1.0.tar.gz`\n\n__linux:__\n\n`sudo -H pip3 install https://github.com/matplotlib/basemap/archive/v1.1.0.tar.gz`\n\n\n__windows:__\n\nplease consult me if you can't find a good solution online. sorry :p\n\n\n\nAfter you've run that in the terminal you can import it in python!->import pandas as pd #Python's data manipulation library\nimport matplotlib.pyplot as plt #Python's graphing library\nfrom mpl_toolkits.basemap import Basemap #The geomapping library built ontop of matplotlib",
        "pred": [
            "matplotlib->Tutorials->Toolkits->The axisartist toolkit->axisartist->import mpl_toolkits.axisartist as AA\nfig = plt.figure()\nfig.add_axes([0.1, 0.1, 0.8, 0.8], axes_class=AA.Axes)",
            "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "torch->PyTorch Recipes->Loading data in PyTorch->5. [Optional] Visualize the data->import matplotlib.pyplot as plt\n\nprint(data[0][0].numpy())\n\nplt.figure()\nplt.plot(waveform.t().numpy())",
            "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation->import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)",
            "statsmodels->Examples->User Notes->Prediction->import numpy as np\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)"
        ]
    },
    {
        "id": "391995",
        "GT": "Exploratory Data Analysis\n\n** Histogram of two FICO distributions on top of each other, one for each credit.policy outcome.**->plt.figure(figsize=(10,6))\ndf[df['credit.policy'] == 0]['fico'].hist(bins=30, alpha=0.5,label='Credit.Policy=0', color='blue')\ndf[df['credit.policy'] == 1]['fico'].hist(bins=30, alpha=0.5, label='Credit.Policy=1', color='red')\nplt.legend()\nplt.xlabel('FICO')",
        "pred": [
            "statsmodels->Examples->Statistics->ANOVA->plt.figure(figsize=(6, 6))\nsymbols = [\"D\", \"^\"]\ncolors = [\"r\", \"g\", \"blue\"]\nfactor_groups = salary_table.groupby([\"E\", \"M\"])\nfor values, group in factor_groups:\n    i, j = values\n    plt.scatter(group[\"X\"], group[\"S\"], marker=symbols[j], color=colors[i - 1], s=144)\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "torch->Image and Video->Adversarial Example Generation->Results->Accuracy vs Epsilon->plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n\n<img alt=\"Accuracy vs Epsilon\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_001.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_001.png\"/>",
            "torch->Reinforcement Learning->Reinforcement Learning (PPO) with TorchRL Tutorial->Results->plt.figure(figsize=(10, 10))\nplt.subplot(2, 2, 1)\nplt.plot(logs[\"reward\"])\nplt.title(\"training rewards (average)\")\nplt.subplot(2, 2, 2)\nplt.plot(logs[\"step_count\"])\nplt.title(\"Max step count (training)\")\nplt.subplot(2, 2, 3)\nplt.plot(logs[\"eval reward (sum)\"])\nplt.title(\"Return (test)\")\nplt.subplot(2, 2, 4)\nplt.plot(logs[\"eval step_count\"])\nplt.title(\"Max step count (test)\")\nplt.show()\n\n\n<img alt=\"training rewards (average), Max step count (training), Return (test), Max step count (test)\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_reinforcement_ppo_001.png\" srcset=\"../_images/sphx_glr_reinforcement_ppo_001.png\"/>",
            "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Customizing the plot->plt.rcParams.update({'figure.autolayout': True})\n\nfig, ax = plt.subplots()\nax.barh(group_names, group_data)\nlabels = ax.get_xticklabels()\nplt.setp(labels, rotation=45, horizontalalignment='right')\n\n\n<img alt=\"lifecycle\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_lifecycle_006.png\" srcset=\"../../_images/sphx_glr_lifecycle_006.png, ../../_images/sphx_glr_lifecycle_006_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->rcParams->plt.rcParams['figure.constrained_layout.use'] = True\nfig, axs = plt.subplots(2, 2, figsize=(3, 3))\nfor ax in axs.flat:\n    example_plot(ax)\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_018.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_018.png, ../../_images/sphx_glr_constrainedlayout_guide_018_2_0x.png 2.0x\"/>"
        ]
    },
    {
        "id": "1492338",
        "GT": "# Apply PCA with two components\npca = PCA(n_components=2, random_state=2)\n\n# Fit and transform the scaled data\nreduced_data = pca.fit_transform(X_train_scaled)\n# Fit and transform the sample data\nreduced_samples = pca.transform(samples_scaled)\n\n# Create a DataFrame for the reduced data\nreduced_data_df = pd.DataFrame(reduced_data, columns = [\"PCA 1\", \"PCA 2\"])",
        "pred": [
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Implementation in Statsmodels->Local level model-># Construct a local level model for inflation\nmod = sm.tsa.UnobservedComponents(dta.infl, 'llevel')\n\n# Fit the model's parameters (sigma2_varepsilon and sigma2_eta)\n# via maximum likelihood\nres = mod.fit()\nprint(res.params)\n\n# Create simulation smoother objects\nsim_kfs = mod.simulation_smoother()              # default method is KFS\nsim_cfa = mod.simulation_smoother(method='cfa')  # can specify CFA method",
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Implementation in Statsmodels->Local level model-># Plot the inflation data along with simulated trends\nfig, axes = plt.subplots(2, figsize=(15, 6))\n\n# Plot data and KFS simulations\ndta.infl.plot(ax=axes[0], color='k')\naxes[0].set_title('Simulations based on KFS approach, MLE parameters')\nsimulated_state_kfs.plot(ax=axes[0], color='C0', alpha=0.25, legend=False)\n\n# Plot data and CFA simulations\ndta.infl.plot(ax=axes[1], color='k')\naxes[1].set_title('Simulations based on CFA approach, MLE parameters')\nsimulated_state_cfa.plot(ax=axes[1], color='C0', alpha=0.25, legend=False)\n\n# Add a legend, clean up layout\nhandles, labels = axes[0].get_legend_handles_labels()\naxes[0].legend(handles[:2], ['Data', 'Simulated state'])\nfig.tight_layout();\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_tvpvar_mcmc_cfa_9_0.png\"/>",
            "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Comparing kernel functions->The available kernel functions on three data points-># Create three equidistant points\ndata = np.linspace(-1, 1, 3)\nkde = sm.nonparametric.KDEUnivariate(data)\n\n# Create a figure\nfig = plt.figure(figsize=(12, 5))\n\n# Enumerate every option for the kernel\nfor i, kernel in enumerate(kernel_switch.keys()):\n\n    # Create a subplot, set the title\n    ax = fig.add_subplot(3, 3, i + 1)\n    ax.set_title('Kernel function \"{}\"'.format(kernel))\n\n    # Fit the model (estimate densities)\n    kde.fit(kernel=kernel, fft=False, gridsize=2 ** 10)\n\n    # Create the plot\n    ax.plot(kde.support, kde.density, lw=3, label=\"KDE from samples\", zorder=10)\n    ax.scatter(data, np.zeros_like(data), marker=\"x\", color=\"red\")\n    plt.grid(True, zorder=-5)\n    ax.set_xlim([-3, 3])\n\nplt.tight_layout()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_24_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_24_0.png\"/>",
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression-># Generate data looking like cosine\nx = np.random.uniform(0, 4 * np.pi, size=200)\ny = np.cos(x) + np.random.random(size=len(x))\n\n# Compute a lowess smoothing of the data\nsmoothed = sm.nonparametric.lowess(exog=x, endog=y, frac=0.2)",
            "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime-># Input to the model\nx = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\ntorch_out = torch_model(x)\n\n# Export the model\n(torch_model,               # model being run\n                  x,                         # model input (or a tuple for multiple inputs)\n                  \"super_resolution.onnx\",   # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  opset_version=10,          # the ONNX version to export the model to\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\n                  input_names = ['input'],   # the model's input names\n                  output_names = ['output'], # the model's output names\n                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n                                'output' : {0 : 'batch_size'}})"
        ]
    },
    {
        "id": "413568",
        "GT": "7. Time taken to fund loans\nThe loan disbursal process works like this in Kiva: the field agent disburses the loan to the borrower at the **disbursed_time**. The agent then posts the loan to Kiva at the **posted_time**. Lenders are then able to see the loan on Kiva and make contributions towards it. The time at which the loan has been completely funded is the **funded_time**\n\nWe can then consider the time taken to completed fund a loan to be **funded_time** - **posted_time**. There is only one case where the posted time is 17 days greater than the funded time. This is probably an error which I need to look into further. (**UPDATE** - The process I described above, where the field agent issues the funds to the borrower before posting it on Kiva is called *Pre-Disbursal* and although it is followed for *most* loans, it is not necessarily followed for all loans)\n\n- The longest time it took for a loan to get funded was 1 year and 2 months\n- Most of the loans are funded within a 100 days. Only 0.1% of all loans in the entire sample take more than a 100 days to be funded->time_to_fund = (loans_df.funded_time - loans_df.posted_time)\ntime_to_fund_in_days = (time_to_fund.astype('timedelta64[s]')/(3600 * 24))\nloans_df = loans_df.assign(time_to_fund=time_to_fund)\nloans_df = loans_df.assign(time_to_fund_in_days=time_to_fund_in_days)\n",
        "pred": [
            "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()->pixel_intensity_distribution = ndimage.histogram(\n    xray_image, min=np.min(xray_image), max=np.max(xray_image), bins=256\n)\n\nplt.plot(pixel_intensity_distribution)\nplt.title(\"Pixel intensity distribution\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\" src=\"../_images/00c12e163a30744c62700ec414e2d514c5c4eae10256e7cd0cf87553d0f4d34c.png\"/>",
            "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: frequentist approach->t_stat_uncorrected = (differences) / ((differences, ddof=1) / n)\np_val_uncorrected = .sf(np.abs(t_stat_uncorrected), df)\n\nprint(\n    f\"Uncorrected t-value: {t_stat_uncorrected:.3f}\\n\"\n    f\"Uncorrected p-value: {p_val_uncorrected:.3f}\"\n)",
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Building the dataset->pollutant_data = np.loadtxt(\"air-quality-data.csv\", dtype=float, delimiter=\",\",\n                            skiprows=1, usecols=range(1, 8))\npollutants_A = pollutant_data[:, 0:5]\npollutants_B = pollutant_data[:, 5:]\n\nprint(pollutants_A.shape)\nprint(pollutants_B.shape)",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Periodic spline features->hour_df = (\n    (0, 26, 1000).reshape(-1, 1),\n    columns=[\"hour\"],\n)\nsplines = periodic_spline_transformer(24, n_splines=12).fit_transform(hour_df)\nsplines_df = (\n    splines,\n    columns=[f\"spline_{i}\" for i in range(splines.shape[1])],\n)\n([hour_df, splines_df], axis=\"columns\").plot(x=\"hour\", cmap=plt.cm.tab20b)\n_ = (\"Periodic spline-based encoding for the 'hour' feature\")\n\n\n<img alt=\"Periodic spline-based encoding for the 'hour' feature\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_005.png\" srcset=\"../../_images/sphx_glr_plot_cyclical_feature_engineering_005.png\"/>",
            "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->4. Post-training static quantization->per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\nper_channel_quantized_model.eval()\nper_channel_quantized_model.fuse_model()\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nper_channel_quantized_model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\nprint(per_channel_quantized_model.qconfig)\n\ntorch.ao.quantization.prepare(per_channel_quantized_model, inplace=True)\nevaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches)\ntorch.ao.quantization.convert(per_channel_quantized_model, inplace=True)\ntop1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\ntorch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file)"
        ]
    },
    {
        "id": "197186",
        "GT": "Notes on Louvain performance\n\n* Much faster than other modularity - based methods->Notes on walk trap method performance\n\n* Very fast, though slower than 'fast greedy' approach\n* Relatively accurate->from sklearn.cluster import spectral_clustering\n\nobj = spectral_clustering(np.matrix(A.data), 5)",
        "pred": [
            "sklearn->Examples->Model Selection->Plotting Cross-Validated Predictions->from sklearn.model_selection import \n\ny_pred = (lr, X, y, cv=10)",
            "sklearn->Examples->Model Selection->Plotting Learning Curves and Checking Models' Scalability->from sklearn.model_selection import \n\ny_pred = (lr, X, y, cv=10)",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->from sklearn.model_selection import \n\nX_train, X_test, y_train, y_test = (X, y, random_state=42)",
            "sklearn->Examples->Generalized Linear Models->Non-negative least squares->from sklearn.model_selection import \n\nX_train, X_test, y_train, y_test = (X, y, test_size=0.5)",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->from statsmodels.formula.api import glm\n\nglm_mod = glm(formula, dta, family=sm.families.Binomial()).fit()",
            "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Plot the most uncertain predictions->from scipy import stats\n\npred_entropies = (lp_model.label_distributions_.T)"
        ]
    },
    {
        "id": "304587",
        "GT": "Saving a Data Frame\n<a id='SavingDataFrame'></a>\nThe `to_csv` method can be used to save a `DataFrame`.\n\nIn theory, this command can be used to save the data as *any* filetype. \n\nOnly files that can be opened as unformatted text seperated by a text delimiter (default = , comma) will be able to be read.\n\nFunction arguments may be used to format the output file.\n\nAs an example, let's store the new, resampled data set, `monthlyrain` to a .csv, .txt, and .xls file:->monthlyrain.to_csv('sample_data/rotterdam_monthly_rainfall_2012.csv')\nmonthlyrain.to_csv('sample_data/rotterdam_monthly_rainfall_2012.txt')\n\n\nmonthlyrain.to_excel('sample_data/rotterdam_monthly_rainfall_2012.xls')\n",
        "pred": [
            "pandas_toms_blog->Indexes->Merging->The merge version->m = pd.merge(flights, daily.reset_index().rename(columns={'date': 'fl_date', 'station': 'origin'}),\n             on=['fl_date', 'origin']).set_index(idx_cols).sort_index()\n\nm.head()",
            "torch->PyTorch Recipes->PyTorch Benchmark->Steps->5. Comparing benchmark results->compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Sampling->rng = default_rng()\n\nbefore_sample = rng.choice(before_lock, size=30, replace=False)\nafter_sample = rng.choice(after_lock, size=30, replace=False)",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()"
        ]
    },
    {
        "id": "343435",
        "GT": "Exploratory Data Analysis->plt.subplots(figsize=(10,7))\nsns.heatmap(df1.corr(), annot=True, cmap='viridis')",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Time Series Filters->Christiano-Fitzgerald approximate band-pass filter: Inflation and Unemployment->fig = plt.figure(figsize=(14, 10))\nax = fig.add_subplot(111)\ncf_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "statsmodels->Examples->Time Series Analysis->Time Series Filters->Baxter-King approximate band-pass filter: Inflation and Unemployment->Explore the hypothesis that inflation and unemployment are counter-cyclical.->fig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111)\nbk_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->sns.jointplot(x='carat', y='price', data=df, size=8, alpha=.25,\n              color='k', marker='.')\nplt.tight_layout()",
            "matplotlib->Tutorials->Intermediate->Artist tutorial->Object containers->Axes container->ax = fig.add_subplot()\nrect = ax.patch  # a Rectangle instance\nrect.set_facecolor('green')",
            "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)->fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)"
        ]
    },
    {
        "id": "1282212",
        "GT": "Exploring the Titanic Dataset->Setting up->import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nplt.style.use('ggplot')",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\n\nnp.random.seed(9876789)",
            "sklearn->Examples->Decision Trees->Understanding the decision tree structure->import numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.model_selection import \nfrom sklearn.datasets import \nfrom sklearn.tree import \nfrom sklearn import tree",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.tsa.arima.model import ARIMA"
        ]
    },
    {
        "id": "934165",
        "GT": "Cleaning Data->del movies['color']\ndel movies['num_critic_for_reviews']\ndel movies['director_facebook_likes']\ndel movies['actor_3_facebook_likes']\ndel movies['actor_1_facebook_likes']\ndel movies['genres']\ndel movies['num_user_for_reviews']\ndel movies['language']\ndel movies['country']\ndel movies['actor_2_facebook_likes']\ndel movies['aspect_ratio']\ndel movies['movie_facebook_likes']\ndel movies['num_voted_users']\ndel movies['cast_total_facebook_likes']\ndel movies['plot_keywords']\ndel movies['movie_imdb_link']\nmovies.head()",
        "pred": [
            "torch->Model Optimization->torch.compile Tutorial->Demonstrating Speedups->eager eval time 0: 0.009543840408325194\neager eval time 1: 0.008958080291748046\neager eval time 2: 0.0089717435836792\neager eval time 3: 0.008945695877075195\neager eval time 4: 0.008947936058044434\neager eval time 5: 0.008927264213562013\neager eval time 6: 0.008937503814697266\neager eval time 7: 0.008948351860046387\neager eval time 8: 0.008941663742065429\neager eval time 9: 0.00894159984588623\n~~~~~~~~~~\ncompile eval time 0: 0.009375519752502441\ncompile eval time 1: 0.009378111839294434\ncompile eval time 2: 0.009247039794921875\ncompile eval time 3: 0.009245856285095215\ncompile eval time 4: 0.008823455810546875\ncompile eval time 5: 0.00714246416091919\ncompile eval time 6: 0.007155712127685547\ncompile eval time 7: 0.007184415817260742\ncompile eval time 8: 0.007147552013397217\ncompile eval time 9: 0.007140128135681152\n~~~~~~~~~~\n(eval) eager median: 0.008946815967559814, compile median: 0.00800393581390381, speedup: 1.1178020633321555x\n~~~~~~~~~~",
            "torch->Model Optimization->torch.compile Tutorial->Demonstrating Speedups->eager train time 0: 0.4737531127929687\neager train time 1: 0.02575574493408203\neager train time 2: 0.025766271591186524\neager train time 3: 0.02569219207763672\neager train time 4: 0.02566761589050293\neager train time 5: 0.026013599395751954\neager train time 6: 0.025726463317871092\neager train time 7: 0.025708160400390624\neager train time 8: 0.025737247467041015\neager train time 9: 0.02180371284484863\n~~~~~~~~~~\ncompile train time 0: 22.97644140625\ncompile train time 1: 0.02164531135559082\ncompile train time 2: 0.02067158317565918\ncompile train time 3: 0.02125984001159668\ncompile train time 4: 0.021174720764160156\ncompile train time 5: 0.020945056915283203\ncompile train time 6: 0.021309440612792968\ncompile train time 7: 0.021041824340820314\ncompile train time 8: 0.020995264053344728\ncompile train time 9: 0.021283327102661134\n~~~~~~~~~~\n(train) eager median: 0.02573185539245605, compile median: 0.02121728038787842, speedup: 1.2127782129493299x\n~~~~~~~~~~",
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->fit1 = Holt(air, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast1 = fit1.forecast(5).rename(\"Holt's linear trend\")\nfit2 = Holt(air, exponential=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2, optimized=False\n)\nfcast2 = fit2.forecast(5).rename(\"Exponential trend\")\nfit3 = Holt(air, damped_trend=True, initialization_method=\"estimated\").fit(\n    smoothing_level=0.8, smoothing_trend=0.2\n)\nfcast3 = fit3.forecast(5).rename(\"Additive damped trend\")\n\nplt.figure(figsize=(12, 8))\nplt.plot(air, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing->fit1 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.2, optimized=False\n)\nfcast1 = fit1.forecast(3).rename(r\"$\\alpha=0.2$\")\nfit2 = SimpleExpSmoothing(oildata, initialization_method=\"heuristic\").fit(\n    smoothing_level=0.6, optimized=False\n)\nfcast2 = fit2.forecast(3).rename(r\"$\\alpha=0.6$\")\nfit3 = SimpleExpSmoothing(oildata, initialization_method=\"estimated\").fit()\nfcast3 = fit3.forecast(3).rename(r\"$\\alpha=%s$\" % fit3.model.params[\"smoothing_level\"])\n\nplt.figure(figsize=(12, 8))\nplt.plot(oildata, marker=\"o\", color=\"black\")\nplt.plot(fit1.fittedvalues, marker=\"o\", color=\"blue\")\n(line1,) = plt.plot(fcast1, marker=\"o\", color=\"blue\")\nplt.plot(fit2.fittedvalues, marker=\"o\", color=\"red\")\n(line2,) = plt.plot(fcast2, marker=\"o\", color=\"red\")\nplt.plot(fit3.fittedvalues, marker=\"o\", color=\"green\")\n(line3,) = plt.plot(fcast3, marker=\"o\", color=\"green\")\nplt.legend([line1, line2, line3], [fcast1.name, fcast2.name, fcast3.name])",
            "torch->Model Optimization->Pruning Tutorial->Global pruning->print(\n    \"Sparsity in conv1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in conv2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc3.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Global sparsity: {:.2f}%\".format(\n        100. * float(\n            ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n        )\n        / float(\n            .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n        )\n    )\n)"
        ]
    },
    {
        "id": "1212139",
        "GT": "Wellness Gaps->wellness_seg.head()",
        "pred": [
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time.head()",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resf_logit.predict()",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_mask.nonzero()",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->arima_res.predict(0, 2)",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines->sinplot()\nsns.despine()\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines->sinplot()\nsns.despine()\n"
        ]
    },
    {
        "id": "1363790",
        "GT": "scatter_compare(subdf[good]);",
        "pred": [
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->sns.heatmap(X.corr());",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->Autocorrelation->tsplot(res_trend.resid, lags=36);",
            "seaborn->User guide and tutorial->Estimating regression fits->Functions for drawing linear regression models->sns.lmplot(x=\"size\", y=\"tip\", data=tips);\n",
            "seaborn->Statistical operations->Estimating regression fits->Functions for drawing linear regression models->sns.lmplot(x=\"size\", y=\"tip\", data=tips);\n",
            "seaborn->User guide and tutorial->Estimating regression fits->Functions for drawing linear regression models->sns.lmplot(x=\"size\", y=\"tip\", data=tips, x_jitter=.05);\n",
            "seaborn->Statistical operations->Estimating regression fits->Functions for drawing linear regression models->sns.lmplot(x=\"size\", y=\"tip\", data=tips, x_jitter=.05);\n",
            "seaborn->User guide and tutorial->Estimating regression fits->Conditioning on other variables->sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", data=tips);\n",
            "seaborn->Statistical operations->Estimating regression fits->Conditioning on other variables->sns.lmplot(x=\"total_bill\", y=\"tip\", hue=\"smoker\", data=tips);\n"
        ]
    },
    {
        "id": "439350",
        "GT": "101 - Training and Evaluating Classifiers with `mmlspark`\n\nIn this example, we try to predict incomes from the *Adult Census* dataset.\n\n$$c = \\sqrt{a^2 + b^2}$$\n\nFirst, we import the packages (use `help(mmlspark)` to view contents),->dataFile = \"AdultCensusIncome.csv\"\nimport os, urllib\nif not os.path.isfile(dataFile):\n    urllib.request.urlretrieve(\"https://mmlspark.azureedge.net/datasets/\"+dataFile, dataFile)\ndata = spark.createDataFrame(pd.read_csv(dataFile, dtype={\" hours-per-week\": np.float64}))\ndata = data.select([\" education\", \" marital-status\", \" hours-per-week\", \" income\"])\ntrain, test = data.randomSplit([0.75, 0.25], seed=123)\ntrain.limit(10).toPandas()",
        "pred": [
            "torch->Model Optimization->(beta) Static Quantization with Eager Mode in PyTorch->3. Define dataset and data loaders->ImageNet Data->data_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\n\n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n# while also improving numerical accuracy. While this can be used with any model, this is\n# especially common with quantized models.\n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)",
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Testing->url = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/csv/COUNT/medpar.csv'\nmedpar = read.csv(url)\nf = los~factor(type)+hmo+white\n\nlibrary(MASS)\nmod = glm.nb(f, medpar)\ncoef(summary(mod))\n                 Estimate Std. Error   z value      Pr(|z|)\n(Intercept)    2.31027893 0.06744676 34.253370 3.885556e-257\nfactor(type)2  0.22124898 0.05045746  4.384861  1.160597e-05\nfactor(type)3  0.70615882 0.07599849  9.291748  1.517751e-20\nhmo           -0.06795522 0.05321375 -1.277024  2.015939e-01\nwhite         -0.12906544 0.06836272 -1.887951  5.903257e-02",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->data = pdr.get_data_fred(\"HOUSTNSA\", \"1959-01-01\", \"2019-06-01\")\nhousing = data.HOUSTNSA.pct_change().dropna()\n# Scale by 100 to get percentages\nhousing = 100 * housing.asfreq(\"MS\")\nfig, ax = plt.subplots()\nax = housing.plot(ax=ax)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_6_0.png\"/>",
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->dots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n",
            "statsmodels->Examples->Linear Regression Models->Rolling Least Squares->exog_vars = [\"Mkt-RF\", \"SMB\", \"HML\"]\nexog = sm.add_constant(factors[exog_vars])\nrols = RollingOLS(endog, exog, window=60)\nrres = rols.fit()\nfig = rres.plot_recursive_coefficient(variables=exog_vars, figsize=(14, 18))\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\" src=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\"/>"
        ]
    },
    {
        "id": "289985",
        "GT": "uber_pred = model.predict(X_test) # Predicting the features of the dataset",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->arima_res.predict(0, 2)",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:->predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->sarima_res.predict(0, 2)",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resf_logit.predict()",
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model->sm_probit_canned = sm.Probit(endog, exog).fit()"
        ]
    },
    {
        "id": "931479",
        "GT": "3. Train each classifier and make prediction  <a name=\"train\"></a>-># Train classifier 1\ntrain_forest = forest.fit(X_1, y_1_train)\n# Obtain predictions\npredict_1 = train_forest.predict(X_test)\n\n# Train classifier 2\ntrain_forest = forest.fit(X_2, y_2_train)\n# Obtain predictions\npredict_2 = train_forest.predict(X_test)\n\n# Train classifier 3\ntrain_forest = forest.fit(X_3, y_3_train)\n# Obtain predictions\npredict_3 = train_forest.predict(X_test)\n\n# Train classifier 4\ntrain_forest = forest.fit(X_4, y_4_train)\n# Obtain predictions\npredict_4 = train_forest.predict(X_test)\n\n# Train classifier 5\ntrain_forest = forest.fit(X_5, y_5_train)\n# Obtain predictions\npredict_5 = train_forest.predict(X_test)",
        "pred": [
            "numpy->NumPy Applications->Deep learning on MNIST->3. Build and train a small neural network from scratch->Compose the model and begin training and testing it-># The training set metrics.\ny_training_error = [\n    store_training_loss[i] / float(len(training_images))\n    for i in range(len(store_training_loss))\n]\nx_training_error = range(1, len(store_training_loss) + 1)\ny_training_accuracy = [\n    store_training_accurate_pred[i] / float(len(training_images))\n    for i in range(len(store_training_accurate_pred))\n]\nx_training_accuracy = range(1, len(store_training_accurate_pred) + 1)\n\n# The test set metrics.\ny_test_error = [\n    store_test_loss[i] / float(len(test_images)) for i in range(len(store_test_loss))\n]\nx_test_error = range(1, len(store_test_loss) + 1)\ny_test_accuracy = [\n    store_training_accurate_pred[i] / float(len(training_images))\n    for i in range(len(store_training_accurate_pred))\n]\nx_test_accuracy = range(1, len(store_test_accurate_pred) + 1)\n\n# Display the plots.\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\naxes[0].set_title(\"Training set error, accuracy\")\naxes[0].plot(x_training_accuracy, y_training_accuracy, label=\"Training set accuracy\")\naxes[0].plot(x_training_error, y_training_error, label=\"Training set error\")\naxes[0].set_xlabel(\"Epochs\")\naxes[1].set_title(\"Test set error, accuracy\")\naxes[1].plot(x_test_accuracy, y_test_accuracy, label=\"Test set accuracy\")\naxes[1].plot(x_test_error, y_test_error, label=\"Test set error\")\naxes[1].set_xlabel(\"Epochs\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/a155f32ad6a3518292224002121b7167dd36f5c2c99a5ac7ac1d53357125a043.png\" src=\"../_images/a155f32ad6a3518292224002121b7167dd36f5c2c99a5ac7ac1d53357125a043.png\"/>",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example-># Step 2: produce one-step-ahead forecasts\nfcast = training_res.forecast()\n\n# Step 3: compute root mean square forecasting error\ntrue = endog.reindex(fcast.index)\nerror = true - fcast\n\n# Print out the results\nprint(pd.concat([true.rename('true'),\n                 fcast.rename('forecast'),\n                 error.rename('error')], axis=1))",
            "statsmodels->Examples->State space models->Unobserved Components: Application->Model-># Output\noutput_mod = sm.tsa.UnobservedComponents(dta['US GNP'], **unrestricted_model)\noutput_res = output_mod.fit(method='powell', disp=False)\n\n# Prices\nprices_mod = sm.tsa.UnobservedComponents(dta['US Prices'], **unrestricted_model)\nprices_res = prices_mod.fit(method='powell', disp=False)\n\nprices_restricted_mod = sm.tsa.UnobservedComponents(dta['US Prices'], **restricted_model)\nprices_restricted_res = prices_restricted_mod.fit(method='powell', disp=False)\n\n# Money\nmoney_mod = sm.tsa.UnobservedComponents(dta['US monetary base'], **unrestricted_model)\nmoney_res = money_mod.fit(method='powell', disp=False)\n\nmoney_restricted_mod = sm.tsa.UnobservedComponents(dta['US monetary base'], **restricted_model)\nmoney_restricted_res = money_restricted_mod.fit(method='powell', disp=False)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example-># Step 2: produce one-step-ahead forecasts\nfcast = append_res.forecast()\n\n# Step 3: compute root mean square forecasting error\ntrue = endog.reindex(fcast.index)\nerror = true - fcast\n\n# Print out the results\nprint(pd.concat([true.rename('true'),\n                 fcast.rename('forecast'),\n                 error.rename('error')], axis=1))",
            "torch->PyTorch Recipes->Dynamic Quantization->Steps->5: Look at Accuracy-># run the float model\nout1, hidden1 = float_lstm(inputs, hidden)\nmag1 = (abs(out1)).item()\nprint('mean absolute value of output tensor values in the FP32 model is {0:.5f} '.format(mag1))\n\n# run the quantized model\nout2, hidden2 = quantized_lstm(inputs, hidden)\nmag2 = (abs(out2)).item()\nprint('mean absolute value of output tensor values in the INT8 model is {0:.5f}'.format(mag2))\n\n# compare them\nmag3 = (abs(out1-out2)).item()\nprint('mean absolute value of the difference between the output tensors is {0:.5f} or {1:.2f} percent'.format(mag3,mag3/mag1*100))"
        ]
    },
    {
        "id": "1272029",
        "GT": "Filter Methods\n\nFilter methods were those that select based on feature information. They should be the first step in any ML analysis.\n\nBasic filter methods consist of removing __constant__, __quasi constant__ or __duplicated__ features. Constant and quasi constant are features that always or almost always are the same value in each case, adding little to no value to the ML.\n\nOne hot encoding may be a source of duplicated features.->Constant Features\n\nConstant features are those that show the same value, just one value, for all the observations of the dataset. This features provide no information that allows a ML model to discriminate or predict a target.\n\nIndentifying and removing constant features is an easy first step towards feature selection and more easily interpretable ML models.\n\nHere, we will demonstrate how to identify constant features using the Santander Customer Satisfaction dataset from Kaggle.\n\nTo identify constant features, we can use the VarianceThreshold function from sklearn, or we can code it ourselves.->data = pd.read_csv('../datasets/santander.csv')\n\ndata.shape",
        "pred": [
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Messy data->anagrams = sns.load_dataset(\"anagrams\")\nanagrams\n",
            "statsmodels->Examples->User Notes->Dates in Time-Series Models->Using Pandas->data.endog.index = dates\nendog = data.endog\nendog",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data->flights = sns.load_dataset(\"flights\")\nflights.head()\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Long-form vs. wide-form data->Long-form data->flights = sns.load_dataset(\"flights\")\nflights.head()\n",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->scores = pd.DataFrame(est.cv_results_)\nscores.head()",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->endog = macrodata['infl']\nendog.plot(figsize=(15, 5))"
        ]
    },
    {
        "id": "928393",
        "GT": "US_coeff = 0.0408\nUK_coeff = 0.0506\n\nprint('Exponentiating the US coefficient {} yields {:.3f}'.\\\n     format(US_coeff, np.exp(US_coeff)))\nprint('Exponentiating the UK coefficient {} yields {:.3f}'.\\\n     format(UK_coeff, np.exp(UK_coeff)))",
        "pred": [
            "sklearn->Examples->Support Vector Machines->RBF SVM parameters->Train classifiers->C_2d_range = [1e-2, 1, 1e2]\ngamma_2d_range = [1e-1, 1, 1e1]\nclassifiers = []\nfor C in C_2d_range:\n    for gamma in gamma_2d_range:\n        clf = (C=C, gamma=gamma)\n        clf.fit(X_2d, y_2d)\n        classifiers.append((C, gamma, clf))",
            "matplotlib->Tutorials->Colors->Colormap Normalization->Symmetric logarithmic->N = 100\nX, Y = np.mgrid[-3:3:complex(0, N), -2:2:complex(0, N)]\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (Z1 - Z2) * 2\n\nfig, ax = plt.subplots(2, 1)\n\npcm = ax[0].pcolormesh(X, Y, Z,\n                       norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0, base=10),\n                       cmap='RdBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[0], extend='both')\n\npcm = ax[1].pcolormesh(X, Y, Z, cmap='RdBu_r', vmin=-np.max(Z), shading='auto')\nfig.colorbar(pcm, ax=ax[1], extend='both')\nplt.show()\n\n\n<img alt=\"colormapnorms\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_003.png\" srcset=\"../../_images/sphx_glr_colormapnorms_003.png, ../../_images/sphx_glr_colormapnorms_003_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Colors->Colormap Normalization->Power-law->N = 100\nX, Y = np.mgrid[0:3:complex(0, N), 0:2:complex(0, N)]\nZ1 = (1 + np.sin(Y * 10.)) * X**2\n\nfig, ax = plt.subplots(2, 1, constrained_layout=True)\n\npcm = ax[0].pcolormesh(X, Y, Z1, norm=colors.PowerNorm(gamma=0.5),\n                       cmap='PuBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[0], extend='max')\nax[0].set_title('PowerNorm()')\n\npcm = ax[1].pcolormesh(X, Y, Z1, cmap='PuBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[1], extend='max')\nax[1].set_title('Normalize()')\nplt.show()\n\n\n<img alt=\"PowerNorm(), Normalize()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_004.png\" srcset=\"../../_images/sphx_glr_colormapnorms_004.png, ../../_images/sphx_glr_colormapnorms_004_2_0x.png 2.0x\"/>",
            "statsmodels->Examples->Statistics->ANOVA->infl = interM_lm.get_influence()\nresid = infl.resid_studentized_internal\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        resid[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"X\")\nplt.ylabel(\"standardized resids\")",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->X_train = textproc.cleantext(train_df,\n                       text_column='review',\n                       remove_stopwords=True,\n                       remove_punc=True)[0:2000]\n\nX_test = textproc.cleantext(test_df,\n                       text_column='review',\n                       remove_stopwords=True,\n                       remove_punc=True)[0:1000]\n\ny_train = train_df['sentiment'].to_numpy()[0:2000]\ny_test = test_df['sentiment'].to_numpy()[0:1000]"
        ]
    },
    {
        "id": "1418017",
        "GT": "hom_var, hom_lags = \"CASES\", [1, 2, 6]\nhom_ts = pd.read_csv(\"./DBs/Hom/HomCDMX4RM.csv\", parse_dates=[\"TMS\"], index_col=\"TMS\")[[hom_var]]\nhom_gt = gt_ins.load(\"./DBs/Hom/HomCDMXHorariosSeM2004-2018.csv\", var=\"GI\")\nhom_dr = DataRegularizer(data=hom_ts, gt=hom_gt, lags=hom_lags, var_list=[], tf=(\"2009-01\", \"2017-01\"))\nhom_dr.proceed()\nhom_lm = GTModelComparison(data=hom_dr.df, train=hom_dr.train, gt_vars=hom_dr.gt_vars)\nhom_lm.run_all()",
        "pred": [
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Working with text->mu, sigma = 100, 15\nx = mu + sigma * np.random.randn(10000)\n\n# the histogram of the data\nn, bins, patches = plt.hist(x, 50, density=True, facecolor='g', alpha=0.75)\n\n\nplt.xlabel('Smarts')\nplt.ylabel('Probability')\nplt.title('Histogram of IQ')\nplt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\nplt.axis([40, 160, 0, 0.03])\nplt.grid(True)\nplt.show()\n\n\n<img alt=\"Histogram of IQ\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_008.png\" srcset=\"../../_images/sphx_glr_pyplot_008.png, ../../_images/sphx_glr_pyplot_008_2_0x.png 2.0x\"/>",
            "scipy->File IO (scipy.io)->MATLAB files->How do I start?->mat_contents = sio.loadmat('octave_a.mat')\n mat_contents\n{'a': array([[[  1.,   4.,   7.,  10.],\n        [  2.,   5.,   8.,  11.],\n        [  3.,   6.,   9.,  12.]]]),\n '__version__': '1.0',\n '__header__': 'MATLAB 5.0 MAT-file, written by\n Octave 3.6.3, 2013-02-17 21:02:11 UTC',\n '__globals__': []}\n oct_a = mat_contents['a']\n oct_a\narray([[[  1.,   4.,   7.,  10.],\n        [  2.,   5.,   8.,  11.],\n        [  3.,   6.,   9.,  12.]]])\n oct_a.shape\n(1, 3, 4)",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables->pred_ols2 = res2.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.plot(x, y, \"o\", label=\"Data\")\nax.plot(x, y_true, \"b-\", label=\"True\")\nax.plot(x, res2.fittedvalues, \"r--.\", label=\"Predicted\")\nax.plot(x, iv_u, \"r--\")\nax.plot(x, iv_l, \"r--\")\nlegend = ax.legend(loc=\"best\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_ols_26_0.png\" src=\"../../../_images/examples_notebooks_generated_ols_26_0.png\"/>",
            "matplotlib->Tutorials->Introductory->Quick start guide->Labelling plots->Axes labels and text->mu, sigma = 115, 15\nx = mu + sigma * np.random.randn(10000)\nfig, ax = plt.subplots(figsize=(5, 2.7), layout='constrained')\n# the histogram of the data\nn, bins, patches = ax.hist(x, 50, density=True, facecolor='C0', alpha=0.75)\n\nax.set_xlabel('Length [cm]')\nax.set_ylabel('Probability')\nax.set_title('Aardvark lengths\\n (not really)')\nax.text(75, .025, r'$\\mu=115,\\ \\sigma=15$')\nax.axis([55, 175, 0, 0.03])\nax.grid(True)\n\n\n<img alt=\"Aardvark lengths  (not really)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_009.png\" srcset=\"../../_images/sphx_glr_quick_start_009.png, ../../_images/sphx_glr_quick_start_009_2_0x.png 2.0x\"/>",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->speech_data_path = 'tutorial-nlp-from-scratch/speeches.csv'\nspeech_df = pd.read_csv(speech_data_path)\nX_pred = textproc.cleantext(speech_df,\n                            text_column='speech',\n                            remove_stopwords=True,\n                            remove_punc=False)\nspeakers = speech_df['speaker'].to_numpy()"
        ]
    },
    {
        "id": "286983",
        "GT": "Problem 1c->Image(filename=\"geomagia_refs.png\")",
        "pred": [
            "statsmodels->Examples->Statistics->ANOVA->Minority Employment Data->Text(0, 0.5, 'JPERF')\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_interactions_anova_39_2.png\" src=\"../../../_images/examples_notebooks_generated_interactions_anova_39_2.png\"/>",
            "matplotlib->Tutorials->Introductory->Animations using Matplotlib->Animation Writers->Saving Animations->ani.save(filename=\"/tmp/pillow_example.gif\", writer=\"pillow\")\nani.save(filename=\"/tmp/pillow_example.apng\", writer=\"pillow\")",
            "sklearn->Examples->Gaussian Mixture Models->Gaussian Mixture Model Selection->Model training and selection->GaussianMixture()\n\n<br/>\n<br/>",
            "seaborn->User guide and tutorial->Estimating regression fits->Fitting different kinds of models->anscombe = sns.load_dataset(\"anscombe\")\n",
            "seaborn->Statistical operations->Estimating regression fits->Fitting different kinds of models->anscombe = sns.load_dataset(\"anscombe\")\n",
            "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content->plt.imshow(img)\nplt.show()\n\n\n\n\n<img alt=\"../_images/0802995ab45723028c194f181d4e07f2e90f6c49c7f49d85ee3de0f218782122.png\" src=\"../_images/0802995ab45723028c194f181d4e07f2e90f6c49c7f49d85ee3de0f218782122.png\"/>"
        ]
    },
    {
        "id": "452562",
        "GT": "# The correlations among the actual, predicted and residual is as follows.\n\ncomb[['gen_tot','losses2', 'losses2_pred_gen_total', 'losses2_resid_gen_total']].corr()",
        "pred": [
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example-># Compute the root mean square error\nrmse = (flattened**2).mean(axis=1)**0.5\n\nprint(rmse)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend-># Compute the root mean square error\nrmse = (flattened**2).mean(axis=1)**0.5\n\nprint(rmse)",
            "numpy->NumPy Applications->Deep learning on MNIST->1. Load the MNIST dataset-># Display the label of the 60,000th image (indexed at 59,999) from the training set.\ny_train[59999]",
            "statsmodels->Examples->User Notes->Formulas->Multiplicative interactions->res1 = ols(formula=\"Lottery ~ Literacy : Wealth - 1\", data=df).fit()\nres2 = ols(formula=\"Lottery ~ Literacy * Wealth - 1\", data=df).fit()\nprint(res1.params, \"\\n\")\nprint(res2.params)",
            "statsmodels->Examples->State space models->Statespace: Custom Models->Bonus: pymc3 for fast Bayesian estimation->Bayesian estimation-># Set sampling params\nndraws = 3000  #  3000 number of draws from the distribution\nnburn = 600  # 600 number of \"burn-in points\" (which will be discarded)",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->feature_names = model[:-1].get_feature_names_out()\n\ncoefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients\"],\n    index=feature_names,\n)\n\ncoefs\n\n\n\n\n\n\n\n\n<br/>\n<br/>"
        ]
    },
    {
        "id": "1408963",
        "GT": "Compute Confusion on HEDGE neurals removed->cm = confusion_matrix(y_deploy, y_pred_HG)\nnp.set_printoptions(precision=2)\nprint('Confusion matrix, without normalization')\nprint(cm)\nplt.figure(1)\nplt.subplot(2,2,1)\nplot_confusion_matrix(cm)\n# Normalize the confusion matrix by row (i.e by the number of samples\n# in each class)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nprint('Normalized confusion matrix')\nprint(cm_normalized)\n#plt.figure()\nplt.subplot(2,2,2)\nplot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\nplt.show()",
        "pred": [
            "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Factor Analysis components - FA->fa_estimator = (n_components=n_components, max_iter=20)\nfa_estimator.fit(faces_centered)\nplot_gallery(\"Factor Analysis (FA)\", fa_estimator.components_[:n_components])\n\n# --- Pixelwise variance\n(figsize=(3.2, 3.6), facecolor=\"white\", tight_layout=True)\nvec = fa_estimator.noise_variance_\nvmax = max(vec.max(), -vec.min())\n(\n    vec.reshape(image_shape),\n    cmap=plt.cm.gray,\n    interpolation=\"nearest\",\n    vmin=-vmax,\n    vmax=vmax,\n)\n(\"off\")\n(\"Pixelwise variance from \\n Factor Analysis (FA)\", size=16, wrap=True)\n(orientation=\"horizontal\", shrink=0.8, pad=0.03)\n()\n\n\n\n<img alt=\"Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_008.png\"/>\n<img alt=\"Pixelwise variance from   Factor Analysis (FA)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_009.png\"/>",
            "statsmodels->Examples->Statistics->ANOVA->resid = lm.resid\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    group_num = i * 2 + j - 1  # for plotting purposes\n    x = [group_num] * len(group)\n    plt.scatter(\n        x,\n        resid[group.index],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"Group\")\nplt.ylabel(\"Residuals\")",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings->data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot(kind=\"barh\", figsize=(9, 7))\n(\"Lasso model, optimum regularization, normalized variables\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Lasso model, optimum regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_016.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_016.png\"/>",
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis->vc = {\"g1\": \"0 + C(group1)\", \"g2\": \"0 + C(group2)\"}\noo = np.ones(df.shape[0])\nmodel3 = sm.MixedLM.from_formula(\"y ~ 1\", groups=oo, vc_formula=vc, data=df)\nresult3 = model3.fit()\nprint(result3.summary())"
        ]
    },
    {
        "id": "842247",
        "GT": "# build a dataframe of cases in which this is not null\ndf_ineq = df_states[~df_states['incshare_top1'].isnull()]\n# check that it worked\nany(df_ineq['incshare_top1'].isnull())",
        "pred": [
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()",
            "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates-># Assign better names for our seasonal terms\ntrue_seasonal_10_3 = terms[0]\ntrue_seasonal_100_2 = terms[1]\ntrue_sum = true_seasonal_10_3 + true_seasonal_100_2\n<br/>",
            "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept and lagged dependent variable-># Fit the model\nmod_fedfunds2 = sm.tsa.MarkovRegression(\n    dta_fedfunds.iloc[1:], k_regimes=2, exog=dta_fedfunds.iloc[:-1]\n)\nres_fedfunds2 = mod_fedfunds2.fit()",
            "statsmodels->Examples->User Notes->Formulas->Using formulas with models that do not (yet) support them->f = \"Lottery ~ Literacy * Wealth\"\ny, X = patsy.dmatrices(f, df, return_type=\"dataframe\")\nprint(y[:5])\nprint(X[:5])"
        ]
    },
    {
        "id": "852877",
        "GT": "Checking the rows of null values->null_rows = wrangle_clean[pd.isnull(wrangle_clean).any(axis=1)]\nnull_rows",
        "pred": [
            "numpy->NumPy Features->Masked Arrays->Missing data->china_masked = nbcases_ma[locations[:, 1] == \"China\"].sum(axis=0)\nchina_masked",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->endog = macrodata['infl']\nendog.plot(figsize=(15, 5))",
            "torch->Code Transforms with FX->(beta) Building a Convolution/Batch Norm fuser in FX->Fusing Convolution with Batch Norm->traced_model = (model)\nprint(traced_model.graph)",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:->predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "pandas_toms_blog->Scaling->Dask->most_common = df.occupation.value_counts().nlargest(100)\nmost_common"
        ]
    },
    {
        "id": "307032",
        "GT": "# Create a DataFrame from a dictionary.\nd = pd.DataFrame({'capital':['Montgomery', 'Juneau', 'Phoenix'], 'state':['AL', 'AK', 'AZ']})\nd.head(2)",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)",
            "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding->from patsy.contrasts import Treatment\n\nlevels = [1, 2, 3, 4]\ncontrast = Treatment(reference=0).code_without_intercept(levels)\nprint(contrast.matrix)",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing->ax = oildata.plot()\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Oil (millions of tonnes)\")\nprint(\"Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\")"
        ]
    },
    {
        "id": "42295",
        "GT": "Analyzing SLAC runs 1930-1938 with `pandas`\n\nThe input will be the `eventTree` from the `SlacAnalyzer`\n\n---\n\n### Import the required libraries->Open the ROOT file and grab the `eventTree`->slact = ROOT.TChain('slacAnalyzer/eventTree')\nfor run_num in range(1930, 1939):\n    slact.Add('../rootOutputs/gm2slac_run0{}.root'.format(run_num))\n\n# we are only going to use the CrystalHits\nslact.SetBranchStatus(\"*\",0)\nfor used_branch in ['EventNum', 'IslandNum', 'XtalNum', 'Energy', 'Time']:\n    slact.SetBranchStatus('XtalHit_' + used_branch)\n\nslact.GetEntries()",
        "pred": [
            "statsmodels->Examples->Statistics->ANOVA->lm_final = ols(\"S ~ X + C(E)*C(M)\", data=salary_table.drop([drop_idx])).fit()\nmf = lm_final.model.data.orig_exog\nlstyle = [\"-\", \"--\"]\n\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    idx = group.index\n    plt.scatter(\n        X[idx],\n        S[idx],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\n    # drop NA because there is no idx 32 in the final model\n    fv = lm_final.fittedvalues.reindex(idx).dropna()\n    x = mf.X.reindex(idx).dropna()\n    plt.plot(x, fv, ls=lstyle[j], color=colors[i - 1])\nplt.xlabel(\"Experience\")\nplt.ylabel(\"Salary\")",
            "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Pairwise comparison of all models: Bayesian approach->pairwise_bayesian = []\n\nfor model_i, model_k in (range(len(model_scores)), 2):\n    model_i_scores = model_scores.iloc[model_i].values\n    model_k_scores = model_scores.iloc[model_k].values\n    differences = model_i_scores - model_k_scores\n    t_post = (\n        df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n    )\n    worse_prob = t_post.cdf(rope_interval[0])\n    better_prob = 1 - t_post.cdf(rope_interval[1])\n    rope_prob = t_post.cdf(rope_interval[1]) - t_post.cdf(rope_interval[0])\n\n    pairwise_bayesian.append([worse_prob, better_prob, rope_prob])\n\npairwise_bayesian_df = (\n    pairwise_bayesian, columns=[\"worse_prob\", \"better_prob\", \"rope_prob\"]\n).round(3)\n\npairwise_comp_df = pairwise_comp_df.join(pairwise_bayesian_df)\npairwise_comp_df\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "sklearn->Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction->Swiss Roll->sr_lle, sr_err = (\n    sr_points, n_neighbors=12, n_components=2\n)\n\nsr_tsne = (n_components=2, perplexity=40, random_state=0).fit_transform(\n    sr_points\n)\n\nfig, axs = (figsize=(8, 8), nrows=2)\naxs[0].scatter(sr_lle[:, 0], sr_lle[:, 1], c=sr_color)\naxs[0].set_title(\"LLE Embedding of Swiss Roll\")\naxs[1].scatter(sr_tsne[:, 0], sr_tsne[:, 1], c=sr_color)\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss Roll\")\n\n\n<img alt=\"LLE Embedding of Swiss Roll, t-SNE Embedding of Swiss Roll\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_002.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_002.png\"/>",
            "sklearn->Examples->Manifold learning->Swiss Roll And Swiss-Hole Reduction->Swiss-Hole->sh_lle, sh_err = (\n    sh_points, n_neighbors=12, n_components=2\n)\n\nsh_tsne = (\n    n_components=2, perplexity=40, init=\"random\", random_state=0\n).fit_transform(sh_points)\n\nfig, axs = (figsize=(8, 8), nrows=2)\naxs[0].scatter(sh_lle[:, 0], sh_lle[:, 1], c=sh_color)\naxs[0].set_title(\"LLE Embedding of Swiss-Hole\")\naxs[1].scatter(sh_tsne[:, 0], sh_tsne[:, 1], c=sh_color)\n_ = axs[1].set_title(\"t-SNE Embedding of Swiss-Hole\")\n\n\n<img alt=\"LLE Embedding of Swiss-Hole, t-SNE Embedding of Swiss-Hole\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_swissroll_004.png\" srcset=\"../../_images/sphx_glr_plot_swissroll_004.png\"/>",
            "torch->PyTorch Recipes->Developing Custom PyTorch Dataloaders->Part 1: The Dataset->1.3 Iterate through data samples->face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break"
        ]
    },
    {
        "id": "595116",
        "GT": "8) Deriving Conclusions->frame.to_csv('order_segmentation_0.0.csv')",
        "pred": [
            "pandas_toms_blog->Scaling->Dask->df.visualize(rankdir='LR')",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->df.info()",
            "statsmodels->Examples->Statistics->ANOVA->df_infl = infl.summary_frame()",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data->data = sm.datasets.fair.load_pandas().data",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first.loc[['AA', 'AS', 'DL'], ['fl_date', 'tail_num']]"
        ]
    },
    {
        "id": "796741",
        "GT": "Cleaning Data\n\n### Making a DataFrame easier to work with\n\nDifficulty: *easy/medium*\n\nIt happens all the time: someone gives you data containing malformed strings, Python, lists and missing data. How do you tidy it up so you can get on with the analysis?\n\nTake this monstrosity as the DataFrame to use in the following puzzles:\n\n```python\ndf = pd.DataFrame({'From_To': ['LoNDon_paris', 'MAdrid_miLAN', 'londON_StockhOlm', \n                               'Budapest_PaRis', 'Brussels_londOn'],\n              'FlightNumber': [10045, np.nan, 10065, np.nan, 10085],\n              'RecentDelays': [[23, 47], [], [24, 43, 87], [13], [67, 32]],\n                   'Airline': ['KLM(!)', '<Air France> (12)', '(British Airways. )', \n                               '12. Air France', '\"Swiss Air\"']})\n```\n(It's some flight data I made up; it's not meant to be accurate in any way.)->temp['From'] = temp['From'].str.capitalize()\ntemp['To'] = temp['To'].str.capitalize()",
        "pred": [
            "torch->PyTorch Recipes->PyTorch Benchmark->Steps->4. Benchmarking with Blocked Autorange->m0 = t0.blocked_autorange()\nm1 = t1.blocked_autorange()\n\nprint(m0)\nprint(m1)\n\n\n\nOutput",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->imdb_train = data.fetch('imdb_train.txt')\nimdb_test = data.fetch('imdb_test.txt')",
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding->print(y_train[0])\nprint(y_train[1])\nprint(y_train[2])",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Comparing trends and exogenous variables in SARIMAX, ARIMA and AutoReg->Using exog in SARIMAX and ARIMA->sarimax_exog_res = SARIMAX(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()\narima_exog_res = ARIMA(y, exog=x, order=(1, 0, 0), trend=\"c\").fit()",
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding->training_labels = one_hot_encoding(y_train[:training_sample])\ntest_labels = one_hot_encoding(y_test[:test_sample])"
        ]
    },
    {
        "id": "1329174",
        "GT": "3g. Turnover V.S. Evaluation \n***\n**Summary:** \n - There seems to be a biomodal distrubtion for those that had a turnover. \n - Employees with **low** performance tend to leave the company more\n - Employees with **high** performance tend to leave the company more\n - The **sweet spot** seems for those that stayed seems to be within **0.6-0.8**-># Kernel Density Plot\nfig = plt.figure(figsize=(15,4),)\nax=sns.kdeplot(df.loc[(df['turnover'] == 0),'evaluation'] , color='b',shade=True,label='no turnover')\nax=sns.kdeplot(df.loc[(df['turnover'] == 1),'evaluation'] , color='r',shade=True, label='turnover')\nplt.title('Last evaluation')",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models-># Graph\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n\n# Plot data points\ninf[\"CPIAUCNS\"].plot(ax=ax, style=\"-\", label=\"Observed data\")\n\n# Plot estimate of the level term\nres_uc_mle.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (MLE)\")\nres_uc_bayes.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (Bayesian)\")\n\nax.legend(loc=\"lower left\");\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\"/>",
            "statsmodels->Examples->State space models->Unobserved Components: Application->Data-># Plot the data\nax = dta.plot(figsize=(13,3))\nylim = ax.get_ylim()\nax.xaxis.grid()\nax.fill_between(dates, ylim[0]+1e-5, ylim[1]-1e-5, recessions, facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\"/>",
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->Confidence interval-># Plot the confidence interval and fit\nfig, ax = pylab.subplots()\nax.scatter(x, y)\nax.plot(eval_x, smoothed, c=\"k\")\nax.fill_between(eval_x, bottom, top, alpha=0.5, color=\"b\")\npylab.autoscale(enable=True, axis=\"x\", tight=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_lowess_7_0.png\" src=\"../../../_images/examples_notebooks_generated_lowess_7_0.png\"/>",
            "statsmodels->Examples->State space models->Unobserved Components: Application->Model-># Create Table I\ntable_i = np.zeros((5,6))\n\nstart = dta.index[0]\nend = dta.index[-1]\ntime_range = '%d:%d-%d:%d' % (start.year, start.quarter, end.year, end.quarter)\nmodels = [\n    ('US GNP', time_range, 'None'),\n    ('US Prices', time_range, 'None'),\n    ('US Prices', time_range, r'$\\sigma_\\eta^2 = 0$'),\n    ('US monetary base', time_range, 'None'),\n    ('US monetary base', time_range, r'$\\sigma_\\eta^2 = 0$'),\n]\nindex = pd.MultiIndex.from_tuples(models, names=['Series', 'Time range', 'Restrictions'])\nparameter_symbols = [\n    r'$\\sigma_\\zeta^2$', r'$\\sigma_\\eta^2$', r'$\\sigma_\\kappa^2$', r'$\\rho$',\n    r'$2 \\pi / \\lambda_c$', r'$\\sigma_\\varepsilon^2$',\n]\n\ni = 0\nfor res in (output_res, prices_res, prices_restricted_res, money_res, money_restricted_res):\n    if res.model.stochastic_level:\n        (sigma_irregular, sigma_level, sigma_trend,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n    else:\n        (sigma_irregular, sigma_level,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n        sigma_trend = np.nan\n    period_cycle = 2 * np.pi / frequency_cycle\n\n    table_i[i, :] = [\n        sigma_level*1e7, sigma_trend*1e7,\n        sigma_cycle*1e7, damping_cycle, period_cycle,\n        sigma_irregular*1e7\n    ]\n    i += 1\n\npd.set_option('float_format', lambda x: '%.4g' % np.round(x, 2) if not np.isnan(x) else '-')\ntable_i = pd.DataFrame(table_i, index=index, columns=parameter_symbols)\ntable_i",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Personal consumption', xlabel='Date', ylabel='Billions of dollars')\n\n# Plot data points\ndata.loc['1977-07-01':, 'consump'].plot(ax=ax, style='o', label='Observed')\n\n# Plot predictions\npredict.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='r--', label='One-step-ahead forecast')\nci = predict_ci.loc['1977-07-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\npredict_dy.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='g', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-07-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='g', alpha=0.1)\n\nlegend = ax.legend(loc='lower right')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>"
        ]
    },
    {
        "id": "715128",
        "GT": "Transforming Data->Calculate percentile-groups based on a student's average daily clicks in a certain decile each semester\nNecessary to alleviate effects from structural changes between semesters->BBB_Train[\"Rank_Clicks\"] = 0\nDDD_Train[\"Rank_Clicks\"] = 0\nFFF_Train[\"Rank_Clicks\"] = 0\n\nBBB_Test[\"Rank_Clicks\"] = 0\nDDD_Test[\"Rank_Clicks\"] = 0\nFFF_Test[\"Rank_Clicks\"] = 0",
        "pred": [
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing->ax = oildata.plot()\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Oil (millions of tonnes)\")\nprint(\"Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\")",
            "torch->Extending PyTorch->Double Backward with Custom Functions->Saving Intermediate Results: What not to do->out = SinhBad.apply(x)\ngrad_x, = torch.autograd.grad(out.sum(), x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out})",
            "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Concentrating out the scale->mod_conc = LocalLevelConcentrated(dta.infl)\nres_conc = mod_conc.fit(disp=False)\nprint(res_conc.summary())"
        ]
    },
    {
        "id": "226104",
        "GT": "accuracy = np.mean(positive_rating[row_idx, col_idx] == validation[:, 2])\naccuracy",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:->predicted = res_log.model.predict(res_log.params, exog=data_student[['pared', 'public', 'gpa']])\npredicted",
            "pandas_toms_blog->Scaling->Dask->most_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total",
            "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Model Support->Simulate Some Data->auto_reg_forecast = res.predict(200, 211)\nauto_reg_forecast",
            "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Applying to all colors->img_array_transposed = np.transpose(img_array, (2, 0, 1))\nimg_array_transposed.shape"
        ]
    },
    {
        "id": "1076191",
        "GT": "Predicting the Post2010 price->50%, it's 0, it cancels out the differences between the price by the fixed features vs renovatable\n    So the company has to work with only working with the extreme values->k = 10 #number of variables for heatmap\ncols = Truth.corr().nlargest(k, 'RenovatablePriceFactor')['RenovatablePriceFactor'].index # Basically just show the K-largest correlations\ncm = np.corrcoef(Truth[cols].values.T)\nsns.set(font_scale=1.25)\nplt.subplots(figsize=[12,12])\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)",
        "pred": [
            "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation->n_features = 100\n# simulation covariance matrix (AR(1) process)\nr = 0.1\nreal_cov = (r ** (n_features))\ncoloring_matrix = (real_cov)\n\nn_samples_range = (6, 31, 1)\nrepeat = 100\nlw_mse = ((n_samples_range.size, repeat))\noa_mse = ((n_samples_range.size, repeat))\nlw_shrinkage = ((n_samples_range.size, repeat))\noa_shrinkage = ((n_samples_range.size, repeat))\nfor i, n_samples in enumerate(n_samples_range):\n    for j in range(repeat):\n        X = ((size=(n_samples, n_features)), coloring_matrix.T)\n\n        lw = (store_precision=False, assume_centered=True)\n        lw.fit(X)\n        lw_mse[i, j] = lw.error_norm(real_cov, scaling=False)\n        lw_shrinkage[i, j] = lw.shrinkage_\n\n        oa = (store_precision=False, assume_centered=True)\n        oa.fit(X)\n        oa_mse[i, j] = oa.error_norm(real_cov, scaling=False)\n        oa_shrinkage[i, j] = oa.shrinkage_\n\n# plot MSE\n(2, 1, 1)\n(\n    n_samples_range,\n    lw_mse.mean(1),\n    yerr=lw_mse.std(1),\n    label=\"Ledoit-Wolf\",\n    color=\"navy\",\n    lw=2,\n)\n(\n    n_samples_range,\n    oa_mse.mean(1),\n    yerr=oa_mse.std(1),\n    label=\"OAS\",\n    color=\"darkorange\",\n    lw=2,\n)\n(\"Squared error\")\n(loc=\"upper right\")\n(\"Comparison of covariance estimators\")\n(5, 31)\n\n# plot shrinkage coefficient\n(2, 1, 2)\n(\n    n_samples_range,\n    lw_shrinkage.mean(1),\n    yerr=lw_shrinkage.std(1),\n    label=\"Ledoit-Wolf\",\n    color=\"navy\",\n    lw=2,\n)\n(\n    n_samples_range,\n    oa_shrinkage.mean(1),\n    yerr=oa_shrinkage.std(1),\n    label=\"OAS\",\n    color=\"darkorange\",\n    lw=2,\n)\n(\"n_samples\")\n(\"Shrinkage\")\n(loc=\"lower right\")\n(()[0], 1.0 + (()[1] - ()[0]) / 10.0)\n(5, 31)\n\n()\n\n\n<img alt=\"Comparison of covariance estimators\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_lw_vs_oas_001.png\" srcset=\"../../_images/sphx_glr_plot_lw_vs_oas_001.png\"/>",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult->lags = int(10 * np.log10(arma_rvs.shape[0]))\narma11 = ARIMA(arma_rvs, order=(1, 0, 1)).fit()\nresid = arma11.resid\nr, q, p = sm.tsa.acf(resid, nlags=lags, fft=True, qstat=True)\ndata = np.c_[range(1, lags + 1), r[1:], q, p]\ntable = pd.DataFrame(data, columns=[\"lag\", \"AC\", \"Q\", \"Prob(Q)\"])\nprint(table.set_index(\"lag\"))",
            "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns->n_comps = 2\n\nmethods = [\n    (\"PCA\", ()),\n    (\"Unrotated FA\", ()),\n    (\"Varimax FA\", (rotation=\"varimax\")),\n]\nfig, axes = (ncols=len(methods), figsize=(10, 8), sharey=True)\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks((len(feature_names)))\n    ax.set_yticklabels(feature_names)\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\nfig.suptitle(\"Factors\")\n()\n()\n\n\n<img alt=\"Factors, PCA, Unrotated FA, Varimax FA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_002.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_002.png\"/>",
            "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->Comparing OLS and RLM->nsample = 50\nx1 = np.linspace(0, 20, nsample)\nX = np.column_stack((x1, (x1 - 5) ** 2))\nX = sm.add_constant(X)\nsig = 0.3  # smaller error variance makes OLS&lt;-RLM contrast bigger\nbeta = [5, 0.5, -0.0]\ny_true2 = np.dot(X, beta)\ny2 = y_true2 + sig * 1.0 * np.random.normal(size=nsample)\ny2[[39, 41, 43, 45, 48]] -= 5  # add some outliers (10% of nsample)",
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Optimisation of kernel hyperparameters in GPR->vmin, vmax = (-log_marginal_likelihood).min(), 50\nlevel = (((vmin), (vmax), num=50), decimals=1)\n(\n    length_scale_grid,\n    noise_level_grid,\n    -log_marginal_likelihood,\n    levels=level,\n    norm=(vmin=vmin, vmax=vmax),\n)\n()\n(\"log\")\n(\"log\")\n(\"Length-scale\")\n(\"Noise-level\")\n(\"Log-marginal-likelihood\")\n()\n\n\n<img alt=\"Log-marginal-likelihood\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\" srcset=\"../../_images/sphx_glr_plot_gpr_noisy_005.png\"/>"
        ]
    },
    {
        "id": "54427",
        "GT": "Acquiring Data\n## Geographical specification\nCenpy uses FIPS codes to specify the geographical extent of the data to be downloaded. The object con is our connection to the api, and the attribute geographies is a dictionary.->Specifying variables to extract\n\nThe other argument taken by query() is cols. This is a list of columns taken from the variables of the API. These variables can be displayed using the variables function, however, due to the number of variables it is easier to use the Social Explorer site to find data you are interested in.\n\nhttps://www.socialexplorer.com/->cols = ['B19083_001E']\ncols.extend(['NAME', 'GEOID'])",
        "pred": [
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors->AB = results.params\nA = AB[0]\nB = AB[1]",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_wide_list = [col for _, col in flights_wide.items()]\nsns.relplot(data=flights_wide_list, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data->flights_wide_list = [col for _, col in flights_wide.items()]\nsns.relplot(data=flights_wide_list, kind=\"line\")\n",
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()"
        ]
    },
    {
        "id": "944408",
        "GT": "DepDelay = flights.loc[lambda df: flights['DepDelay'] < 29, :]\nArrDelay = flights.loc[lambda df: flights['ArrDelay'] < 27, :]\ndelayDep = DepDelay['DepDelay']\ndelayArr = ArrDelay['ArrDelay']",
        "pred": [
            "numpy->NumPy Applications->Determining Static Equilibrium in NumPy->Solving equilibrium with Newton\u2019s second law->forceA = np.array([1, 0, 0])\nforceB = np.array([0, 1, 0])\nprint(\"Force A =\", forceA)\nprint(\"Force B =\", forceB)",
            "sklearn->Examples->Generalized Linear Models->Comparing various online solvers->training SGD\ntraining ASGD\ntraining Perceptron\ntraining Passive-Aggressive I\ntraining Passive-Aggressive II\ntraining SAG\n\n\n\n<br/>",
            "sklearn->Examples->Clustering->Empirical evaluation of the impact of k-means initialization->Evaluation of KMeans with k-means++ init\nEvaluation of KMeans with random init\nEvaluation of MiniBatchKMeans with k-means++ init\nEvaluation of MiniBatchKMeans with random init\n\n\n\n<br/>",
            "pandas_toms_blog->Indexes->Flavors->Indexes for Alignment->dsm = weather.loc['DSM']\n\nhourly = dsm.resample('H').mean()\n\ntemp = hourly['tmpf'].sample(frac=.5, random_state=1).sort_index()\nsped = hourly['sped'].sample(frac=.5, random_state=2).sort_index()",
            "matplotlib->Tutorials->Advanced->Transformations Tutorial->The transformation pipeline->self.transData = (\n    self.transScale + self.transShift + self.transProjection +\n    (self.transProjectionAffine + self.transWedge + self.transAxes))"
        ]
    },
    {
        "id": "921933",
        "GT": "Data Exploration\nFor this project, we will be using a version of the MNIST dataset pulled from the tensorflow tutorials library. The dataset we will be using contains 70,000 grayscale images of handritten digits between 0 and 9. They are stored as a data class, with the images section contsining an array representing the individual pixels for each 28x28 image, and the labels containing a boolean array representing whether or not. The training set is comprised of the first 55,000 images and their labels, and the second 10,000 are used for testing the classification methods. A final set of 5,000 images are saved for validation (we will not be using these).->A Basic Classification Approach - Pairwise Comparison->centroids = []\nfor i in range(10):\n    tmp = np.where((np.where(mnist.train.labels == 1)[1]) == i)[0]\n    tmp = np.average(mnist.train.images[tmp], axis=0)\n    centroids.append(tmp)",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis->oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis->oo = np.ones(df.shape[0])\nmodel4 = sm.MixedLM(df.y, oo[:, None], exog_re=None, groups=oo, exog_vc=vcs)\nresult4 = model4.fit()\nprint(result4.summary())",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()"
        ]
    },
    {
        "id": "640743",
        "GT": "5. Model Training:->*Model Selection*-># Creating Decision Tree model\nfrom sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()",
        "pred": [
            "torch->PyTorch Recipes->Warmstarting model using parameters from a different model in PyTorch->Steps->3. Save model A-># Specify a path to save to\nPATH = \"model.pt\"\n\n(netA.state_dict(), PATH)",
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->TVP-VAR model in Statsmodels-># Create an instance of our TVPVAR class with our observed dataset y\nmod = TVPVAR(y)",
            "sklearn->Examples->Model Selection->Sample pipeline for text feature extraction and evaluation->Pipeline with hyperparameter tuning->from sklearn.feature_extraction.text import \nfrom sklearn.naive_bayes import \nfrom sklearn.pipeline import \n\npipeline = (\n    [\n        (\"vect\", ()),\n        (\"clf\", ()),\n    ]\n)\npipeline",
            "torch->PyTorch Recipes->Saving and loading models for inference in PyTorch->Steps->5. Save and load entire model-># Specify a path\nPATH = \"entire_model.pt\"\n\n# Save\n(net, PATH)\n\n# Load\nmodel = (PATH)\nmodel.eval()",
            "sklearn->Examples->Model Selection->Multiclass Receiver Operating Characteristic (ROC)->Load and prepare data->from sklearn.linear_model import \n\nclassifier = ()\ny_score = classifier.fit(X_train, y_train).predict_proba(X_test)"
        ]
    },
    {
        "id": "9180",
        "GT": "x = tf.placeholder(tf.float32, [None, n_input])\ny = tf.placeholder(tf.float32, [None, n_classes])\nkeep_prob = tf.placeholder(tf.float32)",
        "pred": [
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Data generation->X = (0, 5, num=30).reshape(-1, 1)\ny = target_generator(X, add_noise=False)",
            "sklearn->Examples->Clustering->Demo of affinity propagation clustering algorithm->Generate sample data->centers = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = (\n    n_samples=300, centers=centers, cluster_std=0.5, random_state=0\n)",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "sklearn->Examples->Ensemble methods->Monotonic Constraints->gbdt_with_monotonic_cst = (monotonic_cst=[1, -1])\ngbdt_with_monotonic_cst.fit(X, y)",
            "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 1. Training a Custom Classifier based on a Quantized Feature Extractor->Train and evaluate->new_model = train_model(new_model, criterion, optimizer_ft, exp_lr_scheduler,\n                        num_epochs=25, device='cpu')\n\nvisualize_model(new_model)\nplt.tight_layout()"
        ]
    },
    {
        "id": "1421037",
        "GT": "Filtering DataFrames->Creating a Boolean Series->Filtering with a Boolean Series->df[df.salt > 60]",
        "pred": [
            "statsmodels->Examples->Statistics->ANOVA->df_infl[:5]",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Data exploration on the Bike Sharing Demand dataset->df[\"count\"].max()",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->df.ix[10:15, ['fl_date', 'tail_num']]",
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) on Mauna Loa CO2 data->Build the dataset->co2_data.index.min(), co2_data.index.max()",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Mini Project: Home Court Advantage?->Step 1: Create an outcome variable->df['home_win'] = df.home_points &gt; df.away_points"
        ]
    },
    {
        "id": "566972",
        "GT": "Processing Pclass->process_pclass()",
        "pred": [
            "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Capturing the Model with Symbolic Tracing->traced_rn18 = (rn18)\nprint()",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->textproc = TextPreprocess()",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resf_logit.predict()",
            "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Filardo (1994) Time-Varying Transition Probabilities->res_filardo.summary()",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Binary Model compared to Logit->res_logit.summary()"
        ]
    },
    {
        "id": "714393",
        "GT": "6 - Laplace Distribution->laplace = np.random.laplace(10, 2, 100)\n# Print the histogram\nplt.hist(laplace)\nplt.show()",
        "pred": [
            "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure->mod = ThetaModel(np.log(pce))\nres = mod.fit()\nprint(res.summary())",
            "statsmodels->Examples->Statistics->Copulas->Sampling from a copula->sample = copula.rvs(10000)\nh = sns.jointplot(x=sample[:, 0], y=sample[:, 1], kind=\"hex\")\n_ = h.set_axis_labels(\"X1\", \"X2\", fontsize=16)",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->Condition number->eigs = np.linalg.eigvals(norm_xtx)\ncondition_number = np.sqrt(eigs.max() / eigs.min())\nprint(condition_number)",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->mod = AutoReg(housing, 3, old_names=False)\nres = mod.fit()\nprint(res.summary())"
        ]
    },
    {
        "id": "200955",
        "GT": "The `re` module\n\nSo far our discussion of regular expressions has been relatively agnostic of language-specific implementations.  Those concepts will generally translate well into other settings like Java, Unix, R, etc.  In this section, we will explore a few common use-cases of regular expressions in Python's `re` module.->`re.findall`\n\nThis is your go-to function if you want to search through a body of text and return **all** regular expression matches rather than the first one.  If you supply a regular expression without groups, it will return a list of all non-overlapping matches.->gmail_regex = r'(([a-zA-Z0-9]+)@(gmail)\\.(com))'\ntext  = 'email1@gmail.com, email2@yahoo.com, email3@gmail.com'\nre.findall(gmail_regex, text)",
        "pred": [
            "pandas_toms_blog->Time Series->Timeseries->gs = web.DataReader(\"GS\", data_source='yahoo', start='2006-01-01',\n                    end='2010-01-01')\ngs.head().round(2)",
            "matplotlib->Tutorials->Intermediate->Styling with cycler->Cycling through multiple properties->from cycler import cycler\ncc = (cycler(color=list('rgb')) +\n      cycler(linestyle=['-', '--', '-.']))\nfor d in cc:\n    print(d)",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson->rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "matplotlib->Tutorials->Intermediate->Styling with cycler->Cycling through multiple properties->from cycler import cycler\ncc = (cycler(color=list('rgb')) *\n      cycler(linestyle=['-', '--', '-.']))\nfor d in cc:\n    print(d)"
        ]
    },
    {
        "id": "987800",
        "GT": "Emsemble methods to predict $ET_0$\n- Random Forest\n- Gradient Boosting->model_gb = ensemble.GradientBoostingRegressor(random_state=42)\nn_estimators = [20, 50, 100, 150, 200, 250]\nmax_depth = [3, 4, 5, 6, 7, 8, 9, 10]\nlearning_rate = [ 0.1, 0.05, 0.01, 0.001]\n\nparam_grid = {'n_estimators': n_estimators, \n              'max_depth': max_depth, \n              'learning_rate': learning_rate}\n\ngs = GridSearchCV(estimator = model_gb, \n                  param_grid = param_grid, \n                  scoring = 'neg_mean_squared_error', \n                  cv = 10)\ngs = gs.fit(X_treino, y_treino)\n\nprint(gs.best_params_)",
        "pred": [
            "sklearn->Examples->Support Vector Machines->Scaling the regularization parameter for SVCs->L2-penalty case->model_l2 = (penalty=\"l2\", loss=\"squared_hinge\", dual=True)\nCs = (-4.5, -2, 10)\n\nlabels = [f\"fraction: {train_size}\" for train_size in train_sizes]\nresults = {\"C\": Cs}\nfor label, train_size in zip(labels, train_sizes):\n    cv = (train_size=train_size, test_size=0.3, n_splits=50, random_state=1)\n    train_scores, test_scores = (\n        model_l2, X, y, param_name=\"C\", param_range=Cs, cv=cv\n    )\n    results[label] = test_scores.mean(axis=1)\nresults = (results)",
            "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch->Steps->5. Load the general checkpoint->model = Net()\noptimizer = (net.parameters(), lr=0.001, momentum=0.9)\n\ncheckpoint = (PATH)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\nmodel.eval()\n# - or -\nmodel.train()",
            "sklearn->Examples->Model Selection->Train error vs Test error->Compute train and test errors->alphas = (-5, 1, 60)\nenet = (l1_ratio=0.7, max_iter=10000)\ntrain_errors = list()\ntest_errors = list()\nfor alpha in alphas:\n    enet.set_params(alpha=alpha)\n    enet.fit(X_train, y_train)\n    train_errors.append(enet.score(X_train, y_train))\n    test_errors.append(enet.score(X_test, y_test))\n\ni_alpha_optim = (test_errors)\nalpha_optim = alphas[i_alpha_optim]\nprint(\"Optimal regularization parameter : %s\" % alpha_optim)\n\n# Estimate the coef_ on full data with optimal regularization parameter\nenet.set_params(alpha=alpha_optim)\ncoef_ = enet.fit(X, y).coef_",
            "torch->Model Optimization->Profiling your PyTorch Module->Profile the forward pass->model = MyModule(500, 10).cuda()\ninput = (128, 500).cuda()\nmask = ((500, 500, 500), dtype=torch.double).cuda()\n\n# warm-up\nmodel(input, mask)\n\nwith (with_stack=True, profile_memory=True) as prof:\n    out, idx = model(input, mask)",
            "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->Train models on the diabetes dataset->diabetes = ()\nX = (diabetes.data, columns=diabetes.feature_names)\ny = diabetes.target\n\ntree = ()\nmlp = (\n    (),\n    (hidden_layer_sizes=(100, 100), tol=1e-2, max_iter=500, random_state=0),\n)\ntree.fit(X, y)\nmlp.fit(X, y)"
        ]
    },
    {
        "id": "507966",
        "GT": "5. Data Preprocessing->5.1. New variables' creation->#function that extracts title\ndef title(passenger):\n    name = passenger['Name']\n    start = name.find(',') + 1\n    end = name.find('.')\n    \n    return name[start:end].strip()\n\n#extracting title\ndf_train['title'] = df_train.apply(title, axis=1)\ndf_test['title'] = df_test.apply(title, axis=1)\n\n#showing unique values and count\nprint(df_train.groupby('title').size(), '\\n\\n',\n      df_test.groupby('title').size())",
        "pred": [
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->3. Build the Deep Learning Model\u00b6->Sentiment Analysis on the Speech Data-># To store predicted sentiments\npredictions = {}\n\n# define the length of a paragraph\npara_len = 100\n\n# Retrieve trained values of the parameters\nif os.path.isfile('tutorial-nlp-from-scratch/parameters.npy'):\n    parameters = np.load('tutorial-nlp-from-scratch/parameters.npy', allow_pickle=True).item()\n\n# This is the prediction loop.\nfor index, text in enumerate(X_pred):\n    # split each speech into paragraphs\n    paras = textproc.text_to_paras(text, para_len)\n    # To store the network outputs\n    preds = []\n\n    for para in paras:\n        # split text sample into words/tokens\n        para_tokens = textproc.word_tokeniser(para)\n        # Forward Propagation\n        caches = forward_prop(para_tokens,\n                              parameters,\n                              input_dim)\n\n        # Retrieve the output of the fully connected layer\n        sent_prob = caches['fc_values'][0]['a2'][0][0]\n        preds.append(sent_prob)\n\n    threshold = 0.5\n    preds = np.array(preds)\n    # Mark all predictions &gt; threshold as positive and &lt; threshold as negative\n    pos_indices = np.where(preds &gt; threshold)  # indices where output &gt; 0.5\n    neg_indices = np.where(preds &lt; threshold)  # indices where output &lt; 0.5\n    # Store predictions and corresponding piece of text\n    predictions[speakers[index]] = {'pos_paras': paras[pos_indices[0]],\n                                    'neg_paras': paras[neg_indices[0]]}",
            "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds-># range of admissible distortions\neps_range = (0.1, 0.99, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = (1, 9, 9)\n\n()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = (n_samples_range, eps=eps)\n    (n_samples_range, min_n_components, color=color)\n\n([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n(\"Number of observations to eps-embed\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_samples vs n_components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\"/>",
            "statsmodels->User Guide->Time Series Analysis->Vector Autoregressions tsa.vector_ar->VAR(p) processes->Model fitting-># some example data\nIn [1]: import numpy as np\n\nIn [2]: import pandas\n\nIn [3]: import statsmodels.api as sm\n\nIn [4]: from statsmodels.tsa.api import VAR\n\nIn [5]: mdata = sm.datasets.macrodata.load_pandas().data\n\n# prepare the dates index\nIn [6]: dates = mdata[['year', 'quarter']].astype(int).astype(str)\n\nIn [7]: quarterly = dates[\"year\"] + \"Q\" + dates[\"quarter\"]\n\nIn [8]: from statsmodels.tsa.base.datetools import dates_from_str\n\nIn [9]: quarterly = dates_from_str(quarterly)\n\nIn [10]: mdata = mdata[['realgdp','realcons','realinv']]\n\nIn [11]: mdata.index = pandas.DatetimeIndex(quarterly)\n\nIn [12]: data = np.log(mdata).diff().dropna()\n\n# make a VAR model\nIn [13]: model = VAR(data)",
            "torch->Text->NLP From Scratch: Generating Names with a Character-Level RNN->Training->Preparing for Training-># One-hot vector for category\ndef categoryTensor(category):\n    li = all_categories.index(category)\n    tensor = (1, n_categories)\n    tensor[0][li] = 1\n    return tensor\n\n# One-hot matrix of first to last letters (not including EOS) for input\ndef inputTensor(line):\n    tensor = (len(line), 1, n_letters)\n    for li in range(len(line)):\n        letter = line[li]\n        tensor[li][0][all_letters.find(letter)] = 1\n    return tensor\n\n# LongTensor of second letter to end (EOS) for target\ndef targetTensor(line):\n    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n    letter_indexes.append(n_letters - 1) # EOS\n    return torch.LongTensor(letter_indexes)",
            "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation-># plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>"
        ]
    },
    {
        "id": "1242106",
        "GT": "Grant applications: NSERC's Awards Data->Data sources\n\nThe data is acessible from open data portal of government of Canada.\n\nhttp://open.canada.ca/data/en/dataset/c1b0f627-8c29-427c-ab73-33968ad9176e <br>\nWe start with the 2017 Awards dataset, which is named NSERC_GRT_FYR2016_AWARD.csv->awards2016.info()",
        "pred": [
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time.unique()",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->survey.data.info()",
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time.head()",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->Stack / Unstack->rest.unstack().head()",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Binary Model compared to Logit->res_logit.summary()"
        ]
    },
    {
        "id": "1312124",
        "GT": "Non-high leverage point\n\nThis outlier has some effect on the slope, but not as much as the influential point in the previous example. This is also demonstrated by the Cook's distance values.->x_c = copy.deepcopy(x)\ny_c = copy.deepcopy(y)\n\nx_c = np.append(x_c, 65)\ny_c = np.append(y_c, 65*SLOPE + INTERCEPT + 20)",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Data->spector_data = sm.datasets.spector.load()\nspector_data.exog = sm.add_constant(spector_data.exog, prepend=False)",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson->rand_data = sm.datasets.randhie.load()\nrand_exog = rand_data.exog\nrand_exog = sm.add_constant(rand_exog, prepend=False)",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting many distributions->g = sns.PairGrid(penguins)\ng.map_upper(sns.histplot)\ng.map_lower(sns.kdeplot, fill=True)\ng.map_diag(sns.histplot, kde=True)\n",
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian Processes regression: basic introductory example->Example with noise-free target->rng = (1)\ntraining_indices = rng.choice((y.size), size=6, replace=False)\nX_train, y_train = X[training_indices], y[training_indices]",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit->anes_data = sm.datasets.anes96.load()\nanes_exog = anes_data.exog\nanes_exog = sm.add_constant(anes_exog)"
        ]
    },
    {
        "id": "1232588",
        "GT": "Generator Visualisation->2D Convolution 1 Output->plt.rcParams['figure.figsize'] = (15, 15)\nc1 = np.concatenate([np.concatenate([conv1[:, :, :, j*16+i].squeeze() for i in range(16)], axis=1) for j in range(16)])\nc1img = Image.fromarray((c1 + 1) * 127.5)\nplt.imshow(c1img)",
        "pred": [
            "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->rcParams->plt.rcParams['figure.constrained_layout.use'] = True\nfig, axs = plt.subplots(2, 2, figsize=(3, 3))\nfor ax in axs.flat:\n    example_plot(ax)\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_018.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_018.png, ../../_images/sphx_glr_constrainedlayout_guide_018_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec->plt.rcParams['figure.constrained_layout.use'] = False\nfig = plt.figure(layout=\"constrained\")\n\ngs1 = gridspec.GridSpec(2, 1, figure=fig)\nax1 = fig.add_subplot(gs1[0])\nax2 = fig.add_subplot(gs1[1])\n\nexample_plot(ax1)\nexample_plot(ax2)\n\n\n<img alt=\"Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_019.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_019.png, ../../_images/sphx_glr_constrainedlayout_guide_019_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Introductory->The Lifecycle of a Plot->Customizing the plot->plt.rcParams.update({'figure.autolayout': True})\n\nfig, ax = plt.subplots()\nax.barh(group_names, group_data)\nlabels = ax.get_xticklabels()\nplt.setp(labels, rotation=45, horizontalalignment='right')\n\n\n<img alt=\"lifecycle\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_lifecycle_006.png\" srcset=\"../../_images/sphx_glr_lifecycle_006.png, ../../_images/sphx_glr_lifecycle_006_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Intermediate->Tight Layout guide->Simple Example->plt.close('all')\nfig = plt.figure()\n\nax1 = plt.subplot2grid((3, 3), (0, 0))\nax2 = plt.subplot2grid((3, 3), (0, 1), colspan=2)\nax3 = plt.subplot2grid((3, 3), (1, 0), colspan=2, rowspan=2)\nax4 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)\n\nexample_plot(ax1)\nexample_plot(ax2)\nexample_plot(ax3)\nexample_plot(ax4)\n\nplt.tight_layout()\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_007.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_007.png, ../../_images/sphx_glr_tight_layout_guide_007_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Intermediate->Tight Layout guide->Colorbar->plt.close('all')\narr = np.arange(100).reshape((10, 10))\nfig = plt.figure(figsize=(4, 4))\nim = plt.imshow(arr, interpolation=\"none\")\n\nplt.colorbar(im)\n\nplt.tight_layout()\n\n\n<img alt=\"tight layout guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_014.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_014.png, ../../_images/sphx_glr_tight_layout_guide_014_2_0x.png 2.0x\"/>"
        ]
    },
    {
        "id": "455986",
        "GT": "modeldata['Sex'].drop_duplicates()",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resf_logit.predict()",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.dtypes",
            "seaborn->User guide and tutorial->Estimating regression fits->Fitting different kinds of models->anscombe = sns.load_dataset(\"anscombe\")\n",
            "seaborn->Statistical operations->Estimating regression fits->Fitting different kinds of models->anscombe = sns.load_dataset(\"anscombe\")\n",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->pca_model = PCA(dta.T, standardize=False, demean=True)",
            "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money->res.plot_cusum()"
        ]
    },
    {
        "id": "413255",
        "GT": "countries_json = json.loads(countries_raw.text)\ncountries_json",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Model Support->Simulate Some Data->auto_reg_forecast = res.predict(200, 211)\nauto_reg_forecast",
            "pandas_toms_blog->Scaling->Dask->most_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_total = nbcases_ma[china_mask].sum(axis=0)\nchina_total",
            "numpy->NumPy Features->Masked Arrays->Missing data->china_total = china_masked.data\nchina_total",
            "statsmodels->Examples->Statistics->ANOVA->interM_lm.model.exog\ninterM_lm.model.exog_names"
        ]
    },
    {
        "id": "869915",
        "GT": "match_sum ~ amb_iRateMe_exp + amb_iMeasUp_2->figure = plt.figure(figsize = (12, 8))\nfigure = sm.graphics.plot_regress_exog(model, 'amb_iRateMe_exp', fig = figure)",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))",
            "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)->fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Exercise: How good of in-sample prediction can you do for another series, say, CPI->Hint:->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax = cpi.plot(ax=ax)\nax.legend()",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->Seasonal Dummies->fig = plt.figure(figsize=(16, 9))\nfig = res.plot_diagnostics(lags=30, fig=fig)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_autoregressions_21_0.png\" src=\"../../../_images/examples_notebooks_generated_autoregressions_21_0.png\"/>",
            "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Partial Regression Plots (Duncan)->fig = sm.graphics.plot_partregress_grid(prestige_model)\nfig.tight_layout(pad=1.0)"
        ]
    },
    {
        "id": "196279",
        "GT": "Cleaning and Inspecting the Data.-># Access the data file from the FBI: UCR \ndataset = pd.read_excel(\"NYCCrime.xls\", header=4)",
        "pred": [
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->Forecasting-># The default is to get a one-step-ahead forecast:\nprint(res.forecast())",
            "torch->PyTorch Recipes->Defining a Neural Network in PyTorch->Steps->4. [Optional] Pass data through your model to test-># Equates to one random 28x28 image\nrandom_data = ((1, 1, 28, 28))\n\nmy_nn = Net()\nresult = my_nn(random_data)\nprint (result)",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()",
            "statsmodels->Examples->State space models->State space modeling: Local Linear Trends-># Perform prediction and forecasting\npredict = res.get_prediction()\nforecast = res.get_forecast('2014')"
        ]
    },
    {
        "id": "1094427",
        "GT": "Merge the two databases-># Get connections to the databases\ndb_a = sqlite3.connect('livetweets.db')\ndb_b = sqlite3.connect('histtweets.db')\n\n# Get the contents of a table\nb_cursor = db_b.cursor()\nb_cursor.execute('SELECT * FROM tweets')\noutput = b_cursor.fetchall()   # Returns the results as a list.\n\n# Insert those contents into another table.\na_cursor = db_a.cursor()\nfor row in output:\n    try:\n        a_cursor.execute('INSERT INTO tweets VALUES (?,?,?,?,?,?,?,?,?,?,?)', row)\n    except sqlite3.IntegrityError: # skip duplicate tweet ids\n        pass\n\n# Cleanup\ndb_a.commit()\na_cursor.close()\nb_cursor.close()\n\n# Rename the merged db, and delete the other\nos.rename('livetweets.db', 'tweets.db')\nos.remove('histtweets.db')",
        "pred": [
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example-># Setup forecasts\nnforecasts = 3\nforecasts = {}\n\n# Get the number of initial training observations\nnobs = len(endog)\nn_init_training = int(nobs * 0.8)\n\n# Create model for initial training sample, fit parameters\ninit_training_endog = endog.iloc[:n_init_training]\nmod = sm.tsa.SARIMAX(training_endog, order=(1, 0, 0), trend='c')\nres = mod.fit()\n\n# Save initial forecast\nforecasts[training_endog.index[-1]] = res.forecast(steps=nforecasts)\n\n# Step through the rest of the sample\nfor t in range(n_init_training, nobs):\n    # Update the results by appending the next observation\n    updated_endog = endog.iloc[t:t+1]\n    res = res.append(updated_endog, refit=False)\n\n    # Save the new set of forecasts\n    forecasts[updated_endog.index[0]] = res.forecast(steps=nforecasts)\n\n# Combine all forecasts into a dataframe\nforecasts = pd.concat(forecasts, axis=1)\n\nprint(forecasts.iloc[:5, :5])",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend-># Setup forecasts\nnforecasts = 3\nforecasts = {}\n\n# Get the number of initial training observations\nnobs = len(endog)\nn_init_training = int(nobs * 0.8)\n\n# Create model for initial training sample, fit parameters\ninit_training_endog = endog.iloc[:n_init_training]\nmod = sm.tsa.SARIMAX(training_endog, order=(1, 0, 0), trend='c')\nres = mod.fit()\n\n# Save initial forecast\nforecasts[training_endog.index[-1]] = res.forecast(steps=nforecasts)\n\n# Step through the rest of the sample\nfor t in range(n_init_training, nobs):\n    # Update the results by appending the next observation\n    updated_endog = endog.iloc[t:t+1]\n    res = res.extend(updated_endog)\n\n    # Save the new set of forecasts\n    forecasts[updated_endog.index[0]] = res.forecast(steps=nforecasts)\n\n# Combine all forecasts into a dataframe\nforecasts = pd.concat(forecasts, axis=1)\n\nprint(forecasts.iloc[:5, :5])",
            "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Kim, Nelson, and Startz (1998) Three-state Variance Switching-># Get the dataset\new_excs = requests.get(\"http://econ.korea.ac.kr/~cjkim/MARKOV/data/ew_excs.prn\").content\nraw = pd.read_table(BytesIO(ew_excs), header=None, skipfooter=1, engine=\"python\")\nraw.index = pd.date_range(\"1926-01-01\", \"1995-12-01\", freq=\"MS\")\n\ndta_kns = raw.loc[:\"1986\"] - raw.loc[:\"1986\"].mean()\n\n# Plot the dataset\ndta_kns[0].plot(title=\"Excess returns\", figsize=(12, 3))\n\n# Fit the model\nmod_kns = sm.tsa.MarkovRegression(\n    dta_kns, k_regimes=3, trend=\"n\", switching_variance=True\n)\nres_kns = mod_kns.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\"/>",
            "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Filardo (1994) Time-Varying Transition Probabilities-># Get the dataset\nfilardo = requests.get(\"http://econ.korea.ac.kr/~cjkim/MARKOV/data/filardo.prn\").content\ndta_filardo = pd.read_table(\n    BytesIO(filardo), sep=\" +\", header=None, skipfooter=1, engine=\"python\"\n)\ndta_filardo.columns = [\"month\", \"ip\", \"leading\"]\ndta_filardo.index = pd.date_range(\"1948-01-01\", \"1991-04-01\", freq=\"MS\")\n\ndta_filardo[\"dlip\"] = np.log(dta_filardo[\"ip\"]).diff() * 100\n# Deflated pre-1960 observations by ratio of std. devs.\n# See hmt_tvp.opt or Filardo (1994) p. 302\nstd_ratio = (\n    dta_filardo[\"dlip\"][\"1960-01-01\":].std() / dta_filardo[\"dlip\"][:\"1959-12-01\"].std()\n)\ndta_filardo[\"dlip\"][:\"1959-12-01\"] = dta_filardo[\"dlip\"][:\"1959-12-01\"] * std_ratio\n\ndta_filardo[\"dlleading\"] = np.log(dta_filardo[\"leading\"]).diff() * 100\ndta_filardo[\"dmdlleading\"] = dta_filardo[\"dlleading\"] - dta_filardo[\"dlleading\"].mean()\n\n# Plot the data\ndta_filardo[\"dlip\"].plot(\n    title=\"Standardized growth rate of industrial production\", figsize=(13, 3)\n)\nplt.figure()\ndta_filardo[\"dmdlleading\"].plot(title=\"Leading indicator\", figsize=(13, 3))",
            "statsmodels->User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Models and Estimation->Unobserved Components-># Load the statsmodels api\nimport statsmodels.api as sm\n\n# Load your dataset\nendog = pd.read_csv('your/dataset/here.csv')\n\n# Fit a local level model\nmod_ll = sm.tsa.UnobservedComponents(endog, 'local level')\n# Note that mod_ll is an instance of the UnobservedComponents class\n\n# Fit the model via maximum likelihood\nres_ll = mod_ll.fit()\n# Note that res_ll is an instance of the UnobservedComponentsResults class\n\n# Show the summary of results\nprint(res_ll.summary())\n\n# Show a plot of the estimated level and trend component series\nfig_ll = res_ll.plot_components()\n\n# We could further add a damped stochastic cycle as follows\nmod_cycle = sm.tsa.UnobservedComponents(endog, 'local level', cycle=True,\n                                        damped_cycle=True,\n                                        stochastic_cycle=True)\nres_cycle = mod_cycle.fit()\n\n# Show the summary of results\nprint(res_cycle.summary())\n\n# Show a plot of the estimated level, trend, and cycle component series\nfig_cycle = res_cycle.plot_components()"
        ]
    },
    {
        "id": "126499",
        "GT": "2. Get list of gridded climate points for the watershed\n\nThis example uses a shapefile with the watershed boundary of the Sauk-Suiattle Basin, which is stored in HydroShare at the following url: https://www.hydroshare.org/resource/c532e0578e974201a0bc40a37ef2d284/. \n\nThe data for our processing routines can be retrieved using the getResourceFromHydroShare function by passing in the global identifier from the url above.  In the next cell, we download this resource from HydroShare, and identify that the points in this resource are available for downloading gridded hydrometeorology data, based on the point shapefile at https://www.hydroshare.org/resource/ef2d82bf960144b4bfb1bae6242bcc7f/, which is for the extent of North America and includes the average elevation for each 1/16 degree grid cell.  The file must include columns with station numbers, latitude, longitude, and elevation. The header of these columns must be FID, LAT, LONG_, and ELEV or RASTERVALU, respectively. The station numbers will be used for the remainder of the code to uniquely reference data from each climate station, as well as to identify minimum, maximum, and average elevation of all of the climate stations.  The webserice is currently set to a URL for the smallest geographic overlapping extent - e.g. WRF for Columbia River Basin (to use a limit using data from a FTP service, treatgeoself() would need to be edited in observatory_gridded_hydrometeorology utility).->\"\"\"\n1/16-degree Gridded cell centroids\n\"\"\"\n# List of available data\nhs.getResourceFromHydroShare('ef2d82bf960144b4bfb1bae6242bcc7f')\nNAmer = hs.content['NAmer_dem_list.shp']\n\n\"\"\"\nSauk\n\"\"\"\n# Watershed extent\nhs.getResourceFromHydroShare('c532e0578e974201a0bc40a37ef2d284')\nsauk = hs.content['wbdhub12_17110006_WGS84_Basin.shp']\n\n\"\"\"\nElwha\n\"\"\"\n# Watershed extent\nhs.getResourceFromHydroShare('4aff8b10bc424250b3d7bac2188391e8', )\nelwha = hs.content[\"elwha_ws_bnd_wgs84.shp\"]\n\n\"\"\"\nRio Salado\n\"\"\"\n# Watershed extent\nhs.getResourceFromHydroShare('5c041d95ceb64dce8eb85d2a7db88ed7')\nriosalado = hs.content['UpperRioSalado_delineatedBoundary.shp']",
        "pred": [
            "torch->Parallel and Distributed Training->Writing Distributed Applications with PyTorch->Distributed Training->\"\"\" Dataset partitioning helper \"\"\"\nclass Partition(object):\n\n    def __init__(self, data, index):\n        self.data = data\n        self.index = index\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, index):\n        data_idx = self.index[index]\n        return self.data[data_idx]\n\n\nclass DataPartitioner(object):\n\n    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n        self.data = data\n        self.partitions = []\n        rng = Random()\n        rng.seed(seed)\n        data_len = len(data)\n        indexes = [x for x in range(0, data_len)]\n        rng.shuffle(indexes)\n\n        for frac in sizes:\n            part_len = int(frac * data_len)\n            self.partitions.append(indexes[0:part_len])\n            indexes = indexes[part_len:]\n\n    def use(self, partition):\n        return Partition(self.data, self.partitions[partition])",
            "statsmodels->Examples->State space models->State space modeling: Local Linear Trends->\"\"\"\nUnivariate Local Linear Trend Model\n\"\"\"\nclass LocalLinearTrend(sm.tsa.statespace.MLEModel):\n    def __init__(self, endog):\n        # Model order\n        k_states = k_posdef = 2\n\n        # Initialize the statespace\n        super(LocalLinearTrend, self).__init__(\n            endog, k_states=k_states, k_posdef=k_posdef,\n            initialization='approximate_diffuse',\n            loglikelihood_burn=k_states\n        )\n\n        # Initialize the matrices\n        self.ssm['design'] = np.array([1, 0])\n        self.ssm['transition'] = np.array([[1, 1],\n                                       [0, 1]])\n        self.ssm['selection'] = np.eye(k_states)\n\n        # Cache some indices\n        self._state_cov_idx = ('state_cov',) + np.diag_indices(k_posdef)\n\n    @property\n    def param_names(self):\n        return ['sigma2.measurement', 'sigma2.level', 'sigma2.trend']\n\n    @property\n    def start_params(self):\n        return [np.std(self.endog)]*3\n\n    def transform_params(self, unconstrained):\n        return unconstrained**2\n\n    def untransform_params(self, constrained):\n        return constrained**0.5\n\n    def update(self, params, *args, **kwargs):\n        params = super(LocalLinearTrend, self).update(params, *args, **kwargs)\n\n        # Observation covariance\n        self.ssm['obs_cov',0,0] = params[0]\n\n        # State covariance\n        self.ssm[self._state_cov_idx] = params[1:]",
            "matplotlib->Tutorials->Text->Text rendering with XeLaTeX/LuaLaTeX via the ``pgf`` backend->Choosing the TeX system->\"\"\"\n=============\nPGF texsystem\n=============\n\"\"\"\n\nimport matplotlib.pyplot as plt\nplt.rcParams.update({\n    \"pgf.texsystem\": \"pdflatex\",\n    \"pgf.preamble\": \"\\n\".join([\n         r\"\\usepackage[utf8x]{inputenc}\",\n         r\"\\usepackage[T1]{fontenc}\",\n         r\"\\usepackage{cmbright}\",\n    ]),\n})\n\nfig, ax = plt.subplots(figsize=(4.5, 2.5))\n\nax.plot(range(5))\n\nax.text(0.5, 3., \"serif\", family=\"serif\")\nax.text(0.5, 2., \"monospace\", family=\"monospace\")\nax.text(2.5, 2., \"sans-serif\", family=\"sans-serif\")\nax.set_xlabel(r\"\u00b5 is not $\\mu$\")\n\nfig.tight_layout(pad=.5)",
            "torch->Parallel and Distributed Training->Writing Distributed Applications with PyTorch->Distributed Training->\"\"\" Distributed Synchronous SGD Example \"\"\"\ndef run(rank, size):\n    torch.manual_seed(1234)\n    train_set, bsz = partition_dataset()\n    model = Net()\n    optimizer = optim.SGD(model.parameters(),\n                          lr=0.01, momentum=0.5)\n\n    num_batches = ceil(len(train_set.dataset) / float(bsz))\n    for epoch in range(10):\n        epoch_loss = 0.0\n        for data, target in train_set:\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            average_gradients(model)\n            optimizer.step()\n        print('Rank ', dist.get_rank(), ', epoch ',\n              epoch, ': ', epoch_loss / num_batches)",
            "torch->Parallel and Distributed Training->Writing Distributed Applications with PyTorch->Distributed Training->\"\"\" Partitioning MNIST \"\"\"\ndef partition_dataset():\n    dataset = datasets.MNIST('./data', train=True, download=True,\n                             transform=transforms.Compose([\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.1307,), (0.3081,))\n                             ]))\n    size = dist.get_world_size()\n    bsz = 128 / float(size)\n    partition_sizes = [1.0 / size for _ in range(size)]\n    partition = DataPartitioner(dataset, partition_sizes)\n    partition = partition.use(dist.get_rank())\n    train_set = torch.utils.data.DataLoader(partition,\n                                         batch_size=bsz,\n                                         shuffle=True)\n    return train_set, bsz"
        ]
    },
    {
        "id": "1175442",
        "GT": "Research Notes:\n\n**Eerkes Utwente et al. - Modulation of conductance and superconductivity top gating LAO/STO**:\n- Top gated device were fabricated on STO (001)-oriented substrate, at TiO2-layer\n- isolated measurement structure by lift-off patterend (lithography+photoresist) by using a-LAO layer.\n- LAO deposited by PLD. Crystalline LAO PLD-deposited in oxygen pressure of 2e-3 milibars at 850c\n- Sampled was cooled down to 600c at deposition pressure, post anneal of 1h at 600c at oxygen pressure of 600 milibars, followed a cool down to room temp at 600milibar pressure. (this creates insulating a-LAO/STO layer.\n- Ohmic contacts to the 2DEG is deposited by sputtering of Ti/Au.(Use an argon etcher first to remove some gold layers(?)). \n- Vg = 0V , $n_s = 1.9*10^{13}$ $cm^{-2}$. Which is typical value for metalic top-gates. ( In the experiment, state is the same as devices without top-gates)\n\n**B. Forg  et al.- Field-effect devices utilizing LaAlO3-SrTiO3 interfaces**:\n- 9 uc thick LAO, in situ gate. Depletion mode. operating tested at -100 c to 100c, short 20um-200um and wide 1600um channels)\n- Fabricate all-oxide FET with current and voltage gain.\n- Oxide interface is proven that they can be controlled effectively by gate field induced by top gating config.\n- Thin gate dielectric (LAO) , devices can be operated with small gate voltages in comparison to bottom gating\n- Lower limits to be explored.\n- Subthreshold swing S = 0.7/4 => 175mV/dec\n- \n\n**M. Hosada et al. - Transistor operation and mobility enhancement in top-gated\nLaAlO3/SrTiO3 heterostructures** :\n- Depletion mode too\n- Two probe device geometry prevented the determination of sheet carrier density n and hence hall mobility.\n- Again, TiO2 terminated STO substrate, pre-deposited a-LAO hard mask. \n    - LAO thickness between DS is 4uc to 22uc. \n    - 400um channel length\n    - Channel width 5-200um.\n    - ex-situ gate, sputtered Au with 100nm thickness\n    - Source and drain were contacted with Al ultrasonic wire bonding\n- gate leakage current Ig was significantly smaller than Id when device is on\n- Obtained a clear transistor charactersitics at T = 300k for 16uc LAO\n- Pinch off observed for -0.8V <= Vgs <= +1.2V, (Saturation at higher vds)\n- dId/dVds (conductance) can show the pinch off voltage Vp as a function of Vgs. At Vds = Vp, Id sat is shown.\n- Linear behaviour in the saturation region as expected. Vth = 01.15+0.05V. We can use the Variable range hopping model to calculate Vth too.\n- Small hysterisis.\n- Confirms that trasistor charactersitics is still obtained LAO thichkness of 4uc. Vth does not scale monotonically with thickness of LAO, as would be the case for constant sheet carrier density. confirming that Vth does not depend on:\n\\begin{align}\nV_{th} = \\frac{-ed_{LAO}n}{\\epsilon \\epsilon_0}\n\\end{align}\n\n-  When Vgs -> Vth, enhancement of hall mobility is observed as n decreased.\n\n**C. Woltmann et al. - Field-Effect Transistors with Submicrometer Gate Lengths Fabricated\nfrom LaAlO3-SrTiO3-Based Heterostructures** :\n- Considers short effect behaviour\n- Methods describing semiconductor physics are applicable only with considerable restriction since electronic properties of complex oxides differs fundamentally from conventional semiconductor\n- Conclude that with decreasing gate length, Vt shifts to lower values and is increasingly dependent on Vsd (note that this is depletion mode)\n- On-ff ratio and subthreshold slope decrease (Subthreshold swing increase).  A device characterized by steep subthreshold slope will have a faster transition between off (low current) and on (high current) states.\n\n**Variable Range hopping Conduction (VRH) pdf**: \n\nChapter 4.1:Outline\n- Modeled for amorphous crystal which in this case is SiO2\n- Mott introduced the concept of hopping conduction: VRH\n- Empirically observed $log\\sigma \\propto T^{-1/4}$\n- $log\\sigma \\propto T^{-1/4}$ is valid for low E field strength, an extension is needed for higher field stregth\n- VRH process is often expressed in terms of a percolation problem. VRH/Percolation problem is successful in describing the process in ohmic low-field regime[pg5] but not at medium and high field regime (i.e saturation mode at our case)\n\nChapter 4.2:Hopping Probability\n- Assuming no corellation with occupation probability of different localize states, net electron flow is:\n$$I_{ij} = f_i(1-f_j)w_{ij} - f_j(1-f_i)w_{ji} $$\n\nWhere $f_i$ is occupation probability of state i and \n\n**A.K. Sen - Nonlinear Response, Semi-classical Percolation and Breakdown in the RRTN Model**:\n\nPower-law growth of conductance:\n- non-integer power-law behaviour (for low field),$I \\propto (V-V_g)^\\alpha$ , $V_g$ being the threshold voltage.\n- For conductance G, nonlinearity exponent $\\delta = \\alpha - 1 = 2$. \n- For arrays of normal metal island connected by small tunnel junction,$\\alpha = 1.36 \\pm 0.16$ for 1D and $\\alpha = 1.80 \\pm 0.16$ for 2D.\n\nLow-temperature hopping-dominated conduction:\n- conduction properties of low-temperature regime where conduction is mainly due to phonon-assisted hopping of electrons.\n- Needs to consider Mott's VRH conductance model or some of the other variation \n$$G(T) \\propto \\exp(-(T_0/T))^\\gamma $$\n- VRH exponent being dependent on the concentration of donors/acceptors (n or p)\n- Conductance of the sample goes through ma maximum as temp is increased towards a metalic behaviour.\n\nOrigin of RRTN model and percolative aspect:->Week 4\n### Data from 4uc FET->#Data handling for Labview output. Seperate different line segments, folder in dev15_vsd needs to be deleted first.\n\ndef dataparsing(input_file, output_file, data_width):\n    j = 0\n    with open(input_file, 'r') as f_i:    \n        for line, data in enumerate(f_i):\n            if line % data_width == 0:\n                j = j + 1\n            else:\n                with open(output_file + str(j) + '.txt','a+') as f_o:\n                    f_o.write(data)\n\ninput_file = 'Data\\\\4_uc_new device\\\\dev16\\\\Dev16_Vgs-sweep -2V to 1V -- Vds 0V to 1V.txt'\noutput_file = 'Data\\\\4_uc_new device\\\\dev16\\\\Dev16_RoomT_Vgs_sweep\\\\dev16_dataset'\ndata_width = 606\n\ndataparsing(input_file, output_file, data_width)",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>",
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data-># Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
            "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets-># Set up cluster parameters\n(figsize=(9 * 1.3 + 2, 14.5))\n(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n\n = [\n    (noisy_circles, {\"n_clusters\": 2}),\n    (noisy_moons, {\"n_clusters\": 2}),\n    (varied, {\"n_neighbors\": 2}),\n    (aniso, {\"n_neighbors\": 2}),\n    (blobs, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate():\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = ().fit_transform(X)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ward = (\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\n    )\n    complete = (\n        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\n    )\n    average = (\n        n_clusters=params[\"n_clusters\"], linkage=\"average\"\n    )\n    single = (\n        n_clusters=params[\"n_clusters\"], linkage=\"single\"\n    )\n\n    clustering_algorithms = (\n        (\"Single Linkage\", single),\n        (\"Average Linkage\", average),\n        (\"Complete Linkage\", complete),\n        (\"Ward Linkage\", ward),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = ()\n\n        # catch warnings related to kneighbors_graph\n        with ():\n            (\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \"  1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = ()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        (len(), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            (name, size=18)\n\n        colors = (\n            list(\n                (\n                    (\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        (X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        (-2.5, 2.5)\n        (-2.5, 2.5)\n        (())\n        (())\n        (\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\n()\n\n\n<img alt=\"Single Linkage, Average Linkage, Complete Linkage, Ward Linkage\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\"/>",
            "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results-># Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Dataset\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\ndata['ln_wpi'] = np.log(data['wpi'])\ndata['D.ln_wpi'] = data['ln_wpi'].diff()\n\n\n\n\n\n\n\nIn\u00a0[5]:"
        ]
    },
    {
        "id": "988965",
        "GT": "5.5 \n\nTake the dollar bar series on E-mini S&P 500 futures.->(a) Form a new series as a cumulative sum of log-prices->x = np.log(dbars.price).cumsum()\ncprint(x)\n\nx.plot()",
        "pred": [
            "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure->mod = ThetaModel(np.log(pce))\nres = mod.fit()\nprint(res.summary())",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->dta = sm.datasets.star98.load_pandas().data\nprint(dta.columns)",
            "seaborn->User guide and tutorial->Visualizing categorical data->Categorical scatterplots->tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "seaborn->Plotting functions->Visualizing categorical data->Categorical scatterplots->tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->Estimation->data = sm.datasets.stackloss.load()\ndata.exog = sm.add_constant(data.exog)",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->means75 = exog.mean()\nmeans75[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.75)\nprint(means75)"
        ]
    },
    {
        "id": "9974",
        "GT": "plot_learning_curves(ridge_reg4, X, y,141)\nplot_learning_curves(ridge_reg5, X, y,142)\nplot_learning_curves(ridge_reg6, X, y,143)",
        "pred": [
            "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Partial Regression Plots (Crime Data)->fig = sm.graphics.plot_partregress_grid(crime_model)\nfig.tight_layout(pad=1.0)",
            "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Partial Regression Plots (Duncan)->fig = sm.graphics.plot_partregress_grid(prestige_model)\nfig.tight_layout(pad=1.0)",
            "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Calibration of the confidence interval->coverage_fraction(\n    y_test, all_models[\"q 0.05\"].predict(X_test), all_models[\"q 0.95\"].predict(X_test)\n)",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit->mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)",
            "statsmodels->Examples->Robust Regression->Comparing OLS and RLM->Comparing OLS and RLM->Example 2: linear function with linear truth->resrlm2 = sm.RLM(y2, X2).fit()\nprint(resrlm2.params)\nprint(resrlm2.bse)"
        ]
    },
    {
        "id": "314889",
        "GT": "86 percent is much better! What features were the most important?->rf_all_features = pd.DataFrame(fit_all_rf.feature_importances_,\n                                   index = X.columns,\n                                    columns=['importance']).sort_values('importance',\n                                                                        ascending=False)",
        "pred": [
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Calculating the Air Quality Index->Sub-indices->sub_indices = np.stack((vcompute_indices('PM2.5', pollutants[..., 0]),\n                        vcompute_indices('PM10', pollutants[..., 1]),\n                        vcompute_indices('NO2', pollutants[..., 2]),\n                        vcompute_indices('NH3', pollutants[..., 3]),\n                        vcompute_indices('SO2', pollutants[..., 4]),\n                        vcompute_indices('CO', pollutants[..., 5]),\n                        vcompute_indices('O3', pollutants[..., 6])), axis=1)",
            "sklearn->Examples->Miscellaneous->ROC Curve with Visualization API->Plotting the ROC Curve->svc_disp = (svc, X_test, y_test)\n()\n\n\n<img alt=\"plot roc curve visualization api\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\" srcset=\"../../_images/sphx_glr_plot_roc_curve_visualization_api_001.png\"/>",
            "statsmodels->Examples->State space models->Fixed / constrained parameters in state space models->ucm_trend = pd.Series(res.level.smoothed, index=endog.index)",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->train_result = (\n    rf, X_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n)\ntest_results = (\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_importances_idx = train_result.importances_mean.argsort()",
            "sklearn->Examples->Manifold learning->Comparison of Manifold Learning methods->Define algorithms for the manifold learning->T-distributed Stochastic Neighbor Embedding->t_sne = (\n    n_components=n_components,\n    perplexity=30,\n    init=\"random\",\n    n_iter=250,\n    random_state=0,\n)\nS_t_sne = t_sne.fit_transform(S_points)\n\nplot_2d(S_t_sne, S_color, \"T-distributed Stochastic  \\n Neighbor Embedding\")\n\n\n<img alt=\"T-distributed Stochastic    Neighbor Embedding\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_compare_methods_006.png\" srcset=\"../../_images/sphx_glr_plot_compare_methods_006.png\"/>"
        ]
    },
    {
        "id": "1195980",
        "GT": "EDA and Summary Statistics\n***\n\nLet's explore this data set.  First we use `describe()` to get basic summary statistics for each of the columns.->Scatterplots\n***\n\nLet's look at some scatter plots for three variables: 'CRIM' (per capita crime rate), 'RM' (number of rooms) and 'PTRATIO' (pupil-to-teacher ratio in schools).->your turn: describe relationship\nIn the graph above, there is a negative relationship between price and crime rates, so when crime increases there is a decrease in price. There is a big cluster of houses around 0 per capita crime rates, which span the entire price range. This cluster extends until about 10 per capita crime rate. The relationship seems to be non-linear (quadratic).->plt.scatter(bos.LSTAT, bos.PRICE)\nplt.xlabel(\"% lower status of the population\")\nplt.ylabel(\"Housing Price\")\nplt.title(\"Relationship between LSTAT and Price\")",
        "pred": [
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression->plt.clf()\nplt.grid(True)\nplt.plot(result1.predict(linear=True), result1.resid_pearson, \"o\")\nplt.xlabel(\"Linear predictor\")\nplt.ylabel(\"Residual\")",
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing->ax = oildata.plot()\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Oil (millions of tonnes)\")\nprint(\"Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\")",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression->plt.clf()\nplt.grid(True)\nplt.plot(result2.predict(linear=True), result2.resid_pearson, \"o\")\nplt.xlabel(\"Linear predictor\")\nplt.ylabel(\"Residual\")"
        ]
    },
    {
        "id": "612806",
        "GT": "Conclusion\n\n1. drama, comedy, thriller, action seem to be top four popuolar genres in movie history (by their size/antecedent support/confidence support). They can also be interpreted as a general type because almost all the edges are inward. We can also find the types connected is more specific. \n2. For the family movie, it can also be adventure, animation and comedy. Apparantly, thriller movies are not suitable for a family. \n3. The result is reasonable based on our life experience\n\nto be continued...\n\n## Part 2. Actor/Actresses\n\nFrom Credit dataset, we can obtain all the names of actresses and actors for each movie. We decide to extract them to see the preference of cooperation of each actor/actresses.\n\n### Data Manipulation and Rules are similarly implemented->cast.shape",
        "pred": [
            "numpy->NumPy Features->Saving and sharing your NumPy arrays->Our arrays as a csv file->load_xy.shape",
            "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content->Shape, axis and array properties->img.shape",
            "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Applying to all colors->img_array.shape",
            "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content->Operations on an axis->img_gray.shape",
            "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Products with n-dimensional arrays->reconstructed.shape"
        ]
    },
    {
        "id": "1125698",
        "GT": "Distribution plot for CO2 emssions from liquid fuel consumption->SOLID_FUEL_COLUMN_INDEX = 2\na = sns.jointplot(iran_csv.ix[:,SOLID_FUEL_COLUMN_INDEX],\n              turkey_csv.ix[:,SOLID_FUEL_COLUMN_INDEX]).set_axis_labels(\n    \"IRAN: \" + iran_csv.columns[SOLID_FUEL_COLUMN_INDEX], \n    \"TURKEY: \" + turkey_csv.columns[SOLID_FUEL_COLUMN_INDEX])\na.savefig(\"output.png\")",
        "pred": [
            "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->nvFuser & Dynamic Shapes->SHAPE_COUNT = 20\ndynamic_sizes = deepcopy(input_size)\n\ninputs1: List[] = []\ninputs2: List[] = []\ngrad_outputs: List[] = []\n\n\n# Create some random shapes\nfor _ in range(SHAPE_COUNT):\n    dynamic_sizes[0] = input_size[0] + random.randrange(-2, 3)\n    dynamic_sizes[1] = input_size[1] + random.randrange(-2, 3)\n    input = (*dynamic_sizes, device=device, =, requires_grad=True)\n    inputs1.append(input)\n    inputs2.append((input))\n    grad_outputs.append((input))",
            "matplotlib->Tutorials->Colors->Colormap Normalization->Symmetric logarithmic->N = 100\nX, Y = np.mgrid[-3:3:complex(0, N), -2:2:complex(0, N)]\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (Z1 - Z2) * 2\n\nfig, ax = plt.subplots(2, 1)\n\npcm = ax[0].pcolormesh(X, Y, Z,\n                       norm=colors.SymLogNorm(linthresh=0.03, linscale=0.03,\n                                              vmin=-1.0, vmax=1.0, base=10),\n                       cmap='RdBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[0], extend='both')\n\npcm = ax[1].pcolormesh(X, Y, Z, cmap='RdBu_r', vmin=-np.max(Z), shading='auto')\nfig.colorbar(pcm, ax=ax[1], extend='both')\nplt.show()\n\n\n<img alt=\"colormapnorms\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_003.png\" srcset=\"../../_images/sphx_glr_colormapnorms_003.png, ../../_images/sphx_glr_colormapnorms_003_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Colors->Colormap Normalization->Power-law->N = 100\nX, Y = np.mgrid[0:3:complex(0, N), 0:2:complex(0, N)]\nZ1 = (1 + np.sin(Y * 10.)) * X**2\n\nfig, ax = plt.subplots(2, 1, constrained_layout=True)\n\npcm = ax[0].pcolormesh(X, Y, Z1, norm=colors.PowerNorm(gamma=0.5),\n                       cmap='PuBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[0], extend='max')\nax[0].set_title('PowerNorm()')\n\npcm = ax[1].pcolormesh(X, Y, Z1, cmap='PuBu_r', shading='auto')\nfig.colorbar(pcm, ax=ax[1], extend='max')\nax[1].set_title('Normalize()')\nplt.show()\n\n\n<img alt=\"PowerNorm(), Normalize()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_004.png\" srcset=\"../../_images/sphx_glr_colormapnorms_004.png, ../../_images/sphx_glr_colormapnorms_004_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Colors->Colormap Normalization->Discrete bounds->N = 100\nX, Y = np.meshgrid(np.linspace(-3, 3, N), np.linspace(-2, 2, N))\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = ((Z1 - Z2) * 2)[:-1, :-1]\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 6), constrained_layout=True)\nax = ax.flatten()\n\n# Default norm:\npcm = ax[0].pcolormesh(X, Y, Z, cmap='RdBu_r')\nfig.colorbar(pcm, ax=ax[0], orientation='vertical')\nax[0].set_title('Default norm')\n\n# Even bounds give a contour-like effect:\nbounds = np.linspace(-1.5, 1.5, 7)\nnorm = colors.BoundaryNorm(boundaries=bounds, ncolors=256)\npcm = ax[1].pcolormesh(X, Y, Z, norm=norm, cmap='RdBu_r')\nfig.colorbar(pcm, ax=ax[1], extend='both', orientation='vertical')\nax[1].set_title('BoundaryNorm: 7 boundaries')\n\n# Bounds may be unevenly spaced:\nbounds = np.array([-0.2, -0.1, 0, 0.5, 1])\nnorm = colors.BoundaryNorm(boundaries=bounds, ncolors=256)\npcm = ax[2].pcolormesh(X, Y, Z, norm=norm, cmap='RdBu_r')\nfig.colorbar(pcm, ax=ax[2], extend='both', orientation='vertical')\nax[2].set_title('BoundaryNorm: nonuniform')\n\n# With out-of-bounds colors:\nbounds = np.linspace(-1.5, 1.5, 7)\nnorm = colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend='both')\npcm = ax[3].pcolormesh(X, Y, Z, norm=norm, cmap='RdBu_r')\n# The colorbar inherits the \"extend\" argument from BoundaryNorm.\nfig.colorbar(pcm, ax=ax[3], orientation='vertical')\nax[3].set_title('BoundaryNorm: extend=\"both\"')\nplt.show()\n\n\n<img alt=\"Default norm, BoundaryNorm: 7 boundaries, BoundaryNorm: nonuniform, BoundaryNorm: extend=\" both=\"\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_005.png\" srcset=\"../../_images/sphx_glr_colormapnorms_005.png, ../../_images/sphx_glr_colormapnorms_005_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Colors->Colormap Normalization->Centered->delta = 0.1\nx = np.arange(-3.0, 4.001, delta)\ny = np.arange(-4.0, 3.001, delta)\nX, Y = np.meshgrid(x, y)\nZ1 = np.exp(-X**2 - Y**2)\nZ2 = np.exp(-(X - 1)**2 - (Y - 1)**2)\nZ = (0.9*Z1 - 0.5*Z2) * 2\n\n# select a divergent colormap\ncmap = cm.coolwarm\n\nfig, (ax1, ax2) = plt.subplots(ncols=2)\npc = ax1.pcolormesh(Z, cmap=cmap)\nfig.colorbar(pc, ax=ax1)\nax1.set_title('Normalize()')\n\npc = ax2.pcolormesh(Z, norm=colors.CenteredNorm(), cmap=cmap)\nfig.colorbar(pc, ax=ax2)\nax2.set_title('CenteredNorm()')\n\nplt.show()\n\n\n<img alt=\"Normalize(), CenteredNorm()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_colormapnorms_002.png\" srcset=\"../../_images/sphx_glr_colormapnorms_002.png, ../../_images/sphx_glr_colormapnorms_002_2_0x.png 2.0x\"/>"
        ]
    },
    {
        "id": "236926",
        "GT": "10.5 Packing free energy for a spherical virus->Solution-># Read data into memory\ndf = pd.read_excel('../data/fig6.24.xls', sheet_name='tidy')\n\ndf.head()",
        "pred": [
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "pandas_toms_blog->Scaling->Using Dask->df = dd.read_parquet(\"data/indiv-*.parquet\", engine='pyarrow', columns=['occupation'])\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common.compute().sort_values(ascending=False)",
            "pandas_toms_blog->Fast Pandas->Categoricals->import string\n\ns = pd.Series(np.random.choice(list(string.ascii_letters), 100000))\nprint('{:0.2f} KB'.format(s.memory_usage(index=False) / 1000))",
            "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime->import onnx\n\nonnx_model = onnx.load(\"super_resolution.onnx\")\nonnx.checker.check_model(onnx_model)"
        ]
    },
    {
        "id": "1239988",
        "GT": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.layers import Dense\nfrom keras.layers import Softmax\nfrom keras.layers import Dropout\nfrom keras.models import Sequential",
        "pred": [
            "sklearn->Examples->Calibration->Comparison of Calibration of Classifiers->Calibration curves->from sklearn.calibration import CalibrationDisplay\nfrom sklearn.ensemble import \nfrom sklearn.linear_model import \nfrom sklearn.naive_bayes import \n\n# Create classifiers\nlr = ()\ngnb = ()\nsvc = NaivelyCalibratedLinearSVC(C=1.0)\nrfc = ()\n\nclf_list = [\n    (lr, \"Logistic\"),\n    (gnb, \"Naive Bayes\"),\n    (svc, \"SVC\"),\n    (rfc, \"Random forest\"),\n]",
            "sklearn->Examples->Ensemble methods->Monotonic Constraints->from sklearn.ensemble import \nfrom sklearn.inspection import PartialDependenceDisplay\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nrng = (0)\n\nn_samples = 1000\nf_0 = rng.rand(n_samples)\nf_1 = rng.rand(n_samples)\nX = [f_0, f_1]\nnoise = rng.normal(loc=0.0, scale=0.01, size=n_samples)\n\n# y is positively correlated with f_0, and negatively correlated with f_1\ny = 5 * f_0 + (10 *  * f_0) - 5 * f_1 - (10 *  * f_1) + noise",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->from sklearn.metrics import \nfrom sklearn.metrics import PredictionErrorDisplay\n\nmae_train = (y_train, model.predict(X_train))\ny_pred = model.predict(X_test)\nmae_test = (y_test, y_pred)\nscores = {\n    \"MedAE on training set\": f\"{mae_train:.2f} $/hour\",\n    \"MedAE on testing set\": f\"{mae_test:.2f} $/hour\",\n}",
            "sklearn->Examples->Covariance estimation->Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood->Compare different approaches to setting the regularization parameter->from sklearn.model_selection import \nfrom sklearn.covariance import , \n\n# GridSearch for an optimal shrinkage coefficient\ntuned_parameters = [{\"shrinkage\": shrinkages}]\ncv = ((), tuned_parameters)\ncv.fit(X_train)\n\n# Ledoit-Wolf optimal shrinkage coefficient estimate\nlw = ()\nloglik_lw = lw.fit(X_train).score(X_test)\n\n# OAS coefficient estimate\noa = ()\nloglik_oa = oa.fit(X_train).score(X_test)",
            "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New SequentialFeatureSelector transformer->from sklearn.feature_selection import \nfrom sklearn.neighbors import \nfrom sklearn.datasets import \n\nX, y = (return_X_y=True, as_frame=True)\nfeature_names = X.columns\nknn = (n_neighbors=3)\nsfs = (knn, n_features_to_select=2)\nsfs.fit(X, y)\nprint(\n    \"Features selected by forward sequential selection: \"\n    f\"{feature_names[sfs.get_support()].tolist()}\"\n)"
        ]
    },
    {
        "id": "1157290",
        "GT": "def scotch_recommender(user_scotches, n_recs=5):\n    user_preferences = (scotch_props.loc[scotch_props.index.isin(user_scotches)]\n                                    .mean(axis=0))\n    distances = cdist(scotch_props, user_preferences.to_frame().T).squeeze()\n    distances = pd.Series(data=distances,\n                      index=scotch_props.index)\n    distances = distances[~distances.index.isin(user_scotches)]\n    \n    return distances.sort_values()[:n_recs].index.tolist()",
        "pred": [
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->Autocorrelation->def tsplot(y, lags=None, figsize=(10, 8)):\n    fig = plt.figure(figsize=figsize)\n    layout = (2, 2)\n    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n    acf_ax = plt.subplot2grid(layout, (1, 0))\n    pacf_ax = plt.subplot2grid(layout, (1, 1))\n    \n    y.plot(ax=ts_ax)\n    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n    [ax.set_xlim(1.5) for ax in [acf_ax, pacf_ax]]\n    sns.despine()\n    plt.tight_layout()\n    return ts_ax, acf_ax, pacf_ax",
            "sklearn->Examples->Clustering->Adjustment for chance in clustering performance evaluation->Second experiment: varying number of classes and clusters->def uniform_labelings_scores(score_func, n_samples, n_clusters_range, n_runs=5):\n    scores = ((len(n_clusters_range), n_runs))\n\n    for i, n_clusters in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            labels_a = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            labels_b = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores",
            "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money->def ewma(series, beta, n_window):\n    nobs = len(series)\n    scalar = (1 - beta) / (1 + beta)\n    ma = []\n    k = np.arange(n_window, 0, -1)\n    weights = np.r_[beta ** k, 1, beta ** k[::-1]]\n    for t in range(n_window, nobs - n_window):\n        window = series.iloc[t - n_window : t + n_window + 1].values\n        ma.append(scalar * np.sum(weights * window))\n    return pd.Series(ma, name=series.name, index=series.iloc[n_window:-n_window].index)\n\n\nm2_ewma = ewma(np.log(m2[\"M2SL\"].resample(\"QS\").mean()).diff().iloc[1:], 0.95, 10 * 4)\ncpi_ewma = ewma(\n    np.log(cpi[\"CPIAUCSL\"].resample(\"QS\").mean()).diff().iloc[1:], 0.95, 10 * 4\n)",
            "numpy->NumPy Applications->Plotting Fractals->Julia set->def julia(mesh, c=-1, num_iter=10, radius=2):\n\n    z = mesh.copy()\n    diverge_len = np.zeros(z.shape)\n\n    for i in range(num_iter):\n        conv_mask = np.abs(z) &lt; radius\n        z[conv_mask] = np.square(z[conv_mask]) + c\n        diverge_len[conv_mask] += 1\n\n    return diverge_len",
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis->def f(x):\n    n = x.shape[0]\n    g2 = x.group2\n    u = g2.unique()\n    u.sort()\n    uv = {v: k for k, v in enumerate(u)}\n    mat = np.zeros((n, len(u)))\n    for i in range(n):\n        mat[i, uv[g2.iloc[i]]] = 1\n    colnames = [\"%d\" % z for z in u]\n    return mat, colnames"
        ]
    },
    {
        "id": "14967",
        "GT": "Datashader reveals additional detail, especially when zooming in\n\nYou can now see that there are a lot of points below the linear boundary, representing long trips for very little cost (presumably GPS errors?).->import datashader as ds\nfrom datashader.bokeh_ext import InteractiveImage\nfrom bokeh.models import Range1d\n\np = base_plot()\np.xaxis.axis_label = \"Passengers\"\np.yaxis.axis_label = \"Tip, $\"\np.x_range = Range1d(-0.5, 6.5)\np.y_range = Range1d(0, 60)\n\npipeline = ds.Pipeline(df, ds.Point(\"passenger_count\", \"tip_amount\"), width_scale=0.035)\nInteractiveImage(p, pipeline)",
        "pred": [
            "statsmodels->User Guide->Statistics and Tools->Working with Large Data Sets->Subsetting your data->import pyarrow as pa\nimport pyarrow.parquet as pq\nimport statsmodels.formula.api as smf\n\nclass DataSet(dict):\n    def __init__(self, path):\n        self.parquet = pq.ParquetFile(path)\n\n    def __getitem__(self, key):\n        try:\n            return self.parquet.read([key]).to_pandas()[key]\n        except:\n            raise KeyError\n\nLargeData = DataSet('LargeData.parquet')\n\nres = smf.ols('Profit ~ Sugar + Power + Women', data=LargeData).fit()",
            "matplotlib->Tutorials->Intermediate->Legend guide->Creating artists specifically for adding to the legend (aka. Proxy artists)->import matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nred_patch = mpatches.Patch(color='red', label='The red data')\nax.legend(handles=[red_patch])\n\nplt.show()\n\n\n<img alt=\"legend guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_legend_guide_001.png\" srcset=\"../../_images/sphx_glr_legend_guide_001.png, ../../_images/sphx_glr_legend_guide_001_2_0x.png 2.0x\"/>",
            "statsmodels->Examples->Generalized Linear Models->Using Formulas with GLMs->import statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nstar98 = sm.datasets.star98.load_pandas().data\nformula = \"SUCCESS ~ LOWINC + PERASIAN + PERBLACK + PERHISP + PCTCHRT + \\\n           PCTYRRND + PERMINTE*AVYRSEXP*AVSALK + PERSPENK*PTRATIO*PCTAF\"\ndta = star98[\n    [\n        \"NABOVE\",\n        \"NBELOW\",\n        \"LOWINC\",\n        \"PERASIAN\",\n        \"PERBLACK\",\n        \"PERHISP\",\n        \"PCTCHRT\",\n        \"PCTYRRND\",\n        \"PERMINTE\",\n        \"AVYRSEXP\",\n        \"AVSALK\",\n        \"PERSPENK\",\n        \"PTRATIO\",\n        \"PCTAF\",\n    ]\n].copy()\nendog = dta[\"NABOVE\"] / (dta[\"NABOVE\"] + dta.pop(\"NBELOW\"))\ndel dta[\"NABOVE\"]\ndta[\"SUCCESS\"] = endog",
            "sklearn->Examples->Working with text documents->Clustering text documents using k-means->Clustering evaluation summary->import pandas as pd\nimport matplotlib.pyplot as plt\n\nfig, (ax0, ax1) = (ncols=2, figsize=(16, 6), sharey=True)\n\ndf = (evaluations[::-1]).set_index(\"estimator\")\ndf_std = (evaluations_std[::-1]).set_index(\"estimator\")\n\ndf.drop(\n    [\"train_time\"],\n    axis=\"columns\",\n).plot.barh(ax=ax0, xerr=df_std)\nax0.set_xlabel(\"Clustering scores\")\nax0.set_ylabel(\"\")\n\ndf[\"train_time\"].plot.barh(ax=ax1, xerr=df_std[\"train_time\"])\nax1.set_xlabel(\"Clustering time (s)\")\n()\n\n\n<img alt=\"plot document clustering\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_document_clustering_001.png\" srcset=\"../../_images/sphx_glr_plot_document_clustering_001.png\"/>",
            "sklearn->Examples->Preprocessing->Importance of Feature Scaling->Effect of rescaling on a PCA dimensional reduction->import pandas as pd\nfrom sklearn.decomposition import \n\npca = (n_components=2).fit(X_train)\nscaled_pca = (n_components=2).fit(scaled_X_train)\nX_train_transformed = pca.transform(X_train)\nX_train_std_transformed = scaled_pca.transform(scaled_X_train)\n\nfirst_pca_component = (\n    pca.components_[0], index=X.columns, columns=[\"without scaling\"]\n)\nfirst_pca_component[\"with scaling\"] = scaled_pca.components_[0]\nfirst_pca_component.plot.bar(\n    title=\"Weights of the first principal component\", figsize=(6, 8)\n)\n\n_ = ()\n\n\n<img alt=\"Weights of the first principal component\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_scaling_importance_002.png\" srcset=\"../../_images/sphx_glr_plot_scaling_importance_002.png\"/>"
        ]
    },
    {
        "id": "723290",
        "GT": "if ('applForm' in locals()): del applForm",
        "pred": [
            "torch->Frontend APIs->(beta) Channels Last Memory Format in PyTorch->Memory Format API->print(.is_contiguous(memory_format=))  # Ouputs: True",
            "torch->Parallel and Distributed Training->Training Transformer models using Distributed Data Parallel and Pipeline Parallelism->Define the model->if sys.platform == 'win32':\n    print('Windows platform is not supported for pipeline parallelism')\n    sys.exit(0)\nif () &lt; 4:\n    print('Need at least four GPU devices for this tutorial')\n    sys.exit(0)\n\nclass Encoder():\n    def __init__(self, ntoken, ninp, dropout=0.5):\n        super(Encoder, self).__init__()\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        self.encoder = (ntoken, ninp)\n        self.ninp = ninp\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        # Need (S, N) format for encoder.\n        src = src.t()\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        return self.pos_encoder(src)\n\nclass Decoder():\n    def __init__(self, ntoken, ninp):\n        super(Decoder, self).__init__()\n        self.decoder = (ninp, ntoken)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, inp):\n        # Need batch dimension first for output of pipeline.\n        return self.decoder(inp).permute(1, 0, 2)",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->textproc = TextPreprocess()",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators->sm.robust.scale.iqr(x)",
            "torch->Code Transforms with FX->(beta) Building a Simple CPU Performance Profiler with FX->Capturing the Model with Symbolic Tracing->traced_rn18 = (rn18)\nprint()"
        ]
    },
    {
        "id": "714784",
        "GT": "Unsupervised Clustering->fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n\npivoted.T[labels == 0].T.plot(legend=False, alpha=0.1, ax=ax[0]);\npivoted.T[labels == 1].T.plot(legend=False, alpha=0.1, ax=ax[1]);\n\nax[0].set_title('Purple Cluster')\nax[1].set_title('Red Cluster');",
        "pred": [
            "matplotlib->Tutorials->Introductory->Quick start guide->Labelling plots->Legends->fig, ax = plt.subplots(figsize=(5, 2.7))\nax.plot(np.arange(len(data1)), data1, label='data1')\nax.plot(np.arange(len(data2)), data2, label='data2')\nax.plot(np.arange(len(data3)), data3, 'd', label='data3')\nax.legend()\n\n\n<img alt=\"quick start\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_011.png\" srcset=\"../../_images/sphx_glr_quick_start_011.png, ../../_images/sphx_glr_quick_start_011_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Introductory->Quick start guide->Styling Artists->Linewidths, linestyles, and markersizes->fig, ax = plt.subplots(figsize=(5, 2.7))\nax.plot(data1, 'o', label='data1')\nax.plot(data2, 'd', label='data2')\nax.plot(data3, 'v', label='data3')\nax.plot(data4, 's', label='data4')\nax.legend()\n\n\n<img alt=\"quick start\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_008.png\" srcset=\"../../_images/sphx_glr_quick_start_008.png, ../../_images/sphx_glr_quick_start_008_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Intermediate->Autoscaling->Controlling autoscale->fig, ax = plt.subplots(ncols=2, figsize=(12, 8))\nax[0].plot(x, y)\nax[0].set_xlim(left=-1, right=1)\nax[0].plot(x + np.pi * 0.5, y)\nax[0].set_title(\"set_xlim(left=-1, right=1)\\n\")\nax[1].plot(x, y)\nax[1].set_xlim(left=-1, right=1)\nax[1].plot(x + np.pi * 0.5, y)\nax[1].autoscale()\nax[1].set_title(\"set_xlim(left=-1, right=1)\\nautoscale()\")\n\n\n<img alt=\"set_xlim(left=-1, right=1) , set_xlim(left=-1, right=1) autoscale()\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_autoscale_007.png\" srcset=\"../../_images/sphx_glr_autoscale_007.png, ../../_images/sphx_glr_autoscale_007_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Introductory->Quick start guide->Styling Artists->fig, ax = plt.subplots(figsize=(5, 2.7))\nx = np.arange(len(data1))\nax.plot(x, np.cumsum(data1), color='blue', linewidth=3, linestyle='--')\nl, = ax.plot(x, np.cumsum(data2), color='orange', linewidth=2)\nl.set_linestyle(':')\n\n\n<img alt=\"quick start\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_006.png\" srcset=\"../../_images/sphx_glr_quick_start_006.png, ../../_images/sphx_glr_quick_start_006_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Introductory->Quick start guide->Labelling plots->Annotations->fig, ax = plt.subplots(figsize=(5, 2.7))\n\nt = np.arange(0.0, 5.0, 0.01)\ns = np.cos(2 * np.pi * t)\nline, = ax.plot(t, s, lw=2)\n\nax.annotate('local max', xy=(2, 1), xytext=(3, 1.5),\n            arrowprops=dict(facecolor='black', shrink=0.05))\n\nax.set_ylim(-2, 2)\n\n\n<img alt=\"quick start\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_quick_start_010.png\" srcset=\"../../_images/sphx_glr_quick_start_010.png, ../../_images/sphx_glr_quick_start_010_2_0x.png 2.0x\"/>"
        ]
    },
    {
        "id": "815500",
        "GT": "Homicide in Chicago by year\n\nA large increase in homicides can be sean in 2016 and 2017.-># Chicago domestic violence rate\nym = [i.strftime('%Y') for i in df['Date'][df['Domestic']]]\ncounts = Counter(ym)\nax = plot_counter(counts, sample_frac=1, last_year=True)\nax.set_title('Chicago Domestic Violence Rate')",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Annual frequency, using a PeriodIndex\nindex = pd.period_range(start='2000', periods=4, freq='A')\nendog1 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog1.index)"
        ]
    },
    {
        "id": "1157750",
        "GT": "Inferential Statistics-># Packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport scipy.stats as stats\nfrom scipy.stats import chi2\nfrom scipy.stats import shapiro\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols",
        "pred": [
            "sklearn->Examples->Inspection->Permutation Importance with Multicollinear or Correlated Features->from collections import \n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import \nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import \n\nfrom sklearn.datasets import \nfrom sklearn.ensemble import \nfrom sklearn.inspection import \nfrom sklearn.model_selection import",
            "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime-># Some standard imports\nimport io\nimport numpy as np\n\nfrom torch import nn\nimport torch.utils.model_zoo as model_zoo\nimport torch.onnx",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Dataset\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\ndata['ln_wpi'] = np.log(data['wpi'])\ndata['D.ln_wpi'] = data['ln_wpi'].diff()\n\n\n\n\n\n\n\nIn\u00a0[5]:",
            "seaborn->User guide and tutorial->An introduction to seaborn-># Apply the default theme\nsns.set_theme()\n",
            "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 0. Prerequisites-># Imports\nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\n\nplt.ion()"
        ]
    },
    {
        "id": "1515110",
        "GT": "Testing the accuracy of Logistic Regression->Predicting all the majors together-># Freshman\nalg = LogisticRegression(random_state=1)\nscores = cross_validation.cross_val_score(alg, frs[predictors], frs[\"finalmajor_full\"])\n\nprint(scores.mean())",
        "pred": [
            "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach-># initialize random variable\nt_post = (\n    df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n)",
            "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept and lagged dependent variable-># Fit the model\nmod_fedfunds2 = sm.tsa.MarkovRegression(\n    dta_fedfunds.iloc[1:], k_regimes=2, exog=dta_fedfunds.iloc[:-1]\n)\nres_fedfunds2 = mod_fedfunds2.fit()",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit->mlogit_mod = sm.MNLogit(anes_data.endog, anes_exog)\nmlogit_res = mlogit_mod.fit()\nprint(mlogit_res.params)",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson->poisson_mod = sm.Poisson(rand_data.endog, rand_exog)\npoisson_res = poisson_mod.fit(method=\"newton\")\nprint(poisson_res.summary())"
        ]
    },
    {
        "id": "964580",
        "GT": "Simple Linear Regression->Importing the dataset using pandas->Data = pd.read_csv('Salary_Data.csv')\nX = Data.iloc[:, :1].values\ny = Data.iloc[:, 1].values",
        "pred": [
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "statsmodels->Examples->User Notes->Prediction->Estimation->olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->table = pd.DataFrame(data, columns=[\"lag\", \"AC\", \"Q\", \"Prob(Q)\"])\nprint(table.set_index(\"lag\"))",
            "statsmodels->User Guide->Other Models->Methods for Survival and Duration Analysis->Survival function estimation and inference->fig = sf.plot()\nax = fig.get_axes()[0]\npt = ax.get_lines()[1]\npt.set_visible(False)"
        ]
    },
    {
        "id": "1221941",
        "GT": "Tips for debugging `apply`->Exercise 1\n<span style=\"color:green; font-size:16px\">Use the **`display`** after each line in a custom function that gets used with **`apply`** and **`axis='columns'`** to find the population of the second highest race. Make sure you raise an exception or else you will have to kill your kernel because of the massive output</span>->def second_most(s):\n    ugds, s = s.iloc[0], s.iloc[1:]\n    s = s.sort_values(ascending=False)\n    second_pct = s.iloc[1]\n    second_pop = (second_pct * ugds).astype(int)\n    return second_pop",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 2: Quantity theory of money->def ewma(series, beta, n_window):\n    nobs = len(series)\n    scalar = (1 - beta) / (1 + beta)\n    ma = []\n    k = np.arange(n_window, 0, -1)\n    weights = np.r_[beta ** k, 1, beta ** k[::-1]]\n    for t in range(n_window, nobs - n_window):\n        window = series.iloc[t - n_window : t + n_window + 1].values\n        ma.append(scalar * np.sum(weights * window))\n    return pd.Series(ma, name=series.name, index=series.iloc[n_window:-n_window].index)\n\n\nm2_ewma = ewma(np.log(m2[\"M2SL\"].resample(\"QS\").mean()).diff().iloc[1:], 0.95, 10 * 4)\ncpi_ewma = ewma(\n    np.log(cpi[\"CPIAUCSL\"].resample(\"QS\").mean()).diff().iloc[1:], 0.95, 10 * 4\n)",
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Calculating the Air Quality Index->Moving averages->def moving_mean(a, n):\n    ret = np.cumsum(a, dtype=float, axis=0)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret[n - 1:] / n\n\npollutants_A_24hr_avg = moving_mean(pollutants_A, 24)\npollutants_B_8hr_avg = moving_mean(pollutants_B, 8)[-(pollutants_A_24hr_avg.shape[0]):]",
            "sklearn->Examples->Clustering->Adjustment for chance in clustering performance evaluation->Second experiment: varying number of classes and clusters->def uniform_labelings_scores(score_func, n_samples, n_clusters_range, n_runs=5):\n    scores = ((len(n_clusters_range), n_runs))\n\n    for i, n_clusters in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            labels_a = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            labels_b = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores",
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis->def f(x):\n    n = x.shape[0]\n    g2 = x.group2\n    u = g2.unique()\n    u.sort()\n    uv = {v: k for k, v in enumerate(u)}\n    mat = np.zeros((n, len(u)))\n    for i in range(n):\n        mat[i, uv[g2.iloc[i]]] = 1\n    colnames = [\"%d\" % z for z in u]\n    return mat, colnames",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->def plot_weights(support, weights_func, xlabels, xticks):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(support, weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_ylim(-0.1, 1.1)\n    return ax"
        ]
    },
    {
        "id": "131740",
        "GT": "Make a prediction!->y_pred = gscv.best_estimator_.predict(x_test)",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resf_logit.predict()",
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model->sm_probit_canned = sm.Probit(endog, exog).fit()",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->pca_model = PCA(dta.T, standardize=False, demean=True)",
            "statsmodels->Examples->User Notes->Prediction->In-sample prediction->ypred = olsres.predict(X)\nprint(ypred)",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->sarima_res.predict(0, 2)"
        ]
    },
    {
        "id": "1046703",
        "GT": "Most Experienced Umpires->mlt.subplots(figsize=(10,6))\nump=pd.concat([matches['umpire1'],matches['umpire2']]) \nax=ump.value_counts().head(10).plot.bar(width=0.8,color=sns.color_palette('summer',10))\nfor p in ax.patches:\n    ax.annotate(format(p.get_height()), (p.get_x()+0.15, p.get_height()+0.25))\nmlt.show()",
        "pred": [
            "matplotlib->Tutorials->Colors->Choosing Colormaps in Matplotlib->Grayscale conversion->mpl.rcParams.update({'font.size': 14})\n\n# Indices to step through colormap.\nx = np.linspace(0.0, 1.0, 100)\n\ngradient = np.linspace(0, 1, 256)\ngradient = np.vstack((gradient, gradient))\n\n\ndef plot_color_gradients(cmap_category, cmap_list):\n    fig, axs = plt.subplots(nrows=len(cmap_list), ncols=2)\n    fig.subplots_adjust(top=0.95, bottom=0.01, left=0.2, right=0.99,\n                        wspace=0.05)\n    fig.suptitle(cmap_category + ' colormaps', fontsize=14, y=1.0, x=0.6)\n\n    for ax, name in zip(axs, cmap_list):\n\n        # Get RGB values for colormap.\n        rgb = mpl.colormaps[name](x)[np.newaxis, :, :3]\n\n        # Get colormap in CAM02-UCS colorspace. We want the lightness.\n        lab = cspace_converter(\"sRGB1\", \"CAM02-UCS\")(rgb)\n        L = lab[0, :, 0]\n        L = np.float32(np.vstack((L, L, L)))\n\n        ax[0].imshow(gradient, aspect='auto', cmap=mpl.colormaps[name])\n        ax[1].imshow(L, aspect='auto', cmap='binary_r', vmin=0., vmax=100.)\n        pos = list(ax[0].get_position().bounds)\n        x_text = pos[0] - 0.01\n        y_text = pos[1] + pos[3]/2.\n        fig.text(x_text, y_text, name, va='center', ha='right', fontsize=10)\n\n    # Turn off *all* ticks & spines, not just the ones with colormaps.\n    for ax in axs.flat:\n        ax.set_axis_off()\n\n    plt.show()\n\n\nfor cmap_category, cmap_list in cmaps.items():\n\n    plot_color_gradients(cmap_category, cmap_list)\n\n\n\n<img alt=\"Perceptually Uniform Sequential colormaps\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colormaps_015.png\" srcset=\"../../_images/sphx_glr_colormaps_015.png, ../../_images/sphx_glr_colormaps_015_2_0x.png 2.0x\"/>\n<img alt=\"Sequential colormaps\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colormaps_016.png\" srcset=\"../../_images/sphx_glr_colormaps_016.png, ../../_images/sphx_glr_colormaps_016_2_0x.png 2.0x\"/>\n<img alt=\"Sequential (2) colormaps\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colormaps_017.png\" srcset=\"../../_images/sphx_glr_colormaps_017.png, ../../_images/sphx_glr_colormaps_017_2_0x.png 2.0x\"/>\n<img alt=\"Diverging colormaps\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colormaps_018.png\" srcset=\"../../_images/sphx_glr_colormaps_018.png, ../../_images/sphx_glr_colormaps_018_2_0x.png 2.0x\"/>\n<img alt=\"Cyclic colormaps\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colormaps_019.png\" srcset=\"../../_images/sphx_glr_colormaps_019.png, ../../_images/sphx_glr_colormaps_019_2_0x.png 2.0x\"/>\n<img alt=\"Qualitative colormaps\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colormaps_020.png\" srcset=\"../../_images/sphx_glr_colormaps_020.png, ../../_images/sphx_glr_colormaps_020_2_0x.png 2.0x\"/>\n<img alt=\"Miscellaneous colormaps\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_colormaps_021.png\" srcset=\"../../_images/sphx_glr_colormaps_021.png, ../../_images/sphx_glr_colormaps_021_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->Use with GridSpec->plt.rcParams['figure.constrained_layout.use'] = False\nfig = plt.figure(layout=\"constrained\")\n\ngs1 = gridspec.GridSpec(2, 1, figure=fig)\nax1 = fig.add_subplot(gs1[0])\nax2 = fig.add_subplot(gs1[1])\n\nexample_plot(ax1)\nexample_plot(ax2)\n\n\n<img alt=\"Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_019.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_019.png, ../../_images/sphx_glr_constrainedlayout_guide_019_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Intermediate->Constrained Layout Guide->rcParams->plt.rcParams['figure.constrained_layout.use'] = True\nfig, axs = plt.subplots(2, 2, figsize=(3, 3))\nfor ax in axs.flat:\n    example_plot(ax)\n\n\n<img alt=\"Title, Title, Title, Title\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_constrainedlayout_guide_018.png\" srcset=\"../../_images/sphx_glr_constrainedlayout_guide_018.png, ../../_images/sphx_glr_constrainedlayout_guide_018_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Intermediate->Tight Layout guide->Colorbar->plt.close('all')\narr = np.arange(100).reshape((10, 10))\nfig = plt.figure(figsize=(4, 4))\nim = plt.imshow(arr, interpolation=\"none\")\n\nplt.colorbar(im)\n\nplt.tight_layout()\n\n\n<img alt=\"tight layout guide\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_tight_layout_guide_014.png\" srcset=\"../../_images/sphx_glr_tight_layout_guide_014.png, ../../_images/sphx_glr_tight_layout_guide_014_2_0x.png 2.0x\"/>",
            "seaborn->User guide and tutorial->An introduction to seaborn->Opinionated defaults and flexible customization->sns.set_theme(style=\"ticks\", font_scale=1.25)\ng = sns.relplot(\n    data=penguins,\n    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"body_mass_g\",\n    palette=\"crest\", marker=\"x\", s=100,\n)\ng.set_axis_labels(\"Bill length (mm)\", \"Bill depth (mm)\", labelpad=10)\ng.legend.set_title(\"Body mass (g)\")\ng.figure.set_size_inches(6.5, 4.5)\ng.ax.margins(.15)\ng.despine(trim=True)\n"
        ]
    },
    {
        "id": "74527",
        "GT": "Featurizing Toxic Comments Using Pre-Trained Word Vectors and a Language Model's Encoder\n\n## Overview\n\nThis notebook provides an analysis of featurization methods for text. The central idea we examine is that we can represent text (words, phrases, and even entire sentences or paragraphs) as vectors. However, we'll see that some vector representations may provide more semantic information than others. \n\n## Pre-Trained Word Vectors\n\nAs a first foray we featurize our data using publically available pre-trained word vectors. There are numerous word vectors available, the most common being [Word2Vec](https://code.google.com/archive/p/word2vec/), [GloVe](https://nlp.stanford.edu/projects/glove/), and [fasttext](https://github.com/facebookresearch/fastText/). We'll use the **GloVe** vectors trained on Wikipedia and Gigaword 5. \n\nOur input dataset contains a variable sequence of tokens (words), which we vectorize into a list of real-valued vectors. In order to use a machine learning model with such a representation we need to transform it into a fixed-vector representation. We can do this by many different aggregation schemes: sum/mean, max, min, etc. For this notebook we simply utilize unweighted averages of all the tokens, but you'll likely find that for some applications it may be more useful to consider max/min in addition, and concatenate multiple representations.\n\n\n![](https://image.slidesharecdn.com/starsem-170916142844/95/yejin-choi-2017-from-naive-physics-to-connotation-modeling-commonsense-in-frame-semantics-83-638.jpg?cb=1505572199)\n\n_image credit: Yejin Choi - 2017 - From Naive Physics to Connotation: Modeling Commonsense in Frame Semantics_\n\n_quote credit: Ray Mooney_\n\n\n## Language Model Encoders\n\nWe'll then examine a more advanced method of featurizing our sequence of tokens. In particular, we'll use the encoder from a pre-trained language model. The encoder is a fixed-length vector representation that is typically the last hidden vector in a recurrent neural network trained for machine translation or language modeling.\n\n\n![](http://ruder.io/content/images/2018/07/lm_objective.png)\n\n_image credit: Seabstain Ruder and TheGradient: NLP's ImageNet moment has arrived_\n\nOur hope is that rather than naively aggregating our word vectors by their average representation, the last hidden layer will contain contextual information from the entire sequence of tokens.->Language Model \n\nOur language model encoder utilizes pre-trained language models hosted on [TensorFlow Hub](https://www.tensorflow.org/hub/modules/text). \n\nOur helper script `encoder.py` provides a simple class entitled `encoder` with methods for encoding text using three different encoder models: [ELMO](http://www.aclweb.org/anthology/N18-1202), [USE](https://arxiv.org/pdf/1803.11175.pdf), and [NNLM](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf).->Encoder Imports:\n\nThe class is a bit verbose for readability, but it's conceptually very simple. We load the pre-trained module, which defines a static computational graph with the learned weights from the language model on it's dataset. We initialize this computational graph into a Keras session, which we can then use for fine-tuning or for featurizing an input sequence by computing a forward pass of the computational graph. Note, we could have also just used `tensorflow` directly to do the model building and training, but Keras has some helpful utilities for data min-batching and pre-fetching that makes this very easy (at the cost of some incompatibilities: [issues with fine-tuning may arise](https://groups.google.com/a/tensorflow.org/forum/#!topic/hub/Y4AdAM7HpX0).->from encoder import encoder",
        "pred": [
            "scipy->Statistics (scipy.stats)->Random variables->from scipy import stats",
            "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content->Operations on an axis->from numpy import linalg",
            "numpy->Articles->Deep reinforcement learning with Pong from pixels->Set up Pong->from gym import wrappers\nfrom gym.wrappers import Monitor",
            "sklearn->Examples->Clustering->Online learning of a dictionary of parts of faces->Load the data->from sklearn import datasets\n\nfaces = ()",
            "scipy->Statistics (scipy.stats)->Kernel density estimation->Univariate estimation->from functools import partial"
        ]
    },
    {
        "id": "156597",
        "GT": "Indonesia's Key Indicator Quick Analysis\n\nIn this quick analysis, I'm gonna take a look at how the economic and population indicator have changed over time.->Data Import and Extraction-># Import the dataset\nraw = pd.read_csv(\"../datasets/data_indonesia.csv\", sep=',', \\\n                  dtype = None, error_bad_lines=False, \\\n                  encoding='utf-8', keep_default_na=False)                  \n                  \n# Extract the data into pandas dataframe\ndf = pd.DataFrame(raw)\n\n# Drop rows with missing values and drop duplicate (optional)\ndf.dropna(inplace=True)\ndf.drop_duplicates(inplace=True)",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Kim, Nelson, and Startz (1998) Three-state Variance Switching-># Get the dataset\new_excs = requests.get(\"http://econ.korea.ac.kr/~cjkim/MARKOV/data/ew_excs.prn\").content\nraw = pd.read_table(BytesIO(ew_excs), header=None, skipfooter=1, engine=\"python\")\nraw.index = pd.date_range(\"1926-01-01\", \"1995-12-01\", freq=\"MS\")\n\ndta_kns = raw.loc[:\"1986\"] - raw.loc[:\"1986\"].mean()\n\n# Plot the dataset\ndta_kns[0].plot(title=\"Excess returns\", figsize=(12, 3))\n\n# Fit the model\nmod_kns = sm.tsa.MarkovRegression(\n    dta_kns, k_regimes=3, trend=\"n\", switching_variance=True\n)\nres_kns = mod_kns.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\"/>",
            "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Hamilton (1989) switching model of GNP-># Get the RGNP data to replicate Hamilton\ndta = pd.read_stata(\"https://www.stata-press.com/data/r14/rgnp.dta\").iloc[1:]\ndta.index = pd.DatetimeIndex(dta.date, freq=\"QS\")\ndta_hamilton = dta.rgnp\n\n# Plot the data\ndta_hamilton.plot(title=\"Growth rate of Real GNP\", figsize=(12, 3))\n\n# Fit the model\nmod_hamilton = sm.tsa.MarkovAutoregression(\n    dta_hamilton, k_regimes=2, order=4, switching_ar=False\n)\nres_hamilton = mod_hamilton.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_4_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_4_0.png\"/>",
            "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept-># Get the federal funds rate data\nfrom statsmodels.tsa.regime_switching.tests.test_markov_regression import fedfunds\n\ndta_fedfunds = pd.Series(\n    fedfunds, index=pd.date_range(\"1954-07-01\", \"2010-10-01\", freq=\"QS\")\n)\n\n# Plot the data\ndta_fedfunds.plot(title=\"Federal funds rate\", figsize=(12, 3))\n\n# Fit the model\n# (a switching mean is the default of the MarkovRegession model)\nmod_fedfunds = sm.tsa.MarkovRegression(dta_fedfunds, k_regimes=2)\nres_fedfunds = mod_fedfunds.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_regression_4_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_regression_4_0.png\"/>",
            "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Switching variances-># Get the federal funds rate data\nfrom statsmodels.tsa.regime_switching.tests.test_markov_regression import areturns\n\ndta_areturns = pd.Series(\n    areturns, index=pd.date_range(\"2004-05-04\", \"2014-5-03\", freq=\"W\")\n)\n\n# Plot the data\ndta_areturns.plot(title=\"Absolute returns, S&P500\", figsize=(12, 3))\n\n# Fit the model\nmod_areturns = sm.tsa.MarkovRegression(\n    dta_areturns.iloc[1:],\n    k_regimes=2,\n    exog=dta_areturns.iloc[:-1],\n    switching_variance=True,\n)\nres_areturns = mod_areturns.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_regression_25_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_regression_25_0.png\"/>",
            "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Comparing kernel functions->The available kernel functions on three data points-># Create three equidistant points\ndata = np.linspace(-1, 1, 3)\nkde = sm.nonparametric.KDEUnivariate(data)\n\n# Create a figure\nfig = plt.figure(figsize=(12, 5))\n\n# Enumerate every option for the kernel\nfor i, kernel in enumerate(kernel_switch.keys()):\n\n    # Create a subplot, set the title\n    ax = fig.add_subplot(3, 3, i + 1)\n    ax.set_title('Kernel function \"{}\"'.format(kernel))\n\n    # Fit the model (estimate densities)\n    kde.fit(kernel=kernel, fft=False, gridsize=2 ** 10)\n\n    # Create the plot\n    ax.plot(kde.support, kde.density, lw=3, label=\"KDE from samples\", zorder=10)\n    ax.scatter(data, np.zeros_like(data), marker=\"x\", color=\"red\")\n    plt.grid(True, zorder=-5)\n    ax.set_xlim([-3, 3])\n\nplt.tight_layout()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_24_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_24_0.png\"/>"
        ]
    },
    {
        "id": "870398",
        "GT": "Cleaning dataframe: forest coverage ###\n\nNow onto the third dataset. I'll get the data into dataframe for further analysis.-># It seems that there are a lot of extra null columns; we'll drop those\ndf3.dropna(axis=1, how='all', inplace=True)",
        "pred": [
            "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates-># Assign better names for our seasonal terms\ntrue_seasonal_10_3 = terms[0]\ntrue_seasonal_100_2 = terms[1]\ntrue_sum = true_seasonal_10_3 + true_seasonal_100_2\n<br/>",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Here we'll catch the exception to prevent printing too much of\n# the exception trace output in this notebook\ntry:\n    res.forecast('2000-01-03')\nexcept KeyError as e:\n    print(e)",
            "torch->PyTorch Recipes->PyTorch Benchmark->Steps->6. Saving/Loading benchmark results-># And just to show that we can round trip all of the results from earlier:\nround_tripped_results = pickle.loads(pickle.dumps(results))\nassert(str(benchmark.Compare(results)) == str(benchmark.Compare(round_tripped_results)))",
            "matplotlib->Tutorials->Advanced->Path Tutorial->Compound paths-># histogram our data with numpy\ndata = np.random.randn(1000)\nn, bins = np.histogram(data, 100)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Monthly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='M')\nendog3 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog3.index)"
        ]
    },
    {
        "id": "1245411",
        "GT": "# Steps to normalize these results:\n# Convert data to a single column array.\n# Rehape the array to a single sample\n# Normalize the results\n\nhandgun_array = np.array(wyoming_permit_dat['handgun'])\n\nlong_gun_array = np.array(wyoming_permit_dat['long_gun'])\n\nz, p = wtests.ztest(handgun_array, long_gun_array)\n\nprint(\"The z-value comes out to be {}\".format(z))\n\nprint(\"The p-value comes out to be {}\".format(p))",
        "pred": [
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data-># Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
            "pandas_toms_blog->Indexes->Set Operations-># Bring in the flights data\n\nflights = pd.read_hdf('flights.h5', 'flights')\n\nweather_locs = weather.index.levels[0]\n# The `categories` attribute of a Categorical is an Index\norigin_locs = flights.origin.cat.categories\ndest_locs = flights.dest.cat.categories\n\nairports = weather_locs &amp; origin_locs &amp; dest_locs\nairports",
            "sklearn->Examples->Tutorial exercises->Cross-validation on diabetes Dataset Exercise->Bonus: how much can you trust the selection of alpha?-># To answer this question we use the LassoCV object that sets its alpha\n# parameter automatically from the data by internal cross-validation (i.e. it\n# performs cross-validation on the training data it receives).\n# We use external cross-validation to see how much the automatically obtained\n# alphas differ across different cross-validation folds.\n\nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \n\nlasso_cv = (alphas=alphas, random_state=0, max_iter=10000)\nk_fold = (3)\n\nprint(\"Answer to the bonus question:\", \"how much can you trust the selection of alpha?\")\nprint()\nprint(\"Alpha parameters maximising the generalization score on different\")\nprint(\"subsets of the data:\")\nfor k, (train, test) in enumerate(k_fold.split(X, y)):\n    lasso_cv.fit(X[train], y[train])\n    print(\n        \"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".format(\n            k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])\n        )\n    )\nprint()\nprint(\"Answer: Not very much since we obtained different alphas for different\")\nprint(\"subsets of the data and moreover, the scores for these alphas differ\")\nprint(\"quite substantially.\")\n\n()",
            "torch->PyTorch Recipes->Automatic Mixed Precision->Saving/Resuming-># Read checkpoint as desired, e.g.,\n# dev = torch.cuda.current_device()\n# checkpoint = torch.load(\"filename\",\n#                         map_location = lambda storage, loc: storage.cuda(dev))\nnet.load_state_dict(checkpoint[\"model\"])\nopt.load_state_dict(checkpoint[\"optimizer\"])\nscaler.load_state_dict(checkpoint[\"scaler\"])",
            "statsmodels->Examples->User Notes->Least squares fitting of models to data->Linear models->Check against scipy.optimize.curve_fit-># You can use `scipy.optimize.curve_fit` to get the best-fit parameters and parameter errors.\nfrom scipy.optimize import curve_fit\n\n\ndef f(x, a, b):\n    return a * x + b\n\n\nxdata = data[\"x\"]\nydata = data[\"y\"]\np0 = [0, 0]  # initial parameter estimate\nsigma = data[\"y_err\"]\npopt, pcov = curve_fit(f, xdata, ydata, p0, sigma, absolute_sigma=True)\nperr = np.sqrt(np.diag(pcov))\nprint(\"a = {0:10.3f} +- {1:10.3f}\".format(popt[0], perr[0]))\nprint(\"b = {0:10.3f} +- {1:10.3f}\".format(popt[1], perr[1]))"
        ]
    },
    {
        "id": "360337",
        "GT": "3. Machine Learning Classification Model for Sex->3.2 SVC model-># Support Vector Classifier (SVM/SVC)\nfrom sklearn.svm import SVC\nsvc = SVC(gamma=0.22)\nsvc.fit(X_train, y_train)\n#y_pred = logreg.predict(X_test)\nscore_svc = svc.score(X_test,y_test)\nprint('The accuracy of SVC is', score_svc)",
        "pred": [
            "torch->PyTorch Recipes->Saving and loading a general checkpoint in PyTorch->Steps->4. Save the general checkpoint-># Additional information\nEPOCH = 5\nPATH = \"model.pt\"\nLOSS = 0.4\n\n({\n            'epoch': EPOCH,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': LOSS,\n            }, PATH)",
            "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets-># Set up cluster parameters\n(figsize=(9 * 1.3 + 2, 14.5))\n(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n\n = [\n    (noisy_circles, {\"n_clusters\": 2}),\n    (noisy_moons, {\"n_clusters\": 2}),\n    (varied, {\"n_neighbors\": 2}),\n    (aniso, {\"n_neighbors\": 2}),\n    (blobs, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate():\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = ().fit_transform(X)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ward = (\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\n    )\n    complete = (\n        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\n    )\n    average = (\n        n_clusters=params[\"n_clusters\"], linkage=\"average\"\n    )\n    single = (\n        n_clusters=params[\"n_clusters\"], linkage=\"single\"\n    )\n\n    clustering_algorithms = (\n        (\"Single Linkage\", single),\n        (\"Average Linkage\", average),\n        (\"Complete Linkage\", complete),\n        (\"Ward Linkage\", ward),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = ()\n\n        # catch warnings related to kneighbors_graph\n        with ():\n            (\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \"  1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = ()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        (len(), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            (name, size=18)\n\n        colors = (\n            list(\n                (\n                    (\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        (X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        (-2.5, 2.5)\n        (-2.5, 2.5)\n        (())\n        (())\n        (\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\n()\n\n\n<img alt=\"Single Linkage, Average Linkage, Complete Linkage, Ward Linkage\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\"/>",
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models-># Graph\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n\n# Plot data points\ninf[\"CPIAUCNS\"].plot(ax=ax, style=\"-\", label=\"Observed data\")\n\n# Plot estimate of the level term\nres_uc_mle.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (MLE)\")\nres_uc_bayes.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (Bayesian)\")\n\nax.legend(loc=\"lower left\");\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\"/>",
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->Confidence interval-># Plot the confidence interval and fit\nfig, ax = pylab.subplots()\nax.scatter(x, y)\nax.plot(eval_x, smoothed, c=\"k\")\nax.fill_between(eval_x, bottom, top, alpha=0.5, color=\"b\")\npylab.autoscale(enable=True, axis=\"x\", tight=True)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_lowess_7_0.png\" src=\"../../../_images/examples_notebooks_generated_lowess_7_0.png\"/>",
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data-># Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")"
        ]
    },
    {
        "id": "921659",
        "GT": "1.2 Quantifying Variability->Range->np.ptp(data)",
        "pred": [
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators->np.median(x)",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data->np.bincount(data[\"affairs\"].astype(int))",
            "statsmodels->Examples->Linear Regression Models->Regression Diagnostics->Multicollinearity->np.linalg.cond(results.model.exog)",
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->np.array(nobs1 + nobs2)",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator->Squared error loss->np.array(se_beta).mean()"
        ]
    },
    {
        "id": "628170",
        "GT": "Defining Columns\n\nNow that we have a few validator functions, its time to put together a representation of our table and use that object to help us get us a final data table that is in a form we like. We will start with `col1`.->Validating a Column object\nFor each column object we store the validator functions in a `dict`.\n\nHere is the value for `col4`->col4.validators",
        "pred": [
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time",
            "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution->kde.entropy",
            "statsmodels->Examples->User Notes->Prediction->Predicting with Formulas->res.params",
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables->dta_c.T[0]",
            "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Products with n-dimensional arrays->reconstructed.shape"
        ]
    },
    {
        "id": "835435",
        "GT": "United-Atom Propane Tutorial\nWe're going to recover propane's bonded parameters from a \nshort gromacs simulation (the relevant setup and simulation files\ncan be found in sim_files). \n\nIn order to run this simulation, we had to specify the bonded \nparameters, so we're just going to see if we can verify these\nparameters from a simulation.\n\nIn reality with this set of tools, you would generate an atomistic\ntrajectory, map the system to coarse-grained one, and then\nidentify the (unknown) bonded parameters.->beadtypes = set([a.name for a in traj.topology.atoms])",
        "pred": [
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 1: Probit model->exog = sm.add_constant(exog, prepend=True)",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->pca_model = PCA(dta.T, standardize=False, demean=True)",
            "statsmodels->Examples->Time Series Analysis->Time Series Filters->Baxter-King approximate band-pass filter: Inflation and Unemployment->Explore the hypothesis that inflation and unemployment are counter-cyclical.->bk_cycles = sm.tsa.filters.bkfilter(dta[[\"infl\", \"unemp\"]])",
            "torch->Frontend APIs->Jacobians, Hessians, hvp, vhp, and more: composing function transforms->reverse-mode Jacobian (jacrev) vs forward-mode Jacobian (jacfwd)->get_perf(, \"jacrev\", , \"jacfwd\")",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points->dta = sm.datasets.get_rdataset(\"starsCYG\", \"robustbase\", cache=True).data"
        ]
    },
    {
        "id": "836066",
        "GT": "DMCT Lab Sessional 1 and 2->1. Fetch IRIS Dataset->IRIS_FILE_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\nfile = requests.get(IRIS_FILE_URL).text",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->url = \"https://stats.idre.ucla.edu/stat/data/ologit.dta\"\ndata_student = pd.read_stata(url)",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->if not os.path.exists(\"data/airports.csv.zip\"):\n    download_airports()",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->nobs = len(data_student)\ndata_student[\"dummy\"] = (np.arange(nobs) &lt; (nobs / 2)).astype(float)",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-based cross-validation->all_splits = list(ts_cv.split(X, y))\ntrain_0, test_0 = all_splits[0]",
            "torch->Deploying PyTorch Models in Production->Deploying PyTorch in Python via a REST API with Flask->Integrating the model in our API Server->import requests\n\nresp = requests.post(\"http://localhost:5000/predict\",\n                     files={\"file\": open('&lt;PATH/TO/.jpg/FILE&gt;/cat.jpg','rb')})"
        ]
    },
    {
        "id": "521639",
        "GT": "Now the transfer tree->file=\"Transfers_Ultra.tree\"\n\ntry:\n    f=open(file, 'r')\nexcept IOError:\n    print (\"Unknown file: \"+file)\n    sys.exit()\n\nline = \"\"\nfor l in f:\n    line += l.strip()\n    \nf.close()\n\ntreeToAnnotate = Tree( line )\n\n\nrenumberNodes(treeToAnnotate, leafList2NodeIdRef)\n#print (treeToAnnotate.write(features=[\"support\"], format=2) )\nprint (treeToAnnotate.write( format=2) )",
        "pred": [
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Testing->url = 'https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/csv/COUNT/medpar.csv'\nmedpar = read.csv(url)\nf = los~factor(type)+hmo+white\n\nlibrary(MASS)\nmod = glm.nb(f, medpar)\ncoef(summary(mod))\n                 Estimate Std. Error   z value      Pr(|z|)\n(Intercept)    2.31027893 0.06744676 34.253370 3.885556e-257\nfactor(type)2  0.22124898 0.05045746  4.384861  1.160597e-05\nfactor(type)3  0.70615882 0.07599849  9.291748  1.517751e-20\nhmo           -0.06795522 0.05321375 -1.277024  2.015939e-01\nwhite         -0.12906544 0.06836272 -1.887951  5.903257e-02",
            "torch->PyTorch Recipes->PyTorch Profiler->Steps->4. Using profiler to analyze memory consumption->model = ()\ninputs = (5, 3, 224, 224)\n\nwith (activities=[ProfilerActivity.CPU],\n        profile_memory=True, record_shapes=True) as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------------  ------------  ------------  ------------\n#                              Name       CPU Mem  Self CPU Mem    # of Calls\n# ---------------------------------  ------------  ------------  ------------\n#                       aten::empty      94.79 Mb      94.79 Mb           121\n#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n#                       aten::addmm      19.53 Kb      19.53 Kb             1\n#               aten::empty_strided         572 b         572 b            25\n#                     aten::resize_         240 b         240 b             6\n#                         aten::abs         480 b         240 b             4\n#                         aten::add         160 b         160 b            20\n#               aten::masked_select         120 b         112 b             1\n#                          aten::ne         122 b          53 b             6\n#                          aten::eq          60 b          30 b             2\n# ---------------------------------  ------------  ------------  ------------\n# Self CPU time total: 53.064ms\n\nprint(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------------  ------------  ------------  ------------\n#                              Name       CPU Mem  Self CPU Mem    # of Calls\n# ---------------------------------  ------------  ------------  ------------\n#                       aten::empty      94.79 Mb      94.79 Mb           121\n#                  aten::batch_norm      47.41 Mb           0 b            20\n#      aten::_batch_norm_impl_index      47.41 Mb           0 b            20\n#           aten::native_batch_norm      47.41 Mb           0 b            20\n#                      aten::conv2d      47.37 Mb           0 b            20\n#                 aten::convolution      47.37 Mb           0 b            20\n#                aten::_convolution      47.37 Mb           0 b            20\n#          aten::mkldnn_convolution      47.37 Mb           0 b            20\n#                  aten::max_pool2d      11.48 Mb           0 b             1\n#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n# ---------------------------------  ------------  ------------  ------------\n# Self CPU time total: 53.064ms",
            "torch->PyTorch Recipes->Automatic Mixed Precision->All together: \u201cAutomatic Mixed Precision\u201d->use_amp = True\n\nnet = make_model(in_size, out_size, num_layers)\nopt = (net.parameters(), lr=0.001)\nscaler = (enabled=use_amp)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        with (device_type='cuda', dtype=torch.float16, enabled=use_amp):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\nend_timer_and_print(\"Mixed precision:\")",
            "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor->quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "sklearn->Examples->Decomposition->Factor Analysis (with rotation) to visualize patterns->n_comps = 2\n\nmethods = [\n    (\"PCA\", ()),\n    (\"Unrotated FA\", ()),\n    (\"Varimax FA\", (rotation=\"varimax\")),\n]\nfig, axes = (ncols=len(methods), figsize=(10, 8), sharey=True)\n\nfor ax, (method, fa) in zip(axes, methods):\n    fa.set_params(n_components=n_comps)\n    fa.fit(X)\n\n    components = fa.components_.T\n    print(\"\\n\\n %s :\\n\" % method)\n    print(components)\n\n    vmax = np.abs(components).max()\n    ax.imshow(components, cmap=\"RdBu_r\", vmax=vmax, vmin=-vmax)\n    ax.set_yticks((len(feature_names)))\n    ax.set_yticklabels(feature_names)\n    ax.set_title(str(method))\n    ax.set_xticks([0, 1])\n    ax.set_xticklabels([\"Comp. 1\", \"Comp. 2\"])\nfig.suptitle(\"Factors\")\n()\n()\n\n\n<img alt=\"Factors, PCA, Unrotated FA, Varimax FA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_varimax_fa_002.png\" srcset=\"../../_images/sphx_glr_plot_varimax_fa_002.png\"/>"
        ]
    },
    {
        "id": "1234230",
        "GT": "An aside about BOT\n\nQuoting from Wikipedia: An Internet ***Bot***, also known as web robot, WWW robot or simply bot, is a software application that runs automated tasks (scripts) over the Internet. Typically, bots perform tasks that are both simple and structurally repetitive, at a much higher rate than would be possible for a human alone.\n\nBOT in online social media can be annoying as they keep posting useless stuff. And the existence of posts from BOTs may introduce bias to our analysis.\n\nWe'll need to somehow have some idea about the extent of BOT in our data. One attribute that is distinctive about BOTs is they keep posting duplicated stuff. We can check if authors have this attribute very easily:->men_bot.shape[0], men_bot.sum(), fem_bot.shape[0], fem_bot.sum()",
        "pred": [
            "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Content->Shape, axis and array properties->img_array.max(), img_array.min()",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Remainder->res_a2.pearson_chi2, res_a.pearson_chi2, res_a2.resid_pearson.sum(), res_a.resid_pearson.sum()",
            "numpy->NumPy Features->Linear algebra on n-dimensional arrays->Approximation->Products with n-dimensional arrays->reconstructed.min(), reconstructed.max()",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Investigating Pearson chi-square statistic->(res_e2._results.resid_response ** 2).sum(), (res_e._results.resid_response ** 2).sum()",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Investigating Pearson chi-square statistic->res_e._results.resid_response.mean(), res_e.model.family.variance(res_e.mu)[\n    :5\n], res_e.mu[:5]"
        ]
    },
    {
        "id": "1468231",
        "GT": "English Wikipedia page views, 2008 - 2017  \n\n### Angel Wang  \n\nThe goal of this project is to construct, analyze, and publish a dataset of monthly traffic on English Wikipedia from July 1 2008 through September 30 2017 and create a visulization of the dataset. Following are the steps for data acquisition, data processing and data analysis.\nThe module will then output:  \n1) 5 source data files in JSON format.  \n2) 1 final data file in CSV format.  \n3) 1 .png image of the visualization.->Step 2 Data processing-># get items in api raw data\ndf_pv_mw = pd.DataFrame(pageview_mobileweb)\ndf_pv_ma = pd.DataFrame(pageview_mobileapp)\ndf_pv_dk = pd.DataFrame(pageview_desktop)\ndf_pc_ms = pd.DataFrame(pagecounts_mobile_site)\ndf_pc_ds = pd.DataFrame(pagecounts_desktop_site)\ndf_pv_mw_items = df_pv_mw['items']\ndf_pv_ma_items = df_pv_ma['items']\ndf_pv_dk_items = df_pv_dk['items']\ndf_pc_ms_items = df_pc_ms['items']\ndf_pc_ds_items = df_pc_ds['items']",
        "pred": [
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Logarithmic and other nonlinear axes-># Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n# make up some data in the open interval (0, 1)\ny = np.random.normal(loc=0.5, scale=0.4, size=1000)\ny = y[(y  0) & (y &lt; 1)]\ny.sort()\nx = np.arange(len(y))\n\n# plot with various axes scales\nplt.figure()\n\n# linear\nplt.subplot(221)\nplt.plot(x, y)\nplt.yscale('linear')\nplt.title('linear')\nplt.grid(True)\n\n# log\nplt.subplot(222)\nplt.plot(x, y)\nplt.yscale('log')\nplt.title('log')\nplt.grid(True)\n\n# symmetric log\nplt.subplot(223)\nplt.plot(x, y - y.mean())\nplt.yscale('symlog', linthresh=0.01)\nplt.title('symlog')\nplt.grid(True)\n\n# logit\nplt.subplot(224)\nplt.plot(x, y)\nplt.yscale('logit')\nplt.title('logit')\nplt.grid(True)\n# Adjust the subplot layout, because the logit one may take more space\n# than usual, due to y-tick labels like \"1 - 10^{-3}\"\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n\nplt.show()\n\n\n<img alt=\"linear, log, symlog, logit\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_010.png\" srcset=\"../../_images/sphx_glr_pyplot_010.png, ../../_images/sphx_glr_pyplot_010_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets-># Set up cluster parameters\n(figsize=(9 * 1.3 + 2, 14.5))\n(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n\n = [\n    (noisy_circles, {\"n_clusters\": 2}),\n    (noisy_moons, {\"n_clusters\": 2}),\n    (varied, {\"n_neighbors\": 2}),\n    (aniso, {\"n_neighbors\": 2}),\n    (blobs, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate():\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = ().fit_transform(X)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ward = (\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\n    )\n    complete = (\n        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\n    )\n    average = (\n        n_clusters=params[\"n_clusters\"], linkage=\"average\"\n    )\n    single = (\n        n_clusters=params[\"n_clusters\"], linkage=\"single\"\n    )\n\n    clustering_algorithms = (\n        (\"Single Linkage\", single),\n        (\"Average Linkage\", average),\n        (\"Complete Linkage\", complete),\n        (\"Ward Linkage\", ward),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = ()\n\n        # catch warnings related to kneighbors_graph\n        with ():\n            (\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \"  1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = ()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        (len(), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            (name, size=18)\n\n        colors = (\n            list(\n                (\n                    (\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        (X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        (-2.5, 2.5)\n        (-2.5, 2.5)\n        (())\n        (())\n        (\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\n()\n\n\n<img alt=\"Single Linkage, Average Linkage, Complete Linkage, Ward Linkage\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\"/>",
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data-># Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->Confidence interval-># Now create a bootstrap confidence interval around the a LOWESS fit\n\n\ndef lowess_with_confidence_bounds(\n    x, y, eval_x, N=200, conf_interval=0.95, lowess_kw=None\n):\n    \"\"\"\n    Perform Lowess regression and determine a confidence interval by bootstrap resampling\n    \"\"\"\n    # Lowess smoothing\n    smoothed = sm.nonparametric.lowess(exog=x, endog=y, xvals=eval_x, **lowess_kw)\n\n    # Perform bootstrap resamplings of the data\n    # and  evaluate the smoothing at a fixed set of points\n    smoothed_values = np.empty((N, len(eval_x)))\n    for i in range(N):\n        sample = np.random.choice(len(x), len(x), replace=True)\n        sampled_x = x[sample]\n        sampled_y = y[sample]\n\n        smoothed_values[i] = sm.nonparametric.lowess(\n            exog=sampled_x, endog=sampled_y, xvals=eval_x, **lowess_kw\n        )\n\n    # Get the confidence interval\n    sorted_values = np.sort(smoothed_values, axis=0)\n    bound = int(N * (1 - conf_interval) / 2)\n    bottom = sorted_values[bound - 1]\n    top = sorted_values[-bound]\n\n    return smoothed, bottom, top\n\n\n# Compute the 95% confidence interval\neval_x = np.linspace(0, 4 * np.pi, 31)\nsmoothed, bottom, top = lowess_with_confidence_bounds(\n    x, y, eval_x, lowess_kw={\"frac\": 0.1}\n)",
            "torch->Image and Video->Adversarial Example Generation->Results->Sample Adversarial Examples-># Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()\n\n\n<img alt=\"2 -&gt; 2, 6 -&gt; 6, 4 -&gt; 4, 6 -&gt; 6, 2 -&gt; 2, 8 -&gt; 3, 4 -&gt; 2, 9 -&gt; 5, 7 -&gt; 9, 8 -&gt; 9, 6 -&gt; 0, 1 -&gt; 8, 6 -&gt; 8, 9 -&gt; 8, 8 -&gt; 6, 5 -&gt; 8, 5 -&gt; 2, 4 -&gt; 8, 4 -&gt; 8, 5 -&gt; 8, 3 -&gt; 5, 4 -&gt; 8, 8 -&gt; 2, 8 -&gt; 6, 3 -&gt; 8, 6 -&gt; 1, 1 -&gt; 3, 1 -&gt; 8, 4 -&gt; 8, 8 -&gt; 2, 7 -&gt; 8, 1 -&gt; 2, 0 -&gt; 2, 7 -&gt; 8, 7 -&gt; 8\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_002.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_002.png\"/>"
        ]
    },
    {
        "id": "333615",
        "GT": "def bootstrap_statistic(stat_function, y_test, y_hat):\n    #Setting random seed ensures paired samples\n    np.random.seed(3743)\n    N = 10**3\n    result = np.empty(N)\n    for n in xrange(N):\n        I = np.random.choice(len(y_test), len(y_test))\n        result[n] = stat_function(y_test[I], y_hat[I])\n        \n    return result",
        "pred": [
            "sklearn->Examples->Clustering->Adjustment for chance in clustering performance evaluation->Second experiment: varying number of classes and clusters->def uniform_labelings_scores(score_func, n_samples, n_clusters_range, n_runs=5):\n    scores = ((len(n_clusters_range), n_runs))\n\n    for i, n_clusters in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            labels_a = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            labels_b = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->def plot_weights(support, weights_func, xlabels, xticks):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(support, weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_ylim(-0.1, 1.1)\n    return ax",
            "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence->def scoring_on_bootstrap(estimator, X, y, rng, n_bootstrap=100):\n    results_for_prevalence = (list)\n    for _ in range(n_bootstrap):\n        bootstrap_indices = rng.choice(\n            (X.shape[0]), size=X.shape[0], replace=True\n        )\n        for key, value in scoring(\n            estimator, X[bootstrap_indices], y[bootstrap_indices]\n        ).items():\n            results_for_prevalence[key].append(value)\n    return (results_for_prevalence)",
            "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)->def frame_preprocessing(observation_frame):\n    # Crop the frame.\n    observation_frame = observation_frame[35:195]\n    # Downsample the frame by a factor of 2.\n    observation_frame = observation_frame[::2, ::2, 0]\n    # Remove the background and apply other enhancements.\n    observation_frame[observation_frame == 144] = 0  # Erase the background (type 1).\n    observation_frame[observation_frame == 109] = 0  # Erase the background (type 2).\n    observation_frame[observation_frame != 0] = 1  # Set the items (rackets, ball) to 1.\n    # Return the preprocessed frame as a 1D floating-point array.\n    return observation_frame.astype(float)",
            "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Replace missing values by 0->def get_impute_zero_score(X_missing, y_missing):\n\n    imputer = (\n        missing_values=, add_indicator=True, strategy=\"constant\", fill_value=0\n    )\n    zero_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return zero_impute_scores.mean(), zero_impute_scores.std()\n\n\nmses_california[1], stds_california[1] = get_impute_zero_score(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[1], stds_diabetes[1] = get_impute_zero_score(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Zero imputation\")"
        ]
    },
    {
        "id": "683966",
        "GT": "Data Visualization (15%)->cols_to_visualize = [\"GPA\", \"weight\", \"life_rewarding\", \"healthy_feeling\"]\n\n# Plot histograms\nfor col in cols_to_visualize:\n    plt.title(col + \" histogram\")\n    plt.hist(QOFL_df[col])\n    plt.show()\n\n# Plot scatter matrix\npd.plotting.scatter_matrix(QOFL_df[cols_to_visualize])\nplt.suptitle(\"Scatter matrix of \" + ', '.join(cols_to_visualize))\nplt.show()",
        "pred": [
            "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with partial observations->features_names = [\"experience\", \"parent hourly wage\", \"college degree\"]\n\nregressor_without_ability = ()\nregressor_without_ability.fit(X_train[features_names], y_train)\ny_pred_without_ability = regressor_without_ability.predict(X_test[features_names])\nR2_without_ability = (y_test, y_pred_without_ability)\n\nprint(f\"R2 score without ability: {R2_without_ability:.3f}\")",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->column_to_drop = [\"AGE\"]\n\ncv_model = (\n    model,\n    X.drop(columns=column_to_drop),\n    y,\n    cv=cv,\n    return_estimator=True,\n    n_jobs=2,\n)\n\ncoefs = (\n    [\n        est[-1].regressor_.coef_\n        * est[:-1].transform(X.drop(columns=column_to_drop).iloc[train_idx]).std(axis=0)\n        for est, (train_idx, _) in zip(cv_model[\"estimator\"], cv.split(X, y))\n    ],\n    columns=feature_names[:-1],\n)",
            "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Visualize cross-validation indices for many CV objects->cvs = [\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n]\n\n\nfor cv in cvs:\n    this_cv = cv(n_splits=n_splits)\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n()\n\n\n\n<img alt=\"KFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_006.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_006.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_007.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_007.png\"/>\n<img alt=\"ShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_008.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_008.png\"/>\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_009.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_009.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_010.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_010.png\"/>\n<img alt=\"GroupShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_011.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_011.png\"/>\n<img alt=\"StratifiedShuffleSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_012.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_012.png\"/>\n<img alt=\"TimeSeriesSplit\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_013.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_013.png\"/>",
            "sklearn->Examples->Model Selection->Visualizing cross-validation behavior in scikit-learn->Define a function to visualize cross-validation behavior->cvs = [, , ]\n\nfor cv in cvs:\n    fig, ax = (figsize=(6, 3))\n    plot_cv_indices(cv(n_splits), X, y, groups, ax, n_splits)\n    ax.legend(\n        [(color=cmap_cv(0.8)), (color=cmap_cv(0.02))],\n        [\"Testing set\", \"Training set\"],\n        loc=(1.02, 0.8),\n    )\n    # Make the legend fit\n    ()\n    fig.subplots_adjust(right=0.7)\n\n\n\n<img alt=\"StratifiedKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_003.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_003.png\"/>\n<img alt=\"GroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_004.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_004.png\"/>\n<img alt=\"StratifiedGroupKFold\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_cv_indices_005.png\" srcset=\"../../_images/sphx_glr_plot_cv_indices_005.png\"/>",
            "statsmodels->Examples->Linear Regression Models->Rolling Least Squares->exog_vars = [\"Mkt-RF\", \"SMB\", \"HML\"]\nexog = sm.add_constant(factors[exog_vars])\nrols = RollingOLS(endog, exog, window=60)\nrres = rols.fit()\nfig = rres.plot_recursive_coefficient(variables=exog_vars, figsize=(14, 18))\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\" src=\"../../../_images/examples_notebooks_generated_rolling_ls_12_0.png\"/>"
        ]
    },
    {
        "id": "82504",
        "GT": "*Deck*->stats.binom_test(x=12,n=12+35,p=24/(24.+35.))",
        "pred": [
            "scipy->Statistics (scipy.stats)->Random variables->Broadcasting->stats.t.isf([0.1, 0.05, 0.01], [10, 11, 12])\narray([ 1.37218364,  1.79588482,  2.68099799])",
            "scipy->Statistics (scipy.stats)->Comparing two samples->Kolmogorov-Smirnov test for two samples ks_2samp->stats.ks_2samp(rvs1, rvs3)\nKstestResult(statistic=0.114, pvalue=0.00299005061044668)  # random",
            "scipy->Statistics (scipy.stats)->Comparing two samples->Kolmogorov-Smirnov test for two samples ks_2samp->stats.ks_2samp(rvs1, rvs2)\nKstestResult(statistic=0.026, pvalue=0.9959527565364388)  # random",
            "scipy->Statistics (scipy.stats)->Random variables->Shifting and scaling->norm.stats(loc=3, scale=4, moments=\"mv\")\n(array(3.0), array(16.0))",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->stats.normaltest(resid)",
            "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->Sunspots Data->stats.normaltest(resid)"
        ]
    },
    {
        "id": "581134",
        "GT": "Convert the percentage from *string* type to *float*->#remember only execute it once!\ndf_AAII = pd.read_csv('AAII.csv')\n\"\"\"convert_to_float(df_AAII) # --> df_AAII with \"Bullish\", \"Neutral\", \"Bearish\" set to float \n:param df_AAII: the dataframe to be processed\n:type df_AAII: pandas.core.frame.DataFrame\n:return: df_AAII DataFrame with float type column for the sentiment type\n\"\"\"\ndef convert_to_float(df_AAII):\n    sentimentList = [\"Bullish\", \"Neutral\", \"Bearish\"]\n    for i in range(len(sentimentList)):\n        sentimentType = sentimentList[i]\n        df_AAII[sentimentType] = df_AAII[sentimentType].replace('%','',regex=True).astype('float')/100\n    return df_AAII\ndf_AAII_float = convert_to_float(df_AAII)\n",
        "pred": [
            "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results-># Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "matplotlib->Tutorials->Intermediate->Artist tutorial->Object containers->Axis containers-># plt.figure creates a matplotlib.figure.Figure instance\nfig = plt.figure()\nrect = fig.patch  # a rectangle instance\nrect.set_facecolor('lightgoldenrodyellow')\n\nax1 = fig.add_axes([0.1, 0.3, 0.4, 0.4])\nrect = ax1.patch\nrect.set_facecolor('lightslategray')\n\n\nfor label in ax1.xaxis.get_ticklabels():\n    # label is a Text instance\n    label.set_color('red')\n    label.set_rotation(45)\n    label.set_fontsize(16)\n\nfor line in ax1.yaxis.get_ticklines():\n    # line is a Line2D instance\n    line.set_color('green')\n    line.set_markersize(25)\n    line.set_markeredgewidth(3)\n\nplt.show()\n\n\n<img alt=\"artists\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_artists_004.png\" srcset=\"../../_images/sphx_glr_artists_004.png, ../../_images/sphx_glr_artists_004_2_0x.png 2.0x\"/>",
            "torch->PyTorch Recipes->PyTorch Profiler->Steps->3. Using profiler to analyze execution time-># (omitting some columns)\n# -------------------------------------------------------  ------------  ------------\n#                                                    Name     Self CUDA    CUDA total\n# -------------------------------------------------------  ------------  ------------\n#                                         model_inference       0.000us      11.666ms\n#                                            aten::conv2d       0.000us      10.484ms\n#                                       aten::convolution       0.000us      10.484ms\n#                                      aten::_convolution       0.000us      10.484ms\n#                              aten::_convolution_nogroup       0.000us      10.484ms\n#                                       aten::thnn_conv2d       0.000us      10.484ms\n#                               aten::thnn_conv2d_forward      10.484ms      10.484ms\n# void at::native::im2col_kernel&lt;float&gt;(long, float co...       3.844ms       3.844ms\n#                                       sgemm_32x32x32_NN       3.206ms       3.206ms\n#                                   sgemm_32x32x32_NN_vec       3.093ms       3.093ms\n# -------------------------------------------------------  ------------  ------------\n# Self CPU time total: 23.015ms\n# Self CUDA time total: 11.666ms",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Ordinal regression with a custom cumulative cLogLog distribution:-># using a SciPy distribution\nres_exp = OrderedModel(data_student['apply'],\n                           data_student[['pared', 'public', 'gpa']],\n                           distr=stats.expon).fit(method='bfgs', disp=False)\nres_exp.summary()",
            "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 2. Finetuning the Quantizable Model-># notice `quantize=False`\nmodel = models.resnet18(pretrained=True, progress=True, quantize=False)\nnum_ftrs = model.fc.in_features\n\n# Step 1\nmodel.train()\nmodel.fuse_model()\n# Step 2\nmodel_ft = create_combined_model(model)\nmodel_ft[0].qconfig = torch.quantization.default_qat_qconfig  # Use default QAT configuration\n# Step 3\nmodel_ft = torch.quantization.prepare_qat(model_ft, inplace=True)"
        ]
    },
    {
        "id": "183655",
        "GT": "Data Visualization With Seaborn->import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntips = sns.load_dataset('tips')\n\nsns.lmplot(x = 'total_bill', y = 'tip',hue='sex', data = tips, palette='Set1')\nplt.show()",
        "pred": [
            "seaborn->User guide and tutorial->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n",
            "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "torch->PyTorch Recipes->Loading data in PyTorch->5. [Optional] Visualize the data->import matplotlib.pyplot as plt\n\nprint(data[0][0].numpy())\n\nplt.figure()\nplt.plot(waveform.t().numpy())",
            "matplotlib->Tutorials->Toolkits->The mplot3d toolkit->import matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot(projection='3d')",
            "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 0. Prerequisites-># Imports\nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\n\nplt.ion()"
        ]
    },
    {
        "id": "189019",
        "GT": "Survivors by age\n\nFirst I want to look at the survival rate by age. I imagine that kids were more likely to get to the lifeboats. I'm going to use Pandas `cut` method to create bins on age.->survival_by_age_sex = df.groupby(['Age bins', 'Sex'])['Survived'].mean()",
        "pred": [
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->means25[\"LOWINC\"] = exog[\"LOWINC\"].quantile(0.25)\nprint(means25)",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "statsmodels->Examples->Plotting->Regression Plots->Duncan\u2019s Prestige Dataset->Load the Data->prestige_model = ols(\"prestige ~ income + education\", data=prestige).fit()",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->resp25 = glm_mod.predict(pd.DataFrame(means25).T)\nresp75 = glm_mod.predict(pd.DataFrame(means75).T)\ndiff = resp75 - resp25"
        ]
    },
    {
        "id": "1227801",
        "GT": "Converting the categorical values in Platform, Genre, Publisher into a numerical values->from sklearn.preprocessing import LabelEncoder\nnumber = LabelEncoder()\ndf['Platform'] = number.fit_transform(df['Platform'].astype('str'))\ndf['Genre'] = number.fit_transform(df['Genre'].astype('str'))\ndf['Publisher'] = number.fit_transform(df['Publisher'].astype('str'))",
        "pred": [
            "sklearn->Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Evaluation->from sklearn import metrics\n\nY_pred = rbm_features_classifier.predict(X_test)\nprint(\n    \"Logistic regression using RBM features:\\n%s\\n\"\n    % ((Y_test, Y_pred))\n)",
            "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding->from patsy.contrasts import Treatment\n\nlevels = [1, 2, 3, 4]\ncontrast = Treatment(reference=0).code_without_intercept(levels)\nprint(contrast.matrix)",
            "sklearn->Tutorials->Working With Text Data->Extracting features from text files->Tokenizing text with scikit-learn->from sklearn.feature_extraction.text import CountVectorizer\n count_vect = CountVectorizer()\n X_train_counts = count_vect.fit_transform(twenty_train.data)\n X_train_counts.shape\n(2257, 35788)",
            "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Basic Use->from statsmodels.tsa.deterministic import DeterministicProcess\n\nindex = pd.RangeIndex(0, 100)\ndet_proc = DeterministicProcess(index, constant=True, order=1, seasonal=True, period=5)\ndet_proc.in_sample()",
            "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Load some Data->from statsmodels.tsa.forecasting.theta import ThetaModel\n\ntm = ThetaModel(housing)\nres = tm.fit()\nprint(res.summary())"
        ]
    },
    {
        "id": "779342",
        "GT": "player_age.mean()",
        "pred": [
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resf_logit.predict()",
            "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Hamilton (1989) switching model of GNP->res_hamilton.summary()",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->arima_res.predict(0, 2)",
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time.head()",
            "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Custom Deterministic Terms->det_proc.in_sample().head()"
        ]
    },
    {
        "id": "955497",
        "GT": "This code will replace the string of categories with the first in the list.->plt.figure(figsize=(30,10))\nplt.hist(business_df['categories'].head(350))\nplt.show()",
        "pred": [
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Exercise: How good of in-sample prediction can you do for another series, say, CPI->Hint:->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax = cpi.plot(ax=ax)\nax.legend()",
            "statsmodels->Examples->Time Series Analysis->ARMA: Sunspots Data->Sunspots Data->Simulated ARMA(4,1): Model Identification is Difficult->fig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot(111)\nax.plot(arma_t.generate_sample(nsample=50))",
            "statsmodels->Examples->Time Series Analysis->Time Series Filters->Christiano-Fitzgerald approximate band-pass filter: Inflation and Unemployment->fig = plt.figure(figsize=(14, 10))\nax = fig.add_subplot(111)\ncf_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])",
            "statsmodels->Examples->Time Series Analysis->Time Series Filters->Baxter-King approximate band-pass filter: Inflation and Unemployment->Explore the hypothesis that inflation and unemployment are counter-cyclical.->fig = plt.figure(figsize=(12, 10))\nax = fig.add_subplot(111)\nbk_cycles.plot(ax=ax, style=[\"r--\", \"b-\"])"
        ]
    },
    {
        "id": "1406337",
        "GT": "Here comes the most interesting part. What we want to build is predictor which will take in as a input the few moving avarages.\n# The goal of the predictor is to decide when the position can be entered so that the position returns the profit during particular period. \n\nBelow are moving avarages and return window we are using.\n\n1. Moving Avarage 1 = 400 minutes\n2. Moving Avarage 2 = 120 minutes\n3. Moving Avarage 3 = 20 minutes\n4. The time during which position will return profit\n5. How much minimum profit it should return during above time.->Below is training mode of the predictor. Here predictor decides how to use below parameters \n1. OPEN\n2. VOLUME\n3. Moving Avarage 1\n4. Moving Avarage 2\n5. Moving Avarage 3->#Xtrain, Xtest = X[:int(len(X) * 0.50)], X[int(len(X) * 0.50):] \n#ytrain, ytest = y[:int(len(y) * 0.50)], y[int(len(y) * 0.50):] \nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.50)\nprint(Xtrain.shape)\nprint(Xtest.shape)",
        "pred": [
            "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results-># Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard-># 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)",
            "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation-># plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>",
            "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method-># fit in statsmodels\nmodel = ETSModel(\n    austourists,\n    error=\"add\",\n    trend=\"add\",\n    seasonal=\"add\",\n    damped_trend=True,\n    seasonal_periods=4,\n)\nfit = model.fit()\n\n# fit with R params\nparams_R = [\n    0.35445427,\n    0.03200749,\n    0.39993387,\n    0.97999997,\n    24.01278357,\n    0.97770147,\n    1.76951063,\n    -0.50735902,\n    -6.61171798,\n    5.34956637,\n]\nfit_R = model.smooth(params_R)\n\naustourists.plot(label=\"data\")\nplt.ylabel(\"Australian Tourists\")\n\nfit.fittedvalues.plot(label=\"statsmodels fit\")\nfit_R.fittedvalues.plot(label=\"R fit\", linestyle=\"--\")\nplt.legend()",
            "sklearn->Examples->Generalized Linear Models->Lasso on dense and sparse data->Comparing the two Lasso implementations on Sparse data-># make a copy of the previous data\nXs = X.copy()\n# make Xs sparse by replacing the values lower than 2.5 with 0s\nXs[Xs &lt; 2.5] = 0.0\n# create a copy of Xs in sparse format\nXs_sp = (Xs)\nXs_sp = Xs_sp.tocsc()\n\n# compute the proportion of non-zero coefficient in the data matrix\nprint(f\"Matrix density : {(Xs_sp.nnz / float(X.size) * 100):.3f}%\")\n\nalpha = 0.1\nsparse_lasso = (alpha=alpha, fit_intercept=False, max_iter=10000)\ndense_lasso = (alpha=alpha, fit_intercept=False, max_iter=10000)\n\nt0 = ()\nsparse_lasso.fit(Xs_sp, y)\nprint(f\"Sparse Lasso done in {(() - t0):.3f}s\")\n\nt0 = ()\ndense_lasso.fit(Xs, y)\nprint(f\"Dense Lasso done in  {(() - t0):.3f}s\")\n\n# compare the regression coefficients\ncoeff_diff = (sparse_lasso.coef_ - dense_lasso.coef_)\nprint(f\"Distance between coefficients : {coeff_diff:.2e}\")"
        ]
    },
    {
        "id": "492969",
        "GT": "obs = (obs\n       .resample('D')\n       .mean()\n       .interpolate('linear'))\n\nobs.head(10)",
        "pred": [
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->X = (pd.concat([y.shift(i) for i in range(6)], axis=1,\n               keys=['y'] + ['L%s' % i for i in range(1, 6)])\n       .dropna())\nX.head()",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "pandas_toms_blog->Scaling->Dask->daily = (\n    indiv[['transaction_dt', 'transaction_amt']].dropna()\n        .set_index('transaction_dt')['transaction_amt']\n        .resample(\"D\")\n        .sum()\n).compute()\ndaily",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Poisson->poisson_mod = sm.Poisson(rand_data.endog, rand_exog)\npoisson_res = poisson_mod.fit(method=\"newton\")\nprint(poisson_res.summary())"
        ]
    },
    {
        "id": "111722",
        "GT": "Reliabilty of the consumptions data from SES production DB\n## Import of xml\nIn order to verify if the meter readings stored into the SES production DB are reliable, I reimported into a local mysql DB all the xmls provided by Luigi.\n\nEach xml file contains the meters readings of a whole day, here's the schema, in which the data to be inserted are marked in red:\n![](xml_sesftp1_daily.png)\n\nI wrote a python script that scan every xml and import into a twin table __`smarth2o`.`meter_reading_reimported`__ all the readings found. In case of duplicate reading, an update is executed in order to store the most recent reading.\n\nThe script finally imported 1846 xml files, rejected 10 invalid xml files, and processed a total of 11558687 records.\n\nTake into account that the data were inserted __as is__, without any kind of preprocess.\n## First comparison of readings table imported against the production one\nThe production database has been dumped to a local instance for performance reason.\nLet see the first comparison result:->from sqlalchemy import create_engine\nimport pandas as pd\nengine = create_engine('mysql+pymysql://python:python@localhost:3306/smarth2o')\ndf = pd.read_sql_query(\"\"\"\n SELECT 'xml' as source ,min(a.reading_date_time) as min_date_reading,max(a.reading_date_time) as max_date_reading ,count(*) as count FROM smarth2o.meter_reading_reimported a\n union all\n SELECT 'production' as source ,min(a.reading_date_time) as min_date_reading,max(a.reading_date_time) as max_date_reading,count(*) as count FROM smarth2o.meter_reading a \"\"\",engine)\ndf",
        "pred": [
            "statsmodels->Examples->State space models->Dynamic Factor Models: Application->Macroeconomic data->from pandas_datareader.data import DataReader\n\n# Get the datasets from FRED\nstart = '1979-01-01'\nend = '2014-12-01'\nindprod = DataReader('IPMAN', 'fred', start=start, end=end)\nincome = DataReader('W875RX1', 'fred', start=start, end=end)\nsales = DataReader('CMRMTSPL', 'fred', start=start, end=end)\nemp = DataReader('PAYEMS', 'fred', start=start, end=end)\n# dta = pd.concat((indprod, income, sales, emp), axis=1)\n# dta.columns = ['indprod', 'income', 'sales', 'emp']",
            "matplotlib->Tutorials->Text->Annotations->Coordinate systems for annotations->from matplotlib.text import OffsetFrom\n\nfig, ax = plt.subplots(figsize=(3, 3))\nan1 = ax.annotate(\"Test 1\", xy=(0.5, 0.5), xycoords=\"data\",\n                  va=\"center\", ha=\"center\",\n                  bbox=dict(boxstyle=\"round\", fc=\"w\"))\n\noffset_from = OffsetFrom(an1, (0.5, 0))\nan2 = ax.annotate(\"Test 2\", xy=(0.1, 0.1), xycoords=\"data\",\n                  xytext=(0, -10), textcoords=offset_from,\n                  # xytext is offset points from \"xy=(0.5, 0), xycoords=an1\"\n                  va=\"top\", ha=\"center\",\n                  bbox=dict(boxstyle=\"round\", fc=\"w\"),\n                  arrowprops=dict(arrowstyle=\"-\"))\n\n\n<img alt=\"annotations\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_annotations_021.png\" srcset=\"../../_images/sphx_glr_annotations_021.png, ../../_images/sphx_glr_annotations_021_2_0x.png 2.0x\"/>",
            "matplotlib->Tutorials->Text->Text rendering with XeLaTeX/LuaLaTeX via the ``pgf`` backend->Multi-Page PDF Files->from matplotlib.backends.backend_pgf import PdfPages\nimport matplotlib.pyplot as plt\n\nwith PdfPages('multipage.pdf', metadata={'author': 'Me'}) as pdf:\n\n    fig1, ax1 = plt.subplots()\n    ax1.plot([1, 5, 3])\n    pdf.savefig(fig1)\n\n    fig2, ax2 = plt.subplots()\n    ax2.plot([1, 5, 3])\n    pdf.savefig(fig2)",
            "pandas_toms_blog->Scaling->from pathlib import Path\n\nimport pandas as pd\nimport seaborn as sns\n\ndf = pd.read_parquet(\"data/indiv-10.parq\", columns=['occupation'], engine='pyarrow')\n\nmost_common = df.occupation.value_counts().nlargest(100)\nmost_common",
            "torch->Extending PyTorch->Extending dispatcher for a new backend in C++->Build an extension->from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension\n\nsetup(\n    name='torch_xla',\n    ext_modules=[\n        CppExtension(\n            '_XLAC',\n            torch_xla_sources,\n            include_dirs=include_dirs,\n            extra_compile_args=extra_compile_args,\n            library_dirs=library_dirs,\n            extra_link_args=extra_link_args + \\\n                [make_relative_rpath('torch_xla/lib')],\n        ),\n    ],\n    cmdclass={\n        'build_ext': Build,  # Build is a derived class of BuildExtension\n    }\n    # more configs...\n)"
        ]
    },
    {
        "id": "1223118",
        "GT": "2. Loading google model and defining functions to convert news to vectors with its help->#edit log formatting\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n    level=logging.INFO)\n\n\nfrom gensim.models import Word2Vec",
        "pred": [
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing-># filter the warning for now on\nimport warnings\nwarnings.simplefilter(\"ignore\", DeprecationWarning)",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 0. Prerequisites-># Imports\nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\n\nplt.ion()",
            "sklearn->Examples->Nearest Neighbors->Approximate nearest neighbors in TSNE->import sys\n\ntry:\n    import nmslib\nexcept ImportError:\n    print(\"The package 'nmslib' is required to run this example.\")\n    ()\n\ntry:\n    from pynndescent import PyNNDescentTransformer\nexcept ImportError:\n    print(\"The package 'pynndescent' is required to run this example.\")\n    ()",
            "sklearn->Examples->Semi Supervised Classification->Label Propagation digits: Demonstrating performance->Data generation->from sklearn import datasets\nimport numpy as np\n\ndigits = ()\nrng = (2)\nindices = (len(digits.data))\nrng.shuffle(indices)"
        ]
    },
    {
        "id": "934727",
        "GT": "1. Linear Regression\n\nRegression is a method used in machine learning to predict __continuous__ output. That is, we are predicting a value that can have any possible value, as opposed to discrete values.\n\nWe will begin the workshop by looking at the simple linear regression algorithm to fit a straight line onto a dataset. \n\n## The Investigation\n\nThe dataset we will look at is collected from the following source: \n\nR.J. Gladstone (1905). \"*A Study of the Relations of the Brain to to the Size of the Head*\", Biometrika, Vol. 4, pp105-123\n\nHere, we will investigate the relationship between brain weight (in grams) to head size (cubic cm) for 237 adults classified by gender and age group.->Seperating the Data\n\nHere, we split our dataset into three sections: \n1. Training data\n    * What we train our algorithm on\n2. Testing data\n    * What we use to evaluate the performance of our algorithm, after training\n    \nWe then visualize this split dataset to get a better intuition into the spread of our data.->from sklearn.cross_validation import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.33, \n                                                    random_state=123)",
        "pred": [
            "sklearn->Examples->Miscellaneous->Comparison of kernel ridge regression and SVR->Visualize the learning curves->from sklearn.model_selection import LearningCurveDisplay\n\n_, ax = ()\n\nsvr = (kernel=\"rbf\", C=1e1, gamma=0.1)\nkr = (kernel=\"rbf\", alpha=0.1, gamma=0.1)\n\ncommon_params = {\n    \"X\": X[:100],\n    \"y\": y[:100],\n    \"train_sizes\": (0.1, 1, 10),\n    \"scoring\": \"neg_mean_squared_error\",\n    \"negate_score\": True,\n    \"score_name\": \"Mean Squared Error\",\n    \"std_display_style\": None,\n    \"ax\": ax,\n}\n\n(svr, **common_params)\n(kr, **common_params)\nax.set_title(\"Learning curves\")\nax.legend(handles=ax.get_legend_handles_labels()[0], labels=[\"SVR\", \"KRR\"])\n\n()\n\n\n<img alt=\"Learning curves\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\" srcset=\"../../_images/sphx_glr_plot_kernel_ridge_regression_003.png\"/>",
            "statsmodels->Examples->Statistics->ANOVA->from statsmodels.stats.api import anova_lm\n\ntable1 = anova_lm(lm, interX_lm)\nprint(table1)\n\ninterM_lm = ols(\"S ~ X + C(E)*C(M)\", data=salary_table).fit()\nprint(interM_lm.summary())\n\ntable2 = anova_lm(lm, interM_lm)\nprint(table2)",
            "sklearn->Examples->Feature Selection->Recursive feature elimination with cross-validation->Data generation->from sklearn.datasets import \n\nX, y = (\n    n_samples=500,\n    n_features=15,\n    n_informative=3,\n    n_redundant=2,\n    n_repeated=0,\n    n_classes=8,\n    n_clusters_per_class=1,\n    class_sep=0.8,\n    random_state=0,\n)",
            "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Model comparison->from sklearn.model_selection import \nimport matplotlib.pyplot as plt\n\nscoring = \"neg_mean_absolute_percentage_error\"\nn_cv_folds = 3\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\n\ndef plot_results(figure_title):\n    fig, (ax1, ax2) = (1, 2, figsize=(12, 8))\n\n    plot_info = [\n        (\"fit_time\", \"Fit times (s)\", ax1, None),\n        (\"test_score\", \"Mean Absolute Percentage Error\", ax2, None),\n    ]\n\n    x, width = (4), 0.9\n    for key, title, ax, y_limit in plot_info:\n        items = [\n            dropped_result[key],\n            one_hot_result[key],\n            ordinal_result[key],\n            native_result[key],\n        ]\n\n        mape_cv_mean = [(np.abs(item)) for item in items]\n        mape_cv_std = [(item) for item in items]\n\n        ax.bar(\n            x=x,\n            height=mape_cv_mean,\n            width=width,\n            yerr=mape_cv_std,\n            color=[\"C0\", \"C1\", \"C2\", \"C3\"],\n        )\n        ax.set(\n            xlabel=\"Model\",\n            title=title,\n            xticks=x,\n            xticklabels=[\"Dropped\", \"One Hot\", \"Ordinal\", \"Native\"],\n            ylim=y_limit,\n        )\n    fig.suptitle(figure_title)\n\n\nplot_results(\"Gradient Boosting on Ames Housing\")\n\n\n<img alt=\"Gradient Boosting on Ames Housing, Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\"/>",
            "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.2->New and enhanced displays->from sklearn.inspection import PartialDependenceDisplay\n\nfig, ax = (figsize=(14, 4), constrained_layout=True)\n_ = (\n    model,\n    X,\n    features=[\"age\", \"sex\", (\"pclass\", \"sex\")],\n    categorical_features=categorical_features,\n    ax=ax,\n)\n\n\n<img alt=\"plot release highlights 1 2 0\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_1_2_0_003.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_1_2_0_003.png\"/>"
        ]
    },
    {
        "id": "858120",
        "GT": "Plotting the decision boundary->x1 = np.linspace(start = chip_data[\"Microchip Test 1\"].min(), \n                 stop = chip_data[\"Microchip Test 1\"].max(), num = 100)\nx2 = np.linspace(start = chip_data[\"Microchip Test 2\"].min(), \n                 stop = chip_data[\"Microchip Test 2\"].max(), num = 100)",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals->states1 = pd.DataFrame(\n    np.c_[fit1.level, fit1.trend, fit1.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nstates2 = pd.DataFrame(\n    np.c_[fit2.level, fit2.trend, fit2.season],\n    columns=[\"level\", \"slope\", \"seasonal\"],\n    index=aust.index,\n)\nfig, [[ax1, ax4], [ax2, ax5], [ax3, ax6]] = plt.subplots(3, 2, figsize=(12, 8))\nstates1[[\"level\"]].plot(ax=ax1)\nstates1[[\"slope\"]].plot(ax=ax2)\nstates1[[\"seasonal\"]].plot(ax=ax3)\nstates2[[\"level\"]].plot(ax=ax4)\nstates2[[\"slope\"]].plot(ax=ax5)\nstates2[[\"seasonal\"]].plot(ax=ax6)\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_22_0.png\"/>",
            "numpy->NumPy Applications->X-ray image processing->Edge detection using the Laplacian-Gaussian, Gaussian gradient, Sobel, and Canny filters->The Sobel-Feldman operator (the Sobel filter)->x_sobel = ndimage.sobel(xray_image, axis=0)\ny_sobel = ndimage.sobel(xray_image, axis=1)\n\nxray_image_sobel = np.hypot(x_sobel, y_sobel)\n\nxray_image_sobel *= 255.0 / np.max(xray_image_sobel)",
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->train_importances = (\n    train_result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)\ntest_importances = (\n    test_results.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx],\n)",
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Winters Seasonal->The Internals->fit1 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"add\",\n    initialization_method=\"estimated\",\n).fit()\nfit2 = ExponentialSmoothing(\n    aust,\n    seasonal_periods=4,\n    trend=\"add\",\n    seasonal=\"mul\",\n    initialization_method=\"estimated\",\n).fit()",
            "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Accuracy vs alpha for training and testing sets->train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = ()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\n()\n\n\n<img alt=\"Accuracy vs alpha for training and testing sets\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\" srcset=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\"/>"
        ]
    },
    {
        "id": "1218605",
        "GT": "Manipulates the Data To Better Fit Prophet's Expectations->dataframe = dataframe.reset_index()\ndataframe['y'] = np.log(dataframe['y'])\ndataframe.head()",
        "pred": [
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->X = survey.data[survey.feature_names]\nX.describe(include=\"all\")\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "pandas_toms_blog->Indexes->Merging->The merge version->m = pd.merge(flights, daily.reset_index().rename(columns={'date': 'fl_date', 'station': 'origin'}),\n             on=['fl_date', 'origin']).set_index(idx_cols).sort_index()\n\nm.head()",
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing wide-form data->two_arrays_dict = {s.name: s.to_numpy() for s in two_series}\nsns.relplot(data=two_arrays_dict, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing wide-form data->two_arrays_dict = {s.name: s.to_numpy() for s in two_series}\nsns.relplot(data=two_arrays_dict, kind=\"line\")\n",
            "seaborn->User guide and tutorial->Statistical estimation and error bars->Error bars on regression fits->x = np.random.normal(0, 1, 50)\ny = x * 2 + np.random.normal(0, 2, size=x.size)\nsns.regplot(x=x, y=y)\n",
            "seaborn->Statistical operations->Statistical estimation and error bars->Error bars on regression fits->x = np.random.normal(0, 1, 50)\ny = x * 2 + np.random.normal(0, 2, size=x.size)\nsns.regplot(x=x, y=y)\n"
        ]
    },
    {
        "id": "1469607",
        "GT": "data exploration->statistic->import matplotlib.pyplot as plt\nplt.hist(train['is_iceberg'])",
        "pred": [
            "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Using style sheets->Composing styles->import matplotlib.pyplot as plt\n plt.style.use(['dark_background', 'presentation'])",
            "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Using style sheets->Distributing styles->import matplotlib.pyplot as plt\n plt.style.use(&lt;style-name)",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit",
            "matplotlib->Tutorials->Introductory->Animations using Matplotlib->import matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np",
            "matplotlib->Tutorials->Introductory->Customizing Matplotlib with style sheets and rcParams->Using style sheets->Defining your own style->import matplotlib.pyplot as plt\n plt.style.use('./images/presentation.mplstyle')"
        ]
    },
    {
        "id": "1415295",
        "GT": "Lasso & Reggression #->Train ##->model_lasso = Lasso(alpha=5e-4, max_iter=1150000).fit(X_train, y)",
        "pred": [
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->model.fit(X_train, y_train)",
            "torch->Model Optimization->Pruning Tutorial->Extending torch.nn.utils.prune with custom pruning functions->model = ()\nfoobar_unstructured(, name='bias')\n\nprint()",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resfd2_logit.predict(data_student.iloc[:5])",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - treatment of endog->resf_logit.predict(data_student.iloc[:5])",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes->mod = sm.tsa.SARIMAX(endog4)\nres = mod.fit()"
        ]
    },
    {
        "id": "1134174",
        "GT": "Column Descriptions\n\n'HHKEY' - Customer ID: unique, encrypted customer identification\n\n'ZIP_CODE' - Zip code\n\n'REC' - a variable showing the brand of choice (encrypted)\n\n'FRE' - Number of purchase visits\n\n'MON' - Total net sales\n\n'CC_CARD' - Flag: credit card user\n\n'AVRG' - Average amount spent per visit\n\n'PC_CALC20',\n\n'PSWEATERS' - percentages spent by the customer on Sweaters\n\n'PKNIT_TOPS' - percentages spent by the customer on knit tops\n\n'PKNIT_DRES' - percentages spent by the customer on knit dresses\n\n'PBLOUSES' - percentages spent by the customer on blouses\n\n'PJACKETS' - percentages spent by the customer on jackets\n\n'PCAR_PNTS' - percentages spent by the customer on career pants\n\n'PCAS_PNTS' - percentages spent by the customer on casual pants\n\n'PSHIRTS' - percentages spent by the customer on shirts\n\n'PDRESSES' - percentages spent by the customer on dresses\n\n'PSUITS' - percentages spent by the customer on suits\n\n'POUTERWEAR' - percentages spent by the customer on outerwear\n\n'PJEWELRY' - percentages spent by the customer on jewelry\n\n'PFASHION' - percentages spent by the customer on fashion\n\n'PLEGWEAR' - percentages spent by the customer on legwear\n\n'PCOLLSPND' - percentages spent by the customer on collectibles line\n\n'AMSPEND' - Spending at the AM store\n\n'PSSPEND' - Spending at the PS store\n\n'CCSPEND' - Spending at the CC store\n\n'AXSPEND' - Spending at the AX store\n\n'TMONSPEND' - Amount spent in the past three months\n\n'OMONSPEND' - Amount spent in the past month\n\n'SMONSPEND' - Amount spent in the past six months\n\n'PREVPD',\n\n'GMP' - Gross margin percentage\n\n'PROMOS' - Number of marketing promotions on file\n\n'DAYS' - Number of days the customer has been on file\n\n'FREDAYS' - Number of days between purchases\n\n'MARKDOWN' - Markdown percentage on customer purchases - indicates which customers have purchased merchandise that has been marked down\n\n'CLASSES' - Number of different product classes purchased\n\n'COUPONS' - Number of coupons used by the customer\n\n'STYLES' - Total number of individual items purchased by the customer\n\n'STORES' - Number of stores the customer shopped at\n\n'STORELOY' - Spending in the same period last year (?)\n\n'VALPHON' - Flag: valid phone number on file\n\n'WEB' - Flag: Web shopper\n\n'MAILED' - Number of promotions mailed in the past year\n\n'RESPONDED' - Number of promotions responded to in the past year\n\n'RESPONSERATE' - Promotion response rate for the past year - indicates which customers have ever responded to a marketing promotion before\n\n'HI' - Product uniformity (low score = diverse spending patterns)\n\n'LTFREDAY' - Lifetime average time between visits\n\n'CLUSTYPE' - Microvision lifestyle cluster type\n\n'PERCRET' - Percent of returns\n\n'RESP' - Target variable: response to promotion-># Count how many people responded to the promotion\nprint(clothingFile['RESP'] != 0).sum()",
        "pred": [
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data-># For each team... get number of days between games\ntidy.groupby('team')['date'].diff().dt.days - 1",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Logit ordinal regression:->pred_choice = predicted.argmax(1)\nprint('Fraction of correct choice predictions')\nprint((np.asarray(data_student['apply'].values.codes) == pred_choice).mean())",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "statsmodels->Examples->State space models - Technical notes->State space models: concentrating out the scale->Example: local level model->Typical approach->mod = LocalLevel(dta.infl)\nres = mod.fit(disp=False)\nprint(res.summary())"
        ]
    },
    {
        "id": "1116941",
        "GT": "Effect of Distance Function->color_spec = {'var' : 'g', 'vartas' : '#ee55aa', 'varrotated' : '#55aaee', 'varclosest' : '#aa55ee'}\ncontext_data = all_data[all_data.k_type.isin(color_spec.keys())]\nwith sns.axes_style('darkgrid'):\n    g = sns.FacetGrid(context_data,col=\"pooltype\", size=5,hue=\"k_type\",palette=color_spec, sharey=True)\n    g.map(sns.pointplot, \"num_clusters\", \"purities\")\n    g.add_legend()\n    plt.savefig('purities.png')",
        "pred": [
            "statsmodels->Examples->Statistics->ANOVA->resid = lm.resid\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    group_num = i * 2 + j - 1  # for plotting purposes\n    x = [group_num] * len(group)\n    plt.scatter(\n        x,\n        resid[group.index],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"Group\")\nplt.ylabel(\"Residuals\")",
            "sklearn->Examples->Generalized Linear Models->Quantile regression->Fitting a QuantileRegressor->quantiles = [0.05, 0.5, 0.95]\npredictions = {}\nout_bounds_predictions = (y_true_mean, dtype=)\nfor quantile in quantiles:\n    qr = (quantile=quantile, alpha=0, solver=solver)\n    y_pred = qr.fit(X, y_pareto).predict(X)\n    predictions[quantile] = y_pred\n\n    if quantile == min(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred = y_pareto\n        )\n    elif quantile == max(quantiles):\n        out_bounds_predictions = (\n            out_bounds_predictions, y_pred &lt;= y_pareto\n        )",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with keyword strings->data = {'a': np.arange(50),\n        'c': np.random.randint(0, 50, 50),\n        'd': np.random.randn(50)}\ndata['b'] = data['a'] + 10 * np.random.randn(50)\ndata['d'] = np.abs(data['d']) * 100\n\nplt.scatter('a', 'b', c='c', s='d', data=data)\nplt.xlabel('entry a')\nplt.ylabel('entry b')\nplt.show()\n\n\n<img alt=\"pyplot\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_005.png\" srcset=\"../../_images/sphx_glr_pyplot_005.png, ../../_images/sphx_glr_pyplot_005_2_0x.png 2.0x\"/>",
            "pandas_toms_blog->Indexes->stations = pd.io.json.json_normalize(js['features']).id\nurl = (\"http://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n       \"&amp;data=tmpf&amp;data=relh&amp;data=sped&amp;data=mslp&amp;data=p01i&amp;data=vsby&amp;data=gust_mph&amp;data=skyc1&amp;data=skyc2&amp;data=skyc3\"\n       \"&amp;tz=Etc/UTC&amp;format=comma&amp;latlon=no\"\n       \"&amp;{start:year1=%Y&amp;month1=%m&amp;day1=%d}\"\n       \"&amp;{end:year2=%Y&amp;month2=%m&amp;day2=%d}&amp;{stations}\")\nstations = \"&amp;\".join(\"station=%s\" % s for s in stations)\nstart = pd.Timestamp('2014-01-01')\nend=pd.Timestamp('2014-01-31')\n\nweather = (pd.read_csv(url.format(start=start, end=end, stations=stations),\n                       comment=\"#\"))",
            "statsmodels->Examples->State space models->Seasonality in Time Series Data->Comparison of filtered estimates->time_s = np.s_[:50]  # After this they basically agree\nfig1 = plt.figure()\nax1 = fig1.add_subplot(111)\nidx = np.asarray(series.index)\nh1, = ax1.plot(idx[time_s], res_f.freq_seasonal[0].filtered[time_s], label='Double Freq. Seas')\nh2, = ax1.plot(idx[time_s], res_tf.seasonal.filtered[time_s], label='Mixed Domain Seas')\nh3, = ax1.plot(idx[time_s], true_seasonal_10_3[time_s], label='True Seasonal 10(3)')\nplt.legend([h1, h2, h3], ['Double Freq. Seasonal','Mixed Domain Seasonal','Truth'], loc=2)\nplt.title('Seasonal 10(3) component')\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_seasonal_21_0.png\"/>"
        ]
    },
    {
        "id": "526388",
        "GT": "< Data Wrangle >->1. Exploring Survivors with Each Ticket Class->##Import pandas, numpy, matplotlib, seaborn, patches\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.patches as mpatches",
        "pred": [
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets-># Importing the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pooch\nimport string\nimport re\nimport zipfile\nimport os\n\n# Creating the random instance\nrng = np.random.default_rng()",
            "numpy->NumPy Features->Masked Arrays->Exploring the data->import matplotlib.pyplot as plt\n\nselected_dates = [0, 3, 11, 13]\nplt.plot(dates, nbcases.T, \"--\")\nplt.xticks(selected_dates, dates[selected_dates])\nplt.title(\"COVID-19 cumulative cases from Jan 21 to Feb 3 2020\")",
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)",
            "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "torch->Model Optimization->(beta) Dynamic Quantization on an LSTM Word Language Model->4. Test dynamic quantization->import torch.quantization\n\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {, }, dtype=\n)\nprint(quantized_model)"
        ]
    },
    {
        "id": "432589",
        "GT": "Implementing the Minibatch Gradient Descent->def sgd(model, X_train, y_train, minibatch_size):\n    for iter in range(n_iter):\n        print('Iteration {}'.format(iter))\n\n        # Randomize data point\n        X_train, y_train = shuffle(X_train, y_train)\n\n        for i in range(0, X_train.shape[0], minibatch_size):\n            # Get pair of (X, y) of the current minibatch/chunk\n            X_train_mini = X_train[i:i + minibatch_size]\n            y_train_mini = y_train[i:i + minibatch_size]\n\n            model = sgd_step(model, X_train_mini, y_train_mini)\n\n    return model",
        "pred": [
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Training the model->def train_model(model, , optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for , labels in dataloaders[phase]:\n                 = .to()\n                labels = labels.to()\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with (phase == 'train'):\n                    outputs = model()\n                    _, preds = (outputs, 1)\n                    loss = (outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * .size(0)\n                running_corrects += (preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:4f}')\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model",
            "torch->Parallel and Distributed Training->Getting Started with Fully Sharded Data Parallel(FSDP)->How to use FSDP->def train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n    model.train()\n    ddp_loss = torch.zeros(2).to(rank)\n    if sampler:\n        sampler.set_epoch(epoch)\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(rank), target.to(rank)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target, reduction='sum')\n        loss.backward()\n        optimizer.step()\n        ddp_loss[0] += loss.item()\n        ddp_loss[1] += len(data)\n\n    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n    if rank == 0:\n        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, ddp_loss[0] / ddp_loss[1]))",
            "sklearn->Examples->Model Selection->Class Likelihood Ratios to measure classification performance->Invariance with respect to prevalence->def scoring_on_bootstrap(estimator, X, y, rng, n_bootstrap=100):\n    results_for_prevalence = (list)\n    for _ in range(n_bootstrap):\n        bootstrap_indices = rng.choice(\n            (X.shape[0]), size=X.shape[0], replace=True\n        )\n        for key, value in scoring(\n            estimator, X[bootstrap_indices], y[bootstrap_indices]\n        ).items():\n            results_for_prevalence[key].append(value)\n    return (results_for_prevalence)",
            "sklearn->Examples->Clustering->Adjustment for chance in clustering performance evaluation->Second experiment: varying number of classes and clusters->def uniform_labelings_scores(score_func, n_samples, n_clusters_range, n_runs=5):\n    scores = ((len(n_clusters_range), n_runs))\n\n    for i, n_clusters in enumerate(n_clusters_range):\n        for j in range(n_runs):\n            labels_a = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            labels_b = random_labels(n_samples=n_samples, n_classes=n_clusters)\n            scores[i, j] = score_func(labels_a, labels_b)\n    return scores",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->def plot_weights(support, weights_func, xlabels, xticks):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(support, weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_ylim(-0.1, 1.1)\n    return ax"
        ]
    },
    {
        "id": "104880",
        "GT": "m4=train['KitchenQual'].mode()[0]\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(m4)\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(train['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(train['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(train['SaleType'].mode()[0])",
        "pred": [
            "scipy->Sparse eigenvalue problems with ARPACK->Examples->evals_small, evecs_small = eigsh(X, 3, which='SM', tol=1E-2)\n evals_all[:3]\narray([0.00053181, 0.00298319, 0.01387821])\n evals_small\narray([0.00053181, 0.00298319, 0.01387821])\n np.dot(evecs_small.T, evecs_all[:,:3])\narray([[ 0.99999999  0.00000024 -0.00000049],    # may vary (signs)\n       [-0.00000023  0.99999999  0.00000056],\n       [ 0.00000031 -0.00000037  0.99999852]])",
            "scipy->Sparse eigenvalue problems with ARPACK->Examples->evals_small, evecs_small = eigsh(X, 3, sigma=0, which='LM')\n evals_all[:3]\narray([0.00053181, 0.00298319, 0.01387821])\n evals_small\narray([0.00053181, 0.00298319, 0.01387821])\n np.dot(evecs_small.T, evecs_all[:,:3])\narray([[ 1.  0.  0.],    # may vary (signs)\n       [ 0. -1. -0.],\n       [-0. -0.  1.]])",
            "scipy->Multidimensional image processing (scipy.ndimage)->Filter functions->Generic filter functions->a = np.arange(12).reshape(3,4)\n\n class fnc1d_class:\n...     def __init__(self, shape, axis = -1):\n...         # store the filter axis:\n...         self.axis = axis\n...         # store the shape:\n...         self.shape = shape\n...         # initialize the coordinates:\n...         self.coordinates = [0] * len(shape)\n...\n...     def filter(self, iline, oline):\n...         oline[...] = iline[:-2] + 2 * iline[1:-1] + 3 * iline[2:]\n...         print(self.coordinates)\n...         # calculate the next coordinates:\n...         axes = list(range(len(self.shape)))\n...         # skip the filter axis:\n...         del axes[self.axis]\n...         axes.reverse()\n...         for jj in axes:\n...             if self.coordinates[jj] &lt; self.shape[jj] - 1:\n...                 self.coordinates[jj] += 1\n...                 break\n...             else:\n...                 self.coordinates[jj] = 0\n...\n fnc = fnc1d_class(shape = (3,4))\n generic_filter1d(a, fnc.filter, 3)\n[0, 0]\n[1, 0]\n[2, 0]\narray([[ 3,  8, 14, 17],\n       [27, 32, 38, 41],\n       [51, 56, 62, 65]])",
            "statsmodels->Examples->State space models->VARMAX: Introduction->Example 1: VAR->exog = endog['dln_consump']\nmod = sm.tsa.VARMAX(endog[['dln_inv', 'dln_inc']], order=(2,0), trend='n', exog=exog)\nres = mod.fit(maxiter=1000, disp=False)\nprint(res.summary())",
            "scipy->Statistics (scipy.stats)->Analysing one sample->Tails of the distribution->quantiles = [0.0, 0.01, 0.05, 0.1, 1-0.10, 1-0.05, 1-0.01, 1.0]\n crit = stats.t.ppf(quantiles, 10)\n crit\narray([       -inf, -2.76376946, -1.81246112, -1.37218364,  1.37218364,\n        1.81246112,  2.76376946,         inf])\n n_sample = x.size\n freqcount = np.histogram(x, bins=crit)[0]\n tprob = np.diff(quantiles)\n nprob = np.diff(stats.norm.cdf(crit))\n tch, tpval = stats.chisquare(freqcount, tprob*n_sample)\n nch, npval = stats.chisquare(freqcount, nprob*n_sample)\n print('chisquare for t:      chi2 = %6.2f pvalue = %6.4f' % (tch, tpval))\nchisquare for t:      chi2 =  2.30 pvalue = 0.8901  # random\n print('chisquare for normal: chi2 = %6.2f pvalue = %6.4f' % (nch, npval))\nchisquare for normal: chi2 = 64.60 pvalue = 0.0000  # random"
        ]
    },
    {
        "id": "1298185",
        "GT": "Overview ##\n\nThis is a starter notebook inspired by last year's [Logistic Regression on Tournament Seeds by Kasper P. Lauritzen](https://www.kaggle.com/kplauritzen/notebookde27b18258?scriptVersionId=804590) starter kernel. It creates a basic logistic regression model based on the seed differences between teams. \n\nNote that the predictions for Stage 1's sample submissions file are already based on known outcomes, and the Tourney data this model is trained on includes that data. For Stage 2, you will be predicting future outcomes based on the teams selected for the tournament on March 12.-># This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import GridSearchCV\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n",
        "pred": [
            "sklearn->Examples->Classification->Classifier comparison-># Code source: Ga\u00ebl Varoquaux\n#              Andreas M\u00fcller\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import \nfrom sklearn.model_selection import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.datasets import , , \nfrom sklearn.neural_network import \nfrom sklearn.neighbors import \nfrom sklearn.svm import \nfrom sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import \nfrom sklearn.tree import \nfrom sklearn.ensemble import , \nfrom sklearn.naive_bayes import \nfrom sklearn.discriminant_analysis import \nfrom sklearn.inspection import DecisionBoundaryDisplay\n\nnames = [\n    \"Nearest Neighbors\",\n    \"Linear SVM\",\n    \"RBF SVM\",\n    \"Gaussian Process\",\n    \"Decision Tree\",\n    \"Random Forest\",\n    \"Neural Net\",\n    \"AdaBoost\",\n    \"Naive Bayes\",\n    \"QDA\",\n]\n\nclassifiers = [\n    (3),\n    (kernel=\"linear\", C=0.025),\n    (gamma=2, C=1),\n    (1.0 * (1.0)),\n    (max_depth=5),\n    (max_depth=5, n_estimators=10, max_features=1),\n    (alpha=1, max_iter=1000),\n    (),\n    (),\n    (),\n]\n\nX, y = (\n    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n)\nrng = (2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [\n    (noise=0.3, random_state=0),\n    (noise=0.2, factor=0.5, random_state=1),\n    linearly_separable,\n]\n\nfigure = (figsize=(27, 9))\ni = 1\n# iterate over datasets\nfor ds_cnt, ds in enumerate(datasets):\n    # preprocess dataset, split into training and test part\n    X, y = ds\n    X_train, X_test, y_train, y_test = (\n        X, y, test_size=0.4, random_state=42\n    )\n\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n\n    # just plot the dataset first\n    cm = plt.cm.RdBu\n    cm_bright = ([\"#FF0000\", \"#0000FF\"])\n    ax = (len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n    # Plot the training points\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\")\n    # Plot the testing points\n    ax.scatter(\n        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n    )\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = (len(datasets), len(classifiers) + 1, i)\n\n        clf = ((), clf)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n        (\n            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n        )\n\n        # Plot the training points\n        ax.scatter(\n            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n        )\n        # Plot the testing points\n        ax.scatter(\n            X_test[:, 0],\n            X_test[:, 1],\n            c=y_test,\n            cmap=cm_bright,\n            edgecolors=\"k\",\n            alpha=0.6,\n        )\n\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(y_min, y_max)\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n        ax.text(\n            x_max - 0.3,\n            y_min + 0.3,\n            (\"%.2f\" % score).lstrip(\"0\"),\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        i += 1\n\n()\n()",
            "sklearn->Examples->Generalized Linear Models->Logistic Regression 3-class Classifier-># Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import \nfrom sklearn import datasets\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n# import some data to play with\niris = ()\nX = iris.data[:, :2]  # we only take the first two features.\nY = iris.target\n\n# Create an instance of Logistic Regression Classifier and fit the data.\nlogreg = (C=1e5)\nlogreg.fit(X, Y)\n\n_, ax = (figsize=(4, 3))\n(\n    logreg,\n    X,\n    cmap=plt.cm.Paired,\n    ax=ax,\n    response_method=\"predict\",\n    plot_method=\"pcolormesh\",\n    shading=\"auto\",\n    xlabel=\"Sepal length\",\n    ylabel=\"Sepal width\",\n    eps=0.5,\n)\n\n# Plot also the training points\n(X[:, 0], X[:, 1], c=Y, edgecolors=\"k\", cmap=plt.cm.Paired)\n\n\n(())\n(())\n\n()",
            "sklearn->Examples->Gaussian Process for Machine Learning->Iso-probability lines for Gaussian Processes classification (GPC)-># Author: Vincent Dubourg &lt;vincent.dubourg@gmail.com\n# Adapted to GaussianProcessClassifier:\n#         Jan Hendrik Metzen &lt;jhm@informatik.uni-bremen.de\n# License: BSD 3 clause\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm\n\nfrom sklearn.gaussian_process import \nfrom sklearn.gaussian_process.kernels import , ConstantKernel as \n\n# A few constants\nlim = 8\n\n\ndef g(x):\n    \"\"\"The function to predict (classification will then consist in predicting\n    whether g(x) &lt;= 0 or not)\"\"\"\n    return 5.0 - x[:, 1] - 0.5 * x[:, 0] ** 2.0\n\n\n# Design of experiments\nX = (\n    [\n        [-4.61611719, -6.00099547],\n        [4.10469096, 5.32782448],\n        [0.00000000, -0.50000000],\n        [-6.17289014, -4.6984743],\n        [1.3109306, -6.93271427],\n        [-5.03823144, 3.10584743],\n        [-2.87600388, 6.74310541],\n        [5.21301203, 4.26386883],\n    ]\n)\n\n# Observations\ny = (g(X)  0, dtype=int)\n\n# Instantiate and fit Gaussian Process Model\nkernel = (0.1, (1e-5, )) * (sigma_0=0.1) ** 2\ngp = (kernel=kernel)\ngp.fit(X, y)\nprint(\"Learned kernel: %s \" % gp.kernel_)\n\n# Evaluate real function and the predicted probability\nres = 50\nx1, x2 = ((-lim, lim, res), (-lim, lim, res))\nxx = ([x1.reshape(x1.size), x2.reshape(x2.size)]).T\n\ny_true = g(xx)\ny_prob = gp.predict_proba(xx)[:, 1]\ny_true = y_true.reshape((res, res))\ny_prob = y_prob.reshape((res, res))\n\n# Plot the probabilistic classification iso-values\nfig = (1)\nax = fig.gca()\nax.axes.set_aspect(\"equal\")\n([])\n([])\nax.set_xticklabels([])\nax.set_yticklabels([])\n(\"$x_1$\")\n(\"$x_2$\")\n\ncax = (y_prob, cmap=cm.gray_r, alpha=0.8, extent=(-lim, lim, -lim, lim))\nnorm = plt.matplotlib.colors.Normalize(vmin=0.0, vmax=0.9)\ncb = (cax, ticks=[0.0, 0.2, 0.4, 0.6, 0.8, 1.0], norm=norm)\ncb.set_label(r\"${\\rm \\mathbb{P}}\\left[\\widehat{G}(\\mathbf{x}) \\leq 0\\right]$\")\n(0, 1)\n\n(X[y &lt;= 0, 0], X[y &lt;= 0, 1], \"r.\", markersize=12)\n\n(X[y  0, 0], X[y  0, 1], \"b.\", markersize=12)\n\n(x1, x2, y_true, [0.0], colors=\"k\", linestyles=\"dashdot\")\n\ncs = (x1, x2, y_prob, [0.666], colors=\"b\", linestyles=\"solid\")\n(cs, fontsize=11)\n\ncs = (x1, x2, y_prob, [0.5], colors=\"k\", linestyles=\"dashed\")\n(cs, fontsize=11)\n\ncs = (x1, x2, y_prob, [0.334], colors=\"r\", linestyles=\"solid\")\n(cs, fontsize=11)\n\n()",
            "sklearn->Examples->Generalized Linear Models->Ordinary Least Squares and Ridge Regression Variance-># Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\n\nX_train = [0.5, 1].T\ny_train = [0.5, 1]\nX_test = [0, 2].T\n\n(0)\n\nclassifiers = dict(\n    ols=(), ridge=(alpha=0.1)\n)\n\nfor name, clf in classifiers.items():\n    fig, ax = (figsize=(4, 3))\n\n    for _ in range(6):\n        this_X = 0.1 * (size=(2, 1)) + X_train\n        clf.fit(this_X, y_train)\n\n        ax.plot(X_test, clf.predict(X_test), color=\"gray\")\n        ax.scatter(this_X, y_train, s=3, c=\"gray\", marker=\"o\", zorder=10)\n\n    clf.fit(X_train, y_train)\n    ax.plot(X_test, clf.predict(X_test), linewidth=2, color=\"blue\")\n    ax.scatter(X_train, y_train, s=30, c=\"red\", marker=\"+\", zorder=10)\n\n    ax.set_title(name)\n    ax.set_xlim(0, 2)\n    ax.set_ylim((0, 1.6))\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"y\")\n\n    fig.tight_layout()\n\n()",
            "sklearn->Examples->Clustering->K-means Clustering-># Code source: Ga\u00ebl Varoquaux\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Though the following import is not directly being used, it is required\n# for 3D projection to work with matplotlib &lt; 3.2\nimport mpl_toolkits.mplot3d  # noqa: F401\n\nfrom sklearn.cluster import \nfrom sklearn import datasets\n\n(5)\n\niris = ()\nX = iris.data\ny = iris.target\n\nestimators = [\n    (\"k_means_iris_8\", (n_clusters=8, n_init=\"auto\")),\n    (\"k_means_iris_3\", (n_clusters=3, n_init=\"auto\")),\n    (\"k_means_iris_bad_init\", (n_clusters=3, n_init=1, init=\"random\")),\n]\n\nfig = (figsize=(10, 8))\ntitles = [\"8 clusters\", \"3 clusters\", \"3 clusters, bad initialization\"]\nfor idx, ((name, est), title) in enumerate(zip(estimators, titles)):\n    ax = fig.add_subplot(2, 2, idx + 1, projection=\"3d\", elev=48, azim=134)\n    est.fit(X)\n    labels = est.labels_\n\n    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(float), edgecolor=\"k\")\n\n    ax.xaxis.set_ticklabels([])\n    ax.yaxis.set_ticklabels([])\n    ax.zaxis.set_ticklabels([])\n    ax.set_xlabel(\"Petal width\")\n    ax.set_ylabel(\"Sepal length\")\n    ax.set_zlabel(\"Petal length\")\n    ax.set_title(title)\n\n# Plot the ground truth\nax = fig.add_subplot(2, 2, 4, projection=\"3d\", elev=48, azim=134)\n\nfor name, label in [(\"Setosa\", 0), (\"Versicolour\", 1), (\"Virginica\", 2)]:\n    ax.text3D(\n        X[y == label, 3].mean(),\n        X[y == label, 0].mean(),\n        X[y == label, 2].mean() + 2,\n        name,\n        horizontalalignment=\"center\",\n        bbox=dict(alpha=0.2, edgecolor=\"w\", facecolor=\"w\"),\n    )\n# Reorder the labels to have colors matching the cluster results\ny = (y, [1, 2, 0]).astype(float)\nax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y, edgecolor=\"k\")\n\nax.xaxis.set_ticklabels([])\nax.yaxis.set_ticklabels([])\nax.zaxis.set_ticklabels([])\nax.set_xlabel(\"Petal width\")\nax.set_ylabel(\"Sepal length\")\nax.set_zlabel(\"Petal length\")\nax.set_title(\"Ground Truth\")\n\n(wspace=0.25, hspace=0.25)\n()"
        ]
    },
    {
        "id": "1447973",
        "GT": "Behavioral cloning project\nIn this project, we will make a car drive on its own in a simulated environment. The simulator is provided by Udacity's Self Driving Car Nano Degree program. \n\n### Goal:\nBuild a model that takes images as \"seen\" by three cameras mounted on the car and predict an appropriate steering anlge to autonomously drive on a simulated track.->import pandas as pd\nimport csv\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom sklearn.utils import shuffle",
        "pred": [
            "statsmodels->Examples->State space models->VARMAX: Introduction->import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "statsmodels->Examples->State space models->Trends and cycles in unemployment->import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt",
            "statsmodels->Examples->State space models->Statespace ARMA: Sunspots Data->import numpy as np\nfrom scipy import stats\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm",
            "statsmodels->Examples->Generalized Linear Models->Quasi-binomial regression->import statsmodels.api as sm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom io import StringIO",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\n\nfrom statsmodels.miscmodels.ordinal_model import OrderedModel",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy import stats\nfrom statsmodels.formula.api import logit"
        ]
    },
    {
        "id": "1350574",
        "GT": "Answering Business Questions using SQL\n\n\n## Setup->Sales Data by Country\n\nThe store is interested in analyzing sales data countries where they have had customers, in order to reveal any interesting trends.-># Writing a query that collates data on purchases from different countries, \n# which includes columns for total number of customers, total value of sales\n# average value of sales per customer, and average order value.  Countries\n# with just one customer are lumped together into an \"Other\" group, and\n# that group is listed last.\n\n\ncountry_sales_query = \"\"\"\n\nWITH \n    total_customers AS (\n        SELECT country nation, COUNT(*) num_of_customers \n        FROM customer \n        GROUP BY country\n     ), \n    total_value AS(\n        SELECT c.country nation, SUM(i.total) total_sales, AVG(i.total) avg_sales\n        FROM invoice i \n        INNER JOIN customer c ON c.customer_id = i.customer_id\n        GROUP BY nation\n      ),\n    multi_customer_countries AS (\n        SELECT \n            tc.nation, \n            tc.num_of_customers, \n            tv.total_sales total_sales_country,\n            CAST(tv.total_sales as float)/CAST(tc.num_of_customers as Float) average_value_per_customer,\n            tv.avg_sales average_order_value \n        FROM total_customers tc\n        INNER join total_value tv ON tv.nation = tc.nation\n        WHERE tc.num_of_customers > 1\n      ), \n    other_countries AS (\n        SELECT \n            CASE\n                WHEN tc.num_of_customers = 1 THEN \"Other\"\n            END\n            as nation,\n            COUNT(*) num_of_customers,\n            SUM(tv.total_sales) total_sales_country,\n            CAST(SUM(tv.total_sales) AS float)/CAST(COUNT(*) as FLOAT) average_value_per_customer,\n            AVG(tv.avg_sales) avg_order_value\n        FROM total_customers tc\n        INNER join total_value tv ON tv.nation = tc.nation\n        WHERE tc.num_of_customers = 1\n      ),\n    final_sales as (\n        SELECT * FROM multi_customer_countries\n        UNION\n        SELECT * FROM other_countries\n      )\n      \nSELECT nation, num_of_customers, total_sales_country, average_order_value, average_value_per_customer\nFROM (\n  SELECT\n      fs.*,\n      CASE\n          WHEN fs.nation = \"Other\" THEN 1\n          ELSE 0\n      end AS sort\n  FROM final_sales fs\n  )\norder by sort ASC, total_sales_country DESC\n\n\"\"\"\n\nrun_query(country_sales_query)",
        "pred": [
            "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets-># Set up cluster parameters\n(figsize=(9 * 1.3 + 2, 14.5))\n(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n\n = [\n    (noisy_circles, {\"n_clusters\": 2}),\n    (noisy_moons, {\"n_clusters\": 2}),\n    (varied, {\"n_neighbors\": 2}),\n    (aniso, {\"n_neighbors\": 2}),\n    (blobs, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate():\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = ().fit_transform(X)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ward = (\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\n    )\n    complete = (\n        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\n    )\n    average = (\n        n_clusters=params[\"n_clusters\"], linkage=\"average\"\n    )\n    single = (\n        n_clusters=params[\"n_clusters\"], linkage=\"single\"\n    )\n\n    clustering_algorithms = (\n        (\"Single Linkage\", single),\n        (\"Average Linkage\", average),\n        (\"Complete Linkage\", complete),\n        (\"Ward Linkage\", ward),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = ()\n\n        # catch warnings related to kneighbors_graph\n        with ():\n            (\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \"  1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = ()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        (len(), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            (name, size=18)\n\n        colors = (\n            list(\n                (\n                    (\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        (X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        (-2.5, 2.5)\n        (-2.5, 2.5)\n        (())\n        (())\n        (\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\n()\n\n\n<img alt=\"Single Linkage, Average Linkage, Complete Linkage, Ward Linkage\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\"/>",
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->Confidence interval-># Now create a bootstrap confidence interval around the a LOWESS fit\n\n\ndef lowess_with_confidence_bounds(\n    x, y, eval_x, N=200, conf_interval=0.95, lowess_kw=None\n):\n    \"\"\"\n    Perform Lowess regression and determine a confidence interval by bootstrap resampling\n    \"\"\"\n    # Lowess smoothing\n    smoothed = sm.nonparametric.lowess(exog=x, endog=y, xvals=eval_x, **lowess_kw)\n\n    # Perform bootstrap resamplings of the data\n    # and  evaluate the smoothing at a fixed set of points\n    smoothed_values = np.empty((N, len(eval_x)))\n    for i in range(N):\n        sample = np.random.choice(len(x), len(x), replace=True)\n        sampled_x = x[sample]\n        sampled_y = y[sample]\n\n        smoothed_values[i] = sm.nonparametric.lowess(\n            exog=sampled_x, endog=sampled_y, xvals=eval_x, **lowess_kw\n        )\n\n    # Get the confidence interval\n    sorted_values = np.sort(smoothed_values, axis=0)\n    bound = int(N * (1 - conf_interval) / 2)\n    bottom = sorted_values[bound - 1]\n    top = sorted_values[-bound]\n\n    return smoothed, bottom, top\n\n\n# Compute the 95% confidence interval\neval_x = np.linspace(0, 4 * np.pi, 31)\nsmoothed, bottom, top = lowess_with_confidence_bounds(\n    x, y, eval_x, lowess_kw={\"frac\": 0.1}\n)",
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data-># Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
            "sklearn->Examples->Tutorial exercises->Cross-validation on diabetes Dataset Exercise->Bonus: how much can you trust the selection of alpha?-># To answer this question we use the LassoCV object that sets its alpha\n# parameter automatically from the data by internal cross-validation (i.e. it\n# performs cross-validation on the training data it receives).\n# We use external cross-validation to see how much the automatically obtained\n# alphas differ across different cross-validation folds.\n\nfrom sklearn.linear_model import \nfrom sklearn.model_selection import \n\nlasso_cv = (alphas=alphas, random_state=0, max_iter=10000)\nk_fold = (3)\n\nprint(\"Answer to the bonus question:\", \"how much can you trust the selection of alpha?\")\nprint()\nprint(\"Alpha parameters maximising the generalization score on different\")\nprint(\"subsets of the data:\")\nfor k, (train, test) in enumerate(k_fold.split(X, y)):\n    lasso_cv.fit(X[train], y[train])\n    print(\n        \"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".format(\n            k, lasso_cv.alpha_, lasso_cv.score(X[test], y[test])\n        )\n    )\nprint()\nprint(\"Answer: Not very much since we obtained different alphas for different\")\nprint(\"subsets of the data and moreover, the scores for these alphas differ\")\nprint(\"quite substantially.\")\n\n()",
            "statsmodels->Examples->State space models->Unobserved Components: Application->Model-># Create Table I\ntable_i = np.zeros((5,6))\n\nstart = dta.index[0]\nend = dta.index[-1]\ntime_range = '%d:%d-%d:%d' % (start.year, start.quarter, end.year, end.quarter)\nmodels = [\n    ('US GNP', time_range, 'None'),\n    ('US Prices', time_range, 'None'),\n    ('US Prices', time_range, r'$\\sigma_\\eta^2 = 0$'),\n    ('US monetary base', time_range, 'None'),\n    ('US monetary base', time_range, r'$\\sigma_\\eta^2 = 0$'),\n]\nindex = pd.MultiIndex.from_tuples(models, names=['Series', 'Time range', 'Restrictions'])\nparameter_symbols = [\n    r'$\\sigma_\\zeta^2$', r'$\\sigma_\\eta^2$', r'$\\sigma_\\kappa^2$', r'$\\rho$',\n    r'$2 \\pi / \\lambda_c$', r'$\\sigma_\\varepsilon^2$',\n]\n\ni = 0\nfor res in (output_res, prices_res, prices_restricted_res, money_res, money_restricted_res):\n    if res.model.stochastic_level:\n        (sigma_irregular, sigma_level, sigma_trend,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n    else:\n        (sigma_irregular, sigma_level,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n        sigma_trend = np.nan\n    period_cycle = 2 * np.pi / frequency_cycle\n\n    table_i[i, :] = [\n        sigma_level*1e7, sigma_trend*1e7,\n        sigma_cycle*1e7, damping_cycle, period_cycle,\n        sigma_irregular*1e7\n    ]\n    i += 1\n\npd.set_option('float_format', lambda x: '%.4g' % np.round(x, 2) if not np.isnan(x) else '-')\ntable_i = pd.DataFrame(table_i, index=index, columns=parameter_symbols)\ntable_i"
        ]
    },
    {
        "id": "799682",
        "GT": "ELI5: which features are important for price prediction\n\nOr, explain like I'm 5, how does a linear ridge predict prices?\n\n[ElL5](http://eli5.readthedocs.io/) is a library that can help us with that, let's see it in action. It has support for many models, including XGBoost and LightGBM, but we'll be using it to analyze the Ridge model from scikit-learn.  Overall modelling strategy is inspired by this beatiful kernel [Ridge Script](https://www.kaggle.com/apapiu/ridge-script) by Alexandru Papiu.->y_train = np.log1p(train['price'])\ntrain['category_name'] = train['category_name'].fillna('Other').astype(str)\ntrain['brand_name'] = train['brand_name'].fillna('missing').astype(str)\ntrain['shipping'] = train['shipping'].astype(str)\ntrain['item_condition_id'] = train['item_condition_id'].astype(str)\ntrain['item_description'] = train['item_description'].fillna('None')",
        "pred": [
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Data exploration on the Bike Sharing Demand dataset->y = df[\"count\"] / df[\"count\"].max()",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 1: fitting the model on the available dataset->y_pre = y.iloc[:-5]\ny_pre.plot(figsize=(15, 3), title='Inflation');\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_news_9_0.png\"/>",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points->student_resid   unadj_p  fdr_bh(p)\n16      -2.049393  0.046415   0.764747\n13      -2.035329  0.047868   0.764747\n33       1.905847  0.063216   0.764747\n18      -1.575505  0.122304   0.764747\n1        1.522185  0.135118   0.764747\n3        1.522185  0.135118   0.764747\n21      -1.450418  0.154034   0.764747\n17      -1.426675  0.160731   0.764747\n29       1.388520  0.171969   0.764747\n14      -1.374733  0.176175   0.764747\n35       1.346543  0.185023   0.764747\n34      -1.272159  0.209999   0.764747\n28      -1.186946  0.241618   0.764747\n20      -1.150621  0.256103   0.764747\n44       1.134779  0.262612   0.764747\n39       1.091886  0.280826   0.764747\n19       1.064878  0.292740   0.764747\n6       -1.026873  0.310093   0.764747\n30      -1.009096  0.318446   0.764747\n22      -0.979768  0.332557   0.764747\n8        0.961218  0.341695   0.764747\n5        0.913802  0.365801   0.768599\n11       0.871997  0.387943   0.768599\n12       0.856685  0.396261   0.768599\n46      -0.833923  0.408829   0.768599\n10       0.743920  0.460879   0.770890\n42       0.727179  0.470968   0.770890\n15      -0.689258  0.494280   0.770890\n43       0.688272  0.494895   0.770890\n7        0.655712  0.515424   0.770890\n40      -0.646396  0.521381   0.770890\n26      -0.640978  0.524862   0.770890\n25      -0.545561  0.588123   0.837630\n32       0.472819  0.638680   0.843682\n37       0.472819  0.638680   0.843682\n38       0.462187  0.646225   0.843682\n0        0.430686  0.668799   0.849556\n31       0.341726  0.734184   0.892552\n36       0.318911  0.751303   0.892552\n4        0.307890  0.759619   0.892552\n9        0.235114  0.815211   0.922751\n41       0.187732  0.851950   0.922751\n2       -0.182093  0.856346   0.922751\n23      -0.156014  0.876736   0.922751\n27      -0.147406  0.883485   0.922751\n24       0.065195  0.948314   0.963776\n45       0.045675  0.963776   0.963776",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->y = survey.target.values.ravel()\nsurvey.target.head()",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Hertzprung Russell data for Star Cluster CYG 0B1 - Leverage Points->student_resid   unadj_p  sidak(p)\n16      -2.049393  0.046415  0.892872\n13      -2.035329  0.047868  0.900286\n33       1.905847  0.063216  0.953543\n18      -1.575505  0.122304  0.997826\n1        1.522185  0.135118  0.998911\n3        1.522185  0.135118  0.998911\n21      -1.450418  0.154034  0.999615\n17      -1.426675  0.160731  0.999735\n29       1.388520  0.171969  0.999859\n14      -1.374733  0.176175  0.999889\n35       1.346543  0.185023  0.999933\n34      -1.272159  0.209999  0.999985\n28      -1.186946  0.241618  0.999998\n20      -1.150621  0.256103  0.999999\n44       1.134779  0.262612  0.999999\n39       1.091886  0.280826  1.000000\n19       1.064878  0.292740  1.000000\n6       -1.026873  0.310093  1.000000\n30      -1.009096  0.318446  1.000000\n22      -0.979768  0.332557  1.000000\n8        0.961218  0.341695  1.000000\n5        0.913802  0.365801  1.000000\n11       0.871997  0.387943  1.000000\n12       0.856685  0.396261  1.000000\n46      -0.833923  0.408829  1.000000\n10       0.743920  0.460879  1.000000\n42       0.727179  0.470968  1.000000\n15      -0.689258  0.494280  1.000000\n43       0.688272  0.494895  1.000000\n7        0.655712  0.515424  1.000000\n40      -0.646396  0.521381  1.000000\n26      -0.640978  0.524862  1.000000\n25      -0.545561  0.588123  1.000000\n32       0.472819  0.638680  1.000000\n37       0.472819  0.638680  1.000000\n38       0.462187  0.646225  1.000000\n0        0.430686  0.668799  1.000000\n31       0.341726  0.734184  1.000000\n36       0.318911  0.751303  1.000000\n4        0.307890  0.759619  1.000000\n9        0.235114  0.815211  1.000000\n41       0.187732  0.851950  1.000000\n2       -0.182093  0.856346  1.000000\n23      -0.156014  0.876736  1.000000\n27      -0.147406  0.883485  1.000000\n24       0.065195  0.948314  1.000000\n45       0.045675  0.963776  1.000000"
        ]
    },
    {
        "id": "447074",
        "GT": "Let's look at the policy we learned->Now we'll examine the q values->q_value_observations",
        "pred": [
            "pandas_toms_blog->Scaling->Dask->total_by_employee",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->data_student.dtypes",
            "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->The KDE is a distribution->kde.entropy",
            "statsmodels->Examples->Linear Regression Models->Rolling Least Squares->params.iloc[57:62]",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resf_logit.predict()"
        ]
    },
    {
        "id": "1191469",
        "GT": "Visualizing the Dataset\n* Let's look at clothing departments->grouped = data.groupby('TripType')\ntem_b = grouped['HEALTH AND BEAUTY AIDS', 'SHOES', 'MENS WEAR', 'SWIMWEAR/OUTERWEAR', 'LADIES SOCKS'].agg([np.sum])\n\ntem_b.plot(kind='bar', figsize = (15,12.5))",
        "pred": [
            "sklearn->Examples->Decomposition->Faces dataset decompositions->Decomposition->Sparse components - MiniBatchSparsePCA->batch_pca_estimator = (\n    n_components=n_components, alpha=0.1, max_iter=100, batch_size=3, random_state=rng\n)\nbatch_pca_estimator.fit(faces_centered)\nplot_gallery(\n    \"Sparse components - MiniBatchSparsePCA\",\n    batch_pca_estimator.components_[:n_components],\n)\n\n\n<img alt=\"Sparse components - MiniBatchSparsePCA\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_faces_decomposition_005.png\" srcset=\"../../_images/sphx_glr_plot_faces_decomposition_005.png\"/>",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->delta = (by_game.home_rest - by_game.away_rest).dropna().astype(int)\nax = (delta.value_counts()\n    .reindex(np.arange(delta.min(), delta.max() + 1), fill_value=0)\n    .sort_index()\n    .plot(kind='bar', color='k', width=.9, rot=0, figsize=(12, 6))\n)\nsns.despine()\nax.set(xlabel='Difference in Rest (Home - Away)', ylabel='Games');",
            "statsmodels->Examples->Statistics->Copulas->Sampling from a copula->Reproducibility->marginals = [stats.gamma(2), stats.norm]\njoint_dist = CopulaDistribution(copula=IndependenceCopula(), marginals=marginals)\nsample = joint_dist.rvs(512, random_state=20210801)\nh = sns.jointplot(x=sample[:, 0], y=sample[:, 1], kind=\"scatter\")\n_ = h.set_axis_labels(\"X1\", \"X2\", fontsize=16)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_copula_9_0.png\" src=\"../../../_images/examples_notebooks_generated_copula_9_0.png\"/>",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Condensing and Aggregating observations->Dataset with unique explanatory variables (exog)->gr = data[\"affairs rate_marriage age yrs_married\".split()].groupby(\n    \"rate_marriage age yrs_married\".split()\n)\ndf_a = gr.agg([\"mean\", \"sum\", \"count\"])\n\n\ndef merge_tuple(tpl):\n    if isinstance(tpl, tuple) and len(tpl)  1:\n        return \"_\".join(map(str, tpl))\n    else:\n        return tpl\n\n\ndf_a.columns = df_a.columns.map(merge_tuple)\ndf_a.reset_index(inplace=True)\nprint(df_a.shape)\ndf_a.head()",
            "sklearn->Examples->Inspection->Common pitfalls in the interpretation of coefficients of linear models->->coefs = (\n    model[-1].regressor_.coef_,\n    columns=[\"Coefficients importance\"],\n    index=feature_names,\n)\ncoefs.plot.barh(figsize=(9, 7))\n(\"Ridge model, small regularization, normalized variables\")\n(\"Raw coefficient values\")\n(x=0, color=\".5\")\n(left=0.3)\n\n\n<img alt=\"Ridge model, small regularization, normalized variables\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_010.png\" srcset=\"../../_images/sphx_glr_plot_linear_model_coefficient_interpretation_010.png\"/>"
        ]
    },
    {
        "id": "1152178",
        "GT": "beta = np.array([5000, 2.3, 1.3, 10]).reshape(1, 4)\ny = np.dot(beta, X.T).T\n# Adding noise\ny = y + 2000*np.random.random((10, 1))\ny",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS estimation->nsample = 100\nx = np.linspace(0, 10, 100)\nX = np.column_stack((x, x ** 2))\nbeta = np.array([1, 0.1, 10])\ne = np.random.normal(size=nsample)",
            "sklearn->Examples->Support Vector Machines->Support Vector Regression (SVR) using linear and non-linear kernels->Generate sample data->X = (5 * (40, 1), axis=0)\ny = (X).ravel()\n\n# add noise to targets\ny[::5] += 3 * (0.5 - (8))",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Small group effects->beta = [1.0, 0.3, -0.0, 10]\ny_true = np.dot(X, beta)\ny = y_true + np.random.normal(size=nsample)\n\nres3 = sm.OLS(y, X).fit()",
            "sklearn->Examples->Gaussian Process for Machine Learning->Gaussian process regression (GPR) with noise-level estimation->Data generation->rng = (0)\nX_train = rng.uniform(0, 5, size=20).reshape(-1, 1)\ny_train = target_generator(X_train, add_noise=True)",
            "scipy->Sparse eigenvalue problems with ARPACK->Use of LinearOperator->N = 100\n rng = np.random.default_rng()\n d = rng.normal(0, 1, N).astype(np.float64)\n D = np.diag(d)\n Dop = Diagonal(d, dtype=np.float64)"
        ]
    },
    {
        "id": "31474",
        "GT": "Transforming DataFrame\n\n##### apply\n\nYou can transform dataframe with your own custom function.\n\nIf you call DataFrame `apply` function, it will input `Pandas Series`, you can use `Series` provided functions to manipulate data.->map and lambda function\n\nYou can use `map` and `lambda function` to iterate each element in Pandas series.\n\nIn some simple operation, you can write logic in the lambda function instead of defining a custom function->print(pokemon_df['Type 1'].value_counts().head())\n\nprint(pokemon_df['Type 1'].map(lambda type1: type1.upper()).head())",
        "pred": [
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the labels to floating point through categorical/one-hot encoding->print(training_labels[0])\nprint(training_labels[1])\nprint(training_labels[2])",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables->print(X[:5, :])\nprint(y[:5])\nprint(groups)\nprint(dummy[:5, :])",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Remainder->print(res_f2.summary())\nprint(res_f.summary())",
            "statsmodels->Examples->Discrete Choice Models->Getting Started->Multinomial Logit->print(anes_data.exog.head())\nprint(anes_data.endog.head())",
            "numpy->NumPy Applications->Deep learning on MNIST->2. Preprocess the data->Convert the image data to the floating-point format->print(\"The data type of training images: {}\".format(x_train.dtype))\nprint(\"The data type of test images: {}\".format(x_test.dtype))"
        ]
    },
    {
        "id": "811437",
        "GT": "Hint 2: Use Out-of-Core Learning If You Don't Have Enough Memory\n\nThe size of dataset in the competition (300MB in raw text) is much larger than the example IMDB dataset (80MB in raw text). The dataset, after being represented as feature vectors, may become much larger, and you are unlikely to store all of them in memory. Next, we introduce another training technique called the **Out of Core Learning** to help you train a model using **data streaming**.\n\nThe idea of Out of Core Learning is similar to the stochastic gradient descent, which updates the model when seeing a minibatch, except that each minibatch is loaded from disk via a data stream. Since we only see a part of the dataset at a time, we can only use the `HashingVectorizer` to transform text into feature vectors because the `HashingVectorizer` does not require knowing the vocabulary space in advance.-># import optimized pickle written in C for serializing and \n# de-serializing a Python object\nimport _pickle as pkl\n\n# dump to disk\npkl.dump(hashvec, open('output/hashvec.pkl', 'wb'))\npkl.dump(clf, open('output/clf-sgd.pkl', 'wb'))\n\n# load from disk\nhashvec = pkl.load(open('output/hashvec.pkl', 'rb'))\nclf = pkl.load(open('output/clf-sgd.pkl', 'rb'))\n\ndf_test = pd.read_csv('datasets/test.csv')\nprint('test auc: %.3f' % roc_auc_score(df_test['sentiment'], \\\n            clf.predict_proba(hashvec.transform(df_test['review']))[:,1]))",
        "pred": [
            "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Gradient boosting estimator with native categorical support-># The ordinal encoder will first output the categorical features, and then the\n# continuous (passed-through) features\n\nhist_native = (\n    ordinal_encoder,\n    (\n        random_state=42,\n        categorical_features=categorical_columns,\n    ),\n).set_output(transform=\"pandas\")",
            "numpy->NumPy Applications->Deep learning on MNIST->3. Build and train a small neural network from scratch->Compose the model and begin training and testing it-># To store training and test set losses and accurate predictions\n# for visualization.\nstore_training_loss = []\nstore_training_accurate_pred = []\nstore_test_loss = []\nstore_test_accurate_pred = []\n\n# This is a training loop.\n# Run the learning experiment for a defined number of epochs (iterations).\nfor j in range(epochs):\n\n    #################\n    # Training step #\n    #################\n\n    # Set the initial loss/error and the number of accurate predictions to zero.\n    training_loss = 0.0\n    training_accurate_predictions = 0\n\n    # For all images in the training set, perform a forward pass\n    # and backpropagation and adjust the weights accordingly.\n    for i in range(len(training_images)):\n        # Forward propagation/forward pass:\n        # 1. The input layer:\n        #    Initialize the training image data as inputs.\n        layer_0 = training_images[i]\n        # 2. The hidden layer:\n        #    Take in the training image data into the middle layer by\n        #    matrix-multiplying it by randomly initialized weights.\n        layer_1 = np.dot(layer_0, weights_1)\n        # 3. Pass the hidden layer's output through the ReLU activation function.\n        layer_1 = relu(layer_1)\n        # 4. Define the dropout function for regularization.\n        dropout_mask = rng.integers(low=0, high=2, size=layer_1.shape)\n        # 5. Apply dropout to the hidden layer's output.\n        layer_1 *= dropout_mask * 2\n        # 6. The output layer:\n        #    Ingest the output of the middle layer into the the final layer\n        #    by matrix-multiplying it by randomly initialized weights.\n        #    Produce a 10-dimension vector with 10 scores.\n        layer_2 = np.dot(layer_1, weights_2)\n\n        # Backpropagation/backward pass:\n        # 1. Measure the training error (loss function) between the actual\n        #    image labels (the truth) and the prediction by the model.\n        training_loss += np.sum((training_labels[i] - layer_2) ** 2)\n        # 2. Increment the accurate prediction count.\n        training_accurate_predictions += int(\n            np.argmax(layer_2) == np.argmax(training_labels[i])\n        )\n        # 3. Differentiate the loss function/error.\n        layer_2_delta = training_labels[i] - layer_2\n        # 4. Propagate the gradients of the loss function back through the hidden layer.\n        layer_1_delta = np.dot(weights_2, layer_2_delta) * relu2deriv(layer_1)\n        # 5. Apply the dropout to the gradients.\n        layer_1_delta *= dropout_mask\n        # 6. Update the weights for the middle and input layers\n        #    by multiplying them by the learning rate and the gradients.\n        weights_1 += learning_rate * np.outer(layer_0, layer_1_delta)\n        weights_2 += learning_rate * np.outer(layer_1, layer_2_delta)\n\n    # Store training set losses and accurate predictions.\n    store_training_loss.append(training_loss)\n    store_training_accurate_pred.append(training_accurate_predictions)\n\n    ###################\n    # Evaluation step #\n    ###################\n\n    # Evaluate model performance on the test set at each epoch.\n\n    # Unlike the training step, the weights are not modified for each image\n    # (or batch). Therefore the model can be applied to the test images in a\n    # vectorized manner, eliminating the need to loop over each image\n    # individually:\n\n    results = relu(test_images @ weights_1) @ weights_2\n\n    # Measure the error between the actual label (truth) and prediction values.\n    test_loss = np.sum((test_labels - results) ** 2)\n\n    # Measure prediction accuracy on test set\n    test_accurate_predictions = np.sum(\n        np.argmax(results, axis=1) == np.argmax(test_labels, axis=1)\n    )\n\n    # Store test set losses and accurate predictions.\n    store_test_loss.append(test_loss)\n    store_test_accurate_pred.append(test_accurate_predictions)\n\n    # Summarize error and accuracy metrics at each epoch\n    print(\n        (\n            f\"Epoch: {j}\\n\"\n            f\"  Training set error: {training_loss / len(training_images):.3f}\\n\"\n            f\"  Training set accuracy: {training_accurate_predictions / len(training_images)}\\n\"\n            f\"  Test set error: {test_loss / len(test_images):.3f}\\n\"\n            f\"  Test set accuracy: {test_accurate_predictions / len(test_images)}\"\n        )\n    )",
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models-># Here we follow the same procedure as above, but now we instantiate the\n# Theano wrapper `Loglike` with the UC model instance instead of the\n# SARIMAX model instance\nloglike_uc = Loglike(mod_uc)\n\nwith pm.Model():\n    # Priors\n    sigma2level = pm.InverseGamma(\"sigma2.level\", 1, 1)\n    sigma2ar = pm.InverseGamma(\"sigma2.ar\", 1, 1)\n    arL1 = pm.Uniform(\"ar.L1\", -0.99, 0.99)\n\n    # convert variables to tensor vectors\n    theta_uc = tt.as_tensor_variable([sigma2level, sigma2ar, arL1])\n\n    # use a DensityDist (use a lamdba function to \"call\" the Op)\n    pm.DensityDist(\"likelihood\", loglike_uc, observed=theta_uc)\n\n    # Draw samples\n    trace_uc = pm.sample(\n        ndraws,\n        tune=nburn,\n        return_inferencedata=True,\n        cores=1,\n        compute_convergence_checks=False,\n    )",
            "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results-># Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Comparing kernel functions->The available kernel functions on three data points-># Create three equidistant points\ndata = np.linspace(-1, 1, 3)\nkde = sm.nonparametric.KDEUnivariate(data)\n\n# Create a figure\nfig = plt.figure(figsize=(12, 5))\n\n# Enumerate every option for the kernel\nfor i, kernel in enumerate(kernel_switch.keys()):\n\n    # Create a subplot, set the title\n    ax = fig.add_subplot(3, 3, i + 1)\n    ax.set_title('Kernel function \"{}\"'.format(kernel))\n\n    # Fit the model (estimate densities)\n    kde.fit(kernel=kernel, fft=False, gridsize=2 ** 10)\n\n    # Create the plot\n    ax.plot(kde.support, kde.density, lw=3, label=\"KDE from samples\", zorder=10)\n    ax.scatter(data, np.zeros_like(data), marker=\"x\", color=\"red\")\n    plt.grid(True, zorder=-5)\n    ax.set_xlim([-3, 3])\n\nplt.tight_layout()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_kernel_density_24_0.png\" src=\"../../../_images/examples_notebooks_generated_kernel_density_24_0.png\"/>"
        ]
    },
    {
        "id": "271159",
        "GT": "Exploratory Cells\n\n## Transformations\n\n### Numerical nulls and data leaks-># First looking for features with large numbers of nulls (more than 25%)\n# We'll remove these wholesale, assuming nothing too vital jumps out.\n\nnum_val_counts = train.isnull().sum()\nnum_val_counts[num_val_counts > 0.25*train.shape[0]]",
        "pred": [
            "sklearn->Examples->Examples based on real world datasets->Visualizing the stock market structure->Embedding in 2D space-># Finding a low-dimension embedding for visualization: find the best position of\n# the nodes (the stocks) on a 2D plane\n\nfrom sklearn import manifold\n\nnode_position_model = (\n    n_components=2, eigen_solver=\"dense\", n_neighbors=6\n)\n\nembedding = node_position_model.fit_transform(X.T).T",
            "sklearn->Examples->Clustering->Segmenting the picture of greek coins in regions-># Computing a few extra eigenvectors may speed up the eigen_solver.\n# The spectral clustering quality may also benetif from requesting\n# extra regions for segmentation.\nn_regions_plus = 3\n\n# Apply spectral clustering using the default eigen_solver='arpack'.\n# Any implemented solver can be used: eigen_solver='arpack', 'lobpcg', or 'amg'.\n# Choosing eigen_solver='amg' requires an extra package called 'pyamg'.\n# The quality of segmentation and the speed of calculations is mostly determined\n# by the choice of the solver and the value of the tolerance 'eigen_tol'.\n# TODO: varying eigen_tol seems to have no effect for 'lobpcg' and 'amg' #21243.\nfor assign_labels in (\"kmeans\", \"discretize\", \"cluster_qr\"):\n    t0 = ()\n    labels = (\n        graph,\n        n_clusters=(n_regions + n_regions_plus),\n        eigen_tol=1e-7,\n        assign_labels=assign_labels,\n        random_state=42,\n    )\n\n    t1 = ()\n    labels = labels.reshape(rescaled_coins.shape)\n    (figsize=(5, 5))\n    (rescaled_coins, cmap=plt.cm.gray)\n\n    (())\n    (())\n    title = \"Spectral clustering: %s, %.2fs\" % (assign_labels, (t1 - t0))\n    print(title)\n    (title)\n    for l in range(n_regions):\n        colors = [plt.cm.nipy_spectral((l + 4) / float(n_regions + 4))]\n        (labels == l, colors=colors)\n        # To view individual segments as appear comment in plt.pause(0.5)\n()\n\n# TODO: After #21194 is merged and #21243 is fixed, check which eigen_solver\n# is the best and set eigen_solver='arpack', 'lobpcg', or 'amg' and eigen_tol\n# explicitly in this example.\n\n\n\n<img alt=\"Spectral clustering: kmeans, 2.04s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_001.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_001.png\"/>\n<img alt=\"Spectral clustering: discretize, 1.83s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_002.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_002.png\"/>\n<img alt=\"Spectral clustering: cluster_qr, 1.82s\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_coin_segmentation_003.png\" srcset=\"../../_images/sphx_glr_plot_coin_segmentation_003.png\"/>",
            "torch->Deploying PyTorch Models in Production->Real Time Inference on Raspberry Pi 4 (30 fps!)->Raspberry Pi 4 Setup-># This enables the extended features such as the camera.\nstart_x=1\n\n# This needs to be at least 128M for the camera processing, if it's bigger you can just leave it as is.\ngpu_mem=128\n\n# You need to commment/remove the existing camera_auto_detect line since this causes issues with OpenCV/V4L2 capture.\n#camera_auto_detect=1",
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->3. Fit the model with maximum likelihood-># Create an SARIMAX model instance - here we use it to estimate\n# the parameters via MLE using the `fit` method, but we can\n# also re-use it below for the Bayesian estimation\nmod = sm.tsa.statespace.SARIMAX(inf, order=(1, 0, 1))\n\nres_mle = mod.fit(disp=False)\nprint(res_mle.summary())",
            "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()-># The threshold is \"greater than 150\"\n# Return `1` if true, `0` otherwise\nxray_image_mask_less_noisy = np.where(xray_image &gt; 150, 1, 0)\n\nplt.imshow(xray_image_mask_less_noisy, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/2b2a2a73c09768a3e924a35f2e10a716c3171b763df137f3c1688aa222499701.png\" src=\"../_images/2b2a2a73c09768a3e924a35f2e10a716c3171b763df137f3c1688aa222499701.png\"/>"
        ]
    },
    {
        "id": "633273",
        "GT": "I. Format data\nThe data are split into a training and test set.  The training set uses data from 10-30-2016 to 12-31-2016 and the test set uses data from 1-1-2017 to 1-11-2016.  Here we remove duplicate values in the test set.->standard_formatting from the feateng.py module\n\nThe standard_formatting function from the feateng module will perform preliminary cleaning of the data in preparation for modeling. \n\n1. Outlier removal (see exploratory_analysis.ipynb for justification)\n    - remove prices > $20,000 per month\n    - remove sq_ft > 10,000\n    - remove rooms > 15\n    - remove beds > 8\n    - remove baths > 6\n    - remove data_id = 1965895 (incorrect listing)\n<br><br>\n2. Drop uninformative columns\n    - data_id, link, address, realtor, borough\n<br><br>\n3. Feature encoding\n    - One-hot-feature encoding of unit_type and neighborhood\n<br><br>\n4. Missing values: \n    - For sq_ft, rooms, baths, beds, and days_on_streeteasy, recode missing values from -1 to 0 and add a new indicator variable for missing values.->#get some variable lists before formatting\nunit_type_list = df['unit_type'].unique()\nnhood_list_names = df['neighborhood'].unique()\n#apply standard model formatting\ndf = standard_formatting(df)\ndf = df.drop('index',1)",
        "pred": [
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "statsmodels->Examples->Time Series Analysis->Markov switching dynamic regression->Federal funds rate with switching intercept and lagged dependent variable-># Fit the model\nmod_fedfunds2 = sm.tsa.MarkovRegression(\n    dta_fedfunds.iloc[1:], k_regimes=2, exog=dta_fedfunds.iloc[:-1]\n)\nres_fedfunds2 = mod_fedfunds2.fit()",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack->accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models-># Construct the model instance\nmod_uc = sm.tsa.UnobservedComponents(inf, \"rwalk\", autoregressive=1)\n\n# Fit the model via maximum likelihood\nres_uc_mle = mod_uc.fit()\nprint(res_uc_mle.summary())"
        ]
    },
    {
        "id": "514077",
        "GT": "Impute Function->def knn_impute(data, k_neighbors=3):\n    \"\"\"\n    Imputes missing data using the nearest non-null neighbors.\n    Makes changes in place to dataframe\n    \"\"\"\n    # Iterate over rows\n    for i, row in data.iterrows():\n\n        # Find rows that contain nulls\n        if row.isnull().any():\n\n            # Find K nearest neighbors\n            knn = impute_neighbors(row)\n\n            # Find the cell with the null value and fill it\n            for i, v in row.iteritems():\n                if isinstance(v, float) and isnan(v):\n                    # Fill that with the voted upon value\n                    val = knn[i].mode().values[0]\n                    data.set_value(row.name, i, val)\n\nfin = knn_impute(data)",
        "pred": [
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Load Data->Visualize a few images->def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\n,  = next(iter(dataloaders['train']))\n\n# Make a grid from batch\n = ()\n\nimshow(, title=[class_names[x] for x in ])\n\n\n<img alt=\"['bees', 'bees', 'ants', 'bees']\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\" srcset=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\"/>",
            "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Heterogeneous Data Sources->Creating transformers->def subject_body_extractor(posts):\n    # construct object dtype array with two columns\n    # first column = 'subject' and second column = 'body'\n    features = (shape=(len(posts), 2), dtype=object)\n    for i, text in enumerate(posts):\n        # temporary variable `_` stores '\\n\\n'\n        headers, _, body = text.partition(\"\\n\\n\")\n        # store body text in second column\n        features[i, 1] = body\n\n        prefix = \"Subject:\"\n        sub = \"\"\n        # save text after 'Subject:' in first column\n        for line in headers.split(\"\\n\"):\n            if line.startswith(prefix):\n                sub = line[len(prefix) :]\n                break\n        features[i, 0] = sub\n\n    return features\n\n\nsubject_body_transformer = (subject_body_extractor)",
            "matplotlib->Tutorials->Colors->Creating Colormaps in Matplotlib->Creating listed colormaps->def plot_examples(colormaps):\n    \"\"\"\n    Helper function to plot data with associated colormap.\n    \"\"\"\n    np.random.seed(19680801)\n    data = np.random.randn(30, 30)\n    n = len(colormaps)\n    fig, axs = plt.subplots(1, n, figsize=(n * 2 + 2, 3),\n                            constrained_layout=True, squeeze=False)\n    for [ax, cmap] in zip(axs.flat, colormaps):\n        psm = ax.pcolormesh(data, cmap=cmap, rasterized=True, vmin=-4, vmax=4)\n        fig.colorbar(psm, ax=ax)\n    plt.show()",
            "statsmodels->Examples->Plotting->Box Plots->Bean Plots->def beanplot(data, plot_opts={}, jitter=False):\n    \"\"\"helper function to try out different plot options\"\"\"\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    plot_opts_ = {\n        \"cutoff_val\": 5,\n        \"cutoff_type\": \"abs\",\n        \"label_fontsize\": \"small\",\n        \"label_rotation\": 30,\n    }\n    plot_opts_.update(plot_opts)\n    sm.graphics.beanplot(\n        data, ax=ax, labels=labels, jitter=jitter, plot_opts=plot_opts_\n    )\n    ax.set_xlabel(\"Party identification of respondent.\")\n    ax.set_ylabel(\"Age\")",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->def plot_weights(support, weights_func, xlabels, xticks):\n    fig = plt.figure(figsize=(12, 8))\n    ax = fig.add_subplot(111)\n    ax.plot(support, weights_func(support))\n    ax.set_xticks(xticks)\n    ax.set_xticklabels(xlabels, fontsize=16)\n    ax.set_ylim(-0.1, 1.1)\n    return ax"
        ]
    },
    {
        "id": "1271827",
        "GT": "Data Wrangling\nNow it's time to collect and explore our data.->pd.set_option('display.float_format', lambda x: '%.2f' % x) #Suppress Scientific Notation from Python Pandas\ndf_mov.describe()",
        "pred": [
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\n",
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->iris = sns.load_dataset(\"iris\")\ng = sns.PairGrid(iris)\ng.map(sns.scatterplot)\n",
            "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Train tree classifier->iris = ()\nX = iris.data\ny = iris.target\nX_train, X_test, y_train, y_test = (X, y, random_state=0)\n\nclf = (max_leaf_nodes=3, random_state=0)\nclf.fit(X_train, y_train)",
            "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures->summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]"
        ]
    },
    {
        "id": "692862",
        "GT": "Data Analysis Project - Baseball Metrics->#import all relevant packages for this analysis\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom __future__ import division\n\nfrom IPython.display import HTML\n\n#This is done to hide the Python code\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n</script>\n<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')",
        "pred": [
            "torch->Image and Video->Adversarial Example Generation->Results->Sample Adversarial Examples-># Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()\n\n\n<img alt=\"2 -&gt; 2, 6 -&gt; 6, 4 -&gt; 4, 6 -&gt; 6, 2 -&gt; 2, 8 -&gt; 3, 4 -&gt; 2, 9 -&gt; 5, 7 -&gt; 9, 8 -&gt; 9, 6 -&gt; 0, 1 -&gt; 8, 6 -&gt; 8, 9 -&gt; 8, 8 -&gt; 6, 5 -&gt; 8, 5 -&gt; 2, 4 -&gt; 8, 4 -&gt; 8, 5 -&gt; 8, 3 -&gt; 5, 4 -&gt; 8, 8 -&gt; 2, 8 -&gt; 6, 3 -&gt; 8, 6 -&gt; 1, 1 -&gt; 3, 1 -&gt; 8, 4 -&gt; 8, 8 -&gt; 2, 7 -&gt; 8, 1 -&gt; 2, 0 -&gt; 2, 7 -&gt; 8, 7 -&gt; 8\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_002.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_002.png\"/>",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Logarithmic and other nonlinear axes-># Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n# make up some data in the open interval (0, 1)\ny = np.random.normal(loc=0.5, scale=0.4, size=1000)\ny = y[(y  0) & (y &lt; 1)]\ny.sort()\nx = np.arange(len(y))\n\n# plot with various axes scales\nplt.figure()\n\n# linear\nplt.subplot(221)\nplt.plot(x, y)\nplt.yscale('linear')\nplt.title('linear')\nplt.grid(True)\n\n# log\nplt.subplot(222)\nplt.plot(x, y)\nplt.yscale('log')\nplt.title('log')\nplt.grid(True)\n\n# symmetric log\nplt.subplot(223)\nplt.plot(x, y - y.mean())\nplt.yscale('symlog', linthresh=0.01)\nplt.title('symlog')\nplt.grid(True)\n\n# logit\nplt.subplot(224)\nplt.plot(x, y)\nplt.yscale('logit')\nplt.title('logit')\nplt.grid(True)\n# Adjust the subplot layout, because the logit one may take more space\n# than usual, due to y-tick labels like \"1 - 10^{-3}\"\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n\nplt.show()\n\n\n<img alt=\"linear, log, symlog, logit\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_010.png\" srcset=\"../../_images/sphx_glr_pyplot_010.png, ../../_images/sphx_glr_pyplot_010_2_0x.png 2.0x\"/>",
            "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Transformer Block With a Novel Normalization-># Profile rms_norm\nfunc = functools.partial(\n    with_rms_norm,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"Eager Mode - RMS Norm\")",
            "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard-># imports\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# transforms\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))])\n\n# datasets\ntrainset = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=True,\n    transform=transform)\ntestset = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=False,\n    transform=transform)\n\n# dataloaders\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                        shuffle=True, num_workers=2)\n\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                        shuffle=False, num_workers=2)\n\n# constant for classes\nclasses = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n\n# helper function to show an image\n# (used in the `plot_classes_preds` function below)\ndef matplotlib_imshow(img, one_channel=False):\n    if one_channel:\n        img = img.mean(dim=0)\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    if one_channel:\n        plt.imshow(npimg, cmap=\"Greys\")\n    else:\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))",
            "statsmodels->Examples->State space models->Unobserved Components: Application->Data-># Plot the data\nax = dta.plot(figsize=(13,3))\nylim = ax.get_ylim()\nax.xaxis.grid()\nax.fill_between(dates, ylim[0]+1e-5, ylim[1]-1e-5, recessions, facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\"/>"
        ]
    },
    {
        "id": "140574",
        "GT": "<font id=\"2.5\" color=\"blue\">Plot with Pandas</font>->Bar Graphs\n\nWe can modify our data before we graph it to analyze different things.->yearly_data.plot(kind=\"bar\")\nplt.axis('off')",
        "pred": [
            "seaborn->User guide and tutorial->Data structures accepted by seaborn->Options for visualizing long-form data->year = flights_avg.index\npassengers = flights_avg[\"passengers\"]\nsns.relplot(x=year, y=passengers, kind=\"line\")\n",
            "seaborn->API Overview->Data structures accepted by seaborn->Options for visualizing long-form data->year = flights_avg.index\npassengers = flights_avg[\"passengers\"]\nsns.relplot(x=year, y=passengers, kind=\"line\")\n",
            "seaborn->User guide and tutorial->Visualizing categorical data->Categorical scatterplots->tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "seaborn->Plotting functions->Visualizing categorical data->Categorical scatterplots->tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots->tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots->tips = sns.load_dataset(\"tips\")\nsns.relplot(data=tips, x=\"total_bill\", y=\"tip\")\n",
            "seaborn->User guide and tutorial->Controlling figure aesthetics->Removing axes spines->sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n",
            "seaborn->Figure aesthetics->Controlling figure aesthetics->Removing axes spines->sns.set_style(\"whitegrid\")\nsns.boxplot(data=data, palette=\"deep\")\nsns.despine(left=True)\n"
        ]
    },
    {
        "id": "1192209",
        "GT": "Visual exploratory data analysis\n### Weekly average temperature and visibility\nIs there a correlation between temperature and visibility? Let's find out.\n\nIn this exercise, your job is to plot the weekly average temperature and visibility as subplots. To do this, you need to first select the appropriate columns and then resample by week, aggregating the mean.\n\nIn addition to creating the subplots, you will compute the Pearson correlation coefficient using `.corr()`. The Pearson correlation coefficient, known also as Pearson's r, ranges from -1 (indicating total negative linear correlation) to 1 (indicating total positive linear correlation). A value close to 1 here would indicate that there is a strong correlation between temperature and visibility.\n\nThe DataFrame `df_clean` has been pre-loaded for you.\n\n***Instructions***\n*   Import `matplotlib.pyplot` as `plt`.\n*   Select the `'visibility'` and `'dry_bulb_faren'` columns and resample them by week, aggregating the mean. Assign the result to `weekly_mean`.\n*   Print the output of `weekly_mean.corr()`.\n*   Plot the `weekly_mean` dataframe with `.plot()`, specifying `subplots=True`.->Probability of high temperatures\nWe already know that 2011 was hotter than the climate normals for the previous thirty years. In this final exercise, you will compare the maximum temperature in August 2011 against that of the August 2010 climate normals. More specifically, you will use a CDF plot to determine the probability of the 2011 daily maximum temperature in August being above the 2010 climate normal value. To do this, you will leverage the data manipulation, filtering, resampling, and visualization skills you have acquired throughout this course.\n\nThe two DataFrames `df_clean` and `df_climate` are available in the workspace. Your job is to select the maximum temperature in August in `df_climate`, and then maximum daily temperatures in August 2011\\. You will then filter out the days in August 2011 that were above the August 2010 maximum, and use this to construct a CDF plot.\n\nOnce you've generated the CDF, notice how it shows that there was a 50% probability of the 2011 daily maximum temperature in August being 5 degrees above the 2010 climate normal value!\n\n***Instructions***\n*   From `df_climate`, extract the maximum temperature observed in August 2010\\. The relevant column here is `'Temperature'`. You can select the rows corresponding to August 2010 in multiple ways. For example, `df_climate.loc['2011-Feb']` selects all rows corresponding to February 2011, while `df_climate.loc['2009-09', 'Pressure']` selects the rows corresponding to September 2009 from the `'Pressure'` column.\n*   From `df_clean`, select the August 2011 temperature data from the `'dry_bulb_faren'`. Resample this data by day and aggregate the maximum value. Store the result in `august_2011`.\n*   Filter out days in `august_2011` where the value exceeded `august_max`. Store the result in `august_2011_high`.\n*   Construct a CDF of `august_2011_high` using 25 bins. Remember to specify the `kind`, `normed`, and `cumulative` parameters in addition to `bins`.-># Extract the maximum temperature in August 2010 from df_climate: august_max\naugust_max = df_climate.loc['2010-Aug', 'Temperature'].max()\nprint(august_max)\n\n# Resample the August 2011 temperatures in df_clean by day and aggregate the maximum value: august_2011\naugust_2011 = df_clean.loc['2011-Aug', 'dry_bulb_faren'].resample('D').max()\n\n# Filter out days in august_2011 where the value exceeded august_max: august_2011_high\naugust_2011_high = august_2011[august_2011 > august_max]\n\n# Construct a CDF of august_2011_high\naugust_2011_high.plot(kind = 'hist', bins = 25, normed = True, cumulative = True)\n\n# Display the plot\nplt.show()",
        "pred": [
            "statsmodels->User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Output and postestimation methods and attributes->Applying estimated parameters to an updated or different dataset-># Load in the example macroeconomic dataset\ndta = sm.datasets.macrodata.load_pandas().data\n# Make sure we have an index with an associated frequency, so that\n# we can refer to time periods with date strings or timestamps\ndta.index = pd.date_range('1959Q1', '2009Q3', freq='QS')\n\n# Separate inflation data into a training and test dataset\ntraining_endog = dta['infl'].iloc[:-1]\ntest_endog = dta['infl'].iloc[-1:]\n\n# Fit an SARIMAX model for inflation\ntraining_model = sm.tsa.SARIMAX(training_endog, order=(4, 0, 0))\ntraining_results = training_model.fit()\n\n# Extend the results to the test observations\ntest_results = training_results.extend(test_endog)\n\n# Print the sum of squared errors in the test sample,\n# based on parameters computed using only the training sample\nprint(test_results.sse)",
            "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Hamilton (1989) switching model of GNP-># Get the RGNP data to replicate Hamilton\ndta = pd.read_stata(\"https://www.stata-press.com/data/r14/rgnp.dta\").iloc[1:]\ndta.index = pd.DatetimeIndex(dta.date, freq=\"QS\")\ndta_hamilton = dta.rgnp\n\n# Plot the data\ndta_hamilton.plot(title=\"Growth rate of Real GNP\", figsize=(12, 3))\n\n# Fit the model\nmod_hamilton = sm.tsa.MarkovAutoregression(\n    dta_hamilton, k_regimes=2, order=4, switching_ar=False\n)\nres_hamilton = mod_hamilton.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_4_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_4_0.png\"/>",
            "statsmodels->Examples->State space models->State space modeling: Local Linear Trends-># Load Dataset\ndf.index = pd.date_range(start='%d-01-01' % df.date[0], end='%d-01-01' % df.iloc[-1, 0], freq='AS')\n\n# Log transform\ndf['lff'] = np.log(df['ff'])\n\n# Setup the model\nmod = LocalLinearTrend(df['lff'])\n\n# Fit it using MLE (recall that we are fitting the three variance parameters)\nres = mod.fit(disp=False)\nprint(res.summary())",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend-># Setup forecasts\nnforecasts = 3\nforecasts = {}\n\n# Get the number of initial training observations\nnobs = len(endog)\nn_init_training = int(nobs * 0.8)\n\n# Create model for initial training sample, fit parameters\ninit_training_endog = endog.iloc[:n_init_training]\nmod = sm.tsa.SARIMAX(training_endog, order=(1, 0, 0), trend='c')\nres = mod.fit()\n\n# Save initial forecast\nforecasts[training_endog.index[-1]] = res.forecast(steps=nforecasts)\n\n# Step through the rest of the sample\nfor t in range(n_init_training, nobs):\n    # Update the results by appending the next observation\n    updated_endog = endog.iloc[t:t+1]\n    res = res.extend(updated_endog)\n\n    # Save the new set of forecasts\n    forecasts[updated_endog.index[0]] = res.forecast(steps=nforecasts)\n\n# Combine all forecasts into a dataframe\nforecasts = pd.concat(forecasts, axis=1)\n\nprint(forecasts.iloc[:5, :5])",
            "statsmodels->Examples->Time Series Analysis->Markov switching autoregression->Kim, Nelson, and Startz (1998) Three-state Variance Switching-># Get the dataset\new_excs = requests.get(\"http://econ.korea.ac.kr/~cjkim/MARKOV/data/ew_excs.prn\").content\nraw = pd.read_table(BytesIO(ew_excs), header=None, skipfooter=1, engine=\"python\")\nraw.index = pd.date_range(\"1926-01-01\", \"1995-12-01\", freq=\"MS\")\n\ndta_kns = raw.loc[:\"1986\"] - raw.loc[:\"1986\"].mean()\n\n# Plot the dataset\ndta_kns[0].plot(title=\"Excess returns\", figsize=(12, 3))\n\n# Fit the model\nmod_kns = sm.tsa.MarkovRegression(\n    dta_kns, k_regimes=3, trend=\"n\", switching_variance=True\n)\nres_kns = mod_kns.fit()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\" src=\"../../../_images/examples_notebooks_generated_markov_autoregression_12_0.png\"/>"
        ]
    },
    {
        "id": "453480",
        "GT": "Wins per team->team_wins = df.groupby('WINNING_TEAM').count()['YEAR'].to_frame().reset_index()\nteam_wins.columns = ['team', 'wins']\nteam_wins",
        "pred": [
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures->summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first = df.groupby('airline_id')[['fl_date', 'unique_carrier']].first()\nfirst.head()",
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->daily = df.fl_date.value_counts().sort_index()\ny = daily.resample('MS').mean()\ny.head()",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()"
        ]
    },
    {
        "id": "593940",
        "GT": "Data Exploration and Pre-processing->from pandas.plotting import scatter_matrix\nfig = plt.figure()\nscatter_matrix(train_X, alpha=0.2, figsize=(24, 24), diagonal='kde')\nplt.show()",
        "pred": [
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.kdeplot, lw=3, legend=False)\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Plotting pairwise data relationships->g = sns.PairGrid(iris)\ng.map_upper(sns.scatterplot)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.kdeplot, lw=3, legend=False)\n",
            "sklearn->Examples->Neural Networks->Restricted Boltzmann Machine features for digit classification->Evaluation->from sklearn import metrics\n\nY_pred = rbm_features_classifier.predict(X_test)\nprint(\n    \"Logistic regression using RBM features:\\n%s\\n\"\n    % ((Y_test, Y_pred))\n)",
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)",
            "statsmodels->Examples->Time Series Analysis->Deterministic Terms->Model Support->Simulate Some Data->from statsmodels.tsa.api import AutoReg\n\nmod = AutoReg(y, 1, trend=\"n\", deterministic=det_proc)\nres = mod.fit()\nprint(res.summary())",
            "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding->Backward Difference Coding->from patsy.contrasts import Diff\n\ncontrast = Diff().code_without_intercept(levels)\nprint(contrast.matrix)"
        ]
    },
    {
        "id": "797747",
        "GT": "Palaeovalley_NDVI_linear_regression->Set up the AGDC query-># Choose your site for this example\n# Here we will select our testing site, so that we can demonstrate the code works without waiting for a full area to run\nnum = 48\nStudysite = names.ix[num]\nprint(Studysite)",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Fit the model\nmod = sm.tsa.statespace.SARIMAX(data['ln_wpi'], trend='c', order=(1,1,1))\nres = mod.fit(disp=False)\nprint(res.summary())",
            "seaborn->User guide and tutorial->An introduction to seaborn-># Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Basic example->Constructing and estimating the model-># Construct the model\nmod = sm.tsa.SARIMAX(endog, order=(1, 0, 0), trend='c')\n# Estimate the parameters\nres = mod.fit()\n\nprint(res.summary())",
            "statsmodels->Examples->User Notes->Contrasts->Treatment (Dummy) Coding->from patsy.contrasts import Treatment\n\nlevels = [1, 2, 3, 4]\ncontrast = Treatment(reference=0).code_without_intercept(levels)\nprint(contrast.matrix)",
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing->ax = oildata.plot()\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Oil (millions of tonnes)\")\nprint(\"Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\")"
        ]
    },
    {
        "id": "1151276",
        "GT": "San Francisco County->### San Francisco County\n\nsf_train_log = np.log(sf_train)\n\nsf_train_resid = decomposing(sf_train_log)\n\nsf_train_p = 2\nsf_train_q = 2\n\nsf_model_fit = arima_summary(sf_train_log, sf_train_resid, sf_train_p, 2, sf_train_q)",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Example 2: Arima with additive seasonal effects-># Dataset\ndata = pd.read_stata(BytesIO(wpi1))\ndata.index = data.t\ndata['ln_wpi'] = np.log(data['wpi'])\ndata['D.ln_wpi'] = data['ln_wpi'].diff()\n\n\n\n\n\n\n\nIn\u00a0[5]:",
            "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Transformer Block With a Novel Normalization-># Profile scripted rms_norm\n = (with_rms_norm)\nfunc = functools.partial(\n    ,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"TorchScript - RMS Norm\")",
            "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->Transformer Block With a Novel Normalization-># Profile rms_norm\nfunc = functools.partial(\n    with_rms_norm,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"Eager Mode - RMS Norm\")",
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Bayesian estimation via MCMC-># Prior hyperparameters\n\n# Prior for obs. cov. is inverse-Wishart(v_1^0=k + 3, S10=I)\nv10 = mod.k_endog + 3\nS10 = np.eye(mod.k_endog)\n\n# Prior for state cov. variances is inverse-Gamma(v_{i2}^0 / 2 = 3, S+{i2}^0 / 2 = 0.005)\nvi20 = 6\nSi20 = 0.01",
            "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets-># Set up cluster parameters\n(figsize=(9 * 1.3 + 2, 14.5))\n(\n    left=0.02, right=0.98, bottom=0.001, top=0.96, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\"n_neighbors\": 10, \"n_clusters\": 3}\n\n = [\n    (noisy_circles, {\"n_clusters\": 2}),\n    (noisy_moons, {\"n_clusters\": 2}),\n    (varied, {\"n_neighbors\": 2}),\n    (aniso, {\"n_neighbors\": 2}),\n    (blobs, {}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate():\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = ().fit_transform(X)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ward = (\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\"\n    )\n    complete = (\n        n_clusters=params[\"n_clusters\"], linkage=\"complete\"\n    )\n    average = (\n        n_clusters=params[\"n_clusters\"], linkage=\"average\"\n    )\n    single = (\n        n_clusters=params[\"n_clusters\"], linkage=\"single\"\n    )\n\n    clustering_algorithms = (\n        (\"Single Linkage\", single),\n        (\"Average Linkage\", average),\n        (\"Complete Linkage\", complete),\n        (\"Ward Linkage\", ward),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = ()\n\n        # catch warnings related to kneighbors_graph\n        with ():\n            (\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \"  1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = ()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        (len(), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            (name, size=18)\n\n        colors = (\n            list(\n                (\n                    (\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        (X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        (-2.5, 2.5)\n        (-2.5, 2.5)\n        (())\n        (())\n        (\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=().transAxes,\n            size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\n()\n\n\n<img alt=\"Single Linkage, Average Linkage, Complete Linkage, Ward Linkage\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\" srcset=\"../../_images/sphx_glr_plot_linkage_comparison_001.png\"/>"
        ]
    },
    {
        "id": "621120",
        "GT": "salary.rename(columns = {'2016-17':'Salary'}, inplace = True)\nclean_salaries = salary[['Player','Salary']]\nclean_salaries.head()\nclean_salaries.tail()",
        "pred": [
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "pandas_toms_blog->Fast Pandas->Iteration, Apply, And Vectorization->df = pd.read_csv(\"data/347136217_T_ONTIME.csv\")\ndelays = df['DEP_DELAY']",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->sel = ar_select_order(housing, 13, old_names=False)\nsel.ar_lags\nres = sel.model.fit()\nprint(res.summary())",
            "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.2->Faster parser in->X, y = (\n    \"titanic\", version=1, as_frame=True, return_X_y=True, parser=\"pandas\"\n)\nX.head()\n\n\n\n\n\n\n\n\n<br/>\n<br/>",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->columns = list(map(str, range(1960, 2012)))\ndata.set_index(\"Country Name\", inplace=True)\ndta = data[columns]\ndta = dta.dropna()\ndta.head()"
        ]
    },
    {
        "id": "1439804",
        "GT": "Step by Step Calculation with Walk Network->Run query->results = net.aggregate(1000, type='sum', decay='linear')\nprint('ok')",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Autoregressions->res = mod.fit(cov_type=\"HC0\")\nprint(res.summary())",
            "statsmodels->Examples->Forecasting->Forecasting using the Theta Model->Personal Consumption Expenditure->mod = ThetaModel(np.log(pce))\nres = mod.fit()\nprint(res.summary())",
            "statsmodels->Examples->Linear Regression Models->Recursive Least Squares->Example 1: Copper->mod = sm.RecursiveLS(endog, exog)\nres = mod.fit()\n\nprint(res.summary())",
            "seaborn->User guide and tutorial->Visualizing categorical data->Categorical scatterplots->tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "seaborn->Plotting functions->Visualizing categorical data->Categorical scatterplots->tips = sns.load_dataset(\"tips\")\nsns.catplot(data=tips, x=\"day\", y=\"total_bill\")\n",
            "numpy->NumPy Applications->Determining Moore\u2019s Law with real data in NumPy->Calculating the historical growth curve for transistors->results = model.fit()\nprint(results.summary())"
        ]
    },
    {
        "id": "995072",
        "GT": "Manually Calculating These Terms to Gain Better Understanding of Results->Accuracy\nThe accuracy metric can be constructed using the components of the confusion matrix. With the total population as:->tp = np.sum((y_test == 1) & (predicted == 1))\nfp = np.sum((y_test == 0) & (predicted == 1))\ntn = np.sum((y_test == 0) & (predicted == 0))\nfn = np.sum((y_test == 1) & (predicted == 0))\nprint(tn, fp, fn, tp)",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Nested analysis->oo = np.ones(df.shape[0])\nmodel2 = sm.MixedLM(df.y, oo, exog_re=oo, groups=df.group1, exog_vc=vcs)\nresult2 = model2.fit()\nprint(result2.summary())",
            "statsmodels->Examples->Linear Regression Models->Variance Component Analysis->Crossed analysis->oo = np.ones(df.shape[0])\nmodel4 = sm.MixedLM(df.y, oo[:, None], exog_re=None, groups=oo, exog_vc=vcs)\nresult4 = model4.fit()\nprint(result4.summary())",
            "sklearn->Examples->Model Selection->Statistical comparison of models using grid search->Comparing two models: Bayesian approach-># initialize random variable\nt_post = (\n    df, loc=(differences), scale=corrected_std(differences, n_train, n_test)\n)",
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Replicate fixed effect analysis using GLM with var_weights->weights = 1 / var_eff\nmod_glm = GLM(eff, np.ones(len(eff)), var_weights=weights)\nres_glm = mod_glm.fit(scale=1.0)\nprint(res_glm.summary().tables[1])",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->def core(df, \u03b1=.05):\n    mask = (df &gt; df.quantile(\u03b1)).all(1) &amp; (df &lt; df.quantile(1 - \u03b1)).all(1)\n    return df[mask]"
        ]
    },
    {
        "id": "1354219",
        "GT": "Perceptron with Cross validation->perceptron = Perceptron()\n\nscores = cross_val_score(perceptron, predictors, target, cv=10)\nprint(scores.mean())\n\ny_pred1=cross_val_predict(perceptron,predictors,target,cv=10)",
        "pred": [
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-based cross-validation->all_splits = list(ts_cv.split(X, y))\ntrain_0, test_0 = all_splits[0]",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS non-linear curve but linear in parameters->res = sm.OLS(y, X).fit()\nprint(res.summary())",
            "statsmodels->Examples->Time Series Analysis->Autoregressions->res = mod.fit(cov_type=\"HC0\")\nprint(res.summary())",
            "sklearn->Examples->Ensemble methods->Monotonic Constraints->gbdt_with_monotonic_cst = (monotonic_cst=[1, -1])\ngbdt_with_monotonic_cst.fit(X, y)",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->Generalized Linear Model Example->Aside: Binomial distribution->means25 = exog.mean()\nprint(means25)"
        ]
    },
    {
        "id": "156032",
        "GT": "Building the Recommeder Engines->rating_train, rating_test = train_test_split(relevent_ratings_df, test_size = 0.2, random_state = 42) ",
        "pred": [
            "sklearn->Examples->Pipelines and composite estimators->Effect of transforming the targets in regression model->Real-world data set->X_train, X_test, y_train, y_test = (X, y, random_state=1)",
            "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->X_train, X_test, y_train, y_test = (X, y, random_state=0)",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Comparison->results_all = [res_o, res_f, res_e, res_a]\nnames = \"res_o res_f res_e res_a\".split()",
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets->train_df = textproc.txt_to_df(imdb_train)\ntest_df = textproc.txt_to_df(imdb_test)",
            "torch->Frontend APIs->Per-sample-gradients->Per-sample-grads, the efficient way, using function transforms->ft_per_sample_grads = ft_compute_sample_grad(params, buffers, , )"
        ]
    },
    {
        "id": "1314247",
        "GT": "By Gender:->Pearson's test:->#Becuase Pearson's test and Spearmanr's test need to two variables, and these two variables have the same length.\n#Generate the new variabl with the same length:\nnewM_1 = np.random.choice(newM,len(newF),replace=False,p=None)",
        "pred": [
            "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard-># 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)",
            "torch->Text->NLP From Scratch: Classifying Names with a Character-Level RNN->Evaluating the Results-># Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
            "numpy->NumPy Applications->X-ray image processing->Apply masks to X-rays with np.where()-># The threshold is \"greater than 150\"\n# Return the original image if true, `0` otherwise\nxray_image_mask_noisy = np.where(xray_image &gt; 150, xray_image, 0)\n\nplt.imshow(xray_image_mask_noisy, cmap=\"gray\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n<img alt=\"../_images/976c52ef7b0c1a1d25ff407f99a98e8290bc74a7eebe2564e88373b58fc3cee1.png\" src=\"../_images/976c52ef7b0c1a1d25ff407f99a98e8290bc74a7eebe2564e88373b58fc3cee1.png\"/>",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 2: computing the \u201cnews\u201d from a new observation-># Get the next observation after the \"pre\" dataset\ny_update = y.iloc[-5:-4]\n\n# Print the forecast error\nprint('Forecast error: %.2f' % (y_update.iloc[0] - forecasts_pre.iloc[0]))",
            "statsmodels->Examples->Generalized Estimating Equations->GEE Score Tests-># The coefficients used to define the linear predictors\ncoeff = [[4, 0.4, -0.2], [4, 0.4, -0.2, 0, -0.04]]\n\n# The linear predictors\nlp = [np.dot(x0, coeff[0]), np.dot(x, coeff[1])]\n\n# The mean values\nmu = [np.exp(lp[0]), np.exp(lp[1])]"
        ]
    },
    {
        "id": "23861",
        "GT": "Make a Leaflet map with Jupyter, Pandas, and geoJson->A basic map\n    Is this thing on?-># what long, lat pair should the map be on?\ncenter = [-37, 145]\n\n# what zoom level (rough equiv. to google) should the map be at?\nzoom = 7\n\n# make map\ndemo_map = Map(\n    center=center,\n    zoom=zoom\n)\n\n# print map\ndemo_map",
        "pred": [
            "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds-># range of admissible distortions\neps_range = (0.01, 0.99, 100)\n\n# range of number of samples (observation) to embed\nn_samples_range = (2, 6, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(n_samples_range)))\n\n()\nfor n_samples, color in zip(n_samples_range, colors):\n    min_n_components = (n_samples, eps=eps_range)\n    (eps_range, min_n_components, color=color)\n\n([f\"n_samples = {n}\" for n in n_samples_range], loc=\"upper right\")\n(\"Distortion eps\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_components vs eps\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_components vs eps\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_002.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_002.png\"/>",
            "torch->PyTorch Recipes->Dynamic Quantization->Steps->5: Look at Accuracy-># run the float model\nout1, hidden1 = float_lstm(inputs, hidden)\nmag1 = (abs(out1)).item()\nprint('mean absolute value of output tensor values in the FP32 model is {0:.5f} '.format(mag1))\n\n# run the quantized model\nout2, hidden2 = quantized_lstm(inputs, hidden)\nmag2 = (abs(out2)).item()\nprint('mean absolute value of output tensor values in the INT8 model is {0:.5f}'.format(mag2))\n\n# compare them\nmag3 = (abs(out1-out2)).item()\nprint('mean absolute value of the difference between the output tensors is {0:.5f} or {1:.2f} percent'.format(mag3,mag3/mag1*100))",
            "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds-># range of admissible distortions\neps_range = (0.1, 0.99, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = (1, 9, 9)\n\n()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = (n_samples_range, eps=eps)\n    (n_samples_range, min_n_components, color=color)\n\n([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n(\"Number of observations to eps-embed\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_samples vs n_components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\"/>",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Simple univariate example: AR(1)->Step 1: fitting the model on the available dataset-># Get the estimated AR(1) coefficient\nphi_hat = res_pre.params[0]\n\n# Get the last observed value of the variable\ny_T = y_pre.iloc[-1]\n\n# Directly compute the forecasts at the horizons h=1,2,3,4\nmanual_forecasts = pd.Series([phi_hat * y_T, phi_hat**2 * y_T,\n                              phi_hat**3 * y_T, phi_hat**4 * y_T],\n                             index=forecasts_pre.index)\n\n# We'll print the two to double-check that they're the same\nprint(pd.concat([forecasts_pre, manual_forecasts], axis=1))",
            "statsmodels->Examples->State space models - Technical notes->State space models: Chandrasekhar recursions->Practical example-># Model that will apply Kalman filter recursions\nmod_kf = sm.tsa.SARIMAX(inf_apparel, order=(6, 0, 0), seasonal_order=(15, 0, 0, 12), tolerance=0)\nprint(mod_kf.k_states)\n\n# Model that will apply Chandrasekhar recursions\nmod_ch = sm.tsa.SARIMAX(inf_apparel, order=(6, 0, 0), seasonal_order=(15, 0, 0, 12), tolerance=0)\nmod_ch.ssm.filter_chandrasekhar = True"
        ]
    },
    {
        "id": "329950",
        "GT": "4. Compute and Plot Basic Statistics of the Data\n\nIn the following code cells I will use pandas and numpy to compute and plot basic statistics of the dataset.-># Use pandas .values operator to load the values of relative humidity (the 'RH' column)\n# form the pandas dataframe into a numpy array\nRH = df['RH'].values\n\n# Compute the mean using numpy\nnp.mean(RH, axis=0)",
        "pred": [
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Create the news results\n# Note\nconst_post_plus1 = np.ones(len(y_post) + 1)\nnews = res_pre.news(y_post, exog=const_post_plus1, start='2017-03', end='2017-04')",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example-># Construct the forecast errors\nforecast_errors = forecasts.apply(lambda column: endog - column).reindex(forecasts.index)\n\nprint(forecast_errors.iloc[:5, :5])",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Cross validation->Example->Using extend-># Construct the forecast errors\nforecast_errors = forecasts.apply(lambda column: endog - column).reindex(forecasts.index)\n\nprint(forecast_errors.iloc[:5, :5])",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># In-sample one-step-ahead predictions\npredict = res.get_prediction()\npredict_ci = predict.conf_int()",
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models-># Retrieve the posterior means\nparams = pm.summary(trace_uc)[\"mean\"].values\n\n# Construct results using these posterior means as parameter values\nres_uc_bayes = mod_uc.smooth(params)",
            "torch->Image and Video->Adversarial Example Generation->Implementation->Run Attack->accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)"
        ]
    },
    {
        "id": "646955",
        "GT": "Decison Trees->from IPython.display import Image  \nfrom sklearn.externals.six import StringIO  \nimport pydotplus \n\ndot_data = StringIO()  \ntree.export_graphviz(clf, \n                     out_file=dot_data,  \n                     feature_names=features)  \ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_pdf())  ",
        "pred": [
            "sklearn->Examples->Clustering->Spectral clustering for image segmentation->Plotting four circles->from sklearn.cluster import \nimport matplotlib.pyplot as plt\n\nlabels = (graph, n_clusters=4, eigen_solver=\"arpack\")\nlabel_im = (mask.shape, -1.0)\nlabel_im[mask] = labels\n\nfig, axs = (nrows=1, ncols=2, figsize=(10, 5))\naxs[0].matshow(img)\naxs[1].matshow(label_im)\n\n()\n\n\n<img alt=\"plot segmentation toy\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\" srcset=\"../../_images/sphx_glr_plot_segmentation_toy_001.png\"/>",
            "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Tuning the hyper-parameters of the quantile regressors->from sklearn.base import clone\n\nalpha = 0.95\nneg_mean_pinball_loss_95p_scorer = (\n    ,\n    alpha=alpha,\n    greater_is_better=False,  # maximize the negative loss\n)\nsearch_95p = clone(search_05p).set_params(\n    estimator__alpha=alpha,\n    scoring=neg_mean_pinball_loss_95p_scorer,\n)\nsearch_95p.fit(X_train, y_train)\n(search_95p.best_params_)",
            "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Model comparison->from sklearn.model_selection import \nimport matplotlib.pyplot as plt\n\nscoring = \"neg_mean_absolute_percentage_error\"\nn_cv_folds = 3\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\n\ndef plot_results(figure_title):\n    fig, (ax1, ax2) = (1, 2, figsize=(12, 8))\n\n    plot_info = [\n        (\"fit_time\", \"Fit times (s)\", ax1, None),\n        (\"test_score\", \"Mean Absolute Percentage Error\", ax2, None),\n    ]\n\n    x, width = (4), 0.9\n    for key, title, ax, y_limit in plot_info:\n        items = [\n            dropped_result[key],\n            one_hot_result[key],\n            ordinal_result[key],\n            native_result[key],\n        ]\n\n        mape_cv_mean = [(np.abs(item)) for item in items]\n        mape_cv_std = [(item) for item in items]\n\n        ax.bar(\n            x=x,\n            height=mape_cv_mean,\n            width=width,\n            yerr=mape_cv_std,\n            color=[\"C0\", \"C1\", \"C2\", \"C3\"],\n        )\n        ax.set(\n            xlabel=\"Model\",\n            title=title,\n            xticks=x,\n            xticklabels=[\"Dropped\", \"One Hot\", \"Ordinal\", \"Native\"],\n            ylim=y_limit,\n        )\n    fig.suptitle(figure_title)\n\n\nplot_results(\"Gradient Boosting on Ames Housing\")\n\n\n<img alt=\"Gradient Boosting on Ames Housing, Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_001.png\"/>",
            "sklearn->Examples->Gaussian Process for Machine Learning->Comparison of kernel ridge and Gaussian process regression->Kernel methods: kernel ridge and Gaussian process->Kernel ridge->from sklearn.model_selection import \nfrom sklearn.utils.fixes import loguniform\n\nparam_distributions = {\n    \"alpha\": loguniform(1e0, 1e3),\n    \"kernel__length_scale\": loguniform(1e-2, 1e2),\n    \"kernel__periodicity\": loguniform(1e0, 1e1),\n}\nkernel_ridge_tuned = (\n    kernel_ridge,\n    param_distributions=param_distributions,\n    n_iter=500,\n    random_state=0,\n)\nstart_time = ()\nkernel_ridge_tuned.fit(training_data, training_noisy_target)\nprint(f\"Time for KernelRidge fitting: {() - start_time:.3f} seconds\")",
            "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.1->BisectingKMeans: divide and cluster->from sklearn.datasets import \nfrom sklearn.cluster import , \nimport matplotlib.pyplot as plt\n\nX, _ = (n_samples=1000, centers=2, random_state=0)\n\nkm = (n_clusters=5, random_state=0, n_init=\"auto\").fit(X)\nbisect_km = (n_clusters=5, random_state=0).fit(X)\n\nfig, ax = (1, 2, figsize=(10, 5))\nax[0].scatter(X[:, 0], X[:, 1], s=10, c=km.labels_)\nax[0].scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], s=20, c=\"r\")\nax[0].set_title(\"KMeans\")\n\nax[1].scatter(X[:, 0], X[:, 1], s=10, c=bisect_km.labels_)\nax[1].scatter(\n    bisect_km.cluster_centers_[:, 0], bisect_km.cluster_centers_[:, 1], s=20, c=\"r\"\n)\n_ = ax[1].set_title(\"BisectingKMeans\")\n\n\n<img alt=\"KMeans, BisectingKMeans\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_release_highlights_1_1_0_003.png\" srcset=\"../../_images/sphx_glr_plot_release_highlights_1_1_0_003.png\"/>"
        ]
    },
    {
        "id": "745719",
        "GT": "9B. Concat. result with test and show confidence with undersampled data.->features_sub2 = df_undersampling.iloc[:, 1:].values\ntarget_sub2 = df_undersampling.iloc[:, 0:1]\n\nss = StandardScaler()\nfeaturesS_sub2 = ss.fit_transform(features_sub2)\n\npredictions_RF_model = forest.predict(featuresS_sub2)\n\npredictions_percentage2 = forest.predict_proba(featuresS_sub2)[:, 0]",
        "pred": [
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Plotting with categorical variables->names = ['group_a', 'group_b', 'group_c']\nvalues = [1, 10, 100]\n\nplt.figure(figsize=(9, 3))\n\nplt.subplot(131)\nplt.bar(names, values)\nplt.subplot(132)\nplt.scatter(names, values)\nplt.subplot(133)\nplt.plot(names, values)\nplt.suptitle('Categorical Plotting')\nplt.show()\n\n\n<img alt=\"Categorical Plotting\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_006.png\" srcset=\"../../_images/sphx_glr_pyplot_006.png, ../../_images/sphx_glr_pyplot_006_2_0x.png 2.0x\"/>",
            "sklearn->Examples->Inspection->Failure of Machine Learning to infer causal effects->Income prediction with partial observations->features_names = [\"experience\", \"parent hourly wage\", \"college degree\"]\n\nregressor_without_ability = ()\nregressor_without_ability.fit(X_train[features_names], y_train)\ny_pred_without_ability = regressor_without_ability.predict(X_test[features_names])\nR2_without_ability = (y_test, y_pred_without_ability)\n\nprint(f\"R2 score without ability: {R2_without_ability:.3f}\")",
            "sklearn->Examples->Decision Trees->Understanding the decision tree structure->Decision path->node_indicator = clf.decision_path(X_test)\nleaf_id = clf.apply(X_test)\n\nsample_id = 0\n# obtain ids of the nodes `sample_id` goes through, i.e., row `sample_id`\nnode_index = node_indicator.indices[\n    node_indicator.indptr[sample_id] : node_indicator.indptr[sample_id + 1]\n]\n\nprint(\"Rules used to predict sample {id}:\\n\".format(id=sample_id))\nfor node_id in node_index:\n    # continue to the next node if it is a leaf node\n    if leaf_id[sample_id] == node_id:\n        continue\n\n    # check if value of the split feature for sample 0 is below threshold\n    if X_test[sample_id, feature[node_id]] &lt;= threshold[node_id]:\n        threshold_sign = \"&lt;=\"\n    else:\n        threshold_sign = \"\"\n\n    print(\n        \"decision node {node} : (X_test[{sample}, {feature}] = {value}) \"\n        \"{inequality} {threshold})\".format(\n            node=node_id,\n            sample=sample_id,\n            feature=feature[node_id],\n            value=X_test[sample_id, feature[node_id]],\n            inequality=threshold_sign,\n            threshold=threshold[node_id],\n        )\n    )",
            "sklearn->Examples->Decision Trees->Post pruning decision trees with cost complexity pruning->Accuracy vs alpha for training and testing sets->train_scores = [clf.score(X_train, y_train) for clf in clfs]\ntest_scores = [clf.score(X_test, y_test) for clf in clfs]\n\nfig, ax = ()\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker=\"o\", label=\"train\", drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker=\"o\", label=\"test\", drawstyle=\"steps-post\")\nax.legend()\n()\n\n\n<img alt=\"Accuracy vs alpha for training and testing sets\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\" srcset=\"../../_images/sphx_glr_plot_cost_complexity_pruning_003.png\"/>",
            "torch->PyTorch Recipes->How to use TensorBoard with PyTorch->Log scalars->x = (-5, 5, 0.1).view(-1, 1)\ny = -5 * x + 0.1 * (x.size())\n\nmodel = (1, 1)\ncriterion = ()\noptimizer = (model.parameters(), lr = 0.1)\n\ndef train_model(iter):\n    for epoch in range(iter):\n        y1 = model(x)\n        loss = criterion(y1, y)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ntrain_model(10)\nwriter.flush()"
        ]
    },
    {
        "id": "735119",
        "GT": "Filter out background based on UMIs/cell->First, plot histograms and tweak thresholds  \nNo filtering actually done here->sample_name",
        "pred": [
            "pandas_toms_blog->Scaling->Dask->by_occupation",
            "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types->model score: 0.798",
            "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types->model score: 0.798",
            "numpy->NumPy Features->Masked Arrays->Missing data->nbcases_ma",
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time",
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->Using one-step chi2, DerSimonian-Laird estimate for random effects variance tau->res3.cache_ci"
        ]
    },
    {
        "id": "1036708",
        "GT": "Relatives->def relatives_count(row): return row['SibSp'] + row['Parch']\n\ntrain_df['Relatives'] = train_df.apply(relatives_count, axis=1)\ntest_df['Relatives'] = test_df.apply(relatives_count, axis=1)",
        "pred": [
            "torch->Learning PyTorch->What is torch.nn really?->Using your GPU->def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(), y.to()\n\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
            "numpy->NumPy Applications->Analyzing the impact of the lockdown on air quality in Delhi, India->Paired Student\u2019s t-test on the AQIs->Calculating the test statistics->def t_test(x, y):\n    diff = y - x\n    var = np.var(diff, ddof=1)\n    num = np.mean(diff)\n    denom = np.sqrt(var / len(x))\n    return np.divide(num, denom)\n\nt_value = t_test(before_sample, after_sample)",
            "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Calibration of the confidence interval->def coverage_fraction(y, y_low, y_high):\n    return ((y = y_low, y &lt;= y_high))\n\n\ncoverage_fraction(\n    y_train,\n    all_models[\"q 0.05\"].predict(X_train),\n    all_models[\"q 0.95\"].predict(X_train),\n)",
            "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Impute missing values with mean->def get_impute_mean(X_missing, y_missing):\n    imputer = (missing_values=, strategy=\"mean\", add_indicator=True)\n    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return mean_impute_scores.mean(), mean_impute_scores.std()\n\n\nmses_california[3], stds_california[3] = get_impute_mean(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes, y_miss_diabetes)\nx_labels.append(\"Mean Imputation\")",
            "statsmodels->Examples->Generalized Linear Models->Using Formulas with GLMs->def double_it(x):\n    return 2 * x\n\n\nformula = \"SUCCESS ~ double_it(LOWINC) + PERASIAN + PERBLACK + PERHISP + PCTCHRT + \\\n           PCTYRRND + PERMINTE*AVYRSEXP*AVSALK + PERSPENK*PTRATIO*PCTAF\"\nmod2 = smf.glm(formula=formula, data=dta, family=sm.families.Binomial()).fit()\nprint(mod2.summary())"
        ]
    },
    {
        "id": "280085",
        "GT": "## CODE CELL 24\n\ndf.info()",
        "pred": [
            "seaborn->User guide and tutorial->An introduction to seaborn-># Import seaborn\nimport seaborn as sns\n",
            "seaborn->User guide and tutorial->An introduction to seaborn-># Apply the default theme\nsns.set_theme()\n",
            "statsmodels->Examples->State space models->Forecasting, updating datasets, and the \u201cnews\u201d->Multivariate example: dynamic factor-># Show the summary of the news results\nprint(news.summary())",
            "torch->PyTorch Recipes->Saving and loading models across devices in PyTorch->Steps->6. Saving torch.nn.DataParallel Models-># Save\n(net.module.state_dict(), PATH)\n\n# Load to whatever device you want",
            "seaborn->User guide and tutorial->An introduction to seaborn-># Load an example dataset\ntips = sns.load_dataset(\"tips\")\n"
        ]
    },
    {
        "id": "872298",
        "GT": "**Linear Regression Model** ##->predsTest = lModel.predict(X=dataTest)\nfig,(ax1,ax2)= plt.subplots(ncols=2)\nfig.set_size_inches(20,5)\nsn.distplot(yLabels,ax=ax1,bins=100)\nsn.distplot(np.exp(predsTest),ax=ax2,bins=100)\nax1.set(title=\"Training Set Distribution\")\nax2.set(title=\"Test Set Distribution\")",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS with dummy variables->pred_ols2 = res2.get_prediction()\niv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\niv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.plot(x, y, \"o\", label=\"Data\")\nax.plot(x, y_true, \"b-\", label=\"True\")\nax.plot(x, res2.fittedvalues, \"r--.\", label=\"Predicted\")\nax.plot(x, iv_u, \"r--\")\nax.plot(x, iv_l, \"r--\")\nlegend = ax.legend(loc=\"best\")\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_ols_26_0.png\" src=\"../../../_images/examples_notebooks_generated_ols_26_0.png\"/>",
            "sklearn->Examples->Ensemble methods->Gradient Boosting regression->Plot feature importance->feature_importance = reg.feature_importances_\nsorted_idx = (feature_importance)\npos = (sorted_idx.shape[0]) + 0.5\nfig = (figsize=(12, 6))\n(1, 2, 1)\n(pos, feature_importance[sorted_idx], align=\"center\")\n(pos, (diabetes.feature_names)[sorted_idx])\n(\"Feature Importance (MDI)\")\n\nresult = (\n    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n)\nsorted_idx = result.importances_mean.argsort()\n(1, 2, 2)\n(\n    result.importances[sorted_idx].T,\n    vert=False,\n    labels=(diabetes.feature_names)[sorted_idx],\n)\n(\"Permutation Importance (test set)\")\nfig.tight_layout()\n()\n\n\n<img alt=\"Feature Importance (MDI), Permutation Importance (test set)\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_regression_002.png\"/>",
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->changing data to have positive random effects variance->res5 = combine_effects(eff, var_eff, method_re=\"chi2\", use_t=False)\nres5_df = res5.summary_frame()\nprint(\"method RE:\", res5.method_re)\nprint(res5.summary_frame())\nfig = res5.plot_forest()\nfig.set_figheight(8)\nfig.set_figwidth(6)",
            "statsmodels->Examples->Statistics->ANOVA->resid = lm.resid\nplt.figure(figsize=(6, 6))\nfor values, group in factor_groups:\n    i, j = values\n    group_num = i * 2 + j - 1  # for plotting purposes\n    x = [group_num] * len(group)\n    plt.scatter(\n        x,\n        resid[group.index],\n        marker=symbols[j],\n        color=colors[i - 1],\n        s=144,\n        edgecolors=\"black\",\n    )\nplt.xlabel(\"Group\")\nplt.ylabel(\"Residuals\")",
            "statsmodels->Examples->Plotting->Regression Plots->Statewide Crime 2009 Dataset->Using robust regression to correct for outliers.->weights = rob_crime_model.weights\nidx = weights  0\nX = rob_crime_model.model.exog[idx.values]\nww = weights[idx] / weights[idx].mean()\nhat_matrix_diag = ww * (X * np.linalg.pinv(X).T).sum(1)\nresid = rob_crime_model.resid\nresid2 = resid ** 2\nresid2 /= resid2.sum()\nnobs = int(idx.sum())\nhm = hat_matrix_diag.mean()\nrm = resid2.mean()"
        ]
    },
    {
        "id": "1126720",
        "GT": "Data Cleaning and Pre-Processing (DF_all)->Query all the data from the selected rows:-># Remove the unnecessary columns:\nrm_cols = set(['PRIVATE_CUSTOMER','ENGINE_POWER','ENGINE_POWER_KW_0','HORSE_POWER','HORSE_POWER_0','HORSE_POWER_1', 'MODEL_CODE'])\n#vin_hash_series = df_all['VIN_HASHED'].to_pandas()\n\nfor col in rm_cols:\n    df_all.drop_column(col)",
        "pred": [
            "torch->Model Optimization->Getting Started - Accelerate Your Scripts with nvFuser->nvFuser & Dynamic Shapes-># Perform warm-up iterations\nfor _ in range(3):\n     = inputs1[0]\n     = inputs2[0]\n     = grad_outputs[0]\n    # Run model, forward and backward\n     = (\n        ,\n        ,\n        ,\n        ,\n        ,\n        normalization_axis=2,\n        dropout_prob=0.1,\n    )\n    ()",
            "statsmodels->Examples->Plotting->Box Plots->Advanced Box Plots-># Create a jitter plot.\nfig3 = plt.figure()\nax = fig3.add_subplot(111)\n\nplot_opts = {\n    \"cutoff_val\": 5,\n    \"cutoff_type\": \"abs\",\n    \"label_fontsize\": \"small\",\n    \"label_rotation\": 30,\n    \"violin_fc\": (0.8, 0.8, 0.8),\n    \"jitter_marker\": \".\",\n    \"jitter_marker_size\": 3,\n    \"bean_color\": \"#FF6F00\",\n    \"bean_mean_color\": \"#009D91\",\n}\nsm.graphics.beanplot(age, ax=ax, labels=labels, jitter=True, plot_opts=plot_opts)\n\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\nax.set_title(\"US national election '96 - Age & Party Identification\")",
            "numpy->NumPy Applications->Deep learning on MNIST->1. Load the MNIST dataset-># Display 5 random images from the training set.\nnum_examples = 5\nseed = 147197952744\nrng = np.random.default_rng(seed)\n\nfig, axes = plt.subplots(1, num_examples)\nfor sample, ax in zip(rng.choice(x_train, size=num_examples, replace=False), axes):\n    ax.imshow(sample.reshape(28, 28), cmap=\"gray\")\n\n\n\n\n<img alt=\"../_images/8484ad8185328c820925db98e28702162d6c6d279eb7cd636dfc9e2d94b68215.png\" src=\"../_images/8484ad8185328c820925db98e28702162d6c6d279eb7cd636dfc9e2d94b68215.png\"/>",
            "statsmodels->Examples->Plotting->Box Plots->Advanced Box Plots-># Create a violin plot.\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nsm.graphics.violinplot(\n    age,\n    ax=ax,\n    labels=labels,\n    plot_opts={\n        \"cutoff_val\": 5,\n        \"cutoff_type\": \"abs\",\n        \"label_fontsize\": \"small\",\n        \"label_rotation\": 30,\n    },\n)\n\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\nax.set_title(\"US national election '96 - Age & Party Identification\")",
            "statsmodels->Examples->Plotting->Box Plots->Advanced Box Plots-># Create a bean plot.\nfig2 = plt.figure()\nax = fig2.add_subplot(111)\n\nsm.graphics.beanplot(\n    age,\n    ax=ax,\n    labels=labels,\n    plot_opts={\n        \"cutoff_val\": 5,\n        \"cutoff_type\": \"abs\",\n        \"label_fontsize\": \"small\",\n        \"label_rotation\": 30,\n    },\n)\n\nax.set_xlabel(\"Party identification of respondent.\")\nax.set_ylabel(\"Age\")\nax.set_title(\"US national election '96 - Age & Party Identification\")"
        ]
    },
    {
        "id": "683396",
        "GT": "7 - Use training and testing to identify the best model->#set seed for random split and decision tree state\nseed = 99\n\n#randomly split the date into 70% training and 30% testing sample\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test =\\\ntrain_test_split(df[df.columns[0:df.columns.shape[0]-1]],df['class'],test_size=0.3,random_state=seed)",
        "pred": [
            "sklearn->Examples->Model Selection->Statistical comparison of models using grid search-># create df of model scores ordered by performance\nmodel_scores = results_df.filter(regex=r\"split\\d*_test_score\")\n\n# plot 30 examples of dependency between cv fold and AUC scores\nfig, ax = ()\n(\n    data=model_scores.transpose().iloc[:30],\n    dashes=False,\n    palette=\"Set1\",\n    marker=\"o\",\n    alpha=0.5,\n    ax=ax,\n)\nax.set_xlabel(\"CV test fold\", size=12, labelpad=10)\nax.set_ylabel(\"Model AUC\", size=12)\nax.tick_params(bottom=True, labelbottom=False)\n()\n\n# print correlation of AUC scores across folds\nprint(f\"Correlation of models:\\n {model_scores.transpose().corr()}\")\n\n\n<img alt=\"plot grid search stats\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_grid_search_stats_002.png\" srcset=\"../../_images/sphx_glr_plot_grid_search_stats_002.png\"/>",
            "sklearn->Examples->Miscellaneous->The Johnson-Lindenstrauss bound for embedding with random projections->Theoretical bounds-># range of admissible distortions\neps_range = (0.1, 0.99, 5)\ncolors = plt.cm.Blues((0.3, 1.0, len(eps_range)))\n\n# range of number of samples (observation) to embed\nn_samples_range = (1, 9, 9)\n\n()\nfor eps, color in zip(eps_range, colors):\n    min_n_components = (n_samples_range, eps=eps)\n    (n_samples_range, min_n_components, color=color)\n\n([f\"eps = {eps:0.1f}\" for eps in eps_range], loc=\"lower right\")\n(\"Number of observations to eps-embed\")\n(\"Minimum number of dimensions\")\n(\"Johnson-Lindenstrauss bounds:\\nn_samples vs n_components\")\n()\n\n\n<img alt=\"Johnson-Lindenstrauss bounds: n_samples vs n_components\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\" srcset=\"../../_images/sphx_glr_plot_johnson_lindenstrauss_bound_001.png\"/>",
            "statsmodels->Examples->State space models->ETS models->Holt-Winters\u2019 seasonal method-># fit in statsmodels\nmodel = ETSModel(\n    austourists,\n    error=\"add\",\n    trend=\"add\",\n    seasonal=\"add\",\n    damped_trend=True,\n    seasonal_periods=4,\n)\nfit = model.fit()\n\n# fit with R params\nparams_R = [\n    0.35445427,\n    0.03200749,\n    0.39993387,\n    0.97999997,\n    24.01278357,\n    0.97770147,\n    1.76951063,\n    -0.50735902,\n    -6.61171798,\n    5.34956637,\n]\nfit_R = model.smooth(params_R)\n\naustourists.plot(label=\"data\")\nplt.ylabel(\"Australian Tourists\")\n\nfit.fittedvalues.plot(label=\"statsmodels fit\")\nfit_R.fittedvalues.plot(label=\"R fit\", linestyle=\"--\")\nplt.legend()",
            "sklearn->Examples->Generalized Linear Models->Polynomial and Spline interpolation-># plot function\nlw = 2\nfig, ax = ()\nax.set_prop_cycle(\n    color=[\"black\", \"teal\", \"yellowgreen\", \"gold\", \"darkorange\", \"tomato\"]\n)\nax.plot(x_plot, f(x_plot), linewidth=lw, label=\"ground truth\")\n\n# plot training points\nax.scatter(x_train, y_train, label=\"training points\")\n\n# polynomial features\nfor degree in [3, 4, 5]:\n    model = ((degree), (alpha=1e-3))\n    model.fit(X_train, y_train)\n    y_plot = model.predict(X_plot)\n    ax.plot(x_plot, y_plot, label=f\"degree {degree}\")\n\n# B-spline with 4 + 3 - 1 = 6 basis functions\nmodel = ((n_knots=4, degree=3), (alpha=1e-3))\nmodel.fit(X_train, y_train)\n\ny_plot = model.predict(X_plot)\nax.plot(x_plot, y_plot, label=\"B-spline\")\nax.legend(loc=\"lower center\")\nax.set_ylim(-20, 10)\n()\n\n\n<img alt=\"plot polynomial interpolation\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\" srcset=\"../../_images/sphx_glr_plot_polynomial_interpolation_001.png\"/>",
            "torch->Learning PyTorch->Visualizing Models, Data, and Training with TensorBoard->6. Assessing trained models with TensorBoard-># 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)"
        ]
    },
    {
        "id": "656749",
        "GT": "Project: Investigate Titanic Dataset->INTRODUCTION\nWe will go through \n\n1. <a href=\"#Familiarize-with-the-dataset\">Familiarize with the dataset</a>\n2. <a href=\"#Research-questions\">Research questions</a>\n\nBefore thinking about any questions to ask, let's first make sure be familiarize with this dataset. From [the description page in Kaggle](https://www.kaggle.com/c/titanic), Titanic Dataset: \n\n> Contains demographics and passenger information from 891 of the 2224 passengers and crew on board the Titanic. You can view a description of this dataset on the Kaggle website, where the data was obtained.\n\nHere are the highlights to note:\n\n* On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224.\n* One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\n* Although there was some element of luck involved in surviving the sinking, *some groups of people were more likely to survive than others*, such as women, children, and the upper-class.\n\nNow we have just developed some early understandings about *the domain of our problem*.->Familiarize with the dataset-># Data analysis and wrangling\nimport numpy as np\nimport pandas as pd\n\n# Visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
        "pred": [
            "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "sklearn->Examples->Covariance estimation->Ledoit-Wolf vs OAS estimation->import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import , \n\nfrom sklearn.covariance import , \n\n(0)",
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)",
            "statsmodels->Examples->User Notes->Prediction->import numpy as np\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "statsmodels->Examples->Statistics->Copulas->import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\n\nsns.set_style(\"darkgrid\")\nsns.mpl.rc(\"figure\", figsize=(8, 8))"
        ]
    },
    {
        "id": "1499311",
        "GT": "Capstone Project 2 - Milestone Report\n\nThis report explains what is the problem being solved and how it has been handled so far.->6.Capstone Project 2 notebook->6.1 Load data and libraries-># Import the necessaries libraries for the code\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt",
        "pred": [
            "numpy->Articles->Sentiment Analysis on notable speeches of the last decade->2. Preprocess the datasets-># Importing the necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pooch\nimport string\nimport re\nimport zipfile\nimport os\n\n# Creating the random instance\nrng = np.random.default_rng()",
            "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression->import numpy as np\nimport pylab\nimport seaborn as sns\nimport statsmodels.api as sm\n\nsns.set_style(\"darkgrid\")\npylab.rc(\"figure\", figsize=(16, 8))\npylab.rc(\"font\", size=14)",
            "sklearn->Examples->Clustering->Comparing different hierarchical linkage methods on toy datasets->import time\nimport warnings\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import cluster, \nfrom sklearn.preprocessing import \nfrom itertools import , \n\n(0)",
            "statsmodels->Examples->Statistics->Copulas->import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\n\nsns.set_style(\"darkgrid\")\nsns.mpl.rc(\"figure\", figsize=(8, 8))"
        ]
    },
    {
        "id": "1406454",
        "GT": "The entire graph->[len(graph.nodes) for graph in subgraphs]",
        "pred": [
            "statsmodels->Examples->Nonparametric Statistics->Univariate Kernel Density Estimator->Comparing kernel functions->['gau', 'epa', 'uni', 'tri', 'biw', 'triw', 'cos', 'cos2', 'tric']",
            "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 1.1->Grouping infrequent categories in OneHotEncoder->[array(['dog', 'snake'], dtype=object)]",
            "sklearn->Examples->Pipelines and composite estimators->Column Transformer with Mixed Types->['pclass', 'age', 'fare']",
            "numpy->NumPy Features->Saving and sharing your NumPy arrays->Remove the saved arrays and load them back with NumPy\u2019s->['x_axis', 'y_axis']",
            "statsmodels->Examples->State space models->Fixed / constrained parameters in state space models->['sigma2.irregular', 'sigma2.level', 'sigma2.trend']"
        ]
    },
    {
        "id": "1476769",
        "GT": "Dealing with each individual column to make sure values make sense-># create a dictonary based on what should be replaced\nreplaced = {1:'1-11',2:'12-49',3:'50-99',4:'100-299',5:'300-365',6:'0'}\ndf_age_filtered.replace({'# Days Used Alcohol Past Year (Range)':replaced,'# Days Used Marijuana Past Year (Range)':replaced,\n                        '# Days Used Cocaine Past Year (Range)':replaced,'# Days Used Hallucinogens Past Year (Range)':replaced},\n                       inplace=True)",
        "pred": [
            "pandas_toms_blog->Modern Pandas->Effective Pandas->SettingWithCopy-># ignore the context manager for now\nwith pd.option_context('mode.chained_assignment', None):\n    f[f['a'] &lt;= 3]['b'] = f[f['a'] &lt;= 3 ]['b'] / 10\nf",
            "sklearn->Examples->Miscellaneous->Explicit feature map approximation for RBF kernels->Decision Surfaces of RBF Kernel SVM and Linear SVM-># visualize the decision surface, projected down to the first\n# two principal components of the dataset\npca = (n_components=8).fit(data_train)\n\nX = pca.transform(data_train)\n\n# Generate grid along first two principal components\nmultiples = (-2, 2, 0.1)\n# steps along first component\nfirst = multiples[:, ] * pca.components_[0, :]\n# steps along second component\nsecond = multiples[:, ] * pca.components_[1, :]\n# combine\ngrid = first[, :, :] + second[:, , :]\nflat_grid = grid.reshape(-1, data.shape[1])\n\n# title for the plots\ntitles = [\n    \"SVC with rbf kernel\",\n    \"SVC (linear kernel)\\n with Fourier rbf feature map\\nn_components=100\",\n    \"SVC (linear kernel)\\n with Nystroem rbf feature map\\nn_components=100\",\n]\n\n(figsize=(18, 7.5))\nplt.rcParams.update({\"font.size\": 14})\n# predict and plot\nfor i, clf in enumerate((kernel_svm, nystroem_approx_svm, fourier_approx_svm)):\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    (1, 3, i + 1)\n    Z = clf.predict(flat_grid)\n\n    # Put the result into a color plot\n    Z = Z.reshape(grid.shape[:-1])\n    levels = (10)\n    lv_eps = 0.01  # Adjust a mapping from calculated contour levels to color.\n    (\n        multiples,\n        multiples,\n        Z,\n        levels=levels - lv_eps,\n        cmap=plt.cm.tab10,\n        vmin=0,\n        vmax=10,\n        alpha=0.7,\n    )\n    (\"off\")\n\n    # Plot also the training points\n    (\n        X[:, 0],\n        X[:, 1],\n        c=targets_train,\n        cmap=plt.cm.tab10,\n        edgecolors=(0, 0, 0),\n        vmin=0,\n        vmax=10,\n    )\n\n    (titles[i])\n()\n()\n\n\n<img alt=\"SVC with rbf kernel, SVC (linear kernel)  with Fourier rbf feature map n_components=100, SVC (linear kernel)  with Nystroem rbf feature map n_components=100\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_kernel_approximation_002.png\" srcset=\"../../_images/sphx_glr_plot_kernel_approximation_002.png\"/>",
            "statsmodels->Examples->State space models->Unobserved Components: Application->Model-># Create Table I\ntable_i = np.zeros((5,6))\n\nstart = dta.index[0]\nend = dta.index[-1]\ntime_range = '%d:%d-%d:%d' % (start.year, start.quarter, end.year, end.quarter)\nmodels = [\n    ('US GNP', time_range, 'None'),\n    ('US Prices', time_range, 'None'),\n    ('US Prices', time_range, r'$\\sigma_\\eta^2 = 0$'),\n    ('US monetary base', time_range, 'None'),\n    ('US monetary base', time_range, r'$\\sigma_\\eta^2 = 0$'),\n]\nindex = pd.MultiIndex.from_tuples(models, names=['Series', 'Time range', 'Restrictions'])\nparameter_symbols = [\n    r'$\\sigma_\\zeta^2$', r'$\\sigma_\\eta^2$', r'$\\sigma_\\kappa^2$', r'$\\rho$',\n    r'$2 \\pi / \\lambda_c$', r'$\\sigma_\\varepsilon^2$',\n]\n\ni = 0\nfor res in (output_res, prices_res, prices_restricted_res, money_res, money_restricted_res):\n    if res.model.stochastic_level:\n        (sigma_irregular, sigma_level, sigma_trend,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n    else:\n        (sigma_irregular, sigma_level,\n         sigma_cycle, frequency_cycle, damping_cycle) = res.params\n        sigma_trend = np.nan\n    period_cycle = 2 * np.pi / frequency_cycle\n\n    table_i[i, :] = [\n        sigma_level*1e7, sigma_trend*1e7,\n        sigma_cycle*1e7, damping_cycle, period_cycle,\n        sigma_irregular*1e7\n    ]\n    i += 1\n\npd.set_option('float_format', lambda x: '%.4g' % np.round(x, 2) if not np.isnan(x) else '-')\ntable_i = pd.DataFrame(table_i, index=index, columns=parameter_symbols)\ntable_i",
            "statsmodels->Examples->State space models->Unobserved Components: Application->Data-># Plot the data\nax = dta.plot(figsize=(13,3))\nylim = ax.get_ylim()\nax.xaxis.grid()\nax.fill_between(dates, ylim[0]+1e-5, ylim[1]-1e-5, recessions, facecolor='k', alpha=0.1);\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_structural_harvey_jaeger_6_0.png\"/>",
            "matplotlib->Tutorials->Introductory->Pyplot tutorial->Logarithmic and other nonlinear axes-># Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n# make up some data in the open interval (0, 1)\ny = np.random.normal(loc=0.5, scale=0.4, size=1000)\ny = y[(y  0) & (y &lt; 1)]\ny.sort()\nx = np.arange(len(y))\n\n# plot with various axes scales\nplt.figure()\n\n# linear\nplt.subplot(221)\nplt.plot(x, y)\nplt.yscale('linear')\nplt.title('linear')\nplt.grid(True)\n\n# log\nplt.subplot(222)\nplt.plot(x, y)\nplt.yscale('log')\nplt.title('log')\nplt.grid(True)\n\n# symmetric log\nplt.subplot(223)\nplt.plot(x, y - y.mean())\nplt.yscale('symlog', linthresh=0.01)\nplt.title('symlog')\nplt.grid(True)\n\n# logit\nplt.subplot(224)\nplt.plot(x, y)\nplt.yscale('logit')\nplt.title('logit')\nplt.grid(True)\n# Adjust the subplot layout, because the logit one may take more space\n# than usual, due to y-tick labels like \"1 - 10^{-3}\"\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n                    wspace=0.35)\n\nplt.show()\n\n\n<img alt=\"linear, log, symlog, logit\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_pyplot_010.png\" srcset=\"../../_images/sphx_glr_pyplot_010.png, ../../_images/sphx_glr_pyplot_010_2_0x.png 2.0x\"/>"
        ]
    },
    {
        "id": "1488721",
        "GT": "np.percentile(finalPrice,1)",
        "pred": [
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data->np.bincount(data[\"affairs\"].astype(int))",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Scale Estimators->np.median(x)",
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Exercise: Breakdown points of M-estimator->Squared error loss->np.array(se_beta).mean()",
            "statsmodels->Examples->Linear Regression Models->Regression Diagnostics->Multicollinearity->np.linalg.cond(results.model.exog)",
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Example->np.array(nobs1 + nobs2)"
        ]
    },
    {
        "id": "176258",
        "GT": "Appending column to tips dataset to bin tips->tips[\"tip_bracket\"] = pandas.cut(x = tips[\"tip\"],bins=5)",
        "pred": [
            "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions->sns.relplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\nsns.rugplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions->sns.relplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\nsns.rugplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Relating variables with scatter plots->sns.relplot(data=tips, x=\"total_bill\", y=\"tip\", size=\"size\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Relating variables with scatter plots->sns.relplot(data=tips, x=\"total_bill\", y=\"tip\", size=\"size\")\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", levels=[.01, .05, .1, .8])\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", kind=\"kde\", levels=[.01, .05, .1, .8])\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5), cbar=True)\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Visualizing bivariate distributions->sns.displot(penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\", binwidth=(2, .5), cbar=True)\n",
            "seaborn->User guide and tutorial->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions->sns.jointplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n",
            "seaborn->Plotting functions->Visualizing distributions of data->Distribution visualization in other settings->Plotting joint and marginal distributions->sns.jointplot(data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\")\n"
        ]
    },
    {
        "id": "767468",
        "GT": "Saskatoon House Listings Exploratory Analysis\nThis is an exploratory analysis examining the relationship between the number of house listings in Saskatoon and the number of actual house sales. The aim is to examine if this gap is increasing (or not) so as to better understand the potential for a down-turn in the market.\n\nThe first data set to be explored, \"house-data.csv\", was scraped from http://www.saskatoonrealtors.ca/StatisticMonthly.aspx.\n\nFirst step is importing relevant packages and taking a look at the head of the data frame:->Comparing listings and sales\nLets get a visual of house listings and sales.->listings_sales_differences = listings['Listings'] - listings['Sales']\nprint('Index  Difference')\nprint(listings_sales_differences.sort_values(ascending=False).head())",
        "pred": [
            "pandas_toms_blog->Time Series->Timeseries->Modeling Time Series->ax = y.plot()\nax.set(ylabel='Average Monthly Flights')\nsns.despine()",
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Simple Exponential Smoothing->ax = oildata.plot()\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Oil (millions of tonnes)\")\nprint(\"Figure 7.1: Oil production in Saudi Arabia from 1996 to 2007.\")",
            "pandas_toms_blog->Fast Pandas->Constructors->plt.figure(figsize=(4, 6))\nsns.boxplot(x='Method', y='Time (s)', data=timings)\nsns.despine()\nplt.tight_layout()",
            "statsmodels->Examples->Multivariate Methods->Principal Component Analysis->ax = dta.mean().plot(grid=False)\nax.set_xlabel(\"Year\", size=17)\nax.set_ylabel(\"Fertility rate\", size=17)\nax.set_xlim(0, 51)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes-># Quarterly frequency, using a DatetimeIndex\nindex = pd.date_range(start='2000', periods=4, freq='QS')\nendog2 = pd.Series([1, 2, 3, 4], index=index)\nprint(endog2.index)"
        ]
    },
    {
        "id": "306288",
        "GT": "FITTING THE MODEL->epoch=2\nbatch=100\nsp_epoch=X_train.shape[0]",
        "pred": [
            "torch->Text->Fast Transformer Inference with Better Transformer->Additional Information->input_batch=big_input_batch\n\nmodel_input = to_tensor(transform(input_batch), padding_value=1)\noutput = model(model_input)\noutput.shape",
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->ConvNet as fixed feature extractor->Train and evaluate->model_conv = train_model(model_conv, , ,\n                         , num_epochs=25)",
            "torch->Image and Video->Transfer Learning for Computer Vision Tutorial->Finetuning the convnet->Train and evaluate->model_ft = train_model(model_ft, , , ,\n                       num_epochs=25)",
            "sklearn->Examples->Examples based on real world datasets->Time-related feature engineering->Time-based cross-validation->all_splits = list(ts_cv.split(X, y))\ntrain_0, test_0 = all_splits[0]",
            "torch->Model Optimization->(beta) Quantized Transfer Learning for Computer Vision Tutorial->Part 1. Training a Custom Classifier based on a Quantized Feature Extractor->Train and evaluate->new_model = train_model(new_model, criterion, optimizer_ft, exp_lr_scheduler,\n                        num_epochs=25, device='cpu')\n\nvisualize_model(new_model)\nplt.tight_layout()"
        ]
    },
    {
        "id": "1036325",
        "GT": "4->def observe_matching(matching, verbose):\n    sum_differences = 0\n    for (a, b) in matching.items():\n        propensity_score_a = data_propensity.loc[a][\"propensity\"]\n        propensity_score_b = data_propensity.loc[b][\"propensity\"]\n        diff = abs(propensity_score_a - propensity_score_b)\n        if verbose:\n            print(\"Difference between {} and {} is {}\".format(a, b, diff))\n        sum_differences += diff\n    print(\"Mean difference in prop score is {}\".format(sum_differences / len(matching)))\n    \nobserve_matching(matching_propensity, False)",
        "pred": [
            "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Iterative imputation of the missing values->def get_impute_iterative(X_missing, y_missing):\n    imputer = (\n        missing_values=,\n        add_indicator=True,\n        random_state=0,\n        n_nearest_features=3,\n        max_iter=1,\n        sample_posterior=True,\n    )\n    iterative_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return iterative_impute_scores.mean(), iterative_impute_scores.std()\n\n\nmses_california[4], stds_california[4] = get_impute_iterative(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[4], stds_diabetes[4] = get_impute_iterative(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"Iterative Imputation\")\n\nmses_diabetes = mses_diabetes * -1\nmses_california = mses_california * -1",
            "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->Robust Fitting->def add_stl_plot(fig, res, legend):\n    \"\"\"Add 3 plots from a second STL fit\"\"\"\n    axs = fig.get_axes()\n    comps = [\"trend\", \"seasonal\", \"resid\"]\n    for ax, comp in zip(axs[1:], comps):\n        series = getattr(res, comp)\n        if comp == \"resid\":\n            ax.plot(series, marker=\"o\", linestyle=\"none\")\n        else:\n            ax.plot(series)\n            if comp == \"trend\":\n                ax.legend(legend, frameon=False)\n\n\nstl = STL(elec_equip, period=12, robust=True)\nres_robust = stl.fit()\nfig = res_robust.plot()\nres_non_robust = STL(elec_equip, period=12, robust=False).fit()\nadd_stl_plot(fig, res_non_robust, [\"Robust\", \"Non-robust\"])\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\" src=\"../../../_images/examples_notebooks_generated_stl_decomposition_10_0.png\"/>",
            "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->Impute missing values with mean->def get_impute_mean(X_missing, y_missing):\n    imputer = (missing_values=, strategy=\"mean\", add_indicator=True)\n    mean_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return mean_impute_scores.mean(), mean_impute_scores.std()\n\n\nmses_california[3], stds_california[3] = get_impute_mean(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[3], stds_diabetes[3] = get_impute_mean(X_miss_diabetes, y_miss_diabetes)\nx_labels.append(\"Mean Imputation\")",
            "sklearn->Examples->Missing Value Imputation->Imputing missing values before building an estimator->Impute the missing data and score->kNN-imputation of the missing values->def get_impute_knn_score(X_missing, y_missing):\n    imputer = (missing_values=, add_indicator=True)\n    knn_impute_scores = get_scores_for_imputer(imputer, X_missing, y_missing)\n    return knn_impute_scores.mean(), knn_impute_scores.std()\n\n\nmses_california[2], stds_california[2] = get_impute_knn_score(\n    X_miss_california, y_miss_california\n)\nmses_diabetes[2], stds_diabetes[2] = get_impute_knn_score(\n    X_miss_diabetes, y_miss_diabetes\n)\nx_labels.append(\"KNN Imputation\")",
            "sklearn->Examples->Examples based on real world datasets->Out-of-core classification of text documents->Plot results->def plot_accuracy(x, y, x_legend):\n    \"\"\"Plot accuracy as a function of x.\"\"\"\n    x = (x)\n    y = (y)\n    (\"Classification accuracy as a function of %s\" % x_legend)\n    (\"%s\" % x_legend)\n    (\"Accuracy\")\n    (True)\n    (x, y)\n\n\n[\"legend.fontsize\"] = 10\ncls_names = list(sorted(cls_stats.keys()))\n\n# Plot accuracy evolution\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with #examples\n    accuracy, n_examples = zip(*stats[\"accuracy_history\"])\n    plot_accuracy(n_examples, accuracy, \"training examples (#)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n()\nfor _, stats in sorted(cls_stats.items()):\n    # Plot accuracy evolution with runtime\n    accuracy, runtime = zip(*stats[\"runtime_history\"])\n    plot_accuracy(runtime, accuracy, \"runtime (s)\")\n    ax = ()\n    ax.set_ylim((0.8, 1))\n(cls_names, loc=\"best\")\n\n# Plot fitting times\n()\nfig = ()\ncls_runtime = [stats[\"total_fit_time\"] for cls_name, stats in sorted(cls_stats.items())]\n\ncls_runtime.append(total_vect_time)\ncls_names.append(\"Vectorization\")\nbar_colors = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\"]\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=10)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Training Times\")\n\n\ndef autolabel(rectangles):\n    \"\"\"attach some text vi autolabel on rectangles.\"\"\"\n    for rect in rectangles:\n        height = rect.get_height()\n        ax.text(\n            rect.get_x() + rect.get_width() / 2.0,\n            1.05 * height,\n            \"%.4f\" % height,\n            ha=\"center\",\n            va=\"bottom\",\n        )\n        (()[1], rotation=30)\n\n\nautolabel(rectangles)\n()\n()\n\n# Plot prediction times\n()\ncls_runtime = []\ncls_names = list(sorted(cls_stats.keys()))\nfor cls_name, stats in sorted(cls_stats.items()):\n    cls_runtime.append(stats[\"prediction_time\"])\ncls_runtime.append(parsing_time)\ncls_names.append(\"Read/Parse\\n+Feat.Extr.\")\ncls_runtime.append(vectorizing_time)\ncls_names.append(\"Hashing\\n+Vect.\")\n\nax = (111)\nrectangles = (range(len(cls_names)), cls_runtime, width=0.5, color=bar_colors)\n\nax.set_xticks((0, len(cls_names) - 1, len(cls_names)))\nax.set_xticklabels(cls_names, fontsize=8)\n(()[1], rotation=30)\nymax = max(cls_runtime) * 1.2\nax.set_ylim((0, ymax))\nax.set_ylabel(\"runtime (s)\")\nax.set_title(\"Prediction Times (%d instances)\" % n_test_documents)\nautolabel(rectangles)\n()\n()\n\n\n\n<img alt=\"Classification accuracy as a function of training examples (#)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_001.png\"/>\n<img alt=\"Classification accuracy as a function of runtime (s)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_002.png\"/>\n<img alt=\"Training Times\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_003.png\"/>\n<img alt=\"Prediction Times (1000 instances)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\" srcset=\"../../_images/sphx_glr_plot_out_of_core_classification_004.png\"/>"
        ]
    },
    {
        "id": "1160265",
        "GT": "print(train_label.shape)",
        "pred": [
            "statsmodels->Examples->Robust Regression->M-estimators for Robust Regression->Duncan\u2019s Occupational Prestige data - M-estimation for outliers->print(rlm_model.weights)",
            "matplotlib->Tutorials->Intermediate->Styling with cycler->print(yy.shape)",
            "statsmodels->Examples->User Notes->Forecasting in statsmodels->Indexes->print(endog.index)",
            "statsmodels->Examples->User Notes->Generic Maximum Likelihood->Example 2: Negative Binomial Regression for Count Data->Testing->print(res_nbin.params)",
            "numpy->Articles->Deep reinforcement learning with Pong from pixels->Preprocess frames (the observation)->print(env.observation_space)"
        ]
    },
    {
        "id": "805831",
        "GT": "Filter each split to the top 50 diagnosis/procedure codes->Y = 50",
        "pred": [
            "numpy->Articles->Deep reinforcement learning with Pong from pixels->Create the policy (the neural network) and the forward pass->D = 80 * 80",
            "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Tuning the hyper-parameters of the quantile regressors->0.796",
            "sklearn->Examples->Feature Selection->Recursive feature elimination with cross-validation->Model training and selection->Optimal number of features: 3",
            "sklearn->Examples->Ensemble methods->Prediction Intervals for Gradient Boosting Regression->Calibration of the confidence interval->0.9",
            "sklearn->Examples->Release Highlights->Release Highlights for scikit-learn 0.24->New PolynomialCountSketch kernel approximation function->0.7344"
        ]
    },
    {
        "id": "606503",
        "GT": "Titanic data preprocessing->Drop non-informative features->titanic.drop([\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\"], axis=1, inplace=True)",
        "pred": [
            "seaborn->User guide and tutorial->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "seaborn->Multi-plot grids->Building structured multi-plot grids->Conditional small multiples->g = sns.FacetGrid(tips, col=\"time\")\ng.map(sns.histplot, \"tip\")\n",
            "statsmodels->Examples->Discrete Choice Models->Fair\u2019s Affairs Data->Fair\u2019s Affair data->dta[\"affair\"] = (dta[\"affairs\"]  0).astype(float)\nprint(dta.head(10))",
            "seaborn->User guide and tutorial->Overview of seaborn plotting functions->Combining multiple views on the data->sns.jointplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\")\n",
            "seaborn->API Overview->Overview of seaborn plotting functions->Combining multiple views on the data->sns.jointplot(data=penguins, x=\"flipper_length_mm\", y=\"bill_length_mm\", hue=\"species\")\n",
            "pandas_toms_blog->Indexes->Flavors->Row Slicing->weather2 = weather.reset_index()\nweather2[weather2['station'] == 'DSM'].head()",
            "seaborn->User guide and tutorial->An introduction to seaborn->A high-level API for statistical graphics->Plots for categorical data->sns.catplot(data=tips, kind=\"violin\", x=\"day\", y=\"total_bill\", hue=\"smoker\", split=True)\n"
        ]
    },
    {
        "id": "476671",
        "GT": "header = ['first', 'last', 'gender', 'proportion']\ndf = pd.DataFrame(columns=header)\n\ndf_lt90 = pd.DataFrame(columns=header)",
        "pred": [
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Slicing->first = df.groupby('unique_carrier').first()\nfirst.ix[10:15, ['fl_date', 'tail_num']]",
            "statsmodels->Examples->Generalized Linear Models->Influence Measures for GLMs->get the influence measures->summ_df = infl.summary_frame()\nsumm_df.sort_values(\"cooks_d\", ascending=False)[:10]",
            "pandas_toms_blog->Modern Pandas->Effective Pandas->Multidimensional Indexing->hdf = df.set_index(['unique_carrier', 'origin', 'dest', 'tail_num',\n                    'fl_date']).sort_index()\nhdf[hdf.columns[:4]].head()",
            "pandas_toms_blog->Tidy Data->Reshaping & Tidy Data->NBA Data->tidy['rest'] = tidy.sort_values('date').groupby('team').date.diff().dt.days - 1\ntidy.dropna().head()",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Examples->import feather\ndf = feather.read_dataframe('diamonds.fthr')\ndf.head()"
        ]
    },
    {
        "id": "1315501",
        "GT": "def make_xy(X_col, y_col, vectorizer):\n    X = vectorizer.fit_transform(X_col)\n    y = y_col\n    return X, y",
        "pred": [
            "matplotlib->Tutorials->Introductory->Quick start guide->Coding styles->Making a helper functions->def my_plotter(ax, data1, data2, param_dict):\n    \"\"\"\n    A helper function to make a graph.\n    \"\"\"\n    out = ax.plot(data1, data2, **param_dict)\n    return out",
            "torch->Parallel and Distributed Training->Advanced Model Training with Fully Sharded Data Parallel (FSDP)->Fine-tuning HF T5->def setup_model(model_name):\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    tokenizer =  T5Tokenizer.from_pretrained(model_name)\n    return model, tokenizer",
            "sklearn->Tutorials->Working With Text Data->Extracting features from text files->Tokenizing text with scikit-learn->from sklearn.feature_extraction.text import CountVectorizer\n count_vect = CountVectorizer()\n X_train_counts = count_vect.fit_transform(twenty_train.data)\n X_train_counts.shape\n(2257, 35788)",
            "sklearn->Tutorials->Working With Text Data->Extracting features from text files->From occurrences to frequencies->from sklearn.feature_extraction.text import TfidfTransformer\n tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n X_train_tf = tf_transformer.transform(X_train_counts)\n X_train_tf.shape\n(2257, 35788)",
            "numpy->Articles->Deep reinforcement learning with Pong from pixels->Train the agent for a number of episodes->def update_input(prev_x, cur_x, D):\n    if prev_x is not None:\n        x = cur_x - prev_x\n    else:\n        x = np.zeros(D)\n    return x"
        ]
    },
    {
        "id": "324533",
        "GT": "elizabethSentences[3]",
        "pred": [
            "pandas_toms_blog->Indexes->Merging->The merge version->weather.loc['DSM']",
            "sklearn->Examples->Decomposition->Image denoising using dictionary learning->Generate distorted image->Distorting image...",
            "torch->Model Optimization->torch.compile Tutorial->Comparison to TorchScript and FX Tracing->compile 3: True",
            "statsmodels->Examples->Statistics->Meta-Analysis in statsmodels->Meta-analysis of proportions->Mantel-Hanszel odds-ratio using contingency tables->dta_c.T[0]",
            "statsmodels->Examples->Statistics->ANOVA->df_infl[:5]"
        ]
    },
    {
        "id": "1403304",
        "GT": "2.6 - Identify and explain interesting relationships between features and the class you are trying to predict (i.e., relationships with variables and the target classification).->2.6.1 - What are the reporting times like per offense type?\nFor this plot, we are looking at how long it takes from the time the crime is committed to the time the crime is reported to the police.  There are considerable anomalies with the TIME_TO_REPORT field - many values below 0, implying that the crime was reported before it occurred, and several very large values, which distorted the vertical scale.  For clarity purposes, we report only those values that are greater than 0 and less than 86400 (seconds = 1 day).\n\nThe crimes with the greatest variability are also the most violent: Sex Abuse and Homicide.  This makes sense for homicide, because the victim cannot report the crime and must be discovered (unless there was a witness).  For Sex Abuse, the delays could be due to the reluctance to report or the inability to report (due to coercion or other actions that would prohibit free reporting).  It is interesting to note that Sex Abuse crimes in the Day shift have longer reporting times but lower variability than during the Evening shift, but the shortest reporting times and least variability are during the Midnight shift.-># 2.6.1.1\n# Set up a wide chart so we can see the separation between offenses\nplt.figure(figsize=(30,10))\n\n# Default font was too small to make out the Offense, so scale it\nsns.set(font_scale=2)\n\n# Create a subset with data from 0 to 24 hours response times\nplt_test = dc[dc.TIME_TO_REPORT < 86400][dc.TIME_TO_REPORT > 0]\n\n# Create the box plot - report the response time in hours instead of seconds.  Group by Offense, and color by Shift\nsns.boxplot(x=plt_test.OFFENSE, y=plt_test.TIME_TO_REPORT/3600.0, hue=plt_test.SHIFT)\n\n# Move the legend out of the way\nplt.legend(loc='upper right')",
        "pred": [
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Personal consumption', xlabel='Date', ylabel='Billions of dollars')\n\n# Plot data points\ndata.loc['1977-07-01':, 'consump'].plot(ax=ax, style='o', label='Observed')\n\n# Plot predictions\npredict.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='r--', label='One-step-ahead forecast')\nci = predict_ci.loc['1977-07-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\npredict_dy.predicted_mean.loc['1977-07-01':].plot(ax=ax, style='g', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-07-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='g', alpha=0.1)\n\nlegend = ax.legend(loc='lower right')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>",
            "scipy->Multidimensional image processing (scipy.ndimage)->Extending scipy.ndimage in C-># example.py\nimport numpy as np\nimport ctypes\nfrom scipy import ndimage, LowLevelCallable\nfrom numba import cfunc, types, carray\n\n@cfunc(types.intc(types.CPointer(types.intp),\n                  types.CPointer(types.double),\n                  types.intc,\n                  types.intc,\n                  types.voidptr))\ndef transform(output_coordinates_ptr, input_coordinates_ptr,\n              output_rank, input_rank, user_data):\n    input_coordinates = carray(input_coordinates_ptr, (input_rank,))\n    output_coordinates = carray(output_coordinates_ptr, (output_rank,))\n    shift = carray(user_data, (1,), types.double)[0]\n\n    for i in range(input_rank):\n        input_coordinates[i] = output_coordinates[i] - shift\n\n    return 1\n\nshift = 0.5\n\n# Then call the function\nuser_data = ctypes.c_double(shift)\nptr = ctypes.cast(ctypes.pointer(user_data), ctypes.c_void_p)\ncallback = LowLevelCallable(transform.ctypes, ptr)\n\nim = np.arange(12).reshape(4, 3).astype(np.float64)\nprint(ndimage.geometric_transform(im, callback))",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->ARIMA Postestimation: Example 1 - Dynamic Forecasting-># Prediction error\n\n# Graph\nfig, ax = plt.subplots(figsize=(9,4))\nnpre = 4\nax.set(title='Forecast error', xlabel='Date', ylabel='Forecast - Actual')\n\n# In-sample one-step-ahead predictions and 95% confidence intervals\npredict_error = predict.predicted_mean - endog\npredict_error.loc['1977-10-01':].plot(ax=ax, label='One-step-ahead forecast')\nci = predict_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], alpha=0.1)\n\n# Dynamic predictions and 95% confidence intervals\npredict_dy_error = predict_dy.predicted_mean - endog\npredict_dy_error.loc['1977-10-01':].plot(ax=ax, style='r', label='Dynamic forecast (1978)')\nci = predict_dy_ci.loc['1977-10-01':].copy()\nci.iloc[:,0] -= endog.loc['1977-10-01':]\nci.iloc[:,1] -= endog.loc['1977-10-01':]\nax.fill_between(ci.index, ci.iloc[:,0], ci.iloc[:,1], color='r', alpha=0.1)\n\nlegend = ax.legend(loc='lower left');\nlegend.get_frame().set_facecolor('w')\n\n\n\n\n\n\n\n\n\n<img src=\"data:image/png;base64,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\n\"/>",
            "sklearn->Examples->Nearest Neighbors->Simple 1D Kernel Density Estimation-># Author: Jake Vanderplas &lt;jakevdp@cs.washington.edu\n#\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import \nfrom sklearn.neighbors import \n\n# ----------------------------------------------------------------------\n# Plot the progression of histograms to kernels\n(1)\nN = 20\nX = (\n    ((0, 1, int(0.3 * N)), (5, 1, int(0.7 * N)))\n)[:, ]\nX_plot = (-5, 10, 1000)[:, ]\nbins = (-5, 10, 10)\n\nfig, ax = (2, 2, sharex=True, sharey=True)\nfig.subplots_adjust(hspace=0.05, wspace=0.05)\n\n# histogram 1\nax[0, 0].hist(X[:, 0], bins=bins, fc=\"#AAAAFF\", density=True)\nax[0, 0].text(-3.5, 0.31, \"Histogram\")\n\n# histogram 2\nax[0, 1].hist(X[:, 0], bins=bins + 0.75, fc=\"#AAAAFF\", density=True)\nax[0, 1].text(-3.5, 0.31, \"Histogram, bins shifted\")\n\n# tophat KDE\nkde = (kernel=\"tophat\", bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 0].fill(X_plot[:, 0], (log_dens), fc=\"#AAAAFF\")\nax[1, 0].text(-3.5, 0.31, \"Tophat Kernel Density\")\n\n# Gaussian KDE\nkde = (kernel=\"gaussian\", bandwidth=0.75).fit(X)\nlog_dens = kde.score_samples(X_plot)\nax[1, 1].fill(X_plot[:, 0], (log_dens), fc=\"#AAAAFF\")\nax[1, 1].text(-3.5, 0.31, \"Gaussian Kernel Density\")\n\nfor axi in ax.ravel():\n    axi.plot(X[:, 0], (X.shape[0], -0.01), \"+k\")\n    axi.set_xlim(-4, 9)\n    axi.set_ylim(-0.02, 0.34)\n\nfor axi in ax[:, 0]:\n    axi.set_ylabel(\"Normalized Density\")\n\nfor axi in ax[1, :]:\n    axi.set_xlabel(\"x\")\n\n# ----------------------------------------------------------------------\n# Plot all available kernels\nX_plot = (-6, 6, 1000)[:, None]\nX_src = ((1, 1))\n\nfig, ax = (2, 3, sharex=True, sharey=True)\nfig.subplots_adjust(left=0.05, right=0.95, hspace=0.05, wspace=0.05)\n\n\ndef format_func(x, loc):\n    if x == 0:\n        return \"0\"\n    elif x == 1:\n        return \"h\"\n    elif x == -1:\n        return \"-h\"\n    else:\n        return \"%ih\" % x\n\n\nfor i, kernel in enumerate(\n    [\"gaussian\", \"tophat\", \"epanechnikov\", \"exponential\", \"linear\", \"cosine\"]\n):\n    axi = ax.ravel()[i]\n    log_dens = (kernel=kernel).fit(X_src).score_samples(X_plot)\n    axi.fill(X_plot[:, 0], (log_dens), \"-k\", fc=\"#AAAAFF\")\n    axi.text(-2.6, 0.95, kernel)\n\n    axi.xaxis.set_major_formatter(plt.FuncFormatter(format_func))\n    axi.xaxis.set_major_locator(plt.MultipleLocator(1))\n    axi.yaxis.set_major_locator(plt.NullLocator())\n\n    axi.set_ylim(0, 1.05)\n    axi.set_xlim(-2.9, 2.9)\n\nax[0, 1].set_title(\"Available Kernels\")\n\n# ----------------------------------------------------------------------\n# Plot a 1D density example\nN = 100\n(1)\nX = (\n    ((0, 1, int(0.3 * N)), (5, 1, int(0.7 * N)))\n)[:, ]\n\nX_plot = (-5, 10, 1000)[:, ]\n\ntrue_dens = 0.3 * (0, 1).pdf(X_plot[:, 0]) + 0.7 * (5, 1).pdf(X_plot[:, 0])\n\nfig, ax = ()\nax.fill(X_plot[:, 0], true_dens, fc=\"black\", alpha=0.2, label=\"input distribution\")\ncolors = [\"navy\", \"cornflowerblue\", \"darkorange\"]\nkernels = [\"gaussian\", \"tophat\", \"epanechnikov\"]\nlw = 2\n\nfor color, kernel in zip(colors, kernels):\n    kde = (kernel=kernel, bandwidth=0.5).fit(X)\n    log_dens = kde.score_samples(X_plot)\n    ax.plot(\n        X_plot[:, 0],\n        (log_dens),\n        color=color,\n        lw=lw,\n        linestyle=\"-\",\n        label=\"kernel = '{0}'\".format(kernel),\n    )\n\nax.text(6, 0.38, \"N={0} points\".format(N))\n\nax.legend(loc=\"upper left\")\nax.plot(X[:, 0], -0.005 - 0.01 * (X.shape[0]), \"+k\")\n\nax.set_xlim(-4, 9)\nax.set_ylim(-0.02, 0.4)\n()",
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Appendix A. Application to UnobservedComponents models-># Graph\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n\n# Plot data points\ninf[\"CPIAUCNS\"].plot(ax=ax, style=\"-\", label=\"Observed data\")\n\n# Plot estimate of the level term\nres_uc_mle.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (MLE)\")\nres_uc_bayes.states.smoothed[\"level\"].plot(ax=ax, label=\"Smoothed level (Bayesian)\")\n\nax.legend(loc=\"lower left\");\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_36_0.png\"/>"
        ]
    },
    {
        "id": "850846",
        "GT": "Titanic Dataset Analysis\n## Traditional Analysis\n\nIn this project, we will perform analysis on Dataset containing 891 passenger information from the ill-fated Titanic ship. The dataset contains information about the passengers as below.\n\n\n## Data Dictionary\n\n| Variable          | Definition        |\n|-------------------|-------------------|\n|   survival        | 0 = No or 1 = Yes |\n|pclass| Ticket class \t1 = 1st, 2 = 2nd, 3 = 3rd|\n|sex|sex|\n|Age|Age in years|\n|sibsp|# of siblings / spouses aboard the Titanic|\n|parch|# of parents / children aboard the Titanic|\n|ticket|Ticket number|\n|fare|Passenger fare|\n|cabin|Cabin number|\n|embarked|Port of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton|\n\n\nWe aim to explore some relationships in the dataset. The questions we are exploring are as follows:\n\n1. What factors made people more likely to survive?\n2. How many people survived compared to fatality on the boat?\n3. How many passengers per each ticket class?\n4. How did Ticket class affected survivability?\n5. How did Sex affect survivability?\n6. How did Age affect survivability?\n7. How did Age and Fare correlate?->Cleaning the Data\n\nWe list the number of missing values in the table.->titanic_ds.isnull().sum()",
        "pred": [
            "numpy->NumPy Features->Masked Arrays->Missing data->china_mask.nonzero()",
            "statsmodels->Examples->Discrete Choice Models->Ordinal Regression->Using formulas - no constant in model->resf_logit.predict()",
            "pandas_toms_blog->Indexes->Merging->The merge version->flights.dep_time.head()",
            "torch->Learning PyTorch->What is torch.nn really?->Neural net from scratch (no torch.nn)->tensor(0.1094)",
            "statsmodels->Examples->State space models->SARIMAX: Frequently Asked Questions (FAQ)->Reconstructing residuals, fitted values and forecasts in SARIMAX and ARIMA->arima_res.predict(0, 2)"
        ]
    },
    {
        "id": "603955",
        "GT": "Sign-up Method\n\nThe sign-up method is dominated by the basic method (71.63%) and with nearly all of the remainder of users signing up via faceboook (28.11%).->air.signup_method.value_counts(ascending=False).plot(kind='pie', autopct='%.2f',figsize=(6,6),title='Distribution of Signup Method',fontsize=12)",
        "pred": [
            "scipy->Signal Processing (scipy.signal)->Filtering->Filter Design->IIR Filter->b, a = signal.iirfilter(4, Wn=0.2, rp=5, rs=60, btype='lowpass', ftype='ellip')\n w, h = signal.freqz(b, a)",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->sns.countplot(x='cut', data=df)\nsns.despine()\nplt.tight_layout()",
            "seaborn->User guide and tutorial->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "seaborn->Plotting functions->Visualizing statistical relationships->Emphasizing continuity with line plots->Aggregation and representing uncertainty->fmri = sns.load_dataset(\"fmri\")\nsns.relplot(data=fmri, x=\"timepoint\", y=\"signal\", kind=\"line\")\n",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Pandas Built-in Plotting->df.plot.scatter(x='carat', y='depth', c='k', alpha=.15)\nplt.tight_layout()",
            "pandas_toms_blog->Visualization->Visualization and Exploratory Analysis->Seaborn->sns.barplot(x='cut', y='price', data=df)\nsns.despine()\nplt.tight_layout()"
        ]
    },
    {
        "id": "602423",
        "GT": "for row in range(len(df[(df['Week']==2) & (df['Season']==2017)])):\n    if (df['Home_team'][row]=='MIA') | (df['Home_team'][row]=='TAM') | (df['Away_team'][row]=='MIA') | (df['Away_team'][row]=='TAM'):\n        df = df.drop(row)",
        "pred": [
            "sklearn->Examples->Inspection->Permutation Importance vs Random Forest Feature Importance (MDI)->Tree\u2019s Feature Importance from Mean Decrease in Impurity (MDI)->for name, importances in zip([\"train\", \"test\"], [train_importances, test_importances]):\n    ax = importances.plot.box(vert=False, whis=10)\n    ax.set_title(f\"Permutation Importances ({name} set)\")\n    ax.set_xlabel(\"Decrease in accuracy score\")\n    ax.axvline(x=0, color=\"k\", linestyle=\"--\")\n    ax.figure.tight_layout()\n\n\n\n<img alt=\"Permutation Importances (train set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_004.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_004.png\"/>\n<img alt=\"Permutation Importances (test set)\" class=\"sphx-glr-multi-img\" src=\"../../_images/sphx_glr_plot_permutation_importance_005.png\" srcset=\"../../_images/sphx_glr_plot_permutation_importance_005.png\"/>",
            "sklearn->Examples->Ensemble methods->Categorical Feature Support in Gradient Boosting->Limiting the number of splits->for pipe in (hist_dropped, hist_one_hot, hist_ordinal, hist_native):\n    pipe.set_params(\n        histgradientboostingregressor__max_depth=3,\n        histgradientboostingregressor__max_iter=15,\n    )\n\ndropped_result = (hist_dropped, X, y, cv=n_cv_folds, scoring=scoring)\none_hot_result = (hist_one_hot, X, y, cv=n_cv_folds, scoring=scoring)\nordinal_result = (hist_ordinal, X, y, cv=n_cv_folds, scoring=scoring)\nnative_result = (hist_native, X, y, cv=n_cv_folds, scoring=scoring)\n\nplot_results(\"Gradient Boosting on Ames Housing (few and small trees)\")\n\n()\n\n\n<img alt=\"Gradient Boosting on Ames Housing (few and small trees), Fit times (s), Mean Absolute Percentage Error\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\" srcset=\"../../_images/sphx_glr_plot_gradient_boosting_categorical_002.png\"/>",
            "statsmodels->Examples->State space models->ETS models->Predictions->for i in range(simulated.shape[1]):\n    simulated.iloc[:, i].plot(label=\"_\", color=\"gray\", alpha=0.1)\ndf[\"mean\"].plot(label=\"mean prediction\")\ndf[\"pi_lower\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"95% interval\")\ndf[\"pi_upper\"].plot(linestyle=\"--\", color=\"tab:blue\", label=\"_\")\npred.endog.plot(label=\"data\")\nplt.legend()",
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Application: Bayesian analysis of a TVP-VAR model by MCMC->Bayesian estimation via MCMC->for i in range(niter):\n    mod.update_variances(store_obs_cov[i], store_state_cov[i])\n    sim.simulate()\n\n    # 1. Sample states\n    store_states[i + 1] = sim.simulated_state.T\n\n    # 2. Simulate obs cov\n    fitted = np.matmul(mod['design'].transpose(2, 0, 1), store_states[i + 1][..., None])[..., 0]\n    resid = mod.endog - fitted\n    store_obs_cov[i + 1] = invwishart.rvs(v10 + mod.nobs, S10 + resid.T @ resid)\n\n    # 3. Simulate state cov variances\n    resid = store_states[i + 1, 1:] - store_states[i + 1, :-1]\n    sse = np.sum(resid**2, axis=0)\n\n    for j in range(mod.k_states):\n        rv = invgamma.rvs((vi20 + mod.nobs - 1) / 2, scale=(Si20 + sse[j]) / 2)\n        store_state_cov[i + 1, j] = rv",
            "statsmodels->Examples->Time Series Analysis->Exponential Smoothing->Holt\u2019s Method->Plots of Seasonally Adjusted Data->for fit in [fit2, fit4]:\n    pd.DataFrame(np.c_[fit.level, fit.trend]).rename(\n        columns={0: \"level\", 1: \"slope\"}\n    ).plot(subplots=True)\nplt.show()\nprint(\n    \"Figure 7.4: Level and slope components for Holt\u2019s linear trend method and the additive damped trend method.\"\n)\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_0.png\"/>\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\" src=\"../../../_images/examples_notebooks_generated_exponential_smoothing_12_1.png\"/>"
        ]
    },
    {
        "id": "1006052",
        "GT": "Visualize The Data-># Binarize target labels\nlb = preprocessing.LabelBinarizer()\nSMS_train_target = np.array([number[0] for number in lb.fit_transform(SMS_train_target)])\n\n# Transform data\nsms_count_vect = CountVectorizer(decode_error = 'replace')\nsms_train_counts = sms_count_vect.fit_transform(SMS_train_data)\nsms_tfidf_transformer = TfidfTransformer()\nsms_train_tfidf = sms_tfidf_transformer.fit_transform(sms_train_counts)",
        "pred": [
            "statsmodels->Examples->State space models->TVP-VAR, MCMC, and sparse simulation smoothing->Implementation in Statsmodels->Local level model-># Construct a local level model for inflation\nmod = sm.tsa.UnobservedComponents(dta.infl, 'llevel')\n\n# Fit the model's parameters (sigma2_varepsilon and sigma2_eta)\n# via maximum likelihood\nres = mod.fit()\nprint(res.params)\n\n# Create simulation smoother objects\nsim_kfs = mod.simulation_smoother()              # default method is KFS\nsim_cfa = mod.simulation_smoother(method='cfa')  # can specify CFA method",
            "torch->Deploying PyTorch Models in Production->(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime-># Load pretrained model weights\nmodel_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\nbatch_size = 1    # just a random number\n\n# Initialize model with the pretrained weights\nmap_location = lambda storage, loc: storage\nif torch.cuda.is_available():\n    map_location = None\ntorch_model.load_state_dict((model_url, map_location=map_location))\n\n# set the model to inference mode\ntorch_model.eval()",
            "statsmodels->Examples->State space models->SARIMAX estimation with Bayesian methods->Introduction->6. Application of Bayesian estimates of parameters-># Retrieve the posterior means\nparams = pm.summary(trace)[\"mean\"].values\n\n# Construct results using these posterior means as parameter values\nres_bayes = mod.smooth(params)\n\npredict_bayes = res_bayes.get_prediction()\npredict_bayes_ci = predict_bayes.conf_int()\nlower = predict_bayes_ci[\"lower CPIAUCNS\"]\nupper = predict_bayes_ci[\"upper CPIAUCNS\"]\n\n# Graph\nfig, ax = plt.subplots(figsize=(9, 4), dpi=300)\n\n# Plot data points\ninf.plot(ax=ax, style=\"-\", label=\"Observed\")\n\n# Plot predictions\npredict_bayes.predicted_mean.plot(ax=ax, style=\"r.\", label=\"One-step-ahead forecast\")\nax.fill_between(predict_bayes_ci.index, lower, upper, color=\"r\", alpha=0.1)\nax.legend(loc=\"lower left\")\nplt.show()\n\n\n\n\n\n\n\n<img alt=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_24_0.png\" src=\"../../../_images/examples_notebooks_generated_statespace_sarimax_pymc3_24_0.png\"/>",
            "statsmodels->User Guide->Time Series Analysis->Time Series Analysis by State Space Methods statespace->Additional options and tools->Holding some parameters fixed and estimating the rest-># Construct a model\nmodel = sm.tsa.SARIMAX(endog, order=(1, 0, 0))\n\n# Fit the model with a fixed value for the AR(1) coefficient using the\n# context manager\nwith model.fix_params({'ar.L1': 0.5}):\n    results = model.fit()",
            "statsmodels->Examples->Nonparametric Statistics->Lowess Regression-># Generate data looking like cosine\nx = np.random.uniform(0, 4 * np.pi, size=200)\ny = np.cos(x) + np.random.random(size=len(x))\n\n# Compute a lowess smoothing of the data\nsmoothed = sm.nonparametric.lowess(exog=x, endog=y, frac=0.2)"
        ]
    },
    {
        "id": "667164",
        "GT": "Introduction\nUsing SVM to create a digit recognition system.->import pandas as pd\nimport matplotlib.pyplot as plt, matplotlib.image as mpimg\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.model_selection import KFold",
        "pred": [
            "sklearn->Examples->Miscellaneous->Advanced Plotting With Partial Dependence->import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import \nfrom sklearn.neural_network import \nfrom sklearn.preprocessing import \nfrom sklearn.pipeline import \nfrom sklearn.tree import \nfrom sklearn.inspection import PartialDependenceDisplay",
            "statsmodels->Examples->User Notes->Dates in Time-Series Models->import pandas as pd\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nfrom statsmodels.tsa.ar_model import AutoReg, ar_select_order\n\nplt.rc(\"figure\", figsize=(16, 8))\nplt.rc(\"font\", size=14)",
            "statsmodels->Examples->State space models->SARIMAX: Introduction->import numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport requests\nfrom io import BytesIO",
            "statsmodels->Examples->Time Series Analysis->Seasonal Decomposition->import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom pandas.plotting import register_matplotlib_converters\n\nregister_matplotlib_converters()\nsns.set_style(\"darkgrid\")",
            "statsmodels->Examples->Linear Regression Models->Quantile Regression->Setup->import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\n\ndata = sm.datasets.engel.load_pandas().data\ndata.head()"
        ]
    },
    {
        "id": "501233",
        "GT": "Baseline Model->3. Fitting a basic Linear Model to the entire data->X = sm.add_constant(X)\nmodel = sm.OLS(y,X)\nresults = model.fit()\nresults.summary()",
        "pred": [
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS estimation->model = sm.OLS(y, X)\nresults = model.fit()\nprint(results.summary())",
            "statsmodels->Examples->User Notes->Prediction->Estimation->olsmod = sm.OLS(y, X)\nolsres = olsmod.fit()\nprint(olsres.summary())",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->OLS non-linear curve but linear in parameters->res = sm.OLS(y, X).fit()\nprint(res.summary())",
            "statsmodels->Examples->State space models->Statespace: Custom Models->Model 3: multiple observation and state equations->What do we need to modify?->2) The update() function->mod = MultipleYsModel(i_hat, s_t, m_hat)\nres = mod.fit()\n\nprint(res.summary())",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->ols_model = sm.OLS(y, X)\nols_results = ols_model.fit()\nprint(ols_results.summary())"
        ]
    },
    {
        "id": "1114155",
        "GT": "Statistical Inference->Standard Error->(((x - x.mean()) ** 2).mean()) ** 0.5",
        "pred": [
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Investigating Pearson chi-square statistic->(res_e2._results.resid_response ** 2 / res_e2.model.family.variance(res_e2.mu)).sum()",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Analysis->Investigating Pearson chi-square statistic->(res_e._results.resid_response ** 2 / res_e.model.family.variance(res_e.mu)).sum()",
            "statsmodels->Examples->Generalized Linear Models->Weighting Observations with GLMs->Weighted GLM: Poisson response data->Load data->(data[\"affairs\"] == 0).mean()",
            "matplotlib->Tutorials->Intermediate->Autoscaling->Margins->(0.05, 0.05)",
            "statsmodels->Examples->Linear Regression Models->Ordinary Least Squares->Joint hypothesis test->Multicollinearity->Dropping an observation->2.0 / len(X) ** 0.5"
        ]
    }
]