{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c2f298",
   "metadata": {},
   "source": [
    "# Pump It Up Challenge - Cleaning\n",
    "\n",
    "The approach for this was adapted from Desislava Petkova, who performed the analysis in R. That repository can be viewed [here](https://github.com/dipetkov/DrivenData-PumpItUp/blob/master/transform-data.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d3e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime as dt\n",
    "from scripts import pumpitup\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515e8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5ddf10",
   "metadata": {},
   "source": [
    "# Training Data\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeff121",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/training_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f396a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = pd.read_csv('data/training_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9f8026",
   "metadata": {},
   "source": [
    "First, it's interesting to see how many unique values there are for every feature, which gives us an insight into the granularity and usefulness of each one.\n",
    "\n",
    "We can see that there are some similarly named features which might also share similar levels of detail, while some of them contain thousands of possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca31ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniques = pumpitup.unique_count(train_data)\n",
    "df_uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea55e74a",
   "metadata": {},
   "source": [
    "Next we can look at the percentage of missing values for each feautre. My function treats zeros as missing values for numeric data, but we should bear the context in mind when interpreting this.\n",
    "\n",
    "One interesting example is `num_private` where we previously saw that there were 65 values that it could take, but we now see that almost 99% of them are 0, probably rendering the feature useless to us. We also have a few other features with large proportions of missing data, which we address later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c026075",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['operation_years'][train_data['operation_years'] < 0] = train_data['operation_years'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dff779",
   "metadata": {},
   "source": [
    "### Categorical Features\n",
    "\n",
    "#### Installer and Funder\n",
    "\n",
    "First up, let's look at the `installer` and `funder` features again. If we plot them on a map, we can see that there is some degree of geographical clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85290f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "installer = train_data['installer']\n",
    "installer[pd.isnull(installer)] = 'none'\n",
    "funder = train_data['funder']\n",
    "funder[pd.isnull(funder)] = 'none'\n",
    "\n",
    "isntaller_encoded = pumpitup.label_encode(installer, le)\n",
    "installer_encoded_norm = isntaller_encoded / isntaller_encoded.max()\n",
    "isntaller_cmap = [cmap(x) for x in installer_encoded_norm]\n",
    "\n",
    "funder_encoded = pumpitup.label_encode(funder, le)\n",
    "funder_encoded_norm = funder_encoded / funder_encoded.max()\n",
    "funder_cmap = [cmap(x) for x in funder_encoded_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8944a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].scatter(train_data['longitude'], train_data['latitude'], c=isntaller_cmap, linewidth=0)\n",
    "ax[0].set_ylim(-13, 0)\n",
    "ax[0].set_xlim(28, 42)\n",
    "ax[0].set_ylabel('Latitude')\n",
    "ax[0].set_xlabel('Longitude')\n",
    "\n",
    "ax[1].scatter(train_data['longitude'], train_data['latitude'], c=funder_cmap, linewidth=0)\n",
    "ax[1].set_ylim(-13, 0)\n",
    "ax[1].set_xlim(28, 42)\n",
    "ax[1].set_ylabel('Latitude')\n",
    "ax[1].set_xlabel('Longitude')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab30dbff",
   "metadata": {},
   "source": [
    "To impute these, we will again use imputation by grouping with region and district code. Any missing values in region and district code combinations that do not have any other values will simply be relabeled as *other*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f227c625",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = train_data['installer'].isnull()\n",
    "train_data['installer'][mask] = 'other'\n",
    "train_data.loc[mask, 'installer'] = train_data.groupby(['region', 'district_code'])['installer'].transform(lambda x: x.value_counts().index[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41443899",
   "metadata": {},
   "source": [
    "And now `installer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc9cb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = train_data['funder'].isnull()\n",
    "train_data['funder'][mask] = 'other'\n",
    "train_data.loc[mask, 'funder'] = train_data.groupby(['region', 'district_code'])['funder'].transform(lambda x: x.value_counts().index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f803c91",
   "metadata": {},
   "source": [
    "#### Permit and Public Meeting\n",
    "\n",
    "Both of these features are boolean. A quick look at grouping by `region` and `district_code` shows that different places have different distributions of *True* vs *False*. Without much else to go on at this point, we can use these to fill in the missing values, defaulting to *True* (the more common overall in both features) if there is only missing values in the district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f4226",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = train_data['permit'].isnull()\n",
    "train_data['permit'][mask] = True\n",
    "train_data.loc[mask, 'permit'] = train_data.groupby(['region', 'district_code'])['permit'].transform(lambda x: x.value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05354d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = train_data['public_meeting'].isnull()\n",
    "train_data['public_meeting'][mask] = True\n",
    "train_data.loc[mask, 'public_meeting'] = train_data.groupby(['region', 'district_code'])['public_meeting'].transform(lambda x: x.value_counts().index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1742d35c",
   "metadata": {},
   "source": [
    "Data cleaning done!\n",
    "\n",
    "We can now finally drop `district_code` from our dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d14d06",
   "metadata": {},
   "source": [
    "The regions are shown clearly divided in the left figure. There are 21 regions, with each of them containing a significant number of points from the dataset. Using a colourmap to plot the wards does not make a lot of visual sense beyond simply highlighting the huge increase in precision. In some cases, there is only one point in a ward, making it too granular of a categorical feature for our purposes, so we will drop it.\n",
    "\n",
    "We will also drop `district_code`, but for now I'm going to keep it as it will help to resolve some missing values issues later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ff66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(['region_code', 'subvillage', 'ward'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7317bdde",
   "metadata": {},
   "source": [
    "The `lga` feature also contains geographic information more precise than region, but most likely not representing any other political boundaries. However, it does contain information about whether a point is rural or urban.\n",
    "\n",
    "We can create a recoded feature from this that simply contains *rural*, *urban* and *other* as categories, adding this information on top of the regional information we already have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b8b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = train_data['lga'].copy()\n",
    "series[series.str.contains('Rural')] = 'rural'\n",
    "series[series.str.contains('Urban')] = 'urban'\n",
    "other_flag = series.str.contains('rural') | series.str.contains('urban')\n",
    "other_flag = other_flag == False\n",
    "series[other_flag] = 'other'\n",
    "\n",
    "train_data['lga'] = series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ed1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lga_cmap = train_data['lga'].copy()\n",
    "lga_cmap[series.str.contains('rural')] = 'green'\n",
    "lga_cmap[series.str.contains('urban')] = 'red'\n",
    "other_flag = series.str.contains('rural') | series.str.contains('urban')\n",
    "other_flag = other_flag == False\n",
    "lga_cmap[other_flag] = 'blue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a4f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.scatter(train_data['longitude'], train_data['latitude'], c=lga_cmap, linewidth=0)\n",
    "ax.set_ylim(-13, 0)\n",
    "ax.set_xlim(28, 42)\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_xlabel('Longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb4e0a",
   "metadata": {},
   "source": [
    "Plotting our recoded `lga` feature on a map shows collections of points that look like urban centres. The distinction between *rural* and *other* is less clear.\n",
    "\n",
    "There is also altitude and population information in the `gps_height` and `population` features. We can plot them in the same way to get an idea of how they're distributed. From the missing percentages, we can see that about 35% of both these features are zeros, so we can visualise where that is occuring too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db2ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "viridis = plt.cm.get_cmap('viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c788bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_height_encoded = pumpitup.label_encode(train_data['gps_height'][train_data['gps_height'] > 0], le)\n",
    "gps_height_encoded_norm = gps_height_encoded / gps_height_encoded.max()\n",
    "gps_height_cmap = [viridis(x) for x in gps_height_encoded_norm]\n",
    "\n",
    "population_encoded = pumpitup.label_encode(train_data['population'][train_data['population'] > 0], le)\n",
    "population_encoded_norm = population_encoded / population_encoded.max()\n",
    "population_cmap = [viridis(x) for x in population_encoded_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46f67c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].scatter(train_data['longitude'][train_data['gps_height'] == 0],\\\n",
    "           train_data['latitude'][train_data['gps_height'] == 0], color='gray')\n",
    "ax[0].scatter(train_data['longitude'][train_data['gps_height'] > 0],\\\n",
    "           train_data['latitude'][train_data['gps_height'] > 0], c=gps_height_cmap, linewidth=0)\n",
    "ax[0].set_ylim(-13, 0)\n",
    "ax[0].set_xlim(28, 42)\n",
    "ax[0].set_ylabel('Latitude')\n",
    "ax[0].set_xlabel('Longitude')\n",
    "\n",
    "ax[1].scatter(train_data['longitude'][train_data['population'] == 0],\\\n",
    "           train_data['latitude'][train_data['population'] == 0], color='gray')\n",
    "ax[1].scatter(train_data['longitude'][train_data['population'] > 0],\\\n",
    "           train_data['latitude'][train_data['population'] > 0], c=population_cmap, linewidth=0)\n",
    "ax[1].set_ylim(-13, 0)\n",
    "ax[1].set_xlim(28, 42)\n",
    "ax[1].set_ylabel('Latitude')\n",
    "ax[1].set_xlabel('Longitude')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766aab81",
   "metadata": {},
   "source": [
    "In both the cases, the missing data is concentrated in particular regions, within clean boundaries. This might cause us some problems later for filling in missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75177aa3",
   "metadata": {},
   "source": [
    "### Time\n",
    "\n",
    "There are two features related to time; the `construction_year` and the `date_recorded`.\n",
    "\n",
    "#### Construction Year\n",
    "\n",
    "From our missing data percentages, we can see that `construction_year` is missing around 35% of the data which we should check out in more detail. I also wanted to see whether there was any trend of water pumps being installed in certain parts of the country at different times. Plotting the construction year against longitude and latitude we can see that to a first approximation, installations are distributed randomly through space and time. However we can see that the missing data is not random at all, and is concetrated in specific regions of the country. Again, this makes imputing the missing data intelligently quite difficult, as we have little to compare it to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d75a8",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab293b",
   "metadata": {},
   "source": [
    "We can immediately remove the `id` feature as it is unique for each row and therefore will not help our preduction. Conversely the `recorded_by` takes a single value for the whole dataset, so this can also be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e549f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(['id', 'recorded_by', 'num_private'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c4310",
   "metadata": {},
   "source": [
    "### Extraction Types\n",
    "\n",
    "Looking at `extraction_type_class`, `extraction_type_group` and `extraction_type`, we can see a hierarchy of classification, with `extract_type` as the most granular and `extraction_type_class` the most coarse.\n",
    "\n",
    "We remove the mid level information and refactor the finest feature.\n",
    "\n",
    "There are no missing values to take care of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d60589",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_types = train_data.groupby(['extraction_type_class', 'extraction_type_group', 'extraction_type'])\n",
    "extraction_types.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf49a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['extraction_type'].replace('other - swn 81', 'swn 81', inplace=True)\n",
    "train_data['extraction_type'].replace('other - mkulima/shinyanga', 'other handpump', inplace=True)\n",
    "train_data['extraction_type'].replace('other - play pump', 'other handpump', inplace=True)\n",
    "train_data['extraction_type'].replace('cemo', 'other motorpump', inplace=True)\n",
    "train_data['extraction_type'].replace('climax', 'other motorpump', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cbfa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('extraction_type_group', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b488af1",
   "metadata": {},
   "source": [
    "### Management\n",
    "\n",
    "The management information features `management_group` and `management` capture two different levels of granularity in categories large enough that they need no further modification. There are some unknown values, but it seems that this would refer to the management group being unknown by the recording party, rather than not recorded at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby(['management_group', 'management']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fc72a7",
   "metadata": {},
   "source": [
    "### Scheme\n",
    "\n",
    "The features in `scheme_management` and `scheme_name` are very different in their level of detail, with 13 and 2697 unique values respectively.\n",
    "\n",
    "The `scheme_name` column also has 47% missing `NaN` values, whilst `scheme_management` is over 95% complete. For this reason, we will leave `scheme_management` without modification for now and drop `scheme_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c9c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('scheme_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5602a87d",
   "metadata": {},
   "source": [
    "However, on closer inspection, `scheme_management` essentially encodes the same information as `management`, so we will drop it too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7c55a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = train_data[['latitude', 'longitude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae43d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = backup.gps_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1292ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_filled = pumpitup.fill_missing_knn(series, df_encoded, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84404e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['gps_height'] = height_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff73c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpsf_height_encoded_norm = height_filled / height_filled.max()\n",
    "gpsf_height_cmap = [viridis(x) for x in gpsf_height_encoded_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c1bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.scatter(train_data['longitude'], train_data['latitude'], c=gpsf_height_cmap, linewidth=0)\n",
    "ax.set_ylim(-13, 0)\n",
    "ax.set_xlim(28, 42)\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_xlabel('Longitude')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eb2d7a",
   "metadata": {},
   "source": [
    "That looks pretty reasonable! It's certainly going to be better than simply picking the mean or median. In fact, it basically is picking the mean, but locally to where each point is located.\n",
    "\n",
    "**Population**\n",
    "\n",
    "About 35% of the population data is missing. Population will vary with geographical features such as land type and governmental boundaries. To impute the missing values, we're going to use the `lga`, `region` and `district_code` features. We can use a similar method to that used to impute the missing longitude and latitude values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a7c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = pumpitup.flag_missing_s(train_data['population'])\n",
    "train_data['population'][mask1] = np.nan\n",
    "train_data.loc[mask1, 'population'] = train_data.groupby(['lga', 'region','district_code']).transform('mean')\n",
    "mask2 = train_data['population'].isnull()\n",
    "train_data.loc[mask2, 'population'] = train_data.groupby(['lga', 'region']).transform('mean')\n",
    "mask3 = train_data['population'].isnull()\n",
    "train_data.loc[mask3, 'population'] = train_data.groupby('lga').transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c39e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['population'] = train_data['population'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cccdd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_encoded = pumpitup.label_encode(train_data['population'][train_data['population'] > 0], le)\n",
    "population_encoded_norm = population_encoded / population_encoded.max()\n",
    "population_cmap = [viridis(x) for x in population_encoded_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd4634",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.scatter(train_data['longitude'][train_data['population'] == 0],\\\n",
    "           train_data['latitude'][train_data['population'] == 0], color='gray')\n",
    "ax.scatter(train_data['longitude'][train_data['population'] > 0],\\\n",
    "           train_data['latitude'][train_data['population'] > 0], c=population_cmap, linewidth=0)\n",
    "ax.set_ylim(-13, 0)\n",
    "ax.set_xlim(28, 42)\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_xlabel('Longitude')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59ed5be",
   "metadata": {},
   "source": [
    "We can see that urban areas have higher populations, while rural and others seem to have lower populations, with the data varying across regions and districts. The imputed values don't seem to fall quite as low as those found in some parts of the map with data available.\n",
    "\n",
    "### Time\n",
    "\n",
    "We have some missing values for the `construction_year` feature, or at least, it is unlikely that water pumps installed in the year 0 are still in use today.\n",
    "\n",
    "We looked before at whether there was a strong relationship between the construction year of a pump and the location, but that didn't seem to be too strong. Maybe we can infer some information from the type of pump, who installed it.\n",
    "\n",
    "Just before we do that, we also saw that some of the record dates fell during years that were before the construction years. This can't be right. I'm simply going to replace any where `year_recorded` < `construction_year` with the median year recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd079cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['construction_year'] = train_data['construction_year'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0271e78",
   "metadata": {},
   "source": [
    "We can now calculate the number of years in operation and replace any negative years with the median date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127db06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['operation_years'] = (train_data['year_recorded'] - train_data['construction_year']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5244d4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.scatter(train_data['longitude'][train_data['construction_year'] == 0],\\\n",
    "           train_data['latitude'][train_data['construction_year'] == 0], color='gray')\n",
    "ax.scatter(train_data['longitude'][train_data['construction_year'] > 0],\\\n",
    "           train_data['latitude'][train_data['construction_year'] > 0], c=con_year_cmap, linewidth=0)\n",
    "ax.set_ylim(-13, 0)\n",
    "ax.set_xlim(28, 42)\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_xlabel('Longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b07d57",
   "metadata": {},
   "source": [
    "Another way to look at this is to plot the longitude and latitude of each pump against the construction year in separate scatter plots. This gives us a sense of whether the installations have been concentrated in particular times and places. Besides years at the very beginning of the records and a few slightly more dense clusters, the distribution still seems to be fairly uniformly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].scatter(train_data['construction_year'][train_data['construction_year'] > 0],\\\n",
    "              train_data['longitude'][train_data['construction_year'] > 0], alpha=0.05, linewidth=0)\n",
    "ax[0].set_xlim(1959, 2013)\n",
    "ax[0].set_ylim(28, 42)\n",
    "ax[0].set_ylabel('Longitude')\n",
    "ax[0].set_xlabel('Construction Year')\n",
    "\n",
    "ax[1].scatter(train_data['construction_year'][train_data['construction_year'] > 0],\\\n",
    "              train_data['latitude'][train_data['construction_year'] > 0], alpha=0.05, color='red', linewidth=0)\n",
    "ax[1].set_xlim(1959, 2013)\n",
    "ax[1].set_ylim(-12, 0)\n",
    "ax[1].set_ylabel('Latitude')\n",
    "ax[1].set_xlabel('Construction Year')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e8e46e",
   "metadata": {},
   "source": [
    "One thing we could do is create a new feature by subtracting the `construction_year` from `date_recorded` to see whether the time in operation has any effect on failure rate. Before we do this however, I'd like to sort out the missing values, so we will leave `construction_year` alone for the time being.\n",
    "\n",
    "#### Date Recorded\n",
    "\n",
    "The `date_recorded` feature is a human readable date in a string format, which is not going to be very useful for our classifier. We can implement a helper function that turns the date into just the year and also derives a new feature `operation_years` which is the year of the record minus the year the pump was constructed.\n",
    "\n",
    "There may also be a seasonal effect to pump failure (or at least the observance of pump failure). To take this into account, we create another feature that simply holds the month value from the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c39c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pumpitup.convert_dates(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b3a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.scatter(train_data['construction_year'], train_data['year_recorded'], alpha=0.2, linewidth=0)\n",
    "ax.set_xlim(1959, 2013)\n",
    "ax.set_ylim(2003, 2014)\n",
    "ax.set_ylabel('Year Recorded')\n",
    "ax.set_xlabel('Construction Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d157aa1",
   "metadata": {},
   "source": [
    "Looking at this scatter plot tells us that most of the tests were carried out during the period 2011 through 2013. There are a few points where the `year_recorded` is earlier than the `construction_year`. One other issue is that the missing `construction_year` data has led to records indicating that over a third of pumps have been in operation for around 2000 years. Good going. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[train_data['operation_years'] > 2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3430c",
   "metadata": {},
   "source": [
    "### Other Variables\n",
    "\n",
    "There are a few other variables that need to be picked and recategorised to clean up the data.\n",
    "\n",
    "In particular `wpt_name`, `installer` and `funder` are features with a large number of unique variables.\n",
    "\n",
    "According to the documentation, `wpt_name` refers to the name of the waterpoint. A few minutes spent looking at the entries and translating some Swahali into English gives us a bit more insight. In many cases the waterpoint name seems to be simply a name, but in others the name seems to refer to something of possible local significance that may affect how much a pump is used and how well it is maintained:\n",
    "\n",
    "- Shuleni/Shule/School/Sekondari/Secondary/Msingi/Primary - School\n",
    "- Zahanati/Clinic/Health - Health Clinic\n",
    "- Hospitalini/Hospitali/Hospital - Hospital\n",
    "- Office/Kijiji/Ofisini/Ofisi/Idara - Village Office\n",
    "- Farm/Maziwa - Farm/Dairy\n",
    "- Pump House/Bombani/Well/Kisima - Water related\n",
    "- Kanisani/Kanisa/Church/Anglican/Pentecost/Luther/Msikitini/Msikitini/Mosque - Christian and Muslim places of worship\n",
    "- Center/Market/Sokoni/Madukani - Town/Village Centre + Market/Shopping\n",
    "- Ccm - Government?\n",
    "- Kwa - Seems to accompany an individual's name\n",
    "\n",
    "These groups seem to encompass a large enough number of points or have enough significance to form their own categories. There are lots of other smaller groups which we will group together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c1b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['wpt_name'] = train_data['wpt_name'].str.lower()\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('school')] = 'school'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('shule')] = 'school'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('sekondari')] = 'school'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('secondary')] = 'school'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('sekondari')] = 'school'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('msingi')] = 'school'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('primary')] = 'school'\n",
    "\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('clinic')] = 'health'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('zahanati')] = 'health'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('health')] = 'health'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('hospital')] = 'health'\n",
    "\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('ccm')] = 'official'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('office')] = 'official'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('kijiji')] = 'official'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('ofis')] = 'official'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('idara')] = 'official'\n",
    "\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('farm')] = 'farm'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('maziwa')] = 'farm'\n",
    "\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('pump house')] = 'water'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('pump')] = 'water'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('bombani')] = 'water'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('maji')] = 'water'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('water')] = 'water'\n",
    "\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('kanisani')] = 'religious'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('kanisa')] = 'religious'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('church')] = 'religious'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('luther')] = 'religious'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('anglican')] = 'religious'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('pentecost')] = 'religious'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('msikitini')] = 'religious'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('msikiti')] = 'religious'\n",
    "\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('center')] = 'center'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('market')] = 'center'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('sokoni')] = 'center'\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('madukani')] = 'center'\n",
    "\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('kwa')] = 'name'\n",
    "\n",
    "#finally change any values with less than 500 records to 'other' as well as the 'none' values\n",
    "value_counts = train_data['wpt_name'].value_counts()\n",
    "to_remove = value_counts[value_counts <= 500].index\n",
    "train_data['wpt_name'].replace(to_remove, 'other', inplace=True)\n",
    "\n",
    "train_data['wpt_name'][train_data['wpt_name'].str.contains('none')] = 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['wpt_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed51b95",
   "metadata": {},
   "source": [
    "We're left with some categories of unknown meaning but hopefully they are of some significance.\n",
    "\n",
    "Next we can look at `funder` and `installer`. Both of these have some large categories and lots of small ones.\n",
    "\n",
    "For `funder` the large categories seem to be either governments or large NGOs while the small ones are unknown entities. We can keep the big ones, and group the rest under _other_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a81aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = train_data['funder'].value_counts()\n",
    "to_remove = value_counts[value_counts <= 500].index\n",
    "train_data['funder'].replace(to_remove, 'other', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace8dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['funder'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd16d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.0084*len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23218be9",
   "metadata": {},
   "source": [
    "For `installer`, we can do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = train_data['installer'].value_counts()\n",
    "to_remove = value_counts[value_counts <= 500].index\n",
    "train_data['installer'].replace(to_remove, 'other', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['installer'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e6ea0",
   "metadata": {},
   "source": [
    "Good, it now looks like we're ready to deal with the missing data, before finally making some predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836270d5",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "Let's check our missing percentages again after all that work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9bc6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pumpitup.percent_missing(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0e78c",
   "metadata": {},
   "source": [
    "We can see that there is still quite a bit of work to do. Firstly we can get rid of `amount_tsh` which represents the amount of water available to a pump. This could be useful, but there's just too much missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a303a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('amount_tsh', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b496e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26563a36",
   "metadata": {},
   "source": [
    "### Geographic\n",
    "\n",
    "#### Longitude and Latitude\n",
    "\n",
    "Next let's go back to our longitude and latitude. We have a small percentage of these missing, but we have some pretty granular geographic data in the form of the `district_code` within `regions`. We can find the mean longitude and latitude within each district code and then use these to fill in any missing longitude and latitude data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce8e68",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79a6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('data/training_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d9bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ca82cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels['status_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.get_cmap('Accent')\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c5a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_encoded = pumpitup.label_encode(train_data['region'], le)\n",
    "region_encoded_norm = region_encoded / region_encoded.max()\n",
    "region_cmap = [cmap(x) for x in region_encoded_norm]\n",
    "\n",
    "ward_encoded = pumpitup.label_encode(train_data['ward'], le)\n",
    "ward_encoded_norm = ward_encoded / ward_encoded.max()\n",
    "ward_cmap = [cmap(x) for x in ward_encoded_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71121fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].scatter(train_data['longitude'], train_data['latitude'], c=region_cmap, linewidth=0)\n",
    "ax[0].set_ylim(-13, 0)\n",
    "ax[0].set_xlim(28, 42)\n",
    "ax[0].set_ylabel('Latitude')\n",
    "ax[0].set_xlabel('Longitude')\n",
    "\n",
    "ax[1].scatter(train_data['longitude'], train_data['latitude'], c=ward_cmap, linewidth=0)\n",
    "ax[1].set_ylim(-13, 0)\n",
    "ax[1].set_xlim(28, 42)\n",
    "ax[1].set_ylabel('Latitude')\n",
    "ax[1].set_xlabel('Longitude')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da1f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_encoded = pumpitup.label_encode(train_data['district_code'], le)\n",
    "dist_encoded_norm = dist_encoded / dist_encoded.max()\n",
    "dist_cmap = [cmap(x) for x in dist_encoded_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.scatter(train_data['longitude'], train_data['latitude'], c=dist_cmap, linewidth=0)\n",
    "ax.set_ylim(-13, 0)\n",
    "ax.set_xlim(28, 42)\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_xlabel('Longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eac9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('scheme_management', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325df2e4",
   "metadata": {},
   "source": [
    "### Payment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48cfd71",
   "metadata": {},
   "source": [
    "Besides slightly different labelling, `payment` and `payment_type` are essentially the same. Some unknown values are present. `payment_type` is shorter so we'll keep that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56c92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby(['payment_type', 'payment']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3fdd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('payment', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556c789a",
   "metadata": {},
   "source": [
    "### Water Quality\n",
    "\n",
    "Here we have two almost identical features, `water_quality` and `quality_group`, with `water_quality` containing slightly more granular information. We therefore keep that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e10347",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby(['water_quality', 'quality_group']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45da8108",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('quality_group', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694a4eb",
   "metadata": {},
   "source": [
    "### Quantity\n",
    "\n",
    "Two identical features again, `quantity` and `quantity_group`. We'll keep `quantity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d1ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby(['quantity', 'quantity_group']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('quantity_group', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151281f",
   "metadata": {},
   "source": [
    "### Source\n",
    "\n",
    "From `source_class`, `source_type` and `source`, we keep the highest and lowest levels of information. The *other* `source` category seems to fall under *unknown* in `source_class` so we rename it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79837df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby(['source_class', 'source_type', 'source']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('source_type', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d148ba",
   "metadata": {},
   "source": [
    "### Waterpoint\n",
    "\n",
    "Here we again have two almost identical features, `waterpoint_type` and `waterpoint_type_group`. `waterpoint_type` is slightly more precise, so we keep it and drop the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df12a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['year_recorded'][train_data['year_recorded'] < train_data['operation_years']] = train_data['year_recorded'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a5b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = pumpitup.flag_missing_s(train_data['construction_year'])\n",
    "train_data['construction_year'][mask1] = np.nan\n",
    "train_data.loc[mask1, 'construction_year'] = train_data.groupby(['extraction_type', 'installer']).transform('median')\n",
    "mask2 = train_data['construction_year'].isnull()\n",
    "train_data.loc[mask2, 'construction_year'] = train_data.groupby(['extraction_type']).transform('median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ec218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby(['waterpoint_type_group', 'waterpoint_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e309b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('waterpoint_type_group', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e12555",
   "metadata": {},
   "source": [
    "### Geographical Information\n",
    "\n",
    "The geographical position of a region is recorded at multiple levels of precision, in seemingly increasing precision, as given by:\n",
    "- `region` or `region_code`\n",
    "- `district_code` (subcategory within each region)\n",
    "- `ward`\n",
    "- `subvillage`\n",
    "- `longitude` and `latitude`\n",
    "\n",
    "Each `region` contains one or more `region_code`, with some overlap (eg. `region_code` 11 appearing in *Iringa* and *Shinyanga*, while the same `district_code` can appear in many regions, indicating that this is a generic sub-division of a `region`. Some `region_code` and `district_code` numbers do not seem to fit into the regular pattern exactly, but without a better understanding of this categorisation system, it's probably best to leave them as they are. \n",
    "\n",
    "The `ward` and `subvillage` features are much more precise, with 2092 and 19288 unique values respectively. Although individual wards or villages may differ in governance surrounding a water pump, regions and districts likely indicate different political boundaries that may have a larger effect on water pump failure. It probably doesn't make sense to keep this increasingly granular geographical information while we have latitude and longitude as numeric features.\n",
    "\n",
    "To get an idea of the precision, we can make a quick scatter plot of the geographic positions of every point colour mapped to their `region` and `ward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8862b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby(['region', 'region_code', 'district_code']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b70d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_year_encoded = pumpitup.label_encode(train_data['construction_year'][train_data['construction_year'] > 0], le)\n",
    "con_year_encoded_norm = con_year_encoded / con_year_encoded.max()\n",
    "con_year_cmap = [viridis(x) for x in con_year_encoded_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d21e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = pumpitup.flag_missing_s(train_data['longitude'])\n",
    "train_data['longitude'][mask1] = np.nan\n",
    "train_data.loc[mask1, 'longitude'] = train_data.groupby(['region', 'district_code']).transform('mean')\n",
    "mask2 = train_data['longitude'].isnull()\n",
    "train_data.loc[mask2, 'longitude'] = train_data.groupby(['region']).transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59319746",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask3 = pumpitup.flag_missing_s(train_data['latitude'])\n",
    "train_data['latitude'][mask3] = np.nan\n",
    "train_data.loc[mask3, 'latitude'] = train_data.groupby(['region', 'district_code']).transform('mean')\n",
    "mask4 = train_data['latitude'].isnull()\n",
    "train_data.loc[mask4, 'latitude'] = train_data.groupby(['region']).transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14092d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "ax.scatter(train_data['longitude'], train_data['latitude'], color='gray', linewidth=0)\n",
    "ax.scatter(train_data['longitude'][mask1], train_data['latitude'][mask1], color='red', linewidth=0)\n",
    "ax.scatter(train_data['longitude'][mask2], train_data['latitude'][mask2], color='blue', linewidth=0)\n",
    "ax.set_ylim(-13, 0)\n",
    "ax.set_xlim(28, 42)\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_xlabel('Longitude')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d2d029",
   "metadata": {},
   "source": [
    "The red dots show the locations of points where latitude and longitude were imputed using `region` and `district_code`, while the blue does show the locations of points where latitude and longitude were imputed using `region` only\n",
    "\n",
    "**GPS Height**\n",
    "\n",
    "From the previous map of the altitude of each pump, we can see a trend of lower altitudes closer to the coast, with elevation increasing further inland. It's difficult to imput the missing `gps_height` values precisely, but we can try to resample the map and use some mean values."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
