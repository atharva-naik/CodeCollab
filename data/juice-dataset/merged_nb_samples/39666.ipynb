{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3054f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression example\n",
    "import numpy as np #Linear Algebra library\n",
    "import pandas as pd #Data Frames library\n",
    "import matplotlib.pyplot as plt #data visualization library\n",
    "import seaborn as sns #data visualizaiton library\n",
    "from sklearn.linear_model import LinearRegression #importing supervised learning model\n",
    "from sklearn.model_selection import train_test_split #function for splitting data randomly\n",
    "from sklearn import metrics #helps test error in the end\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ad004",
   "metadata": {},
   "source": [
    "<h2>Input file read using pandas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452e7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_data = pd.read_csv(\"C:/users/Bryan/Documents/Comp Sci/Python Hype train/Data_Science/Salary_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2345023d",
   "metadata": {},
   "source": [
    "<h1>Exploring the Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use .head() if the dataset is to large to print the entire thing.\n",
    "salary_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6689aa",
   "metadata": {},
   "source": [
    "<p>Seeing if we have any missing values</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de19dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8bb142",
   "metadata": {},
   "source": [
    "<h1>Importing Dataset and Extracting Dependent and Independent Variables</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef93d38c",
   "metadata": {},
   "source": [
    "<p>Setting up independent and dependent variables. Note: Our predictor values X is formatted into a 2-dimensional array containing while our target value is set up as a 1-dimensional array. To do this, we first take the salary_data and call the method iloc which is short for integer location. This helps us slice our dataset for the values we want. In our predictor variable X, we select all the rows and then we select the 0th column to the -1th column. Notice that this is done so the array is 2 dimensional, reason being if we were to have more than one predictor value, it is important we keep it as an organized array. Finally .values is called onto the sliced data to return us a numpy array that will allow us to make predictions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb674807",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = salary_data.iloc[:,:-1].values\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5871b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = salary_data.iloc[:,1].values\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747b5f3a",
   "metadata": {},
   "source": [
    "<h1>Visualizing the Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988d6ed",
   "metadata": {},
   "source": [
    "<p>this is a distribution plot of the Years of Experience of the employees in the dataset. Bins represent how many different bars the data is sorted into by rounding the values. We can see from above the numbers are not all intergers thus putting them into bins help visualize. First value passed is a one dimensional array, KDE stands for kernel density estimation. This creates a Gaussian Curve, or a normal distribution of the bar. Basically a curve that estimates the bars.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba124e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(salary_data['YearsExperience'],kde=False,bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5969c8c",
   "metadata": {},
   "source": [
    "<p>This is a countplot which is an extremely flexible function for graphing. The data that is passed to it can exist in multiple forms and countplot knows exactly how to handle it. Notice that both graphs are the exact same, but the data can be passed to the function in multiple ways. The first function passes the data straight to the parameter for the y axis and selects the \"YearsExperience\" series, which is a pandas class. The second implementation passes the entire dataset to the function and then tells the graph to only countplot on the y axis value of YearsExperience.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dcb347",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(y=salary_data[\"YearsExperience\"])\n",
    "sns.countplot(y=\"YearsExperience\",data=salary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2c074",
   "metadata": {},
   "source": [
    "<p>The following function .corr() performs correlation calculations. The default form of correlation calculation is the Pearson method. We can specify which method of calculation we would like so. Each have it's own benefits but one should study statistics to attempt to learn more about it.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f06c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_data.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_data.corr(method='kendall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5608b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_data.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2886fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a31fb63",
   "metadata": {},
   "source": [
    "<p>The following heatmap will allow us to visualize the correlation much better.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(salary_data.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e79ad",
   "metadata": {},
   "source": [
    "<p>Visualize data to see if there is any trend between the Years of Experience people have and their salary</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x ='YearsExperience', y = 'Salary', data=salary_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3e3c6d",
   "metadata": {},
   "source": [
    "<h1>Machine Learning Process</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e7095a",
   "metadata": {},
   "source": [
    "<p>Data splitting</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bf7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 1/3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cee2a6",
   "metadata": {},
   "source": [
    "Fitting training data into the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2b8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab4e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "LoBF = lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d4f67b",
   "metadata": {},
   "source": [
    "<p>Making predictions with our trained model onto our split test set</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda14004",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c2c77f",
   "metadata": {},
   "source": [
    "<p>The following code creates and scatter plot using matplotlib instead of the seaborn library. In blue we used the scatter function to display all our points of data that were randomly chosen as the training data and what the actual value of the data was. Now we take the linear regression object that we made early and fit with data to display visually the line of best fit that was created for our data. We add the appropriate titles to our plot as well.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, color = 'blue')\n",
    "plt.plot(X_train, LoBF, color = 'red')\n",
    "plt.title('Salary vs Experience (Training Set)')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317b9f50",
   "metadata": {},
   "source": [
    "<p>Below is the actual predictions that we have made on the test set that we have split from our original data to evaluate how well our model would perform on data that we already have.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6270ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test, y_test, color = 'blue')\n",
    "plt.plot(X_train, LoBF, color = 'red')\n",
    "plt.title('Salary vs Experience (Test set)')\n",
    "plt.xlabel('Years of Experience')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81945585",
   "metadata": {},
   "source": [
    "<p>Now we check how well our model performed on the test data we split from earlier using three types of error calculations metrics from sci-kit. The three error calculations that I have used are the Mean absolute error, Mean squared error and root mean square error.</p>"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
