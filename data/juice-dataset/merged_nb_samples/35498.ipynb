{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d04ae67",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Project 2: Build a Traffic Sign Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e491de",
   "metadata": {},
   "source": [
    "** Given a labelled data set of German road signs; build, train, and test a deep network classifier in Tensorflow.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad80614",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0. Load the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d81fb1",
   "metadata": {},
   "source": [
    "### Load the training, validation, and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22143954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Read in the sign names from the CSV file\n",
    "with open('signnames.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader)\n",
    "    signnames = {int(rows[0]):rows[1] for rows in reader}\n",
    "numsigns = len(signnames)    \n",
    "\n",
    "# Load the pickled training, validation, and test data\n",
    "import pickle\n",
    "training_file = \"data/train.p\"\n",
    "validation_file = \"data/valid.p\"\n",
    "testing_file = \"data/test.p\"\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']\n",
    "\n",
    "print(\"Data is loaded for {} sign types\".format(numsigns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593eee46",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1. Dataset Summary & Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea494be",
   "metadata": {},
   "source": [
    "### A basic summary of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f5566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = y_train.size\n",
    "n_validation = y_valid.size\n",
    "n_test = y_test.size\n",
    "image_shape = X_train[0].shape\n",
    "n_classes = np.unique(y_train).size\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of validation examples =\", n_validation)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109524ab",
   "metadata": {},
   "source": [
    "### An exploratory visualization of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572755cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Plotting images ... this may take a few seconds\")\n",
    "fig = plt.figure(figsize=(12,120))\n",
    "numsamples = 5\n",
    "for ii in range(numsigns):\n",
    "    indices = np.nonzero(y_train==ii)[0]\n",
    "    np.random.shuffle(indices)\n",
    "    for jj in range(min(numsamples,len(indices))):\n",
    "        index = indices[jj]\n",
    "        image = X_train[index].squeeze()\n",
    "        ax = plt.subplot(numsigns,numsamples,numsamples*ii+jj+1)\n",
    "        plt.imshow(image)\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "        if(jj==0):\n",
    "            ax.set_xlabel(\"%d : %s (count = %d)\" % (ii,signnames[ii],len(indices)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a0cc7b",
   "metadata": {},
   "source": [
    "### Plot a histogram indicating the number of each type of image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e398dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.hist(y_train, bins=numsigns, range=(0,43), facecolor='green', edgecolor='black', alpha=0.5);\n",
    "plt.xlabel(\"Sign Label\");\n",
    "plt.ylabel(\"Count\");\n",
    "plt.title(\"Histogram of Sign Types in Training Data\")\n",
    "x_pos = np.arange(numsigns)+0.5\n",
    "ax = plt.gca()\n",
    "ax.set_xticks(x_pos)\n",
    "names = list(signnames.values())\n",
    "ax.set_xticklabels(names, rotation='vertical');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360b42d7",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 2. Preprocess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345bc47",
   "metadata": {},
   "source": [
    "### Define the pre-processing of each image (conversion to grayscale, and normalization)\n",
    "* I used conversion to grayscale, as I did not notice any appreciable improvement when using all three original channels.\n",
    "* The conversion to grayscale is just a simple average of the channels.\n",
    "* Each image is normalized to ensure that it has zero mean across the pixel values with standard deviation of unity.\n",
    "* Normalization helps model training, and ensures that very underexposed images are put on equal footing with bright images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a8fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(X):\n",
    "    X = X.astype(np.float)\n",
    "\n",
    "    # Make grayscale\n",
    "    X = np.mean(X,axis=3)\n",
    "    X = X[...,None]\n",
    "    \n",
    "    # Normalize\n",
    "    for ii in range(X.shape[0]):\n",
    "        meanval = np.mean(X[ii])\n",
    "        stdval = np.std(X[ii])\n",
    "        X[ii] = (X[ii] - meanval)/stdval\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f899cc",
   "metadata": {},
   "source": [
    "### Exploit symmetry in the data, to generate additional training data.\n",
    "* Some of the image classes are closed under horizontal flipping.  For example, a \"yield\" sign remains a \"yield\" sign after horizontal flipping.  \n",
    "* Furthermore, some pairs of classes are complementary under horizontal flipping.  For example, \"Keep Left\" signs become \"Keep Right\" signs under horizontal flipping.\n",
    "* Using these elementary symmetries allows us to augment the data set with 13469 additional images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c4e9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def augmentData(X,y):\n",
    "    # Augment data through trivial reflections\n",
    "    xflip = [11,12,13,15,17,18,22,30,35]\n",
    "    swaps = ((19,20),(33,34),(36,37),(38,39))\n",
    "\n",
    "    xadd = []\n",
    "    yadd = []\n",
    "    for label in xflip:\n",
    "        ind = np.nonzero(y==label)    \n",
    "        xadd.extend(np.flip(X[ind],axis=2))\n",
    "        yadd.extend(y[ind])\n",
    "\n",
    "    for s1,s2 in swaps:\n",
    "        ind1 = np.nonzero(y==s1) \n",
    "        xadd.extend(np.flip(X[ind1],axis=2))\n",
    "        yadd.extend(s2*np.ones_like(y[ind1]))\n",
    "\n",
    "        ind2 = np.nonzero(y==s2) \n",
    "        xadd.extend(np.flip(X[ind2],axis=2))\n",
    "        yadd.extend(s1*np.ones_like(y[ind2]))\n",
    "\n",
    "    xadd = np.array(xadd)\n",
    "    yadd = np.array(yadd)\n",
    "    X = np.concatenate((X,xadd),axis=0)\n",
    "    y = np.concatenate((y,yadd),axis=0)\n",
    "    \n",
    "    print(\"Augmented the image set with {} additional images using symmetry.\".format(yadd.size))\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e627163d",
   "metadata": {},
   "source": [
    "### Pre-process all the images.  Also, augment the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25293e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train = augmentData(X_train,y_train)\n",
    "X_train = preProcess(X_train)\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "X_valid = preProcess(X_valid)\n",
    "X_test = preProcess(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e597e",
   "metadata": {},
   "source": [
    "### Use the Keras ImageDataGenerator() class to further augment the data.\n",
    "* The ImageDataGenerator class allows for generation of additional data by applying rotations, shifts, zooming, and shearing to the images in the original dataset.\n",
    "* We define the parameters of the ImageDataGenerator() class here.\n",
    "* During training, we will use the ImageDataGenerator.flow() method to generate new data \"inline\" as the training epochs proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bbd70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=10.,\n",
    "    width_shift_range=0.,\n",
    "    height_shift_range=0.,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=None,\n",
    "    preprocessing_function=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d43674",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3. Define the Model Architecture\n",
    "### Define the layers in the deep learning network. \n",
    "* I started with the LeCun network from the tutorial as a baseline.\n",
    "* After some experimentation, the final architecture consists of two convolutional layers, and three fully connected layers.  \n",
    "* Drop-out is used to provide regularization.  I found drop-out to provide better performance than max-pooling.  \n",
    "* I did experiment with additional layers, but found little improvement.  Perhaps if I was willing to allow for longer training times with more epochs, more layers would have proven beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0236a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 128\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "# Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "mu = 0.0\n",
    "sigma = 0.1\n",
    "\n",
    "# TF placeholders\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1)) \n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, numsigns)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Layer 1: Convolutional.\n",
    "conv1_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma))\n",
    "conv1_b = tf.Variable(tf.zeros(6))\n",
    "conv1   = tf.nn.conv2d(x, conv1_W, strides=[1, 1, 1, 1], padding='VALID') + conv1_b\n",
    "\n",
    "# Activation.\n",
    "conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "# Drop out\n",
    "conv1 = tf.nn.dropout(conv1, keep_prob)\n",
    "\n",
    "#     # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "#     conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# Layer 2: Convolutional. Output = 10x10x16.\n",
    "conv2_W = tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma))\n",
    "conv2_b = tf.Variable(tf.zeros(16))\n",
    "conv2   = tf.nn.conv2d(conv1, conv2_W, strides=[1, 1, 1, 1], padding='VALID') + conv2_b\n",
    "\n",
    "# Activation.\n",
    "conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "# Drop out\n",
    "conv2 = tf.nn.dropout(conv2, keep_prob)\n",
    "\n",
    "# Flatten. Input = 5x5x16. Output = 400.\n",
    "fc0   = flatten(conv2)\n",
    "\n",
    "# Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "fc1_W = tf.Variable(tf.truncated_normal(shape=(9216, 120), mean = mu, stddev = sigma))\n",
    "fc1_b = tf.Variable(tf.zeros(120))\n",
    "fc1   = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "\n",
    "# Activation.\n",
    "fc1    = tf.nn.relu(fc1)\n",
    "\n",
    "# Drop out\n",
    "fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "# Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "fc2_W  = tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma))\n",
    "fc2_b  = tf.Variable(tf.zeros(84))\n",
    "fc2    = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "\n",
    "# Activation.\n",
    "fc2    = tf.nn.relu(fc2)\n",
    "\n",
    "# Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "fc3_W  = tf.Variable(tf.truncated_normal(shape=(84, 43), mean = mu, stddev = sigma))\n",
    "fc3_b  = tf.Variable(tf.zeros(43))\n",
    "logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7337ec67",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4.  Train and Validate the Model\n",
    "* The model is trained in TensorFlow using the AdamOptimizer.\n",
    "* Learning rate : 0.001\n",
    "* Number of Epochs : 15  (I was trying to get good performance with comparatively little learning).\n",
    "* Batch size : 128\n",
    "* This approach achieves an accuracy on the Test data set of 95%.\n",
    "### Define functional objects for use in TensorFlow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 0.001\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob:1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e3428d",
   "metadata": {},
   "source": [
    "### Unleash the training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c23ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    for i in range(EPOCHS):\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "#         for offset in range(0, num_examples, BATCH_SIZE):\n",
    "#             end = offset + BATCH_SIZE\n",
    "#             batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "        batches = 0   \n",
    "        for batch_x, batch_y in datagen.flow(X_train, y_train, batch_size=BATCH_SIZE):\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob:0.5})\n",
    "            batches += 1\n",
    "            if batches >= len(X_train) / BATCH_SIZE:\n",
    "                break\n",
    "        train_accuracy = evaluate(X_train, y_train)\n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Train Accuracy = {:.3f}\".format(train_accuracy))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    saver.save(sess, './trafficsign')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c16fda4",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5. Test Performance of the Trained Model on the Test Data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6567e866",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"\\nAccuracy on Test Images = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b8329",
   "metadata": {},
   "source": [
    "# ---\n",
    "## Step 6. Test the Trained Model on New Images\n",
    "* We now test our model on 10 new images of German signs downloaded from the web.\n",
    "* The images are resized to 32x32x3 and pre-processed just as described above.\n",
    "* The images appear to be good quality.  There are no occlusions, shadows, glare, etc., so we expect good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dc0244",
   "metadata": {},
   "source": [
    "### Load and plot the new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e88586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "\n",
    "numimages = 10\n",
    "pics0 = []\n",
    "for ii in range(numimages):\n",
    "    fname = \"data/signs/sign{}.jpg\".format(ii)\n",
    "    image = imread(fname)\n",
    "    image = resize(image,(32,32,3),mode='reflect')\n",
    "    pics0.append(image)\n",
    "\n",
    "ylabels0 = [14,13,1,35,22,3,18,5,9,11]    \n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "for ii,pic in enumerate(pics0):\n",
    "    ax = plt.subplot(2,numimages/2,ii+1)\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    ax.get_yaxis().set_ticks([])\n",
    "    plt.xlabel(signnames[ylabels0[ii]])\n",
    "    plt.imshow(pic)\n",
    "\n",
    "pics = np.array(pics0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673fdd4b",
   "metadata": {},
   "source": [
    "### Pre-process the new image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bac016",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pics = preProcess(pics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3ce7fc",
   "metadata": {},
   "source": [
    "### Evaluate the model accuracy on the new image data\n",
    "* The model performs exceptionally well (as expected).  \n",
    "* All images are correctly classified.\n",
    "* In most cases, the model asserts the classifications with very high probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb26e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict_top = {x:X_pics,keep_prob: 1}\n",
    "labels_pred = tf.nn.softmax(logits)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'));\n",
    "    predictions = sess.run(labels_pred,feed_dict = feed_dict_top)\n",
    "    mytop5 = sess.run(tf.nn.top_k(tf.constant(predictions), k=5))\n",
    "    new_accuracy = evaluate(X_pics, ylabels0)\n",
    "\n",
    "print(\"\\n\\nClassification accuracy percentage on new images : {0:.1f}%\\n\".format(new_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7663cecf",
   "metadata": {},
   "source": [
    "### Plot the classification results\n",
    "* We now show the probabilty distribution across the five most likely classifications.\n",
    "* Except for the case of the 60km/hr sign, the model is very certain of the classification.\n",
    "* The 60km/hr sign is somewhat confused with the 80 km/hr sign (I often do the same thing!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for ii,pic in enumerate(pics0):\n",
    "    objects = [signnames[lab] for lab in mytop5.indices[ii]]\n",
    "    y_pos = np.arange(len(objects))+0.5\n",
    "    performance = mytop5.values[ii]\n",
    "    ax = plt.subplot(numimages,2,2*ii+1)\n",
    "    plt.imshow(np.squeeze(pic))\n",
    "    plt.xlabel(signnames[ylabels0[ii]])\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "    ax.get_yaxis().set_ticks([])\n",
    "    ax = plt.subplot(numimages,2,2*ii+2)\n",
    "    ax.barh(y_pos, performance, align='center', alpha=0.8)\n",
    "    ax.invert_yaxis()\n",
    "    plt.axis([0.0,1.0,5.0,0.0])\n",
    "    plt.yticks(y_pos, objects)\n",
    "    plt.xlabel('Probability')\n",
    "#     plt.title('Classification Probabilities')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e8e8cd",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7. Visualize the Neural Network's State with Test Images\n",
    "### Define the network state visualizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):\n",
    "    pimg = preProcess(image_input)\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('.'));\n",
    "        activation = tf_activation.eval(session=sess,feed_dict={x : pimg, keep_prob : 0.5})\n",
    "        featuremaps = activation.shape[3]\n",
    "        plt.figure(plt_num, figsize=(15,15))\n",
    "        ax = plt.subplot(numimages,8, 1)\n",
    "        plt.imshow(np.squeeze(image_input))\n",
    "        ax.get_xaxis().set_ticks([])\n",
    "        ax.get_yaxis().set_ticks([])\n",
    "        for featuremap in range(featuremaps):\n",
    "            ax = plt.subplot(6,9, featuremap+2) # sets the number of feature maps to show on each row and column\n",
    "            plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number\n",
    "            if activation_min != -1 and activation_max != -1:\n",
    "                plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin =activation_min, vmax=activation_max, cmap=\"gray\")\n",
    "            elif activation_max != -1:\n",
    "                plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmax=activation_max, cmap=\"gray\")\n",
    "            elif activation_min !=-1:\n",
    "                plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", vmin=activation_min, cmap=\"gray\")\n",
    "            else:\n",
    "                plt.imshow(activation[0,:,:, featuremap], interpolation=\"nearest\", cmap=\"gray\")\n",
    "            ax.get_xaxis().set_ticks([])\n",
    "            ax.get_yaxis().set_ticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9c9cf",
   "metadata": {},
   "source": [
    "### Visualize the network state for each new image at the first convolutional layer."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
