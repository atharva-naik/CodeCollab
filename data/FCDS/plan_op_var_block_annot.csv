id,blockid,block,plan operator,variable,answer,answer with test
18502,1,"genre_movie = movies[genres].T
#display(genre_movie)


# get sum of all ratings per movie and count of ratings per movie
movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})
display(movies_ratings)

genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]",TODO,genre_movie,"# ever row is what movies are in one category
genre_movie = movies[genres].T
#display(genre_movie)


# get sum of all ratings per movie and count of ratings per movie
movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})
display(movies_ratings)

genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]

#display(
result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)
display(result)","# ever row is what movies are in one category
genre_movie = movies[genres].T
#display(genre_movie)


# get sum of all ratings per movie and count of ratings per movie
movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})
display(movies_ratings)

genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]

#display(
result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)
display(result)
def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # ever row is what movies are in one category
    genre_movie = movies[genres].T

    # get sum of all ratings per movie and count of ratings per movie
    movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})

    genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
    genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]
    
    result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)

    return result.to_dict()
    
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
18502,2,"movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})
display(movies_ratings)

genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]",TODO,movies_ratings,"# ever row is what movies are in one category
genre_movie = movies[genres].T
#display(genre_movie)


# get sum of all ratings per movie and count of ratings per movie
movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})
display(movies_ratings)

genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]

#display(
result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)
display(result)","# ever row is what movies are in one category
genre_movie = movies[genres].T
#display(genre_movie)


# get sum of all ratings per movie and count of ratings per movie
movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})
display(movies_ratings)

genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]

#display(
result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)
display(result)
def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # ever row is what movies are in one category
    genre_movie = movies[genres].T

    # get sum of all ratings per movie and count of ratings per movie
    movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})

    genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
    genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]
    
    result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)

    return result.to_dict()
    
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
18502,3,"genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]

#display(
result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)",TODO,genre_movie_ratings,"# ever row is what movies are in one category
genre_movie = movies[genres].T
#display(genre_movie)


# get sum of all ratings per movie and count of ratings per movie
movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})
display(movies_ratings)

genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]

#display(
result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)
display(result)","# ever row is what movies are in one category
genre_movie = movies[genres].T
#display(genre_movie)


# get sum of all ratings per movie and count of ratings per movie
movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})
display(movies_ratings)

genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]

#display(
result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)
display(result)
def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # ever row is what movies are in one category
    genre_movie = movies[genres].T

    # get sum of all ratings per movie and count of ratings per movie
    movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})

    genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
    genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]
    
    result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)

    return result.to_dict()
    
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
18502,4,"genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]

#display(
result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)",TODO,genre_movie_count,"# ever row is what movies are in one category
genre_movie = movies[genres].T
#display(genre_movie)


# get sum of all ratings per movie and count of ratings per movie
movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})
display(movies_ratings)

genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]

#display(
result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)
display(result)","# ever row is what movies are in one category
genre_movie = movies[genres].T
#display(genre_movie)


# get sum of all ratings per movie and count of ratings per movie
movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})
display(movies_ratings)

genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]

#display(
result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)
display(result)
def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # ever row is what movies are in one category
    genre_movie = movies[genres].T

    # get sum of all ratings per movie and count of ratings per movie
    movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})

    genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
    genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]
    
    result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)

    return result.to_dict()
    
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
18502,5,"result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)
display(result)",TODO,result,"# ever row is what movies are in one category
genre_movie = movies[genres].T
#display(genre_movie)


# get sum of all ratings per movie and count of ratings per movie
movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})
display(movies_ratings)

genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]

#display(
result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)
display(result)","# ever row is what movies are in one category
genre_movie = movies[genres].T
#display(genre_movie)


# get sum of all ratings per movie and count of ratings per movie
movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})
display(movies_ratings)

genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]

#display(
result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)
display(result)
def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # ever row is what movies are in one category
    genre_movie = movies[genres].T

    # get sum of all ratings per movie and count of ratings per movie
    movies_ratings = ratings.groupby('item_id').agg({'rating': [""sum"", 'count']})

    genre_movie_ratings = genre_movie * movies_ratings[('rating', 'sum')]
    genre_movie_count = genre_movie * movies_ratings[('rating', 'count')]
    
    result = np.sum(genre_movie_ratings, axis = 1) / np.sum(genre_movie_count, axis = 1)

    return result.to_dict()
    
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
19672,1,"    Dict = {}
    for key in genres:
        movie_ids = movies[movies[key] == 1].index
        Dict[key] = ratings[ratings['item_id'].isin(movie_ids)]['rating'].mean()
    return Dict",TODO,Dict,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    Dict = {}
    for key in genres:
        movie_ids = movies[movies[key] == 1].index
        Dict[key] = ratings[ratings['item_id'].isin(movie_ids)]['rating'].mean()
    return Dict
    pass","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    Dict = {}
    for key in genres:
        movie_ids = movies[movies[key] == 1].index
        Dict[key] = ratings[ratings['item_id'].isin(movie_ids)]['rating'].mean()
    return Dict
    pass
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
19672,2,"        movie_ids = movies[movies[key] == 1].index
        Dict[key] = ratings[ratings['item_id'].isin(movie_ids)]['rating'].mean()",TODO,movie_ids,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    Dict = {}
    for key in genres:
        movie_ids = movies[movies[key] == 1].index
        Dict[key] = ratings[ratings['item_id'].isin(movie_ids)]['rating'].mean()
    return Dict
    pass","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    Dict = {}
    for key in genres:
        movie_ids = movies[movies[key] == 1].index
        Dict[key] = ratings[ratings['item_id'].isin(movie_ids)]['rating'].mean()
    return Dict
    pass
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
18619,1,"    joined_df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    return {genre : joined_df[""rating""].mul(joined_df[genre]).sum() / joined_df[genre].sum() for genre in genres}",TODO,joined_df,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    joined_df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    return {genre : joined_df[""rating""].mul(joined_df[genre]).sum() / joined_df[genre].sum() for genre in genres}","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    joined_df = ratings.merge(movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    return {genre : joined_df[""rating""].mul(joined_df[genre]).sum() / joined_df[genre].sum() for genre in genres}
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
17341,1,"    avg_rating= dict()
    df=ratings.merge(movies, left_on=""item_id"", right_on=""movie_id"", how=""left"")
    
    for genre in genres:
        avg_rating[genre]=(df[df[genre]==1][""rating""].mean(axis=0))
        
    return avg_rating",TODO,avg_rating,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    
    avg_rating= dict()
    df=ratings.merge(movies, left_on=""item_id"", right_on=""movie_id"", how=""left"")
    
    for genre in genres:
        avg_rating[genre]=(df[df[genre]==1][""rating""].mean(axis=0))
        
    return avg_rating
    
    pass","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    
    avg_rating= dict()
    df=ratings.merge(movies, left_on=""item_id"", right_on=""movie_id"", how=""left"")
    
    for genre in genres:
        avg_rating[genre]=(df[df[genre]==1][""rating""].mean(axis=0))
        
    return avg_rating
    
    pass
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
17341,2,"    df=ratings.merge(movies, left_on=""item_id"", right_on=""movie_id"", how=""left"")
    
    for genre in genres:
        avg_rating[genre]=(df[df[genre]==1][""rating""].mean(axis=0))",TODO,df,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    
    avg_rating= dict()
    df=ratings.merge(movies, left_on=""item_id"", right_on=""movie_id"", how=""left"")
    
    for genre in genres:
        avg_rating[genre]=(df[df[genre]==1][""rating""].mean(axis=0))
        
    return avg_rating
    
    pass","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    
    avg_rating= dict()
    df=ratings.merge(movies, left_on=""item_id"", right_on=""movie_id"", how=""left"")
    
    for genre in genres:
        avg_rating[genre]=(df[df[genre]==1][""rating""].mean(axis=0))
        
    return avg_rating
    
    pass
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
21968,1,"    mov = movies[['Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    rat = ratings[['item_id', 'rating']] 
    # print(mov.head())
    merged = rat.merge(mov, left_on = ""item_id"", right_index = True, how = ""left"")",TODO,mov,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # pass
    # print(movies.head())
    mov = movies[['Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    rat = ratings[['item_id', 'rating']] 
    # print(mov.head())
    merged = rat.merge(mov, left_on = ""item_id"", right_index = True, how = ""left"")
    # print(merged.head())
    ans = {}
    for i in genres:
      subs = merged[merged[i]==1]
      subs = subs[[""rating""]]
      # print(subs.head())
      av = subs.sum(axis=0)/len(subs)
      # print(av[0])
      # for x in av:
      #   print(""yep"")
      #   print(x)
      ans[i] = av[0]
      # break
    # print(ans)
    return ans
","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # pass
    # print(movies.head())
    mov = movies[['Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    rat = ratings[['item_id', 'rating']] 
    # print(mov.head())
    merged = rat.merge(mov, left_on = ""item_id"", right_index = True, how = ""left"")
    # print(merged.head())
    ans = {}
    for i in genres:
      subs = merged[merged[i]==1]
      subs = subs[[""rating""]]
      # print(subs.head())
      av = subs.sum(axis=0)/len(subs)
      # print(av[0])
      # for x in av:
      #   print(""yep"")
      #   print(x)
      ans[i] = av[0]
      # break
    # print(ans)
    return ans

# movie_rating_by_genre(ratings, movies, genres)
# genres = columns[ columns.index('Action'):]
# print(genres)
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
21968,2,"    rat = ratings[['item_id', 'rating']] 
    # print(mov.head())
    merged = rat.merge(mov, left_on = ""item_id"", right_index = True, how = ""left"")",TODO,rat,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # pass
    # print(movies.head())
    mov = movies[['Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    rat = ratings[['item_id', 'rating']] 
    # print(mov.head())
    merged = rat.merge(mov, left_on = ""item_id"", right_index = True, how = ""left"")
    # print(merged.head())
    ans = {}
    for i in genres:
      subs = merged[merged[i]==1]
      subs = subs[[""rating""]]
      # print(subs.head())
      av = subs.sum(axis=0)/len(subs)
      # print(av[0])
      # for x in av:
      #   print(""yep"")
      #   print(x)
      ans[i] = av[0]
      # break
    # print(ans)
    return ans
","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # pass
    # print(movies.head())
    mov = movies[['Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    rat = ratings[['item_id', 'rating']] 
    # print(mov.head())
    merged = rat.merge(mov, left_on = ""item_id"", right_index = True, how = ""left"")
    # print(merged.head())
    ans = {}
    for i in genres:
      subs = merged[merged[i]==1]
      subs = subs[[""rating""]]
      # print(subs.head())
      av = subs.sum(axis=0)/len(subs)
      # print(av[0])
      # for x in av:
      #   print(""yep"")
      #   print(x)
      ans[i] = av[0]
      # break
    # print(ans)
    return ans

# movie_rating_by_genre(ratings, movies, genres)
# genres = columns[ columns.index('Action'):]
# print(genres)
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
21968,3,"    merged = rat.merge(mov, left_on = ""item_id"", right_index = True, how = ""left"")
    # print(merged.head())
    ans = {}
    for i in genres:
      subs = merged[merged[i]==1]",TODO,merged,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # pass
    # print(movies.head())
    mov = movies[['Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    rat = ratings[['item_id', 'rating']] 
    # print(mov.head())
    merged = rat.merge(mov, left_on = ""item_id"", right_index = True, how = ""left"")
    # print(merged.head())
    ans = {}
    for i in genres:
      subs = merged[merged[i]==1]
      subs = subs[[""rating""]]
      # print(subs.head())
      av = subs.sum(axis=0)/len(subs)
      # print(av[0])
      # for x in av:
      #   print(""yep"")
      #   print(x)
      ans[i] = av[0]
      # break
    # print(ans)
    return ans
","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # pass
    # print(movies.head())
    mov = movies[['Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    rat = ratings[['item_id', 'rating']] 
    # print(mov.head())
    merged = rat.merge(mov, left_on = ""item_id"", right_index = True, how = ""left"")
    # print(merged.head())
    ans = {}
    for i in genres:
      subs = merged[merged[i]==1]
      subs = subs[[""rating""]]
      # print(subs.head())
      av = subs.sum(axis=0)/len(subs)
      # print(av[0])
      # for x in av:
      #   print(""yep"")
      #   print(x)
      ans[i] = av[0]
      # break
    # print(ans)
    return ans

# movie_rating_by_genre(ratings, movies, genres)
# genres = columns[ columns.index('Action'):]
# print(genres)
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
21968,4,"    ans = {}
    for i in genres:
      subs = merged[merged[i]==1]
      subs = subs[[""rating""]]
      # print(subs.head())
      av = subs.sum(axis=0)/len(subs)
      # print(av[0])
      # for x in av:
      #   print(""yep"")
      #   print(x)
      ans[i] = av[0]
      # break
    # print(ans)
    return ans",TODO,ans,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # pass
    # print(movies.head())
    mov = movies[['Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    rat = ratings[['item_id', 'rating']] 
    # print(mov.head())
    merged = rat.merge(mov, left_on = ""item_id"", right_index = True, how = ""left"")
    # print(merged.head())
    ans = {}
    for i in genres:
      subs = merged[merged[i]==1]
      subs = subs[[""rating""]]
      # print(subs.head())
      av = subs.sum(axis=0)/len(subs)
      # print(av[0])
      # for x in av:
      #   print(""yep"")
      #   print(x)
      ans[i] = av[0]
      # break
    # print(ans)
    return ans
","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # pass
    # print(movies.head())
    mov = movies[['Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    rat = ratings[['item_id', 'rating']] 
    # print(mov.head())
    merged = rat.merge(mov, left_on = ""item_id"", right_index = True, how = ""left"")
    # print(merged.head())
    ans = {}
    for i in genres:
      subs = merged[merged[i]==1]
      subs = subs[[""rating""]]
      # print(subs.head())
      av = subs.sum(axis=0)/len(subs)
      # print(av[0])
      # for x in av:
      #   print(""yep"")
      #   print(x)
      ans[i] = av[0]
      # break
    # print(ans)
    return ans

# movie_rating_by_genre(ratings, movies, genres)
# genres = columns[ columns.index('Action'):]
# print(genres)
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
21968,5,"      subs = merged[merged[i]==1]
      subs = subs[[""rating""]]
      # print(subs.head())
      av = subs.sum(axis=0)/len(subs)",TODO,subs,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # pass
    # print(movies.head())
    mov = movies[['Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    rat = ratings[['item_id', 'rating']] 
    # print(mov.head())
    merged = rat.merge(mov, left_on = ""item_id"", right_index = True, how = ""left"")
    # print(merged.head())
    ans = {}
    for i in genres:
      subs = merged[merged[i]==1]
      subs = subs[[""rating""]]
      # print(subs.head())
      av = subs.sum(axis=0)/len(subs)
      # print(av[0])
      # for x in av:
      #   print(""yep"")
      #   print(x)
      ans[i] = av[0]
      # break
    # print(ans)
    return ans
","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # pass
    # print(movies.head())
    mov = movies[['Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    rat = ratings[['item_id', 'rating']] 
    # print(mov.head())
    merged = rat.merge(mov, left_on = ""item_id"", right_index = True, how = ""left"")
    # print(merged.head())
    ans = {}
    for i in genres:
      subs = merged[merged[i]==1]
      subs = subs[[""rating""]]
      # print(subs.head())
      av = subs.sum(axis=0)/len(subs)
      # print(av[0])
      # for x in av:
      #   print(""yep"")
      #   print(x)
      ans[i] = av[0]
      # break
    # print(ans)
    return ans

# movie_rating_by_genre(ratings, movies, genres)
# genres = columns[ columns.index('Action'):]
# print(genres)
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
21968,6,"      av = subs.sum(axis=0)/len(subs)
      # print(av[0])
      # for x in av:
      #   print(""yep"")
      #   print(x)
      ans[i] = av[0]",TODO,av,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # pass
    # print(movies.head())
    mov = movies[['Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    rat = ratings[['item_id', 'rating']] 
    # print(mov.head())
    merged = rat.merge(mov, left_on = ""item_id"", right_index = True, how = ""left"")
    # print(merged.head())
    ans = {}
    for i in genres:
      subs = merged[merged[i]==1]
      subs = subs[[""rating""]]
      # print(subs.head())
      av = subs.sum(axis=0)/len(subs)
      # print(av[0])
      # for x in av:
      #   print(""yep"")
      #   print(x)
      ans[i] = av[0]
      # break
    # print(ans)
    return ans
","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    # pass
    # print(movies.head())
    mov = movies[['Action', 'Adventure', 'Animation', 'Children', 'Comedy',
           'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',
           'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']]
    rat = ratings[['item_id', 'rating']] 
    # print(mov.head())
    merged = rat.merge(mov, left_on = ""item_id"", right_index = True, how = ""left"")
    # print(merged.head())
    ans = {}
    for i in genres:
      subs = merged[merged[i]==1]
      subs = subs[[""rating""]]
      # print(subs.head())
      av = subs.sum(axis=0)/len(subs)
      # print(av[0])
      # for x in av:
      #   print(""yep"")
      #   print(x)
      ans[i] = av[0]
      # break
    # print(ans)
    return ans

# movie_rating_by_genre(ratings, movies, genres)
# genres = columns[ columns.index('Action'):]
# print(genres)
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
17656,1,"    movies = movies.melt(id_vars=[ 'movie_title', 'release_date', 'video_release_date', 'IMDb_URL',
      'unknown'],ignore_index=False)
    
    movies = movies[movies[""value""]==1]
    
    serie = ratings.set_index(""item_id"")\
    .join(movies).groupby(""variable"")[""rating""].mean()",TODO,movies,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    movies = movies.melt(id_vars=[ 'movie_title', 'release_date', 'video_release_date', 'IMDb_URL',
      'unknown'],ignore_index=False)
    
    movies = movies[movies[""value""]==1]
    
    serie = ratings.set_index(""item_id"")\
    .join(movies).groupby(""variable"")[""rating""].mean()

    return {genre:serie[genre] for genre in genres}
      ","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    movies = movies.melt(id_vars=[ 'movie_title', 'release_date', 'video_release_date', 'IMDb_URL',
      'unknown'],ignore_index=False)
    
    movies = movies[movies[""value""]==1]
    
    serie = ratings.set_index(""item_id"")\
    .join(movies).groupby(""variable"")[""rating""].mean()

    return {genre:serie[genre] for genre in genres}
      
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
17656,2,"    serie = ratings.set_index(""item_id"")\
    .join(movies).groupby(""variable"")[""rating""].mean()

    return {genre:serie[genre] for genre in genres}",TODO,serie,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    movies = movies.melt(id_vars=[ 'movie_title', 'release_date', 'video_release_date', 'IMDb_URL',
      'unknown'],ignore_index=False)
    
    movies = movies[movies[""value""]==1]
    
    serie = ratings.set_index(""item_id"")\
    .join(movies).groupby(""variable"")[""rating""].mean()

    return {genre:serie[genre] for genre in genres}
      ","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    movies = movies.melt(id_vars=[ 'movie_title', 'release_date', 'video_release_date', 'IMDb_URL',
      'unknown'],ignore_index=False)
    
    movies = movies[movies[""value""]==1]
    
    serie = ratings.set_index(""item_id"")\
    .join(movies).groupby(""variable"")[""rating""].mean()

    return {genre:serie[genre] for genre in genres}
      
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
23120,1,"    ans_dict = {}
    for i in genres:
        genre_specific = movies[movies[i] == 1]
        joined_db_avg = np.average(ratings.merge(genre_specific, how = 'inner', left_on = ""item_id"", right_on = ""movie_id"")[""rating""].values)
        ans_dict[i] = joined_db_avg
    
    return ans_dict",TODO,ans_dict,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    ans_dict = {}
    for i in genres:
        genre_specific = movies[movies[i] == 1]
        joined_db_avg = np.average(ratings.merge(genre_specific, how = 'inner', left_on = ""item_id"", right_on = ""movie_id"")[""rating""].values)
        ans_dict[i] = joined_db_avg
    
    return ans_dict","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    ans_dict = {}
    for i in genres:
        genre_specific = movies[movies[i] == 1]
        joined_db_avg = np.average(ratings.merge(genre_specific, how = 'inner', left_on = ""item_id"", right_on = ""movie_id"")[""rating""].values)
        ans_dict[i] = joined_db_avg
    
    return ans_dict
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
23120,2,"        genre_specific = movies[movies[i] == 1]
        joined_db_avg = np.average(ratings.merge(genre_specific, how = 'inner', left_on = ""item_id"", right_on = ""movie_id"")[""rating""].values)",TODO,genre_specific,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    ans_dict = {}
    for i in genres:
        genre_specific = movies[movies[i] == 1]
        joined_db_avg = np.average(ratings.merge(genre_specific, how = 'inner', left_on = ""item_id"", right_on = ""movie_id"")[""rating""].values)
        ans_dict[i] = joined_db_avg
    
    return ans_dict","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    ans_dict = {}
    for i in genres:
        genre_specific = movies[movies[i] == 1]
        joined_db_avg = np.average(ratings.merge(genre_specific, how = 'inner', left_on = ""item_id"", right_on = ""movie_id"")[""rating""].values)
        ans_dict[i] = joined_db_avg
    
    return ans_dict
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
23120,3,"        joined_db_avg = np.average(ratings.merge(genre_specific, how = 'inner', left_on = ""item_id"", right_on = ""movie_id"")[""rating""].values)
        ans_dict[i] = joined_db_avg",TODO,joined_db_avg,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    ans_dict = {}
    for i in genres:
        genre_specific = movies[movies[i] == 1]
        joined_db_avg = np.average(ratings.merge(genre_specific, how = 'inner', left_on = ""item_id"", right_on = ""movie_id"")[""rating""].values)
        ans_dict[i] = joined_db_avg
    
    return ans_dict","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    ans_dict = {}
    for i in genres:
        genre_specific = movies[movies[i] == 1]
        joined_db_avg = np.average(ratings.merge(genre_specific, how = 'inner', left_on = ""item_id"", right_on = ""movie_id"")[""rating""].values)
        ans_dict[i] = joined_db_avg
    
    return ans_dict
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
1837,1,"    diction = dict()
    for gen in genres:
        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length
    
    return diction",TODO,diction,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    diction = dict()
    for gen in genres:
        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length
    
    return diction","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    diction = dict()
    for gen in genres:
        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length
    
    return diction
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
1837,2,"        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length",TODO,value,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    diction = dict()
    for gen in genres:
        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length
    
    return diction","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    diction = dict()
    for gen in genres:
        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length
    
    return diction
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
1837,3,"        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length",TODO,length,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    diction = dict()
    for gen in genres:
        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length
    
    return diction","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    diction = dict()
    for gen in genres:
        value = 0
        length = 0
        for id in movies[movies[gen] == 1].index:
            value += ratings.loc[ratings['item_id'] == id, ""rating""].sum()
            length += len(ratings.loc[ratings['item_id'] == id, ""rating""])
        diction[gen] = value/length
    
    return diction
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
2278,1,"    genres_dict = {}
 
    # iterating through the elements of list
    for i in genres:
        genres_dict[i] = None
     
    #print(genres_dict)

    movies_ratings = pd.merge(movies, ratings, how='inner', left_on = 'movie_id', right_on = 'item_id')
    #display(movies_ratings.head())
    
    genres_dict['Action'] = movies_ratings.loc[movies_ratings['Action'] == 1]['rating'].mean()
    genres_dict['Adventure'] = movies_ratings.loc[movies_ratings['Adventure'] == 1]['rating'].mean()
    genres_dict['Animation'] = movies_ratings.loc[movies_ratings['Animation'] == 1]['rating'].mean()
    genres_dict['Children'] = movies_ratings.loc[movies_ratings['Children'] == 1]['rating'].mean()
    genres_dict['Comedy'] = movies_ratings.loc[movies_ratings['Comedy'] == 1]['rating'].mean()
    genres_dict['Documentary'] = movies_ratings.loc[movies_ratings['Documentary'] == 1]['rating'].mean()
    genres_dict['Drama'] = movies_ratings.loc[movies_ratings['Drama'] == 1]['rating'].mean()
    genres_dict['Fantasy'] = movies_ratings.loc[movies_ratings['Fantasy'] == 1]['rating'].mean()
    genres_dict['Film-Noir'] = movies_ratings.loc[movies_ratings['Film-Noir'] == 1]['rating'].mean()
    genres_dict['Horror'] = movies_ratings.loc[movies_ratings['Horror'] == 1]['rating'].mean()
    genres_dict['Musical'] = movies_ratings.loc[movies_ratings['Musical'] == 1]['rating'].mean()
    genres_dict['Mystery'] = movies_ratings.loc[movies_ratings['Mystery'] == 1]['rating'].mean()
    genres_dict['Sci-Fi'] = movies_ratings.loc[movies_ratings['Sci-Fi'] == 1]['rating'].mean()
    genres_dict['Thriller'] = movies_ratings.loc[movies_ratings['Thriller'] == 1]['rating'].mean()
    genres_dict['War'] = movies_ratings.loc[movies_ratings['War'] == 1]['rating'].mean()
    genres_dict['Western'] = movies_ratings.loc[movies_ratings['Western'] == 1]['rating'].mean()
    genres_dict['Crime'] = movies_ratings.loc[movies_ratings['Crime'] == 1]['rating'].mean()
    genres_dict['Romance'] = movies_ratings.loc[movies_ratings['Romance'] == 1]['rating'].mean()
    
    return genres_dict",TODO,genres_dict,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    
    #display(movies.head())
    #display(ratings.head())
    #display(genres)
    
    
    genres_dict = {}
 
    # iterating through the elements of list
    for i in genres:
        genres_dict[i] = None
     
    #print(genres_dict)

    movies_ratings = pd.merge(movies, ratings, how='inner', left_on = 'movie_id', right_on = 'item_id')
    #display(movies_ratings.head())
    
    genres_dict['Action'] = movies_ratings.loc[movies_ratings['Action'] == 1]['rating'].mean()
    genres_dict['Adventure'] = movies_ratings.loc[movies_ratings['Adventure'] == 1]['rating'].mean()
    genres_dict['Animation'] = movies_ratings.loc[movies_ratings['Animation'] == 1]['rating'].mean()
    genres_dict['Children'] = movies_ratings.loc[movies_ratings['Children'] == 1]['rating'].mean()
    genres_dict['Comedy'] = movies_ratings.loc[movies_ratings['Comedy'] == 1]['rating'].mean()
    genres_dict['Documentary'] = movies_ratings.loc[movies_ratings['Documentary'] == 1]['rating'].mean()
    genres_dict['Drama'] = movies_ratings.loc[movies_ratings['Drama'] == 1]['rating'].mean()
    genres_dict['Fantasy'] = movies_ratings.loc[movies_ratings['Fantasy'] == 1]['rating'].mean()
    genres_dict['Film-Noir'] = movies_ratings.loc[movies_ratings['Film-Noir'] == 1]['rating'].mean()
    genres_dict['Horror'] = movies_ratings.loc[movies_ratings['Horror'] == 1]['rating'].mean()
    genres_dict['Musical'] = movies_ratings.loc[movies_ratings['Musical'] == 1]['rating'].mean()
    genres_dict['Mystery'] = movies_ratings.loc[movies_ratings['Mystery'] == 1]['rating'].mean()
    genres_dict['Sci-Fi'] = movies_ratings.loc[movies_ratings['Sci-Fi'] == 1]['rating'].mean()
    genres_dict['Thriller'] = movies_ratings.loc[movies_ratings['Thriller'] == 1]['rating'].mean()
    genres_dict['War'] = movies_ratings.loc[movies_ratings['War'] == 1]['rating'].mean()
    genres_dict['Western'] = movies_ratings.loc[movies_ratings['Western'] == 1]['rating'].mean()
    genres_dict['Crime'] = movies_ratings.loc[movies_ratings['Crime'] == 1]['rating'].mean()
    genres_dict['Romance'] = movies_ratings.loc[movies_ratings['Romance'] == 1]['rating'].mean()
    
    return genres_dict
     
    pass","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    
    #display(movies.head())
    #display(ratings.head())
    #display(genres)
    
    
    genres_dict = {}
 
    # iterating through the elements of list
    for i in genres:
        genres_dict[i] = None
     
    #print(genres_dict)

    movies_ratings = pd.merge(movies, ratings, how='inner', left_on = 'movie_id', right_on = 'item_id')
    #display(movies_ratings.head())
    
    genres_dict['Action'] = movies_ratings.loc[movies_ratings['Action'] == 1]['rating'].mean()
    genres_dict['Adventure'] = movies_ratings.loc[movies_ratings['Adventure'] == 1]['rating'].mean()
    genres_dict['Animation'] = movies_ratings.loc[movies_ratings['Animation'] == 1]['rating'].mean()
    genres_dict['Children'] = movies_ratings.loc[movies_ratings['Children'] == 1]['rating'].mean()
    genres_dict['Comedy'] = movies_ratings.loc[movies_ratings['Comedy'] == 1]['rating'].mean()
    genres_dict['Documentary'] = movies_ratings.loc[movies_ratings['Documentary'] == 1]['rating'].mean()
    genres_dict['Drama'] = movies_ratings.loc[movies_ratings['Drama'] == 1]['rating'].mean()
    genres_dict['Fantasy'] = movies_ratings.loc[movies_ratings['Fantasy'] == 1]['rating'].mean()
    genres_dict['Film-Noir'] = movies_ratings.loc[movies_ratings['Film-Noir'] == 1]['rating'].mean()
    genres_dict['Horror'] = movies_ratings.loc[movies_ratings['Horror'] == 1]['rating'].mean()
    genres_dict['Musical'] = movies_ratings.loc[movies_ratings['Musical'] == 1]['rating'].mean()
    genres_dict['Mystery'] = movies_ratings.loc[movies_ratings['Mystery'] == 1]['rating'].mean()
    genres_dict['Sci-Fi'] = movies_ratings.loc[movies_ratings['Sci-Fi'] == 1]['rating'].mean()
    genres_dict['Thriller'] = movies_ratings.loc[movies_ratings['Thriller'] == 1]['rating'].mean()
    genres_dict['War'] = movies_ratings.loc[movies_ratings['War'] == 1]['rating'].mean()
    genres_dict['Western'] = movies_ratings.loc[movies_ratings['Western'] == 1]['rating'].mean()
    genres_dict['Crime'] = movies_ratings.loc[movies_ratings['Crime'] == 1]['rating'].mean()
    genres_dict['Romance'] = movies_ratings.loc[movies_ratings['Romance'] == 1]['rating'].mean()
    
    return genres_dict
     
    pass
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
2278,2,"    movies_ratings = pd.merge(movies, ratings, how='inner', left_on = 'movie_id', right_on = 'item_id')
    #display(movies_ratings.head())
    
    genres_dict['Action'] = movies_ratings.loc[movies_ratings['Action'] == 1]['rating'].mean()
    genres_dict['Adventure'] = movies_ratings.loc[movies_ratings['Adventure'] == 1]['rating'].mean()
    genres_dict['Animation'] = movies_ratings.loc[movies_ratings['Animation'] == 1]['rating'].mean()
    genres_dict['Children'] = movies_ratings.loc[movies_ratings['Children'] == 1]['rating'].mean()
    genres_dict['Comedy'] = movies_ratings.loc[movies_ratings['Comedy'] == 1]['rating'].mean()
    genres_dict['Documentary'] = movies_ratings.loc[movies_ratings['Documentary'] == 1]['rating'].mean()
    genres_dict['Drama'] = movies_ratings.loc[movies_ratings['Drama'] == 1]['rating'].mean()
    genres_dict['Fantasy'] = movies_ratings.loc[movies_ratings['Fantasy'] == 1]['rating'].mean()
    genres_dict['Film-Noir'] = movies_ratings.loc[movies_ratings['Film-Noir'] == 1]['rating'].mean()
    genres_dict['Horror'] = movies_ratings.loc[movies_ratings['Horror'] == 1]['rating'].mean()
    genres_dict['Musical'] = movies_ratings.loc[movies_ratings['Musical'] == 1]['rating'].mean()
    genres_dict['Mystery'] = movies_ratings.loc[movies_ratings['Mystery'] == 1]['rating'].mean()
    genres_dict['Sci-Fi'] = movies_ratings.loc[movies_ratings['Sci-Fi'] == 1]['rating'].mean()
    genres_dict['Thriller'] = movies_ratings.loc[movies_ratings['Thriller'] == 1]['rating'].mean()
    genres_dict['War'] = movies_ratings.loc[movies_ratings['War'] == 1]['rating'].mean()
    genres_dict['Western'] = movies_ratings.loc[movies_ratings['Western'] == 1]['rating'].mean()
    genres_dict['Crime'] = movies_ratings.loc[movies_ratings['Crime'] == 1]['rating'].mean()
    genres_dict['Romance'] = movies_ratings.loc[movies_ratings['Romance'] == 1]['rating'].mean()",TODO,movies_ratings,"def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    
    #display(movies.head())
    #display(ratings.head())
    #display(genres)
    
    
    genres_dict = {}
 
    # iterating through the elements of list
    for i in genres:
        genres_dict[i] = None
     
    #print(genres_dict)

    movies_ratings = pd.merge(movies, ratings, how='inner', left_on = 'movie_id', right_on = 'item_id')
    #display(movies_ratings.head())
    
    genres_dict['Action'] = movies_ratings.loc[movies_ratings['Action'] == 1]['rating'].mean()
    genres_dict['Adventure'] = movies_ratings.loc[movies_ratings['Adventure'] == 1]['rating'].mean()
    genres_dict['Animation'] = movies_ratings.loc[movies_ratings['Animation'] == 1]['rating'].mean()
    genres_dict['Children'] = movies_ratings.loc[movies_ratings['Children'] == 1]['rating'].mean()
    genres_dict['Comedy'] = movies_ratings.loc[movies_ratings['Comedy'] == 1]['rating'].mean()
    genres_dict['Documentary'] = movies_ratings.loc[movies_ratings['Documentary'] == 1]['rating'].mean()
    genres_dict['Drama'] = movies_ratings.loc[movies_ratings['Drama'] == 1]['rating'].mean()
    genres_dict['Fantasy'] = movies_ratings.loc[movies_ratings['Fantasy'] == 1]['rating'].mean()
    genres_dict['Film-Noir'] = movies_ratings.loc[movies_ratings['Film-Noir'] == 1]['rating'].mean()
    genres_dict['Horror'] = movies_ratings.loc[movies_ratings['Horror'] == 1]['rating'].mean()
    genres_dict['Musical'] = movies_ratings.loc[movies_ratings['Musical'] == 1]['rating'].mean()
    genres_dict['Mystery'] = movies_ratings.loc[movies_ratings['Mystery'] == 1]['rating'].mean()
    genres_dict['Sci-Fi'] = movies_ratings.loc[movies_ratings['Sci-Fi'] == 1]['rating'].mean()
    genres_dict['Thriller'] = movies_ratings.loc[movies_ratings['Thriller'] == 1]['rating'].mean()
    genres_dict['War'] = movies_ratings.loc[movies_ratings['War'] == 1]['rating'].mean()
    genres_dict['Western'] = movies_ratings.loc[movies_ratings['Western'] == 1]['rating'].mean()
    genres_dict['Crime'] = movies_ratings.loc[movies_ratings['Crime'] == 1]['rating'].mean()
    genres_dict['Romance'] = movies_ratings.loc[movies_ratings['Romance'] == 1]['rating'].mean()
    
    return genres_dict
     
    pass","def movie_rating_by_genre(ratings, movies, genres):
    """"""
    Compute the average movie ratings by genres.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, float]  : a mapping from movie genres to their average rating scores
    """"""
    
    #display(movies.head())
    #display(ratings.head())
    #display(genres)
    
    
    genres_dict = {}
 
    # iterating through the elements of list
    for i in genres:
        genres_dict[i] = None
     
    #print(genres_dict)

    movies_ratings = pd.merge(movies, ratings, how='inner', left_on = 'movie_id', right_on = 'item_id')
    #display(movies_ratings.head())
    
    genres_dict['Action'] = movies_ratings.loc[movies_ratings['Action'] == 1]['rating'].mean()
    genres_dict['Adventure'] = movies_ratings.loc[movies_ratings['Adventure'] == 1]['rating'].mean()
    genres_dict['Animation'] = movies_ratings.loc[movies_ratings['Animation'] == 1]['rating'].mean()
    genres_dict['Children'] = movies_ratings.loc[movies_ratings['Children'] == 1]['rating'].mean()
    genres_dict['Comedy'] = movies_ratings.loc[movies_ratings['Comedy'] == 1]['rating'].mean()
    genres_dict['Documentary'] = movies_ratings.loc[movies_ratings['Documentary'] == 1]['rating'].mean()
    genres_dict['Drama'] = movies_ratings.loc[movies_ratings['Drama'] == 1]['rating'].mean()
    genres_dict['Fantasy'] = movies_ratings.loc[movies_ratings['Fantasy'] == 1]['rating'].mean()
    genres_dict['Film-Noir'] = movies_ratings.loc[movies_ratings['Film-Noir'] == 1]['rating'].mean()
    genres_dict['Horror'] = movies_ratings.loc[movies_ratings['Horror'] == 1]['rating'].mean()
    genres_dict['Musical'] = movies_ratings.loc[movies_ratings['Musical'] == 1]['rating'].mean()
    genres_dict['Mystery'] = movies_ratings.loc[movies_ratings['Mystery'] == 1]['rating'].mean()
    genres_dict['Sci-Fi'] = movies_ratings.loc[movies_ratings['Sci-Fi'] == 1]['rating'].mean()
    genres_dict['Thriller'] = movies_ratings.loc[movies_ratings['Thriller'] == 1]['rating'].mean()
    genres_dict['War'] = movies_ratings.loc[movies_ratings['War'] == 1]['rating'].mean()
    genres_dict['Western'] = movies_ratings.loc[movies_ratings['Western'] == 1]['rating'].mean()
    genres_dict['Crime'] = movies_ratings.loc[movies_ratings['Crime'] == 1]['rating'].mean()
    genres_dict['Romance'] = movies_ratings.loc[movies_ratings['Romance'] == 1]['rating'].mean()
    
    return genres_dict
     
    pass
# local tests
def test_movie_rating_by_genre():
    genre_rating = movie_rating_by_genre(ratings, movies, genres)
    assert len(genre_rating) == 18
    assert np.allclose(genre_rating['Action'], 3.480245417953027)
    print(""All tests passed!"")

test_movie_rating_by_genre()"
23447,1,"    return_val = {}
    #filter out only correct genres
    for genre in genres:
        return_val[genre] = {}
        genre_movies = movies[movies[genre] == 1]
        years = genre_movies['release_date'].str[-4:]
        counts = years.value_counts()
        counts.index = counts.index.astype('int')
        counts.astype('int')
        genre_dict = counts.to_dict()
        return_val[genre] = genre_dict
    return return_val",TODO,return_val,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    return_val = {}
    #filter out only correct genres
    for genre in genres:
        return_val[genre] = {}
        genre_movies = movies[movies[genre] == 1]
        years = genre_movies['release_date'].str[-4:]
        counts = years.value_counts()
        counts.index = counts.index.astype('int')
        counts.astype('int')
        genre_dict = counts.to_dict()
        return_val[genre] = genre_dict
    return return_val","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    return_val = {}
    #filter out only correct genres
    for genre in genres:
        return_val[genre] = {}
        genre_movies = movies[movies[genre] == 1]
        years = genre_movies['release_date'].str[-4:]
        counts = years.value_counts()
        counts.index = counts.index.astype('int')
        counts.astype('int')
        genre_dict = counts.to_dict()
        return_val[genre] = genre_dict
    return return_val
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
23447,2,"        genre_movies = movies[movies[genre] == 1]
        years = genre_movies['release_date'].str[-4:]",TODO,genre_movies,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    return_val = {}
    #filter out only correct genres
    for genre in genres:
        return_val[genre] = {}
        genre_movies = movies[movies[genre] == 1]
        years = genre_movies['release_date'].str[-4:]
        counts = years.value_counts()
        counts.index = counts.index.astype('int')
        counts.astype('int')
        genre_dict = counts.to_dict()
        return_val[genre] = genre_dict
    return return_val","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    return_val = {}
    #filter out only correct genres
    for genre in genres:
        return_val[genre] = {}
        genre_movies = movies[movies[genre] == 1]
        years = genre_movies['release_date'].str[-4:]
        counts = years.value_counts()
        counts.index = counts.index.astype('int')
        counts.astype('int')
        genre_dict = counts.to_dict()
        return_val[genre] = genre_dict
    return return_val
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
23447,3,"        years = genre_movies['release_date'].str[-4:]
        counts = years.value_counts()",TODO,years,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    return_val = {}
    #filter out only correct genres
    for genre in genres:
        return_val[genre] = {}
        genre_movies = movies[movies[genre] == 1]
        years = genre_movies['release_date'].str[-4:]
        counts = years.value_counts()
        counts.index = counts.index.astype('int')
        counts.astype('int')
        genre_dict = counts.to_dict()
        return_val[genre] = genre_dict
    return return_val","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    return_val = {}
    #filter out only correct genres
    for genre in genres:
        return_val[genre] = {}
        genre_movies = movies[movies[genre] == 1]
        years = genre_movies['release_date'].str[-4:]
        counts = years.value_counts()
        counts.index = counts.index.astype('int')
        counts.astype('int')
        genre_dict = counts.to_dict()
        return_val[genre] = genre_dict
    return return_val
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
23447,4,"        counts = years.value_counts()
        counts.index = counts.index.astype('int')
        counts.astype('int')
        genre_dict = counts.to_dict()",TODO,counts,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    return_val = {}
    #filter out only correct genres
    for genre in genres:
        return_val[genre] = {}
        genre_movies = movies[movies[genre] == 1]
        years = genre_movies['release_date'].str[-4:]
        counts = years.value_counts()
        counts.index = counts.index.astype('int')
        counts.astype('int')
        genre_dict = counts.to_dict()
        return_val[genre] = genre_dict
    return return_val","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    return_val = {}
    #filter out only correct genres
    for genre in genres:
        return_val[genre] = {}
        genre_movies = movies[movies[genre] == 1]
        years = genre_movies['release_date'].str[-4:]
        counts = years.value_counts()
        counts.index = counts.index.astype('int')
        counts.astype('int')
        genre_dict = counts.to_dict()
        return_val[genre] = genre_dict
    return return_val
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
23447,5,"        genre_dict = counts.to_dict()
        return_val[genre] = genre_dict",TODO,genre_dict,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    return_val = {}
    #filter out only correct genres
    for genre in genres:
        return_val[genre] = {}
        genre_movies = movies[movies[genre] == 1]
        years = genre_movies['release_date'].str[-4:]
        counts = years.value_counts()
        counts.index = counts.index.astype('int')
        counts.astype('int')
        genre_dict = counts.to_dict()
        return_val[genre] = genre_dict
    return return_val","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    return_val = {}
    #filter out only correct genres
    for genre in genres:
        return_val[genre] = {}
        genre_movies = movies[movies[genre] == 1]
        years = genre_movies['release_date'].str[-4:]
        counts = years.value_counts()
        counts.index = counts.index.astype('int')
        counts.astype('int')
        genre_dict = counts.to_dict()
        return_val[genre] = genre_dict
    return return_val
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
17036,1,"    column_genre = movies.groupby(""release_year"").sum()
    
    # Create a dictionary of dictionaries
    nested_mapping = (column_genre[genres]).apply(lambda x : (x[x > 0]).to_dict()).to_dict()",TODO,column_genre,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # create a column for the year
    movies[""release_year""] = pd.Series(movies[""release_date""].apply(lambda x: 0 if pd.isnull(x) else str(x)[-4:]), dtype=""int32"")
    
    # get the number of movies for each year with genre as the column
    column_genre = movies.groupby(""release_year"").sum()
    
    # Create a dictionary of dictionaries
    nested_mapping = (column_genre[genres]).apply(lambda x : (x[x > 0]).to_dict()).to_dict()
    return nested_mapping","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # create a column for the year
    movies[""release_year""] = pd.Series(movies[""release_date""].apply(lambda x: 0 if pd.isnull(x) else str(x)[-4:]), dtype=""int32"")
    
    # get the number of movies for each year with genre as the column
    column_genre = movies.groupby(""release_year"").sum()
    
    # Create a dictionary of dictionaries
    nested_mapping = (column_genre[genres]).apply(lambda x : (x[x > 0]).to_dict()).to_dict()
    return nested_mapping
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
17036,2,"    nested_mapping = (column_genre[genres]).apply(lambda x : (x[x > 0]).to_dict()).to_dict()
    return nested_mapping",TODO,nested_mapping,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # create a column for the year
    movies[""release_year""] = pd.Series(movies[""release_date""].apply(lambda x: 0 if pd.isnull(x) else str(x)[-4:]), dtype=""int32"")
    
    # get the number of movies for each year with genre as the column
    column_genre = movies.groupby(""release_year"").sum()
    
    # Create a dictionary of dictionaries
    nested_mapping = (column_genre[genres]).apply(lambda x : (x[x > 0]).to_dict()).to_dict()
    return nested_mapping","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # create a column for the year
    movies[""release_year""] = pd.Series(movies[""release_date""].apply(lambda x: 0 if pd.isnull(x) else str(x)[-4:]), dtype=""int32"")
    
    # get the number of movies for each year with genre as the column
    column_genre = movies.groupby(""release_year"").sum()
    
    # Create a dictionary of dictionaries
    nested_mapping = (column_genre[genres]).apply(lambda x : (x[x > 0]).to_dict()).to_dict()
    return nested_mapping
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
24293,1,"    movies_genre_year = movies.copy()
    movies_genre_year['release_date'] = pd.to_datetime(movies_genre_year['release_date'], format='%d-%b-%Y', errors='coerce')
    movies_genre_year = movies_genre_year.dropna(subset=['release_date'])
    movies_genre_year['year'] = movies_genre_year['release_date'].dt.year

    #group by year and sum to get counts for each year for genre
    movies_by_genre_count = movies_genre_year.groupby('year')[genres].sum()",TODO,movies_genre_year,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""

    # preprocess to get year to be able to groupby that
    movies_genre_year = movies.copy()
    movies_genre_year['release_date'] = pd.to_datetime(movies_genre_year['release_date'], format='%d-%b-%Y', errors='coerce')
    movies_genre_year = movies_genre_year.dropna(subset=['release_date'])
    movies_genre_year['year'] = movies_genre_year['release_date'].dt.year

    #group by year and sum to get counts for each year for genre
    movies_by_genre_count = movies_genre_year.groupby('year')[genres].sum()
    genre_year_count_df = movies_by_genre_count.to_dict()

    #filter out the 0s
    genre_year_count = {genre: {year: count for year, count in year.items() if count != 0} 
               for genre, year in genre_year_count_df.items()}


    return genre_year_count","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""

    # preprocess to get year to be able to groupby that
    movies_genre_year = movies.copy()
    movies_genre_year['release_date'] = pd.to_datetime(movies_genre_year['release_date'], format='%d-%b-%Y', errors='coerce')
    movies_genre_year = movies_genre_year.dropna(subset=['release_date'])
    movies_genre_year['year'] = movies_genre_year['release_date'].dt.year

    #group by year and sum to get counts for each year for genre
    movies_by_genre_count = movies_genre_year.groupby('year')[genres].sum()
    genre_year_count_df = movies_by_genre_count.to_dict()

    #filter out the 0s
    genre_year_count = {genre: {year: count for year, count in year.items() if count != 0} 
               for genre, year in genre_year_count_df.items()}


    return genre_year_count
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
24293,2,"    movies_by_genre_count = movies_genre_year.groupby('year')[genres].sum()
    genre_year_count_df = movies_by_genre_count.to_dict()",TODO,movies_by_genre_count,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""

    # preprocess to get year to be able to groupby that
    movies_genre_year = movies.copy()
    movies_genre_year['release_date'] = pd.to_datetime(movies_genre_year['release_date'], format='%d-%b-%Y', errors='coerce')
    movies_genre_year = movies_genre_year.dropna(subset=['release_date'])
    movies_genre_year['year'] = movies_genre_year['release_date'].dt.year

    #group by year and sum to get counts for each year for genre
    movies_by_genre_count = movies_genre_year.groupby('year')[genres].sum()
    genre_year_count_df = movies_by_genre_count.to_dict()

    #filter out the 0s
    genre_year_count = {genre: {year: count for year, count in year.items() if count != 0} 
               for genre, year in genre_year_count_df.items()}


    return genre_year_count","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""

    # preprocess to get year to be able to groupby that
    movies_genre_year = movies.copy()
    movies_genre_year['release_date'] = pd.to_datetime(movies_genre_year['release_date'], format='%d-%b-%Y', errors='coerce')
    movies_genre_year = movies_genre_year.dropna(subset=['release_date'])
    movies_genre_year['year'] = movies_genre_year['release_date'].dt.year

    #group by year and sum to get counts for each year for genre
    movies_by_genre_count = movies_genre_year.groupby('year')[genres].sum()
    genre_year_count_df = movies_by_genre_count.to_dict()

    #filter out the 0s
    genre_year_count = {genre: {year: count for year, count in year.items() if count != 0} 
               for genre, year in genre_year_count_df.items()}


    return genre_year_count
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
24293,3,"    genre_year_count_df = movies_by_genre_count.to_dict()

    #filter out the 0s
    genre_year_count = {genre: {year: count for year, count in year.items() if count != 0} 
               for genre, year in genre_year_count_df.items()}",TODO,genre_year_count_df,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""

    # preprocess to get year to be able to groupby that
    movies_genre_year = movies.copy()
    movies_genre_year['release_date'] = pd.to_datetime(movies_genre_year['release_date'], format='%d-%b-%Y', errors='coerce')
    movies_genre_year = movies_genre_year.dropna(subset=['release_date'])
    movies_genre_year['year'] = movies_genre_year['release_date'].dt.year

    #group by year and sum to get counts for each year for genre
    movies_by_genre_count = movies_genre_year.groupby('year')[genres].sum()
    genre_year_count_df = movies_by_genre_count.to_dict()

    #filter out the 0s
    genre_year_count = {genre: {year: count for year, count in year.items() if count != 0} 
               for genre, year in genre_year_count_df.items()}


    return genre_year_count","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""

    # preprocess to get year to be able to groupby that
    movies_genre_year = movies.copy()
    movies_genre_year['release_date'] = pd.to_datetime(movies_genre_year['release_date'], format='%d-%b-%Y', errors='coerce')
    movies_genre_year = movies_genre_year.dropna(subset=['release_date'])
    movies_genre_year['year'] = movies_genre_year['release_date'].dt.year

    #group by year and sum to get counts for each year for genre
    movies_by_genre_count = movies_genre_year.groupby('year')[genres].sum()
    genre_year_count_df = movies_by_genre_count.to_dict()

    #filter out the 0s
    genre_year_count = {genre: {year: count for year, count in year.items() if count != 0} 
               for genre, year in genre_year_count_df.items()}


    return genre_year_count
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
24293,4,"    genre_year_count = {genre: {year: count for year, count in year.items() if count != 0} 
               for genre, year in genre_year_count_df.items()}


    return genre_year_count",TODO,genre_year_count,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""

    # preprocess to get year to be able to groupby that
    movies_genre_year = movies.copy()
    movies_genre_year['release_date'] = pd.to_datetime(movies_genre_year['release_date'], format='%d-%b-%Y', errors='coerce')
    movies_genre_year = movies_genre_year.dropna(subset=['release_date'])
    movies_genre_year['year'] = movies_genre_year['release_date'].dt.year

    #group by year and sum to get counts for each year for genre
    movies_by_genre_count = movies_genre_year.groupby('year')[genres].sum()
    genre_year_count_df = movies_by_genre_count.to_dict()

    #filter out the 0s
    genre_year_count = {genre: {year: count for year, count in year.items() if count != 0} 
               for genre, year in genre_year_count_df.items()}


    return genre_year_count","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""

    # preprocess to get year to be able to groupby that
    movies_genre_year = movies.copy()
    movies_genre_year['release_date'] = pd.to_datetime(movies_genre_year['release_date'], format='%d-%b-%Y', errors='coerce')
    movies_genre_year = movies_genre_year.dropna(subset=['release_date'])
    movies_genre_year['year'] = movies_genre_year['release_date'].dt.year

    #group by year and sum to get counts for each year for genre
    movies_by_genre_count = movies_genre_year.groupby('year')[genres].sum()
    genre_year_count_df = movies_by_genre_count.to_dict()

    #filter out the 0s
    genre_year_count = {genre: {year: count for year, count in year.items() if count != 0} 
               for genre, year in genre_year_count_df.items()}


    return genre_year_count
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
21248,1,"    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre",TODO,movie_count_genre,"from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre","from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
21248,2,"    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count",TODO,year_count,"from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre","from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
21248,3,"      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()",TODO,update_movies,"from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre","from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
21248,4,"      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()",TODO,count,"from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre","from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
21167,1,"    result = dict()
    def convert(x):
        return int(x[-4:])
    movies = movies.dropna(subset=['release_date'])
    movies[""release_date""] = movies[""release_date""].apply(convert)
    for genre in genres:
        genre_dict = dict()
        genre_df = movies[movies[genre] == 1]
        count = genre_df.groupby(['release_date'], group_keys=True)['release_date'].count().reset_index(name=""count"")
        for i in range(len(count)):
            genre_dict[int(count.iloc[[i]][""release_date""])] = int(count.iloc[[i]][""count""])
        result[genre] = genre_dict

    return result",TODO,result,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    result = dict()
    def convert(x):
        return int(x[-4:])
    movies = movies.dropna(subset=['release_date'])
    movies[""release_date""] = movies[""release_date""].apply(convert)
    for genre in genres:
        genre_dict = dict()
        genre_df = movies[movies[genre] == 1]
        count = genre_df.groupby(['release_date'], group_keys=True)['release_date'].count().reset_index(name=""count"")
        for i in range(len(count)):
            genre_dict[int(count.iloc[[i]][""release_date""])] = int(count.iloc[[i]][""count""])
        result[genre] = genre_dict

    return result","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    result = dict()
    def convert(x):
        return int(x[-4:])
    movies = movies.dropna(subset=['release_date'])
    movies[""release_date""] = movies[""release_date""].apply(convert)
    for genre in genres:
        genre_dict = dict()
        genre_df = movies[movies[genre] == 1]
        count = genre_df.groupby(['release_date'], group_keys=True)['release_date'].count().reset_index(name=""count"")
        for i in range(len(count)):
            genre_dict[int(count.iloc[[i]][""release_date""])] = int(count.iloc[[i]][""count""])
        result[genre] = genre_dict

    return result
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
21167,2,"    movies = movies.dropna(subset=['release_date'])
    movies[""release_date""] = movies[""release_date""].apply(convert)
    for genre in genres:
        genre_dict = dict()
        genre_df = movies[movies[genre] == 1]",TODO,movies,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    result = dict()
    def convert(x):
        return int(x[-4:])
    movies = movies.dropna(subset=['release_date'])
    movies[""release_date""] = movies[""release_date""].apply(convert)
    for genre in genres:
        genre_dict = dict()
        genre_df = movies[movies[genre] == 1]
        count = genre_df.groupby(['release_date'], group_keys=True)['release_date'].count().reset_index(name=""count"")
        for i in range(len(count)):
            genre_dict[int(count.iloc[[i]][""release_date""])] = int(count.iloc[[i]][""count""])
        result[genre] = genre_dict

    return result","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    result = dict()
    def convert(x):
        return int(x[-4:])
    movies = movies.dropna(subset=['release_date'])
    movies[""release_date""] = movies[""release_date""].apply(convert)
    for genre in genres:
        genre_dict = dict()
        genre_df = movies[movies[genre] == 1]
        count = genre_df.groupby(['release_date'], group_keys=True)['release_date'].count().reset_index(name=""count"")
        for i in range(len(count)):
            genre_dict[int(count.iloc[[i]][""release_date""])] = int(count.iloc[[i]][""count""])
        result[genre] = genre_dict

    return result
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
21167,3,"        genre_dict = dict()
        genre_df = movies[movies[genre] == 1]
        count = genre_df.groupby(['release_date'], group_keys=True)['release_date'].count().reset_index(name=""count"")
        for i in range(len(count)):
            genre_dict[int(count.iloc[[i]][""release_date""])] = int(count.iloc[[i]][""count""])
        result[genre] = genre_dict",TODO,genre_dict,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    result = dict()
    def convert(x):
        return int(x[-4:])
    movies = movies.dropna(subset=['release_date'])
    movies[""release_date""] = movies[""release_date""].apply(convert)
    for genre in genres:
        genre_dict = dict()
        genre_df = movies[movies[genre] == 1]
        count = genre_df.groupby(['release_date'], group_keys=True)['release_date'].count().reset_index(name=""count"")
        for i in range(len(count)):
            genre_dict[int(count.iloc[[i]][""release_date""])] = int(count.iloc[[i]][""count""])
        result[genre] = genre_dict

    return result","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    result = dict()
    def convert(x):
        return int(x[-4:])
    movies = movies.dropna(subset=['release_date'])
    movies[""release_date""] = movies[""release_date""].apply(convert)
    for genre in genres:
        genre_dict = dict()
        genre_df = movies[movies[genre] == 1]
        count = genre_df.groupby(['release_date'], group_keys=True)['release_date'].count().reset_index(name=""count"")
        for i in range(len(count)):
            genre_dict[int(count.iloc[[i]][""release_date""])] = int(count.iloc[[i]][""count""])
        result[genre] = genre_dict

    return result
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
21167,4,"        genre_df = movies[movies[genre] == 1]
        count = genre_df.groupby(['release_date'], group_keys=True)['release_date'].count().reset_index(name=""count"")",TODO,genre_df,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    result = dict()
    def convert(x):
        return int(x[-4:])
    movies = movies.dropna(subset=['release_date'])
    movies[""release_date""] = movies[""release_date""].apply(convert)
    for genre in genres:
        genre_dict = dict()
        genre_df = movies[movies[genre] == 1]
        count = genre_df.groupby(['release_date'], group_keys=True)['release_date'].count().reset_index(name=""count"")
        for i in range(len(count)):
            genre_dict[int(count.iloc[[i]][""release_date""])] = int(count.iloc[[i]][""count""])
        result[genre] = genre_dict

    return result","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    result = dict()
    def convert(x):
        return int(x[-4:])
    movies = movies.dropna(subset=['release_date'])
    movies[""release_date""] = movies[""release_date""].apply(convert)
    for genre in genres:
        genre_dict = dict()
        genre_df = movies[movies[genre] == 1]
        count = genre_df.groupby(['release_date'], group_keys=True)['release_date'].count().reset_index(name=""count"")
        for i in range(len(count)):
            genre_dict[int(count.iloc[[i]][""release_date""])] = int(count.iloc[[i]][""count""])
        result[genre] = genre_dict

    return result
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
21167,5,"        count = genre_df.groupby(['release_date'], group_keys=True)['release_date'].count().reset_index(name=""count"")
        for i in range(len(count)):
            genre_dict[int(count.iloc[[i]][""release_date""])] = int(count.iloc[[i]][""count""])",TODO,count,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    result = dict()
    def convert(x):
        return int(x[-4:])
    movies = movies.dropna(subset=['release_date'])
    movies[""release_date""] = movies[""release_date""].apply(convert)
    for genre in genres:
        genre_dict = dict()
        genre_df = movies[movies[genre] == 1]
        count = genre_df.groupby(['release_date'], group_keys=True)['release_date'].count().reset_index(name=""count"")
        for i in range(len(count)):
            genre_dict[int(count.iloc[[i]][""release_date""])] = int(count.iloc[[i]][""count""])
        result[genre] = genre_dict

    return result","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    result = dict()
    def convert(x):
        return int(x[-4:])
    movies = movies.dropna(subset=['release_date'])
    movies[""release_date""] = movies[""release_date""].apply(convert)
    for genre in genres:
        genre_dict = dict()
        genre_df = movies[movies[genre] == 1]
        count = genre_df.groupby(['release_date'], group_keys=True)['release_date'].count().reset_index(name=""count"")
        for i in range(len(count)):
            genre_dict[int(count.iloc[[i]][""release_date""])] = int(count.iloc[[i]][""count""])
        result[genre] = genre_dict

    return result
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
16991,1,"    movies_long = movies.melt(id_vars = ['release_date'], value_vars=genres, var_name = 'genre', value_name = 'is_genre')
    # filter is_genre = 1
    movies_long = movies_long[movies_long.is_genre == 1]
    # create year column
    movies_long['year'] = movies_long['release_date'].apply(lambda x: int(x[-4:]))
    return {k: f.groupby('year').agg(cnt = ('is_genre', 'count')).to_dict()['cnt'] for k, f in movies_long.groupby('genre')}",TODO,movies_long,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # convert movies dataframe from wide to long
    movies_long = movies.melt(id_vars = ['release_date'], value_vars=genres, var_name = 'genre', value_name = 'is_genre')
    # filter is_genre = 1
    movies_long = movies_long[movies_long.is_genre == 1]
    # create year column
    movies_long['year'] = movies_long['release_date'].apply(lambda x: int(x[-4:]))
    return {k: f.groupby('year').agg(cnt = ('is_genre', 'count')).to_dict()['cnt'] for k, f in movies_long.groupby('genre')}","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # convert movies dataframe from wide to long
    movies_long = movies.melt(id_vars = ['release_date'], value_vars=genres, var_name = 'genre', value_name = 'is_genre')
    # filter is_genre = 1
    movies_long = movies_long[movies_long.is_genre == 1]
    # create year column
    movies_long['year'] = movies_long['release_date'].apply(lambda x: int(x[-4:]))
    return {k: f.groupby('year').agg(cnt = ('is_genre', 'count')).to_dict()['cnt'] for k, f in movies_long.groupby('genre')}
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
14720,1,"    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre",TODO,movie_count_genre,"from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre","from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
14720,2,"    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count",TODO,year_count,"from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre","from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
14720,3,"      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()",TODO,update_movies,"from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre","from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
14720,4,"      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()",TODO,count,"from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre","from pandas.core.dtypes.cast import Dtype
def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    movie_count_genre = {}
    year_count={}

    for i in genres:
      update_movies = movies.dropna(subset=['release_date'])
      update_movies['year'] = update_movies['release_date'].str[7:].astype(int)
      count = update_movies[update_movies[i] == 1].groupby('year')[i].count()
      year_count = count.to_dict()
      movie_count_genre[i]=year_count

    return movie_count_genre
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
14693,1,"dates = movies['release_date'].values

def get_year(date):
    return int(date.split('-')[-1]) if not date == 'nan' else 0

extract_year = np.vectorize(get_year)
years = extract_year(dates.astype('str'))",TODO,dates,"# process year from release_date column
dates = movies['release_date'].values

def get_year(date):
    return int(date.split('-')[-1]) if not date == 'nan' else 0

extract_year = np.vectorize(get_year)
years = extract_year(dates.astype('str'))

movies_cp = movies.copy()
movies_cp['release_date'] = years

movies_cp = movies_cp[['release_date'] + genres]

# prepares for pivot table to get date movie matrix
movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
date_movie_hotmap.fillna(0, inplace=True)

del date_movie_hotmap[0]

movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)

#display(movies_cp.T)
display(date_movie_hotmap)

result = movies_cp.T @ date_movie_hotmap

result
","# process year from release_date column
dates = movies['release_date'].values

def get_year(date):
    return int(date.split('-')[-1]) if not date == 'nan' else 0

extract_year = np.vectorize(get_year)
years = extract_year(dates.astype('str'))

movies_cp = movies.copy()
movies_cp['release_date'] = years

movies_cp = movies_cp[['release_date'] + genres]

# prepares for pivot table to get date movie matrix
movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
date_movie_hotmap.fillna(0, inplace=True)

del date_movie_hotmap[0]

movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)

#display(movies_cp.T)
display(date_movie_hotmap)

result = movies_cp.T @ date_movie_hotmap

result

def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # process year from release_date column
    dates = movies['release_date'].values

    def get_year(date):
        return int(date.split('-')[-1]) if not date == 'nan' else 0

    extract_year = np.vectorize(get_year)
    years = extract_year(dates.astype('str'))

    movies_cp = movies.copy()
    movies_cp['release_date'] = years

    movies_cp = movies_cp[['release_date'] + genres]

    # prepares for pivot table to get date movie matrix
    movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

    date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
    date_movie_hotmap.fillna(0, inplace=True)

    del date_movie_hotmap[0]

    movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)
    
    result = movies_cp.T @ date_movie_hotmap


    return result.astype('int').T.to_dict()


# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
14693,2,"extract_year = np.vectorize(get_year)
years = extract_year(dates.astype('str'))",TODO,extract_year,"# process year from release_date column
dates = movies['release_date'].values

def get_year(date):
    return int(date.split('-')[-1]) if not date == 'nan' else 0

extract_year = np.vectorize(get_year)
years = extract_year(dates.astype('str'))

movies_cp = movies.copy()
movies_cp['release_date'] = years

movies_cp = movies_cp[['release_date'] + genres]

# prepares for pivot table to get date movie matrix
movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
date_movie_hotmap.fillna(0, inplace=True)

del date_movie_hotmap[0]

movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)

#display(movies_cp.T)
display(date_movie_hotmap)

result = movies_cp.T @ date_movie_hotmap

result
","# process year from release_date column
dates = movies['release_date'].values

def get_year(date):
    return int(date.split('-')[-1]) if not date == 'nan' else 0

extract_year = np.vectorize(get_year)
years = extract_year(dates.astype('str'))

movies_cp = movies.copy()
movies_cp['release_date'] = years

movies_cp = movies_cp[['release_date'] + genres]

# prepares for pivot table to get date movie matrix
movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
date_movie_hotmap.fillna(0, inplace=True)

del date_movie_hotmap[0]

movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)

#display(movies_cp.T)
display(date_movie_hotmap)

result = movies_cp.T @ date_movie_hotmap

result

def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # process year from release_date column
    dates = movies['release_date'].values

    def get_year(date):
        return int(date.split('-')[-1]) if not date == 'nan' else 0

    extract_year = np.vectorize(get_year)
    years = extract_year(dates.astype('str'))

    movies_cp = movies.copy()
    movies_cp['release_date'] = years

    movies_cp = movies_cp[['release_date'] + genres]

    # prepares for pivot table to get date movie matrix
    movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

    date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
    date_movie_hotmap.fillna(0, inplace=True)

    del date_movie_hotmap[0]

    movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)
    
    result = movies_cp.T @ date_movie_hotmap


    return result.astype('int').T.to_dict()


# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
14693,3,"years = extract_year(dates.astype('str'))

movies_cp = movies.copy()
movies_cp['release_date'] = years",TODO,years,"# process year from release_date column
dates = movies['release_date'].values

def get_year(date):
    return int(date.split('-')[-1]) if not date == 'nan' else 0

extract_year = np.vectorize(get_year)
years = extract_year(dates.astype('str'))

movies_cp = movies.copy()
movies_cp['release_date'] = years

movies_cp = movies_cp[['release_date'] + genres]

# prepares for pivot table to get date movie matrix
movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
date_movie_hotmap.fillna(0, inplace=True)

del date_movie_hotmap[0]

movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)

#display(movies_cp.T)
display(date_movie_hotmap)

result = movies_cp.T @ date_movie_hotmap

result
","# process year from release_date column
dates = movies['release_date'].values

def get_year(date):
    return int(date.split('-')[-1]) if not date == 'nan' else 0

extract_year = np.vectorize(get_year)
years = extract_year(dates.astype('str'))

movies_cp = movies.copy()
movies_cp['release_date'] = years

movies_cp = movies_cp[['release_date'] + genres]

# prepares for pivot table to get date movie matrix
movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
date_movie_hotmap.fillna(0, inplace=True)

del date_movie_hotmap[0]

movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)

#display(movies_cp.T)
display(date_movie_hotmap)

result = movies_cp.T @ date_movie_hotmap

result

def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # process year from release_date column
    dates = movies['release_date'].values

    def get_year(date):
        return int(date.split('-')[-1]) if not date == 'nan' else 0

    extract_year = np.vectorize(get_year)
    years = extract_year(dates.astype('str'))

    movies_cp = movies.copy()
    movies_cp['release_date'] = years

    movies_cp = movies_cp[['release_date'] + genres]

    # prepares for pivot table to get date movie matrix
    movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

    date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
    date_movie_hotmap.fillna(0, inplace=True)

    del date_movie_hotmap[0]

    movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)
    
    result = movies_cp.T @ date_movie_hotmap


    return result.astype('int').T.to_dict()


# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
14693,4,"movies_cp = movies.copy()
movies_cp['release_date'] = years

movies_cp = movies_cp[['release_date'] + genres]

# prepares for pivot table to get date movie matrix
movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
date_movie_hotmap.fillna(0, inplace=True)

del date_movie_hotmap[0]

movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)

#display(movies_cp.T)
display(date_movie_hotmap)

result = movies_cp.T @ date_movie_hotmap",TODO,movies_cp,"# process year from release_date column
dates = movies['release_date'].values

def get_year(date):
    return int(date.split('-')[-1]) if not date == 'nan' else 0

extract_year = np.vectorize(get_year)
years = extract_year(dates.astype('str'))

movies_cp = movies.copy()
movies_cp['release_date'] = years

movies_cp = movies_cp[['release_date'] + genres]

# prepares for pivot table to get date movie matrix
movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
date_movie_hotmap.fillna(0, inplace=True)

del date_movie_hotmap[0]

movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)

#display(movies_cp.T)
display(date_movie_hotmap)

result = movies_cp.T @ date_movie_hotmap

result
","# process year from release_date column
dates = movies['release_date'].values

def get_year(date):
    return int(date.split('-')[-1]) if not date == 'nan' else 0

extract_year = np.vectorize(get_year)
years = extract_year(dates.astype('str'))

movies_cp = movies.copy()
movies_cp['release_date'] = years

movies_cp = movies_cp[['release_date'] + genres]

# prepares for pivot table to get date movie matrix
movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
date_movie_hotmap.fillna(0, inplace=True)

del date_movie_hotmap[0]

movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)

#display(movies_cp.T)
display(date_movie_hotmap)

result = movies_cp.T @ date_movie_hotmap

result

def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # process year from release_date column
    dates = movies['release_date'].values

    def get_year(date):
        return int(date.split('-')[-1]) if not date == 'nan' else 0

    extract_year = np.vectorize(get_year)
    years = extract_year(dates.astype('str'))

    movies_cp = movies.copy()
    movies_cp['release_date'] = years

    movies_cp = movies_cp[['release_date'] + genres]

    # prepares for pivot table to get date movie matrix
    movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

    date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
    date_movie_hotmap.fillna(0, inplace=True)

    del date_movie_hotmap[0]

    movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)
    
    result = movies_cp.T @ date_movie_hotmap


    return result.astype('int').T.to_dict()


# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
14693,5,"date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
date_movie_hotmap.fillna(0, inplace=True)

del date_movie_hotmap[0]

movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)

#display(movies_cp.T)
display(date_movie_hotmap)

result = movies_cp.T @ date_movie_hotmap",TODO,date_movie_hotmap,"# process year from release_date column
dates = movies['release_date'].values

def get_year(date):
    return int(date.split('-')[-1]) if not date == 'nan' else 0

extract_year = np.vectorize(get_year)
years = extract_year(dates.astype('str'))

movies_cp = movies.copy()
movies_cp['release_date'] = years

movies_cp = movies_cp[['release_date'] + genres]

# prepares for pivot table to get date movie matrix
movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
date_movie_hotmap.fillna(0, inplace=True)

del date_movie_hotmap[0]

movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)

#display(movies_cp.T)
display(date_movie_hotmap)

result = movies_cp.T @ date_movie_hotmap

result
","# process year from release_date column
dates = movies['release_date'].values

def get_year(date):
    return int(date.split('-')[-1]) if not date == 'nan' else 0

extract_year = np.vectorize(get_year)
years = extract_year(dates.astype('str'))

movies_cp = movies.copy()
movies_cp['release_date'] = years

movies_cp = movies_cp[['release_date'] + genres]

# prepares for pivot table to get date movie matrix
movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
date_movie_hotmap.fillna(0, inplace=True)

del date_movie_hotmap[0]

movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)

#display(movies_cp.T)
display(date_movie_hotmap)

result = movies_cp.T @ date_movie_hotmap

result

def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # process year from release_date column
    dates = movies['release_date'].values

    def get_year(date):
        return int(date.split('-')[-1]) if not date == 'nan' else 0

    extract_year = np.vectorize(get_year)
    years = extract_year(dates.astype('str'))

    movies_cp = movies.copy()
    movies_cp['release_date'] = years

    movies_cp = movies_cp[['release_date'] + genres]

    # prepares for pivot table to get date movie matrix
    movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

    date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
    date_movie_hotmap.fillna(0, inplace=True)

    del date_movie_hotmap[0]

    movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)
    
    result = movies_cp.T @ date_movie_hotmap


    return result.astype('int').T.to_dict()


# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
14693,6,"result = movies_cp.T @ date_movie_hotmap

result",TODO,result,"# process year from release_date column
dates = movies['release_date'].values

def get_year(date):
    return int(date.split('-')[-1]) if not date == 'nan' else 0

extract_year = np.vectorize(get_year)
years = extract_year(dates.astype('str'))

movies_cp = movies.copy()
movies_cp['release_date'] = years

movies_cp = movies_cp[['release_date'] + genres]

# prepares for pivot table to get date movie matrix
movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
date_movie_hotmap.fillna(0, inplace=True)

del date_movie_hotmap[0]

movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)

#display(movies_cp.T)
display(date_movie_hotmap)

result = movies_cp.T @ date_movie_hotmap

result
","# process year from release_date column
dates = movies['release_date'].values

def get_year(date):
    return int(date.split('-')[-1]) if not date == 'nan' else 0

extract_year = np.vectorize(get_year)
years = extract_year(dates.astype('str'))

movies_cp = movies.copy()
movies_cp['release_date'] = years

movies_cp = movies_cp[['release_date'] + genres]

# prepares for pivot table to get date movie matrix
movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
date_movie_hotmap.fillna(0, inplace=True)

del date_movie_hotmap[0]

movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)

#display(movies_cp.T)
display(date_movie_hotmap)

result = movies_cp.T @ date_movie_hotmap

result

def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    # process year from release_date column
    dates = movies['release_date'].values

    def get_year(date):
        return int(date.split('-')[-1]) if not date == 'nan' else 0

    extract_year = np.vectorize(get_year)
    years = extract_year(dates.astype('str'))

    movies_cp = movies.copy()
    movies_cp['release_date'] = years

    movies_cp = movies_cp[['release_date'] + genres]

    # prepares for pivot table to get date movie matrix
    movies_cp['value'] = movies_cp['release_date'].astype(bool).astype(int)# > 0 

    date_movie_hotmap = movies_cp[['release_date', 'value']].pivot_table(index = 'movie_id', columns = 'release_date', values = 'value')
    date_movie_hotmap.fillna(0, inplace=True)

    del date_movie_hotmap[0]

    movies_cp.drop(['release_date', 'value'], axis = 1, inplace=True)
    
    result = movies_cp.T @ date_movie_hotmap


    return result.astype('int').T.to_dict()


# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
23193,1,"    movies_valid_year = movies[movies['release_year'] != None]
    year_count = dict(zip(genres, [None] * len(genres)))
    for genre in genres:
        year_count[genre] = dict(movies_valid_year[movies_valid_year[genre] == 1].groupby('release_year').count()[genre])",TODO,movies_valid_year,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    def convert_to_year(string):
        try:
            return int(string.split('-')[-1])
        except:
            return None
        
    movies['release_year'] = movies['release_date'].apply(convert_to_year)
    movies_valid_year = movies[movies['release_year'] != None]
    year_count = dict(zip(genres, [None] * len(genres)))
    for genre in genres:
        year_count[genre] = dict(movies_valid_year[movies_valid_year[genre] == 1].groupby('release_year').count()[genre])
    return year_count","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    def convert_to_year(string):
        try:
            return int(string.split('-')[-1])
        except:
            return None
        
    movies['release_year'] = movies['release_date'].apply(convert_to_year)
    movies_valid_year = movies[movies['release_year'] != None]
    year_count = dict(zip(genres, [None] * len(genres)))
    for genre in genres:
        year_count[genre] = dict(movies_valid_year[movies_valid_year[genre] == 1].groupby('release_year').count()[genre])
    return year_count
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
23193,2,"    year_count = dict(zip(genres, [None] * len(genres)))
    for genre in genres:
        year_count[genre] = dict(movies_valid_year[movies_valid_year[genre] == 1].groupby('release_year').count()[genre])
    return year_count",TODO,year_count,"def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    def convert_to_year(string):
        try:
            return int(string.split('-')[-1])
        except:
            return None
        
    movies['release_year'] = movies['release_date'].apply(convert_to_year)
    movies_valid_year = movies[movies['release_year'] != None]
    year_count = dict(zip(genres, [None] * len(genres)))
    for genre in genres:
        year_count[genre] = dict(movies_valid_year[movies_valid_year[genre] == 1].groupby('release_year').count()[genre])
    return year_count","def movie_count_by_genre(movies, genres):
    """"""
    Count the number of movies in each movie genre.

    args:
        movies (pd.DataFrame) : Dataframe containing movie attributes
        genres (List[str]) :  the list of movie genres

    return:
        Dict[str, Dict[int, int]]  : a nested mapping from movie genre to year to number of movies in that year
    """"""
    def convert_to_year(string):
        try:
            return int(string.split('-')[-1])
        except:
            return None
        
    movies['release_year'] = movies['release_date'].apply(convert_to_year)
    movies_valid_year = movies[movies['release_year'] != None]
    year_count = dict(zip(genres, [None] * len(genres)))
    for genre in genres:
        year_count[genre] = dict(movies_valid_year[movies_valid_year[genre] == 1].groupby('release_year').count()[genre])
    return year_count
# local test
def test_movie_count_by_genre():
    genre_count = movie_count_by_genre(movies, genres)
    assert genre_count['Crime'][1997] == 30
    assert genre_count['Action'][1998] == 12
    assert genre_count['Drama'][1939] == 5
    print(""All tests passed!"")

test_movie_count_by_genre()"
309,1,"    before_fil = movies.copy()
    before_fil['release_date'] = before_fil['release_date'].str[-4:].apply(pd.to_numeric, downcast=""integer"", errors='coerce')
    filtered = before_fil[before_fil['release_date'] >= starting_year]",TODO,before_fil,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    before_fil = movies.copy()
    before_fil['release_date'] = before_fil['release_date'].str[-4:].apply(pd.to_numeric, downcast=""integer"", errors='coerce')
    filtered = before_fil[before_fil['release_date'] >= starting_year]
    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')
    merged_df = merged_df[[""release_date"", ""rating""]].dropna().astype(int)
    merged_df.rename(columns={'release_date':'release_year'}, inplace = True)
    return merged_df","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    before_fil = movies.copy()
    before_fil['release_date'] = before_fil['release_date'].str[-4:].apply(pd.to_numeric, downcast=""integer"", errors='coerce')
    filtered = before_fil[before_fil['release_date'] >= starting_year]
    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')
    merged_df = merged_df[[""release_date"", ""rating""]].dropna().astype(int)
    merged_df.rename(columns={'release_date':'release_year'}, inplace = True)
    return merged_df
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
309,2,"    filtered = before_fil[before_fil['release_date'] >= starting_year]
    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')",TODO,filtered,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    before_fil = movies.copy()
    before_fil['release_date'] = before_fil['release_date'].str[-4:].apply(pd.to_numeric, downcast=""integer"", errors='coerce')
    filtered = before_fil[before_fil['release_date'] >= starting_year]
    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')
    merged_df = merged_df[[""release_date"", ""rating""]].dropna().astype(int)
    merged_df.rename(columns={'release_date':'release_year'}, inplace = True)
    return merged_df","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    before_fil = movies.copy()
    before_fil['release_date'] = before_fil['release_date'].str[-4:].apply(pd.to_numeric, downcast=""integer"", errors='coerce')
    filtered = before_fil[before_fil['release_date'] >= starting_year]
    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')
    merged_df = merged_df[[""release_date"", ""rating""]].dropna().astype(int)
    merged_df.rename(columns={'release_date':'release_year'}, inplace = True)
    return merged_df
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
309,3,"    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')
    merged_df = merged_df[[""release_date"", ""rating""]].dropna().astype(int)
    merged_df.rename(columns={'release_date':'release_year'}, inplace = True)
    return merged_df",TODO,merged_df,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    before_fil = movies.copy()
    before_fil['release_date'] = before_fil['release_date'].str[-4:].apply(pd.to_numeric, downcast=""integer"", errors='coerce')
    filtered = before_fil[before_fil['release_date'] >= starting_year]
    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')
    merged_df = merged_df[[""release_date"", ""rating""]].dropna().astype(int)
    merged_df.rename(columns={'release_date':'release_year'}, inplace = True)
    return merged_df","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    before_fil = movies.copy()
    before_fil['release_date'] = before_fil['release_date'].str[-4:].apply(pd.to_numeric, downcast=""integer"", errors='coerce')
    filtered = before_fil[before_fil['release_date'] >= starting_year]
    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')
    merged_df = merged_df[[""release_date"", ""rating""]].dropna().astype(int)
    merged_df.rename(columns={'release_date':'release_year'}, inplace = True)
    return merged_df
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
15325,1,"    df = movies.replace(0, np.nan)
    df = df[df['release_date'].notna()]
    df_movies = df['release_date'].apply(lambda x: x[-4:])",TODO,df,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    df = movies.replace(0, np.nan)
    df = df[df['release_date'].notna()]
    df_movies = df['release_date'].apply(lambda x: x[-4:])
    # print(df_movies.head())
    df_ratings = ratings[['item_id','rating']]
    # print(df_ratings.head())
    df2 = df_ratings.merge(df_movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    # print(df2.head())
    df2 = df2[df2['release_date'].notna()]
    df2['release_date'] = df2['release_date'].astype(int)
    df2['rating'] = df2['rating'].astype(int)
    result = df2[df2['release_date']>=starting_year]
    # print(result.head())
    result = result[['release_date','rating']]
    result.rename(columns = {'release_date':'release_year'}, inplace = True)
    return result","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    df = movies.replace(0, np.nan)
    df = df[df['release_date'].notna()]
    df_movies = df['release_date'].apply(lambda x: x[-4:])
    # print(df_movies.head())
    df_ratings = ratings[['item_id','rating']]
    # print(df_ratings.head())
    df2 = df_ratings.merge(df_movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    # print(df2.head())
    df2 = df2[df2['release_date'].notna()]
    df2['release_date'] = df2['release_date'].astype(int)
    df2['rating'] = df2['rating'].astype(int)
    result = df2[df2['release_date']>=starting_year]
    # print(result.head())
    result = result[['release_date','rating']]
    result.rename(columns = {'release_date':'release_year'}, inplace = True)
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
15325,2,"    df_movies = df['release_date'].apply(lambda x: x[-4:])
    # print(df_movies.head())
    df_ratings = ratings[['item_id','rating']]
    # print(df_ratings.head())
    df2 = df_ratings.merge(df_movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")",TODO,df_movies,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    df = movies.replace(0, np.nan)
    df = df[df['release_date'].notna()]
    df_movies = df['release_date'].apply(lambda x: x[-4:])
    # print(df_movies.head())
    df_ratings = ratings[['item_id','rating']]
    # print(df_ratings.head())
    df2 = df_ratings.merge(df_movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    # print(df2.head())
    df2 = df2[df2['release_date'].notna()]
    df2['release_date'] = df2['release_date'].astype(int)
    df2['rating'] = df2['rating'].astype(int)
    result = df2[df2['release_date']>=starting_year]
    # print(result.head())
    result = result[['release_date','rating']]
    result.rename(columns = {'release_date':'release_year'}, inplace = True)
    return result","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    df = movies.replace(0, np.nan)
    df = df[df['release_date'].notna()]
    df_movies = df['release_date'].apply(lambda x: x[-4:])
    # print(df_movies.head())
    df_ratings = ratings[['item_id','rating']]
    # print(df_ratings.head())
    df2 = df_ratings.merge(df_movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    # print(df2.head())
    df2 = df2[df2['release_date'].notna()]
    df2['release_date'] = df2['release_date'].astype(int)
    df2['rating'] = df2['rating'].astype(int)
    result = df2[df2['release_date']>=starting_year]
    # print(result.head())
    result = result[['release_date','rating']]
    result.rename(columns = {'release_date':'release_year'}, inplace = True)
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
15325,3,"    df_ratings = ratings[['item_id','rating']]
    # print(df_ratings.head())
    df2 = df_ratings.merge(df_movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")",TODO,df_ratings,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    df = movies.replace(0, np.nan)
    df = df[df['release_date'].notna()]
    df_movies = df['release_date'].apply(lambda x: x[-4:])
    # print(df_movies.head())
    df_ratings = ratings[['item_id','rating']]
    # print(df_ratings.head())
    df2 = df_ratings.merge(df_movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    # print(df2.head())
    df2 = df2[df2['release_date'].notna()]
    df2['release_date'] = df2['release_date'].astype(int)
    df2['rating'] = df2['rating'].astype(int)
    result = df2[df2['release_date']>=starting_year]
    # print(result.head())
    result = result[['release_date','rating']]
    result.rename(columns = {'release_date':'release_year'}, inplace = True)
    return result","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    df = movies.replace(0, np.nan)
    df = df[df['release_date'].notna()]
    df_movies = df['release_date'].apply(lambda x: x[-4:])
    # print(df_movies.head())
    df_ratings = ratings[['item_id','rating']]
    # print(df_ratings.head())
    df2 = df_ratings.merge(df_movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    # print(df2.head())
    df2 = df2[df2['release_date'].notna()]
    df2['release_date'] = df2['release_date'].astype(int)
    df2['rating'] = df2['rating'].astype(int)
    result = df2[df2['release_date']>=starting_year]
    # print(result.head())
    result = result[['release_date','rating']]
    result.rename(columns = {'release_date':'release_year'}, inplace = True)
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
15325,4,"    df2 = df_ratings.merge(df_movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    # print(df2.head())
    df2 = df2[df2['release_date'].notna()]
    df2['release_date'] = df2['release_date'].astype(int)
    df2['rating'] = df2['rating'].astype(int)
    result = df2[df2['release_date']>=starting_year]",TODO,df2,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    df = movies.replace(0, np.nan)
    df = df[df['release_date'].notna()]
    df_movies = df['release_date'].apply(lambda x: x[-4:])
    # print(df_movies.head())
    df_ratings = ratings[['item_id','rating']]
    # print(df_ratings.head())
    df2 = df_ratings.merge(df_movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    # print(df2.head())
    df2 = df2[df2['release_date'].notna()]
    df2['release_date'] = df2['release_date'].astype(int)
    df2['rating'] = df2['rating'].astype(int)
    result = df2[df2['release_date']>=starting_year]
    # print(result.head())
    result = result[['release_date','rating']]
    result.rename(columns = {'release_date':'release_year'}, inplace = True)
    return result","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    df = movies.replace(0, np.nan)
    df = df[df['release_date'].notna()]
    df_movies = df['release_date'].apply(lambda x: x[-4:])
    # print(df_movies.head())
    df_ratings = ratings[['item_id','rating']]
    # print(df_ratings.head())
    df2 = df_ratings.merge(df_movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    # print(df2.head())
    df2 = df2[df2['release_date'].notna()]
    df2['release_date'] = df2['release_date'].astype(int)
    df2['rating'] = df2['rating'].astype(int)
    result = df2[df2['release_date']>=starting_year]
    # print(result.head())
    result = result[['release_date','rating']]
    result.rename(columns = {'release_date':'release_year'}, inplace = True)
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
15325,5,"    result = df2[df2['release_date']>=starting_year]
    # print(result.head())
    result = result[['release_date','rating']]
    result.rename(columns = {'release_date':'release_year'}, inplace = True)
    return result",TODO,result,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    df = movies.replace(0, np.nan)
    df = df[df['release_date'].notna()]
    df_movies = df['release_date'].apply(lambda x: x[-4:])
    # print(df_movies.head())
    df_ratings = ratings[['item_id','rating']]
    # print(df_ratings.head())
    df2 = df_ratings.merge(df_movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    # print(df2.head())
    df2 = df2[df2['release_date'].notna()]
    df2['release_date'] = df2['release_date'].astype(int)
    df2['rating'] = df2['rating'].astype(int)
    result = df2[df2['release_date']>=starting_year]
    # print(result.head())
    result = result[['release_date','rating']]
    result.rename(columns = {'release_date':'release_year'}, inplace = True)
    return result","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    df = movies.replace(0, np.nan)
    df = df[df['release_date'].notna()]
    df_movies = df['release_date'].apply(lambda x: x[-4:])
    # print(df_movies.head())
    df_ratings = ratings[['item_id','rating']]
    # print(df_ratings.head())
    df2 = df_ratings.merge(df_movies, left_on = ""item_id"", right_on = ""movie_id"", how = ""left"")
    # print(df2.head())
    df2 = df2[df2['release_date'].notna()]
    df2['release_date'] = df2['release_date'].astype(int)
    df2['rating'] = df2['rating'].astype(int)
    result = df2[df2['release_date']>=starting_year]
    # print(result.head())
    result = result[['release_date','rating']]
    result.rename(columns = {'release_date':'release_year'}, inplace = True)
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
18135,1,"    movies_temp = movies[['release_date']]

    #drop nan values for year and convert rest
    movies_temp = movies_temp.dropna(subset=['release_date'])
    def release_year(release_date):
        # last 4 chracters of values in release_date column
        year_string = release_date[-4:]
        # value to int from string
        return int(year_string)

    movies_temp['release_date'] = movies_temp['release_date'].apply(release_year)

    movies_temp['item_id'] = movies_temp.index
    movies_temp=movies_temp.merge(ratings, left_on = ""item_id"", right_on = ""item_id"", how = ""right"")

    movies_temp = movies_temp.drop(['user_id', 'timestamp', 'item_id'], axis=1)

    movies_temp = movies_temp.dropna(subset=['rating'])

    movies_temp = movies_temp.rename(columns={'release_date': 'release_year'})
    

    movies_temp = movies_temp[movies_temp['release_year'] >= starting_year]
    movies_temp = movies_temp.dropna(subset=['release_year'])
    movies_temp['release_year'] =movies_temp['release_year'].astype('int64')


    return movies_temp",TODO,movies_temp,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    
    #drop all columns except genre, release_date
    movies_temp = movies[['release_date']]

    #drop nan values for year and convert rest
    movies_temp = movies_temp.dropna(subset=['release_date'])
    def release_year(release_date):
        # last 4 chracters of values in release_date column
        year_string = release_date[-4:]
        # value to int from string
        return int(year_string)

    movies_temp['release_date'] = movies_temp['release_date'].apply(release_year)

    movies_temp['item_id'] = movies_temp.index
    movies_temp=movies_temp.merge(ratings, left_on = ""item_id"", right_on = ""item_id"", how = ""right"")

    movies_temp = movies_temp.drop(['user_id', 'timestamp', 'item_id'], axis=1)

    movies_temp = movies_temp.dropna(subset=['rating'])

    movies_temp = movies_temp.rename(columns={'release_date': 'release_year'})
    

    movies_temp = movies_temp[movies_temp['release_year'] >= starting_year]
    movies_temp = movies_temp.dropna(subset=['release_year'])
    movies_temp['release_year'] =movies_temp['release_year'].astype('int64')


    return movies_temp
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    pass","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    
    #drop all columns except genre, release_date
    movies_temp = movies[['release_date']]

    #drop nan values for year and convert rest
    movies_temp = movies_temp.dropna(subset=['release_date'])
    def release_year(release_date):
        # last 4 chracters of values in release_date column
        year_string = release_date[-4:]
        # value to int from string
        return int(year_string)

    movies_temp['release_date'] = movies_temp['release_date'].apply(release_year)

    movies_temp['item_id'] = movies_temp.index
    movies_temp=movies_temp.merge(ratings, left_on = ""item_id"", right_on = ""item_id"", how = ""right"")

    movies_temp = movies_temp.drop(['user_id', 'timestamp', 'item_id'], axis=1)

    movies_temp = movies_temp.dropna(subset=['rating'])

    movies_temp = movies_temp.rename(columns={'release_date': 'release_year'})
    

    movies_temp = movies_temp[movies_temp['release_year'] >= starting_year]
    movies_temp = movies_temp.dropna(subset=['release_year'])
    movies_temp['release_year'] =movies_temp['release_year'].astype('int64')


    return movies_temp
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    pass
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
18135,2,"        year_string = release_date[-4:]
        # value to int from string
        return int(year_string)",TODO,year_string,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    
    #drop all columns except genre, release_date
    movies_temp = movies[['release_date']]

    #drop nan values for year and convert rest
    movies_temp = movies_temp.dropna(subset=['release_date'])
    def release_year(release_date):
        # last 4 chracters of values in release_date column
        year_string = release_date[-4:]
        # value to int from string
        return int(year_string)

    movies_temp['release_date'] = movies_temp['release_date'].apply(release_year)

    movies_temp['item_id'] = movies_temp.index
    movies_temp=movies_temp.merge(ratings, left_on = ""item_id"", right_on = ""item_id"", how = ""right"")

    movies_temp = movies_temp.drop(['user_id', 'timestamp', 'item_id'], axis=1)

    movies_temp = movies_temp.dropna(subset=['rating'])

    movies_temp = movies_temp.rename(columns={'release_date': 'release_year'})
    

    movies_temp = movies_temp[movies_temp['release_year'] >= starting_year]
    movies_temp = movies_temp.dropna(subset=['release_year'])
    movies_temp['release_year'] =movies_temp['release_year'].astype('int64')


    return movies_temp
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    pass","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    
    #drop all columns except genre, release_date
    movies_temp = movies[['release_date']]

    #drop nan values for year and convert rest
    movies_temp = movies_temp.dropna(subset=['release_date'])
    def release_year(release_date):
        # last 4 chracters of values in release_date column
        year_string = release_date[-4:]
        # value to int from string
        return int(year_string)

    movies_temp['release_date'] = movies_temp['release_date'].apply(release_year)

    movies_temp['item_id'] = movies_temp.index
    movies_temp=movies_temp.merge(ratings, left_on = ""item_id"", right_on = ""item_id"", how = ""right"")

    movies_temp = movies_temp.drop(['user_id', 'timestamp', 'item_id'], axis=1)

    movies_temp = movies_temp.dropna(subset=['rating'])

    movies_temp = movies_temp.rename(columns={'release_date': 'release_year'})
    

    movies_temp = movies_temp[movies_temp['release_year'] >= starting_year]
    movies_temp = movies_temp.dropna(subset=['release_year'])
    movies_temp['release_year'] =movies_temp['release_year'].astype('int64')


    return movies_temp
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    pass
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
1290,1,"    before_fil = movies.copy()
    before_fil['release_date'] = before_fil['release_date'].str[-4:].apply(pd.to_numeric, downcast=""integer"", errors='coerce')
    filtered = before_fil[before_fil['release_date'] >= starting_year]",TODO,before_fil,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    before_fil = movies.copy()
    before_fil['release_date'] = before_fil['release_date'].str[-4:].apply(pd.to_numeric, downcast=""integer"", errors='coerce')
    filtered = before_fil[before_fil['release_date'] >= starting_year]
    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')
    merged_df = merged_df[[""release_date"", ""rating""]].dropna().astype(int)
    merged_df.rename(columns={'release_date':'release_year'}, inplace = True)
    return merged_df","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    before_fil = movies.copy()
    before_fil['release_date'] = before_fil['release_date'].str[-4:].apply(pd.to_numeric, downcast=""integer"", errors='coerce')
    filtered = before_fil[before_fil['release_date'] >= starting_year]
    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')
    merged_df = merged_df[[""release_date"", ""rating""]].dropna().astype(int)
    merged_df.rename(columns={'release_date':'release_year'}, inplace = True)
    return merged_df
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
1290,2,"    filtered = before_fil[before_fil['release_date'] >= starting_year]
    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')",TODO,filtered,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    before_fil = movies.copy()
    before_fil['release_date'] = before_fil['release_date'].str[-4:].apply(pd.to_numeric, downcast=""integer"", errors='coerce')
    filtered = before_fil[before_fil['release_date'] >= starting_year]
    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')
    merged_df = merged_df[[""release_date"", ""rating""]].dropna().astype(int)
    merged_df.rename(columns={'release_date':'release_year'}, inplace = True)
    return merged_df","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    before_fil = movies.copy()
    before_fil['release_date'] = before_fil['release_date'].str[-4:].apply(pd.to_numeric, downcast=""integer"", errors='coerce')
    filtered = before_fil[before_fil['release_date'] >= starting_year]
    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')
    merged_df = merged_df[[""release_date"", ""rating""]].dropna().astype(int)
    merged_df.rename(columns={'release_date':'release_year'}, inplace = True)
    return merged_df
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
1290,3,"    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')
    merged_df = merged_df[[""release_date"", ""rating""]].dropna().astype(int)
    merged_df.rename(columns={'release_date':'release_year'}, inplace = True)
    return merged_df",TODO,merged_df,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    before_fil = movies.copy()
    before_fil['release_date'] = before_fil['release_date'].str[-4:].apply(pd.to_numeric, downcast=""integer"", errors='coerce')
    filtered = before_fil[before_fil['release_date'] >= starting_year]
    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')
    merged_df = merged_df[[""release_date"", ""rating""]].dropna().astype(int)
    merged_df.rename(columns={'release_date':'release_year'}, inplace = True)
    return merged_df","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    before_fil = movies.copy()
    before_fil['release_date'] = before_fil['release_date'].str[-4:].apply(pd.to_numeric, downcast=""integer"", errors='coerce')
    filtered = before_fil[before_fil['release_date'] >= starting_year]
    merged_df = pd.merge(filtered, ratings, right_on='item_id', left_index=True, how='right')
    merged_df = merged_df[[""release_date"", ""rating""]].dropna().astype(int)
    merged_df.rename(columns={'release_date':'release_year'}, inplace = True)
    return merged_df
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
22933,1,"    merged = pd.merge(ratings[['item_id', 'rating']], movies[['release_date']], how='left', left_on='item_id', right_on='movie_id')

    # filter out rows with invalid year values
    merged = merged.loc[merged['release_date'].notna(), :]

    # get year value
    merged['release_date'] = merged['release_date'].astype('datetime64[D]').apply(lambda date: date.year)
    
    # change column name
    merged = merged.rename(columns={'release_date': 'release_year'})
    return merged.loc[merged['release_year'] >= starting_year,['release_year', 'rating']]",TODO,merged,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    merged = pd.merge(ratings[['item_id', 'rating']], movies[['release_date']], how='left', left_on='item_id', right_on='movie_id')

    # filter out rows with invalid year values
    merged = merged.loc[merged['release_date'].notna(), :]

    # get year value
    merged['release_date'] = merged['release_date'].astype('datetime64[D]').apply(lambda date: date.year)
    
    # change column name
    merged = merged.rename(columns={'release_date': 'release_year'})
    return merged.loc[merged['release_year'] >= starting_year,['release_year', 'rating']]","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    merged = pd.merge(ratings[['item_id', 'rating']], movies[['release_date']], how='left', left_on='item_id', right_on='movie_id')

    # filter out rows with invalid year values
    merged = merged.loc[merged['release_date'].notna(), :]

    # get year value
    merged['release_date'] = merged['release_date'].astype('datetime64[D]').apply(lambda date: date.year)
    
    # change column name
    merged = merged.rename(columns={'release_date': 'release_year'})
    return merged.loc[merged['release_year'] >= starting_year,['release_year', 'rating']]
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
2325,1,"    filtered_year = movies[pd.to_datetime(movies['release_date']) >= pd.to_datetime('01-01-{}'.format(starting_year))][['release_date']]
    filtered_year['release_date'] = pd.DatetimeIndex(filtered_year['release_date']).year
    combined = filtered_year.join(ratings.set_index('item_id'))[['release_date','rating']]",TODO,filtered_year,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    filtered_year = movies[pd.to_datetime(movies['release_date']) >= pd.to_datetime('01-01-{}'.format(starting_year))][['release_date']]
    filtered_year['release_date'] = pd.DatetimeIndex(filtered_year['release_date']).year
    combined = filtered_year.join(ratings.set_index('item_id'))[['release_date','rating']]
    combined.rename(columns={""release_date"": ""release_year"", ""rating"": ""rating""},inplace=True)

    return combined","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    filtered_year = movies[pd.to_datetime(movies['release_date']) >= pd.to_datetime('01-01-{}'.format(starting_year))][['release_date']]
    filtered_year['release_date'] = pd.DatetimeIndex(filtered_year['release_date']).year
    combined = filtered_year.join(ratings.set_index('item_id'))[['release_date','rating']]
    combined.rename(columns={""release_date"": ""release_year"", ""rating"": ""rating""},inplace=True)

    return combined
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
2325,2,"    combined = filtered_year.join(ratings.set_index('item_id'))[['release_date','rating']]
    combined.rename(columns={""release_date"": ""release_year"", ""rating"": ""rating""},inplace=True)

    return combined",TODO,combined,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    filtered_year = movies[pd.to_datetime(movies['release_date']) >= pd.to_datetime('01-01-{}'.format(starting_year))][['release_date']]
    filtered_year['release_date'] = pd.DatetimeIndex(filtered_year['release_date']).year
    combined = filtered_year.join(ratings.set_index('item_id'))[['release_date','rating']]
    combined.rename(columns={""release_date"": ""release_year"", ""rating"": ""rating""},inplace=True)

    return combined","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    filtered_year = movies[pd.to_datetime(movies['release_date']) >= pd.to_datetime('01-01-{}'.format(starting_year))][['release_date']]
    filtered_year['release_date'] = pd.DatetimeIndex(filtered_year['release_date']).year
    combined = filtered_year.join(ratings.set_index('item_id'))[['release_date','rating']]
    combined.rename(columns={""release_date"": ""release_year"", ""rating"": ""rating""},inplace=True)

    return combined
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
714,1,"    new_movies=movies
    new_movies['Year'] = new_movies['release_date'].str[-4:].dropna().astype(int)
    new_movies[""release_year""]=new_movies['Year']
    new_movies_frame=new_movies.loc[new_movies[""release_year""]>=starting_year]",TODO,new_movies,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    new_movies=movies
    new_movies['Year'] = new_movies['release_date'].str[-4:].dropna().astype(int)
    new_movies[""release_year""]=new_movies['Year']
    new_movies_frame=new_movies.loc[new_movies[""release_year""]>=starting_year]
    output_frame=new_movies_frame.merge(ratings,left_on=""movie_id"",right_on=""item_id"",how=""inner"")
    final_output_frame=output_frame[[""release_year"",'rating']]
    return final_output_frame","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    new_movies=movies
    new_movies['Year'] = new_movies['release_date'].str[-4:].dropna().astype(int)
    new_movies[""release_year""]=new_movies['Year']
    new_movies_frame=new_movies.loc[new_movies[""release_year""]>=starting_year]
    output_frame=new_movies_frame.merge(ratings,left_on=""movie_id"",right_on=""item_id"",how=""inner"")
    final_output_frame=output_frame[[""release_year"",'rating']]
    return final_output_frame
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
714,2,"    new_movies_frame=new_movies.loc[new_movies[""release_year""]>=starting_year]
    output_frame=new_movies_frame.merge(ratings,left_on=""movie_id"",right_on=""item_id"",how=""inner"")",TODO,new_movies_frame,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    new_movies=movies
    new_movies['Year'] = new_movies['release_date'].str[-4:].dropna().astype(int)
    new_movies[""release_year""]=new_movies['Year']
    new_movies_frame=new_movies.loc[new_movies[""release_year""]>=starting_year]
    output_frame=new_movies_frame.merge(ratings,left_on=""movie_id"",right_on=""item_id"",how=""inner"")
    final_output_frame=output_frame[[""release_year"",'rating']]
    return final_output_frame","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    new_movies=movies
    new_movies['Year'] = new_movies['release_date'].str[-4:].dropna().astype(int)
    new_movies[""release_year""]=new_movies['Year']
    new_movies_frame=new_movies.loc[new_movies[""release_year""]>=starting_year]
    output_frame=new_movies_frame.merge(ratings,left_on=""movie_id"",right_on=""item_id"",how=""inner"")
    final_output_frame=output_frame[[""release_year"",'rating']]
    return final_output_frame
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
714,3,"    output_frame=new_movies_frame.merge(ratings,left_on=""movie_id"",right_on=""item_id"",how=""inner"")
    final_output_frame=output_frame[[""release_year"",'rating']]",TODO,output_frame,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    new_movies=movies
    new_movies['Year'] = new_movies['release_date'].str[-4:].dropna().astype(int)
    new_movies[""release_year""]=new_movies['Year']
    new_movies_frame=new_movies.loc[new_movies[""release_year""]>=starting_year]
    output_frame=new_movies_frame.merge(ratings,left_on=""movie_id"",right_on=""item_id"",how=""inner"")
    final_output_frame=output_frame[[""release_year"",'rating']]
    return final_output_frame","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    new_movies=movies
    new_movies['Year'] = new_movies['release_date'].str[-4:].dropna().astype(int)
    new_movies[""release_year""]=new_movies['Year']
    new_movies_frame=new_movies.loc[new_movies[""release_year""]>=starting_year]
    output_frame=new_movies_frame.merge(ratings,left_on=""movie_id"",right_on=""item_id"",how=""inner"")
    final_output_frame=output_frame[[""release_year"",'rating']]
    return final_output_frame
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
714,4,"    final_output_frame=output_frame[[""release_year"",'rating']]
    return final_output_frame",TODO,final_output_frame,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    new_movies=movies
    new_movies['Year'] = new_movies['release_date'].str[-4:].dropna().astype(int)
    new_movies[""release_year""]=new_movies['Year']
    new_movies_frame=new_movies.loc[new_movies[""release_year""]>=starting_year]
    output_frame=new_movies_frame.merge(ratings,left_on=""movie_id"",right_on=""item_id"",how=""inner"")
    final_output_frame=output_frame[[""release_year"",'rating']]
    return final_output_frame","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    new_movies=movies
    new_movies['Year'] = new_movies['release_date'].str[-4:].dropna().astype(int)
    new_movies[""release_year""]=new_movies['Year']
    new_movies_frame=new_movies.loc[new_movies[""release_year""]>=starting_year]
    output_frame=new_movies_frame.merge(ratings,left_on=""movie_id"",right_on=""item_id"",how=""inner"")
    final_output_frame=output_frame[[""release_year"",'rating']]
    return final_output_frame
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
15127,1,"    movies_na = movies.dropna(subset=['release_date'])
    movies_na[""release_year""] = movies_na[""release_date""].str[7:].astype(int)
    #ratings[""year""] = pd.to_datetime(ratings['timestamp'],unit='s').dt.year
    ratings = pd.merge(movies_na, ratings, left_on='movie_id', right_on='item_id')",TODO,movies_na,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    movies_na = movies.dropna(subset=['release_date'])
    movies_na[""release_year""] = movies_na[""release_date""].str[7:].astype(int)
    #ratings[""year""] = pd.to_datetime(ratings['timestamp'],unit='s').dt.year
    ratings = pd.merge(movies_na, ratings, left_on='movie_id', right_on='item_id')
    ratings = ratings[ratings['release_year'] >= starting_year]
    ratings = ratings[['release_year', 'rating']]
    return ratings","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    movies_na = movies.dropna(subset=['release_date'])
    movies_na[""release_year""] = movies_na[""release_date""].str[7:].astype(int)
    #ratings[""year""] = pd.to_datetime(ratings['timestamp'],unit='s').dt.year
    ratings = pd.merge(movies_na, ratings, left_on='movie_id', right_on='item_id')
    ratings = ratings[ratings['release_year'] >= starting_year]
    ratings = ratings[['release_year', 'rating']]
    return ratings
movie_rating_distribution(movies, ratings, starting_year = 1990)
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
15127,2,"    ratings = pd.merge(movies_na, ratings, left_on='movie_id', right_on='item_id')
    ratings = ratings[ratings['release_year'] >= starting_year]
    ratings = ratings[['release_year', 'rating']]
    return ratings",TODO,ratings,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    movies_na = movies.dropna(subset=['release_date'])
    movies_na[""release_year""] = movies_na[""release_date""].str[7:].astype(int)
    #ratings[""year""] = pd.to_datetime(ratings['timestamp'],unit='s').dt.year
    ratings = pd.merge(movies_na, ratings, left_on='movie_id', right_on='item_id')
    ratings = ratings[ratings['release_year'] >= starting_year]
    ratings = ratings[['release_year', 'rating']]
    return ratings","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    movies_na = movies.dropna(subset=['release_date'])
    movies_na[""release_year""] = movies_na[""release_date""].str[7:].astype(int)
    #ratings[""year""] = pd.to_datetime(ratings['timestamp'],unit='s').dt.year
    ratings = pd.merge(movies_na, ratings, left_on='movie_id', right_on='item_id')
    ratings = ratings[ratings['release_year'] >= starting_year]
    ratings = ratings[['release_year', 'rating']]
    return ratings
movie_rating_distribution(movies, ratings, starting_year = 1990)
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
14289,1,"    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]",TODO,relyears,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
14289,2,"    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])",TODO,relmovies,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
14289,3,"    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:",TODO,relmoviegroups,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
14289,4,"    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}",TODO,ratingslist,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
14289,5,"    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,",TODO,yearlist,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
14289,6,"    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)",TODO,resultdict,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
14289,7,"    result = pd.DataFrame(resultdict)
    return result",TODO,result,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
14289,8,"        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings",TODO,relratings,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
14289,9,"        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol",TODO,yearcol,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    relyears = map(str, list(range(starting_year, 2000)))
    relmovies = movies[movies[""release_date""].str[-4:].isin(relyears)]
    relmoviegroups = relmovies.groupby(relmovies[""release_date""].str[-4:])
    # Build new dataframe through lists first to avoid repeated concats
    ratingslist = []
    yearlist = []
    for year, rows in relmoviegroups:
        # Create new dataframe with ratings and year
        relratings = ratings[ratings[""item_id""].isin(rows.index.tolist())][""rating""].tolist()
        yearcol = [int(year)] * len(relratings)
        ratingslist += relratings
        yearlist += yearcol
    resultdict = {""release_year"": yearlist,
                  ""rating"": ratingslist}
    result = pd.DataFrame(resultdict)
    return result
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
20286,1,"    movies = movies.dropna(subset=['release_date'])
    x = movies['release_date'].str[-4:].astype('int')>=starting_year
    a = movies[x].index
    df1 = pd.DataFrame({""movie_id"" : movies[x].index, ""release_year"" : list(movies[x]['release_date'].str[-4:].astype('int'))})",TODO,movies,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    
    movies = movies.dropna(subset=['release_date'])
    x = movies['release_date'].str[-4:].astype('int')>=starting_year
    a = movies[x].index
    df1 = pd.DataFrame({""movie_id"" : movies[x].index, ""release_year"" : list(movies[x]['release_date'].str[-4:].astype('int'))})
    return df1.merge(ratings, left_on = ""movie_id"", right_on = ""item_id"", how = ""inner"")[['release_year', 'rating']]
    ","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    
    movies = movies.dropna(subset=['release_date'])
    x = movies['release_date'].str[-4:].astype('int')>=starting_year
    a = movies[x].index
    df1 = pd.DataFrame({""movie_id"" : movies[x].index, ""release_year"" : list(movies[x]['release_date'].str[-4:].astype('int'))})
    return df1.merge(ratings, left_on = ""movie_id"", right_on = ""item_id"", how = ""inner"")[['release_year', 'rating']]
    
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
20286,2,"    x = movies['release_date'].str[-4:].astype('int')>=starting_year
    a = movies[x].index
    df1 = pd.DataFrame({""movie_id"" : movies[x].index, ""release_year"" : list(movies[x]['release_date'].str[-4:].astype('int'))})",TODO,x,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    
    movies = movies.dropna(subset=['release_date'])
    x = movies['release_date'].str[-4:].astype('int')>=starting_year
    a = movies[x].index
    df1 = pd.DataFrame({""movie_id"" : movies[x].index, ""release_year"" : list(movies[x]['release_date'].str[-4:].astype('int'))})
    return df1.merge(ratings, left_on = ""movie_id"", right_on = ""item_id"", how = ""inner"")[['release_year', 'rating']]
    ","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    
    movies = movies.dropna(subset=['release_date'])
    x = movies['release_date'].str[-4:].astype('int')>=starting_year
    a = movies[x].index
    df1 = pd.DataFrame({""movie_id"" : movies[x].index, ""release_year"" : list(movies[x]['release_date'].str[-4:].astype('int'))})
    return df1.merge(ratings, left_on = ""movie_id"", right_on = ""item_id"", how = ""inner"")[['release_year', 'rating']]
    
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
20286,3,"    df1 = pd.DataFrame({""movie_id"" : movies[x].index, ""release_year"" : list(movies[x]['release_date'].str[-4:].astype('int'))})
    return df1.merge(ratings, left_on = ""movie_id"", right_on = ""item_id"", how = ""inner"")[['release_year', 'rating']]",TODO,df1,"def movie_rating_distribution(movies, ratings, starting_year = 1990):
    
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    
    movies = movies.dropna(subset=['release_date'])
    x = movies['release_date'].str[-4:].astype('int')>=starting_year
    a = movies[x].index
    df1 = pd.DataFrame({""movie_id"" : movies[x].index, ""release_year"" : list(movies[x]['release_date'].str[-4:].astype('int'))})
    return df1.merge(ratings, left_on = ""movie_id"", right_on = ""item_id"", how = ""inner"")[['release_year', 'rating']]
    ","def movie_rating_distribution(movies, ratings, starting_year = 1990):
    
    """"""
    Record the release year and rating of every movie released on or after a starting year in the dataset.

    args:
        movies (pd.DataFrame)   : Dataframe containing movie attributes
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        starting_year (int) : the earliest release year for a movie to be considered

    return:
        pd.DataFrame  : a DataFrame of the movie release years and ratings
    """"""
    
    movies = movies.dropna(subset=['release_date'])
    x = movies['release_date'].str[-4:].astype('int')>=starting_year
    a = movies[x].index
    df1 = pd.DataFrame({""movie_id"" : movies[x].index, ""release_year"" : list(movies[x]['release_date'].str[-4:].astype('int'))})
    return df1.merge(ratings, left_on = ""movie_id"", right_on = ""item_id"", how = ""inner"")[['release_year', 'rating']]
    
# local test
def test_movie_rating_distribution():
    ratings_dist = movie_rating_distribution(movies, ratings)
    assert set(ratings_dist.columns) == set([""release_year"", ""rating""])
    assert ratings_dist.shape == (70445, 2)
    assert ratings_dist.release_year.shape == (70445,)
    assert len(ratings_dist[ratings_dist[""release_year""] == 1995]) == 10499
    print(""All tests passed!"")

test_movie_rating_distribution()
def plot_movie_rating_dist(ratings_dist):
    return sns.catplot(x = ""release_year"", y = ""rating"", data = ratings_dist, kind = ""violin"", height = 5, aspect = 2)
    
plot_movie_rating_dist(movie_rating_distribution(movies,ratings))"
21241,1,"    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')
    
    #Filter the ratings by year
    ratings = ratings[ratings['timestamp'].dt.year == year]

    #Finally, count the num of ratings of each months
    rat_counts = ratings['timestamp'].dt.month.value_counts().sort_index() #sorting based on year",TODO,ratings,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    #12 months = 1 year
    
    #Convert the timestamp col to datetime format, easier to filter by year
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')
    
    #Filter the ratings by year
    ratings = ratings[ratings['timestamp'].dt.year == year]

    #Finally, count the num of ratings of each months
    rat_counts = ratings['timestamp'].dt.month.value_counts().sort_index() #sorting based on year

    #New df with the month and rat_counts
    monthly_movie_counts = pd.DataFrame({'month': rat_counts.index, 'rating_count': rat_counts.values})

    return monthly_movie_counts
""""""
The column names will be ""month"" and ""rating_count"". 
Months that do not have any rating count will be excluded from the dataframe. 
The month column will contain integer values from 1 to 12 representing the months of the year.
""""""","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    #12 months = 1 year
    
    #Convert the timestamp col to datetime format, easier to filter by year
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')
    
    #Filter the ratings by year
    ratings = ratings[ratings['timestamp'].dt.year == year]

    #Finally, count the num of ratings of each months
    rat_counts = ratings['timestamp'].dt.month.value_counts().sort_index() #sorting based on year

    #New df with the month and rat_counts
    monthly_movie_counts = pd.DataFrame({'month': rat_counts.index, 'rating_count': rat_counts.values})

    return monthly_movie_counts
""""""
The column names will be ""month"" and ""rating_count"". 
Months that do not have any rating count will be excluded from the dataframe. 
The month column will contain integer values from 1 to 12 representing the months of the year.
""""""
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
21241,2,"    rat_counts = ratings['timestamp'].dt.month.value_counts().sort_index() #sorting based on year

    #New df with the month and rat_counts
    monthly_movie_counts = pd.DataFrame({'month': rat_counts.index, 'rating_count': rat_counts.values})",TODO,rat_counts,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    #12 months = 1 year
    
    #Convert the timestamp col to datetime format, easier to filter by year
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')
    
    #Filter the ratings by year
    ratings = ratings[ratings['timestamp'].dt.year == year]

    #Finally, count the num of ratings of each months
    rat_counts = ratings['timestamp'].dt.month.value_counts().sort_index() #sorting based on year

    #New df with the month and rat_counts
    monthly_movie_counts = pd.DataFrame({'month': rat_counts.index, 'rating_count': rat_counts.values})

    return monthly_movie_counts
""""""
The column names will be ""month"" and ""rating_count"". 
Months that do not have any rating count will be excluded from the dataframe. 
The month column will contain integer values from 1 to 12 representing the months of the year.
""""""","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    #12 months = 1 year
    
    #Convert the timestamp col to datetime format, easier to filter by year
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')
    
    #Filter the ratings by year
    ratings = ratings[ratings['timestamp'].dt.year == year]

    #Finally, count the num of ratings of each months
    rat_counts = ratings['timestamp'].dt.month.value_counts().sort_index() #sorting based on year

    #New df with the month and rat_counts
    monthly_movie_counts = pd.DataFrame({'month': rat_counts.index, 'rating_count': rat_counts.values})

    return monthly_movie_counts
""""""
The column names will be ""month"" and ""rating_count"". 
Months that do not have any rating count will be excluded from the dataframe. 
The month column will contain integer values from 1 to 12 representing the months of the year.
""""""
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
21241,3,"    monthly_movie_counts = pd.DataFrame({'month': rat_counts.index, 'rating_count': rat_counts.values})

    return monthly_movie_counts",TODO,monthly_movie_counts,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    #12 months = 1 year
    
    #Convert the timestamp col to datetime format, easier to filter by year
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')
    
    #Filter the ratings by year
    ratings = ratings[ratings['timestamp'].dt.year == year]

    #Finally, count the num of ratings of each months
    rat_counts = ratings['timestamp'].dt.month.value_counts().sort_index() #sorting based on year

    #New df with the month and rat_counts
    monthly_movie_counts = pd.DataFrame({'month': rat_counts.index, 'rating_count': rat_counts.values})

    return monthly_movie_counts
""""""
The column names will be ""month"" and ""rating_count"". 
Months that do not have any rating count will be excluded from the dataframe. 
The month column will contain integer values from 1 to 12 representing the months of the year.
""""""","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    #12 months = 1 year
    
    #Convert the timestamp col to datetime format, easier to filter by year
    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit = 's')
    
    #Filter the ratings by year
    ratings = ratings[ratings['timestamp'].dt.year == year]

    #Finally, count the num of ratings of each months
    rat_counts = ratings['timestamp'].dt.month.value_counts().sort_index() #sorting based on year

    #New df with the month and rat_counts
    monthly_movie_counts = pd.DataFrame({'month': rat_counts.index, 'rating_count': rat_counts.values})

    return monthly_movie_counts
""""""
The column names will be ""month"" and ""rating_count"". 
Months that do not have any rating count will be excluded from the dataframe. 
The month column will contain integer values from 1 to 12 representing the months of the year.
""""""
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
17740,1,"    movies_reviewed = ratings[(ratings['year'] == year)]

    count = movies_reviewed.groupby('month').size().reset_index(name='rating_count')",TODO,movies_reviewed,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings['rating_datetime'] = pd.to_datetime(ratings['timestamp'],unit='s')
    ratings['month'] = ratings['rating_datetime'].dt.month.astype(str).str.lstrip('0').astype(int)
    ratings['year'] = ratings['rating_datetime'].dt.year
    movies_reviewed = ratings[(ratings['year'] == year)]

    count = movies_reviewed.groupby('month').size().reset_index(name='rating_count')
    return count","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings['rating_datetime'] = pd.to_datetime(ratings['timestamp'],unit='s')
    ratings['month'] = ratings['rating_datetime'].dt.month.astype(str).str.lstrip('0').astype(int)
    ratings['year'] = ratings['rating_datetime'].dt.year
    movies_reviewed = ratings[(ratings['year'] == year)]

    count = movies_reviewed.groupby('month').size().reset_index(name='rating_count')
    return count
# local test
def test_movies_reviewed_by_month():
    movies_reviewed_by_month(ratings, 1997)
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
17740,2,"    count = movies_reviewed.groupby('month').size().reset_index(name='rating_count')
    return count",TODO,count,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings['rating_datetime'] = pd.to_datetime(ratings['timestamp'],unit='s')
    ratings['month'] = ratings['rating_datetime'].dt.month.astype(str).str.lstrip('0').astype(int)
    ratings['year'] = ratings['rating_datetime'].dt.year
    movies_reviewed = ratings[(ratings['year'] == year)]

    count = movies_reviewed.groupby('month').size().reset_index(name='rating_count')
    return count","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings['rating_datetime'] = pd.to_datetime(ratings['timestamp'],unit='s')
    ratings['month'] = ratings['rating_datetime'].dt.month.astype(str).str.lstrip('0').astype(int)
    ratings['year'] = ratings['rating_datetime'].dt.year
    movies_reviewed = ratings[(ratings['year'] == year)]

    count = movies_reviewed.groupby('month').size().reset_index(name='rating_count')
    return count
# local test
def test_movies_reviewed_by_month():
    movies_reviewed_by_month(ratings, 1997)
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
1210,1,"    temp = ratings[ratings[""year""] == year].groupby(""month"").count()
    return temp.reset_index()[[""month"", ""user_id""]].rename(columns={""user_id"":""rating_count""})",TODO,temp,"from datetime import datetime
def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings[""year""] = pd.to_datetime(ratings[""timestamp""], unit=""s"").dt.year
    ratings[""month""] = pd.to_datetime(ratings[""timestamp""], unit=""s"").dt.month
    temp = ratings[ratings[""year""] == year].groupby(""month"").count()
    return temp.reset_index()[[""month"", ""user_id""]].rename(columns={""user_id"":""rating_count""})","from datetime import datetime
def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings[""year""] = pd.to_datetime(ratings[""timestamp""], unit=""s"").dt.year
    ratings[""month""] = pd.to_datetime(ratings[""timestamp""], unit=""s"").dt.month
    temp = ratings[ratings[""year""] == year].groupby(""month"").count()
    return temp.reset_index()[[""month"", ""user_id""]].rename(columns={""user_id"":""rating_count""})
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
19945,1,"    ratings[""datetime""] = pd.to_datetime(ratings[""timestamp""], unit=""s"", origin=""unix"")
    
    # filter rows by year
    ratings = ratings[ratings[""datetime""].dt.year == year].copy()
        
    # extract month from datetime
    ratings[""month""] = ratings[""datetime""].dt.month

    # count rating_count by month
    df_rating_count = ratings.groupby(""month"").size().reset_index(name=""rating_count"")",TODO,ratings,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    
    # convert timestamp to the standard datetime format
    ratings[""datetime""] = pd.to_datetime(ratings[""timestamp""], unit=""s"", origin=""unix"")
    
    # filter rows by year
    ratings = ratings[ratings[""datetime""].dt.year == year].copy()
        
    # extract month from datetime
    ratings[""month""] = ratings[""datetime""].dt.month

    # count rating_count by month
    df_rating_count = ratings.groupby(""month"").size().reset_index(name=""rating_count"")
    
    # display(df_rating_count)
    
    return df_rating_count","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    
    # convert timestamp to the standard datetime format
    ratings[""datetime""] = pd.to_datetime(ratings[""timestamp""], unit=""s"", origin=""unix"")
    
    # filter rows by year
    ratings = ratings[ratings[""datetime""].dt.year == year].copy()
        
    # extract month from datetime
    ratings[""month""] = ratings[""datetime""].dt.month

    # count rating_count by month
    df_rating_count = ratings.groupby(""month"").size().reset_index(name=""rating_count"")
    
    # display(df_rating_count)
    
    return df_rating_count
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
19945,2,"    df_rating_count = ratings.groupby(""month"").size().reset_index(name=""rating_count"")
    
    # display(df_rating_count)
    
    return df_rating_count",TODO,df_rating_count,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    
    # convert timestamp to the standard datetime format
    ratings[""datetime""] = pd.to_datetime(ratings[""timestamp""], unit=""s"", origin=""unix"")
    
    # filter rows by year
    ratings = ratings[ratings[""datetime""].dt.year == year].copy()
        
    # extract month from datetime
    ratings[""month""] = ratings[""datetime""].dt.month

    # count rating_count by month
    df_rating_count = ratings.groupby(""month"").size().reset_index(name=""rating_count"")
    
    # display(df_rating_count)
    
    return df_rating_count","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    
    # convert timestamp to the standard datetime format
    ratings[""datetime""] = pd.to_datetime(ratings[""timestamp""], unit=""s"", origin=""unix"")
    
    # filter rows by year
    ratings = ratings[ratings[""datetime""].dt.year == year].copy()
        
    # extract month from datetime
    ratings[""month""] = ratings[""datetime""].dt.month

    # count rating_count by month
    df_rating_count = ratings.groupby(""month"").size().reset_index(name=""rating_count"")
    
    # display(df_rating_count)
    
    return df_rating_count
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
20629,1,"    year_df = ratings[ratings[""timestamp""].dt.year == year]
    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()",TODO,year_df,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings[""timestamp""] = pd.to_datetime(ratings[""timestamp""], unit='s')
    year_df = ratings[ratings[""timestamp""].dt.year == year]
    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()

    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})

    return result_df","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings[""timestamp""] = pd.to_datetime(ratings[""timestamp""], unit='s')
    year_df = ratings[ratings[""timestamp""].dt.year == year]
    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()

    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})

    return result_df
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
20629,2,"    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()

    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})",TODO,count_df,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings[""timestamp""] = pd.to_datetime(ratings[""timestamp""], unit='s')
    year_df = ratings[ratings[""timestamp""].dt.year == year]
    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()

    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})

    return result_df","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings[""timestamp""] = pd.to_datetime(ratings[""timestamp""], unit='s')
    year_df = ratings[ratings[""timestamp""].dt.year == year]
    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()

    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})

    return result_df
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
20629,3,"    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})

    return result_df",TODO,result_df,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings[""timestamp""] = pd.to_datetime(ratings[""timestamp""], unit='s')
    year_df = ratings[ratings[""timestamp""].dt.year == year]
    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()

    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})

    return result_df","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    ratings[""timestamp""] = pd.to_datetime(ratings[""timestamp""], unit='s')
    year_df = ratings[ratings[""timestamp""].dt.year == year]
    count_df = year_df.groupby(year_df[""timestamp""].dt.month)[""rating""].count()

    result_df = pd.DataFrame({""month"": count_df.index, ""rating_count"": count_df.values})

    return result_df
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
562,1,"    df = ratings.copy()
    df.loc[:,'timestamp']=pd.to_datetime(ratings.loc[:,'timestamp'],unit='s').dt.strftime('%Y-%m')
    df = df[df.timestamp.isin([str(year)+""-""+str(""{:02d}"".format(x)) for x in range(1,13)])]
    df.loc[:,'timestamp']=pd.to_datetime(ratings.loc[:,'timestamp'],unit='s').dt.strftime('%m')
    df=df.loc[:,'rating':'timestamp'].groupby('timestamp').count().reset_index(level=0)
    df.loc[:,'timestamp'] = df.loc[:,'timestamp'].astype(int)
    return df.rename(columns={'timestamp':'month','rating':'rating_count'}, inplace=False)",TODO,df,"def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    df = ratings.copy()
    df.loc[:,'timestamp']=pd.to_datetime(ratings.loc[:,'timestamp'],unit='s').dt.strftime('%Y-%m')
    df = df[df.timestamp.isin([str(year)+""-""+str(""{:02d}"".format(x)) for x in range(1,13)])]
    df.loc[:,'timestamp']=pd.to_datetime(ratings.loc[:,'timestamp'],unit='s').dt.strftime('%m')
    df=df.loc[:,'rating':'timestamp'].groupby('timestamp').count().reset_index(level=0)
    df.loc[:,'timestamp'] = df.loc[:,'timestamp'].astype(int)
    return df.rename(columns={'timestamp':'month','rating':'rating_count'}, inplace=False)
      ","def movies_reviewed_by_month(ratings, year):
    """"""
    Count the number of ratings in each month of a given year.

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings
        year (int)  : the year in which ratings are considered

    return:
        pd.DataFrame  : a DataFrame of the months and movie rating count in each month
    """"""
    df = ratings.copy()
    df.loc[:,'timestamp']=pd.to_datetime(ratings.loc[:,'timestamp'],unit='s').dt.strftime('%Y-%m')
    df = df[df.timestamp.isin([str(year)+""-""+str(""{:02d}"".format(x)) for x in range(1,13)])]
    df.loc[:,'timestamp']=pd.to_datetime(ratings.loc[:,'timestamp'],unit='s').dt.strftime('%m')
    df=df.loc[:,'rating':'timestamp'].groupby('timestamp').count().reset_index(level=0)
    df.loc[:,'timestamp'] = df.loc[:,'timestamp'].astype(int)
    return df.rename(columns={'timestamp':'month','rating':'rating_count'}, inplace=False)
      
# local test
def test_movies_reviewed_by_month():
    ratings_by_month = movies_reviewed_by_month(ratings, 1997)
    assert list(ratings_by_month.loc[ratings_by_month['month']==9, 'rating_count'])[0] == 6704
    print(""All tests passed!"")

test_movies_reviewed_by_month()"
2255,1,"    count_movies = ratings[""item_id""].value_counts().reset_index()
    count_movies.columns = [""item_id"", ""rating_count""]
    count_movies = count_movies[count_movies[""rating_count""] > threshold]
    filtered_df = ratings[ratings['item_id'].isin(count_movies[""item_id""].tolist())]",TODO,count_movies,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    count_movies = ratings[""item_id""].value_counts().reset_index()
    count_movies.columns = [""item_id"", ""rating_count""]
    count_movies = count_movies[count_movies[""rating_count""] > threshold]
    filtered_df = ratings[ratings['item_id'].isin(count_movies[""item_id""].tolist())]
    average_df = filtered_df.groupby('item_id')['rating'].mean().reset_index()
    highest_average_rating = average_df.sort_values('rating', ascending=False)
    lowest_average_rating = average_df.sort_values('rating', ascending=True)
    return highest_average_rating.iloc[:size]['item_id'].tolist(), lowest_average_rating.iloc[:size]['item_id'].tolist()","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    count_movies = ratings[""item_id""].value_counts().reset_index()
    count_movies.columns = [""item_id"", ""rating_count""]
    count_movies = count_movies[count_movies[""rating_count""] > threshold]
    filtered_df = ratings[ratings['item_id'].isin(count_movies[""item_id""].tolist())]
    average_df = filtered_df.groupby('item_id')['rating'].mean().reset_index()
    highest_average_rating = average_df.sort_values('rating', ascending=False)
    lowest_average_rating = average_df.sort_values('rating', ascending=True)
    return highest_average_rating.iloc[:size]['item_id'].tolist(), lowest_average_rating.iloc[:size]['item_id'].tolist()
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
2255,2,"    filtered_df = ratings[ratings['item_id'].isin(count_movies[""item_id""].tolist())]
    average_df = filtered_df.groupby('item_id')['rating'].mean().reset_index()",TODO,filtered_df,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    count_movies = ratings[""item_id""].value_counts().reset_index()
    count_movies.columns = [""item_id"", ""rating_count""]
    count_movies = count_movies[count_movies[""rating_count""] > threshold]
    filtered_df = ratings[ratings['item_id'].isin(count_movies[""item_id""].tolist())]
    average_df = filtered_df.groupby('item_id')['rating'].mean().reset_index()
    highest_average_rating = average_df.sort_values('rating', ascending=False)
    lowest_average_rating = average_df.sort_values('rating', ascending=True)
    return highest_average_rating.iloc[:size]['item_id'].tolist(), lowest_average_rating.iloc[:size]['item_id'].tolist()","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    count_movies = ratings[""item_id""].value_counts().reset_index()
    count_movies.columns = [""item_id"", ""rating_count""]
    count_movies = count_movies[count_movies[""rating_count""] > threshold]
    filtered_df = ratings[ratings['item_id'].isin(count_movies[""item_id""].tolist())]
    average_df = filtered_df.groupby('item_id')['rating'].mean().reset_index()
    highest_average_rating = average_df.sort_values('rating', ascending=False)
    lowest_average_rating = average_df.sort_values('rating', ascending=True)
    return highest_average_rating.iloc[:size]['item_id'].tolist(), lowest_average_rating.iloc[:size]['item_id'].tolist()
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
2255,3,"    average_df = filtered_df.groupby('item_id')['rating'].mean().reset_index()
    highest_average_rating = average_df.sort_values('rating', ascending=False)
    lowest_average_rating = average_df.sort_values('rating', ascending=True)",TODO,average_df,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    count_movies = ratings[""item_id""].value_counts().reset_index()
    count_movies.columns = [""item_id"", ""rating_count""]
    count_movies = count_movies[count_movies[""rating_count""] > threshold]
    filtered_df = ratings[ratings['item_id'].isin(count_movies[""item_id""].tolist())]
    average_df = filtered_df.groupby('item_id')['rating'].mean().reset_index()
    highest_average_rating = average_df.sort_values('rating', ascending=False)
    lowest_average_rating = average_df.sort_values('rating', ascending=True)
    return highest_average_rating.iloc[:size]['item_id'].tolist(), lowest_average_rating.iloc[:size]['item_id'].tolist()","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    count_movies = ratings[""item_id""].value_counts().reset_index()
    count_movies.columns = [""item_id"", ""rating_count""]
    count_movies = count_movies[count_movies[""rating_count""] > threshold]
    filtered_df = ratings[ratings['item_id'].isin(count_movies[""item_id""].tolist())]
    average_df = filtered_df.groupby('item_id')['rating'].mean().reset_index()
    highest_average_rating = average_df.sort_values('rating', ascending=False)
    lowest_average_rating = average_df.sort_values('rating', ascending=True)
    return highest_average_rating.iloc[:size]['item_id'].tolist(), lowest_average_rating.iloc[:size]['item_id'].tolist()
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
2255,4,"    highest_average_rating = average_df.sort_values('rating', ascending=False)
    lowest_average_rating = average_df.sort_values('rating', ascending=True)
    return highest_average_rating.iloc[:size]['item_id'].tolist(), lowest_average_rating.iloc[:size]['item_id'].tolist()",TODO,highest_average_rating,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    count_movies = ratings[""item_id""].value_counts().reset_index()
    count_movies.columns = [""item_id"", ""rating_count""]
    count_movies = count_movies[count_movies[""rating_count""] > threshold]
    filtered_df = ratings[ratings['item_id'].isin(count_movies[""item_id""].tolist())]
    average_df = filtered_df.groupby('item_id')['rating'].mean().reset_index()
    highest_average_rating = average_df.sort_values('rating', ascending=False)
    lowest_average_rating = average_df.sort_values('rating', ascending=True)
    return highest_average_rating.iloc[:size]['item_id'].tolist(), lowest_average_rating.iloc[:size]['item_id'].tolist()","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    count_movies = ratings[""item_id""].value_counts().reset_index()
    count_movies.columns = [""item_id"", ""rating_count""]
    count_movies = count_movies[count_movies[""rating_count""] > threshold]
    filtered_df = ratings[ratings['item_id'].isin(count_movies[""item_id""].tolist())]
    average_df = filtered_df.groupby('item_id')['rating'].mean().reset_index()
    highest_average_rating = average_df.sort_values('rating', ascending=False)
    lowest_average_rating = average_df.sort_values('rating', ascending=True)
    return highest_average_rating.iloc[:size]['item_id'].tolist(), lowest_average_rating.iloc[:size]['item_id'].tolist()
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
2255,5,"    lowest_average_rating = average_df.sort_values('rating', ascending=True)
    return highest_average_rating.iloc[:size]['item_id'].tolist(), lowest_average_rating.iloc[:size]['item_id'].tolist()",TODO,lowest_average_rating,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    count_movies = ratings[""item_id""].value_counts().reset_index()
    count_movies.columns = [""item_id"", ""rating_count""]
    count_movies = count_movies[count_movies[""rating_count""] > threshold]
    filtered_df = ratings[ratings['item_id'].isin(count_movies[""item_id""].tolist())]
    average_df = filtered_df.groupby('item_id')['rating'].mean().reset_index()
    highest_average_rating = average_df.sort_values('rating', ascending=False)
    lowest_average_rating = average_df.sort_values('rating', ascending=True)
    return highest_average_rating.iloc[:size]['item_id'].tolist(), lowest_average_rating.iloc[:size]['item_id'].tolist()","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    count_movies = ratings[""item_id""].value_counts().reset_index()
    count_movies.columns = [""item_id"", ""rating_count""]
    count_movies = count_movies[count_movies[""rating_count""] > threshold]
    filtered_df = ratings[ratings['item_id'].isin(count_movies[""item_id""].tolist())]
    average_df = filtered_df.groupby('item_id')['rating'].mean().reset_index()
    highest_average_rating = average_df.sort_values('rating', ascending=False)
    lowest_average_rating = average_df.sort_values('rating', ascending=True)
    return highest_average_rating.iloc[:size]['item_id'].tolist(), lowest_average_rating.iloc[:size]['item_id'].tolist()
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
23187,1,"    ratings_temp = ratings.drop(['user_id', 'timestamp'], axis=1)
    
    #adding number of ratings per movie and average rating columns
    ratings_temp = ratings_temp.groupby(""item_id"").agg(rating_count = (""rating"", ""count""), rating_score = (""rating"", ""mean""))

    #filtering for only more than threshold ratings
    ratings_temp = ratings_temp[ratings_temp['rating_count'] > 50]
    
    ratings_temp['movie_id'] = ratings_temp.index

    
    #temp df for best movies sorted descending order
    ratings_temp = ratings_temp.sort_values(by=['rating_score'])

    #temp df for best movies sorted descending order
    ratings_temp_b = ratings_temp.sort_values(by=['rating_score'], ascending = False)

    #making dfs with only first 'size' number of movie ids
    ratings_temp_best = ratings_temp_b.iloc[:size]
    ratings_temp_worst = ratings_temp.iloc[:size]",TODO,ratings_temp,"def best_worst_movies(ratings, threshold = 50, size = 10):   

    ratings_temp = ratings.drop(['user_id', 'timestamp'], axis=1)
    
    #adding number of ratings per movie and average rating columns
    ratings_temp = ratings_temp.groupby(""item_id"").agg(rating_count = (""rating"", ""count""), rating_score = (""rating"", ""mean""))

    #filtering for only more than threshold ratings
    ratings_temp = ratings_temp[ratings_temp['rating_count'] > 50]
    
    ratings_temp['movie_id'] = ratings_temp.index

    
    #temp df for best movies sorted descending order
    ratings_temp = ratings_temp.sort_values(by=['rating_score'])

    #temp df for best movies sorted descending order
    ratings_temp_b = ratings_temp.sort_values(by=['rating_score'], ascending = False)

    #making dfs with only first 'size' number of movie ids
    ratings_temp_best = ratings_temp_b.iloc[:size]
    ratings_temp_worst = ratings_temp.iloc[:size]


    best_movie_id =ratings_temp_best['movie_id'].to_list()
    worst_movie_id = ratings_temp_worst['movie_id'].to_list()
 
    
    return best_movie_id, worst_movie_id
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):   

    ratings_temp = ratings.drop(['user_id', 'timestamp'], axis=1)
    
    #adding number of ratings per movie and average rating columns
    ratings_temp = ratings_temp.groupby(""item_id"").agg(rating_count = (""rating"", ""count""), rating_score = (""rating"", ""mean""))

    #filtering for only more than threshold ratings
    ratings_temp = ratings_temp[ratings_temp['rating_count'] > 50]
    
    ratings_temp['movie_id'] = ratings_temp.index

    
    #temp df for best movies sorted descending order
    ratings_temp = ratings_temp.sort_values(by=['rating_score'])

    #temp df for best movies sorted descending order
    ratings_temp_b = ratings_temp.sort_values(by=['rating_score'], ascending = False)

    #making dfs with only first 'size' number of movie ids
    ratings_temp_best = ratings_temp_b.iloc[:size]
    ratings_temp_worst = ratings_temp.iloc[:size]


    best_movie_id =ratings_temp_best['movie_id'].to_list()
    worst_movie_id = ratings_temp_worst['movie_id'].to_list()
 
    
    return best_movie_id, worst_movie_id
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
23187,2,"    ratings_temp_b = ratings_temp.sort_values(by=['rating_score'], ascending = False)

    #making dfs with only first 'size' number of movie ids
    ratings_temp_best = ratings_temp_b.iloc[:size]",TODO,ratings_temp_b,"def best_worst_movies(ratings, threshold = 50, size = 10):   

    ratings_temp = ratings.drop(['user_id', 'timestamp'], axis=1)
    
    #adding number of ratings per movie and average rating columns
    ratings_temp = ratings_temp.groupby(""item_id"").agg(rating_count = (""rating"", ""count""), rating_score = (""rating"", ""mean""))

    #filtering for only more than threshold ratings
    ratings_temp = ratings_temp[ratings_temp['rating_count'] > 50]
    
    ratings_temp['movie_id'] = ratings_temp.index

    
    #temp df for best movies sorted descending order
    ratings_temp = ratings_temp.sort_values(by=['rating_score'])

    #temp df for best movies sorted descending order
    ratings_temp_b = ratings_temp.sort_values(by=['rating_score'], ascending = False)

    #making dfs with only first 'size' number of movie ids
    ratings_temp_best = ratings_temp_b.iloc[:size]
    ratings_temp_worst = ratings_temp.iloc[:size]


    best_movie_id =ratings_temp_best['movie_id'].to_list()
    worst_movie_id = ratings_temp_worst['movie_id'].to_list()
 
    
    return best_movie_id, worst_movie_id
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):   

    ratings_temp = ratings.drop(['user_id', 'timestamp'], axis=1)
    
    #adding number of ratings per movie and average rating columns
    ratings_temp = ratings_temp.groupby(""item_id"").agg(rating_count = (""rating"", ""count""), rating_score = (""rating"", ""mean""))

    #filtering for only more than threshold ratings
    ratings_temp = ratings_temp[ratings_temp['rating_count'] > 50]
    
    ratings_temp['movie_id'] = ratings_temp.index

    
    #temp df for best movies sorted descending order
    ratings_temp = ratings_temp.sort_values(by=['rating_score'])

    #temp df for best movies sorted descending order
    ratings_temp_b = ratings_temp.sort_values(by=['rating_score'], ascending = False)

    #making dfs with only first 'size' number of movie ids
    ratings_temp_best = ratings_temp_b.iloc[:size]
    ratings_temp_worst = ratings_temp.iloc[:size]


    best_movie_id =ratings_temp_best['movie_id'].to_list()
    worst_movie_id = ratings_temp_worst['movie_id'].to_list()
 
    
    return best_movie_id, worst_movie_id
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
23187,3,"    ratings_temp_best = ratings_temp_b.iloc[:size]
    ratings_temp_worst = ratings_temp.iloc[:size]


    best_movie_id =ratings_temp_best['movie_id'].to_list()",TODO,ratings_temp_best,"def best_worst_movies(ratings, threshold = 50, size = 10):   

    ratings_temp = ratings.drop(['user_id', 'timestamp'], axis=1)
    
    #adding number of ratings per movie and average rating columns
    ratings_temp = ratings_temp.groupby(""item_id"").agg(rating_count = (""rating"", ""count""), rating_score = (""rating"", ""mean""))

    #filtering for only more than threshold ratings
    ratings_temp = ratings_temp[ratings_temp['rating_count'] > 50]
    
    ratings_temp['movie_id'] = ratings_temp.index

    
    #temp df for best movies sorted descending order
    ratings_temp = ratings_temp.sort_values(by=['rating_score'])

    #temp df for best movies sorted descending order
    ratings_temp_b = ratings_temp.sort_values(by=['rating_score'], ascending = False)

    #making dfs with only first 'size' number of movie ids
    ratings_temp_best = ratings_temp_b.iloc[:size]
    ratings_temp_worst = ratings_temp.iloc[:size]


    best_movie_id =ratings_temp_best['movie_id'].to_list()
    worst_movie_id = ratings_temp_worst['movie_id'].to_list()
 
    
    return best_movie_id, worst_movie_id
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):   

    ratings_temp = ratings.drop(['user_id', 'timestamp'], axis=1)
    
    #adding number of ratings per movie and average rating columns
    ratings_temp = ratings_temp.groupby(""item_id"").agg(rating_count = (""rating"", ""count""), rating_score = (""rating"", ""mean""))

    #filtering for only more than threshold ratings
    ratings_temp = ratings_temp[ratings_temp['rating_count'] > 50]
    
    ratings_temp['movie_id'] = ratings_temp.index

    
    #temp df for best movies sorted descending order
    ratings_temp = ratings_temp.sort_values(by=['rating_score'])

    #temp df for best movies sorted descending order
    ratings_temp_b = ratings_temp.sort_values(by=['rating_score'], ascending = False)

    #making dfs with only first 'size' number of movie ids
    ratings_temp_best = ratings_temp_b.iloc[:size]
    ratings_temp_worst = ratings_temp.iloc[:size]


    best_movie_id =ratings_temp_best['movie_id'].to_list()
    worst_movie_id = ratings_temp_worst['movie_id'].to_list()
 
    
    return best_movie_id, worst_movie_id
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
23187,4,"    ratings_temp_worst = ratings_temp.iloc[:size]


    best_movie_id =ratings_temp_best['movie_id'].to_list()
    worst_movie_id = ratings_temp_worst['movie_id'].to_list()",TODO,ratings_temp_worst,"def best_worst_movies(ratings, threshold = 50, size = 10):   

    ratings_temp = ratings.drop(['user_id', 'timestamp'], axis=1)
    
    #adding number of ratings per movie and average rating columns
    ratings_temp = ratings_temp.groupby(""item_id"").agg(rating_count = (""rating"", ""count""), rating_score = (""rating"", ""mean""))

    #filtering for only more than threshold ratings
    ratings_temp = ratings_temp[ratings_temp['rating_count'] > 50]
    
    ratings_temp['movie_id'] = ratings_temp.index

    
    #temp df for best movies sorted descending order
    ratings_temp = ratings_temp.sort_values(by=['rating_score'])

    #temp df for best movies sorted descending order
    ratings_temp_b = ratings_temp.sort_values(by=['rating_score'], ascending = False)

    #making dfs with only first 'size' number of movie ids
    ratings_temp_best = ratings_temp_b.iloc[:size]
    ratings_temp_worst = ratings_temp.iloc[:size]


    best_movie_id =ratings_temp_best['movie_id'].to_list()
    worst_movie_id = ratings_temp_worst['movie_id'].to_list()
 
    
    return best_movie_id, worst_movie_id
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):   

    ratings_temp = ratings.drop(['user_id', 'timestamp'], axis=1)
    
    #adding number of ratings per movie and average rating columns
    ratings_temp = ratings_temp.groupby(""item_id"").agg(rating_count = (""rating"", ""count""), rating_score = (""rating"", ""mean""))

    #filtering for only more than threshold ratings
    ratings_temp = ratings_temp[ratings_temp['rating_count'] > 50]
    
    ratings_temp['movie_id'] = ratings_temp.index

    
    #temp df for best movies sorted descending order
    ratings_temp = ratings_temp.sort_values(by=['rating_score'])

    #temp df for best movies sorted descending order
    ratings_temp_b = ratings_temp.sort_values(by=['rating_score'], ascending = False)

    #making dfs with only first 'size' number of movie ids
    ratings_temp_best = ratings_temp_b.iloc[:size]
    ratings_temp_worst = ratings_temp.iloc[:size]


    best_movie_id =ratings_temp_best['movie_id'].to_list()
    worst_movie_id = ratings_temp_worst['movie_id'].to_list()
 
    
    return best_movie_id, worst_movie_id
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
23187,5,"    best_movie_id =ratings_temp_best['movie_id'].to_list()
    worst_movie_id = ratings_temp_worst['movie_id'].to_list()
 
    
    return best_movie_id, worst_movie_id",TODO,best_movie_id,"def best_worst_movies(ratings, threshold = 50, size = 10):   

    ratings_temp = ratings.drop(['user_id', 'timestamp'], axis=1)
    
    #adding number of ratings per movie and average rating columns
    ratings_temp = ratings_temp.groupby(""item_id"").agg(rating_count = (""rating"", ""count""), rating_score = (""rating"", ""mean""))

    #filtering for only more than threshold ratings
    ratings_temp = ratings_temp[ratings_temp['rating_count'] > 50]
    
    ratings_temp['movie_id'] = ratings_temp.index

    
    #temp df for best movies sorted descending order
    ratings_temp = ratings_temp.sort_values(by=['rating_score'])

    #temp df for best movies sorted descending order
    ratings_temp_b = ratings_temp.sort_values(by=['rating_score'], ascending = False)

    #making dfs with only first 'size' number of movie ids
    ratings_temp_best = ratings_temp_b.iloc[:size]
    ratings_temp_worst = ratings_temp.iloc[:size]


    best_movie_id =ratings_temp_best['movie_id'].to_list()
    worst_movie_id = ratings_temp_worst['movie_id'].to_list()
 
    
    return best_movie_id, worst_movie_id
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):   

    ratings_temp = ratings.drop(['user_id', 'timestamp'], axis=1)
    
    #adding number of ratings per movie and average rating columns
    ratings_temp = ratings_temp.groupby(""item_id"").agg(rating_count = (""rating"", ""count""), rating_score = (""rating"", ""mean""))

    #filtering for only more than threshold ratings
    ratings_temp = ratings_temp[ratings_temp['rating_count'] > 50]
    
    ratings_temp['movie_id'] = ratings_temp.index

    
    #temp df for best movies sorted descending order
    ratings_temp = ratings_temp.sort_values(by=['rating_score'])

    #temp df for best movies sorted descending order
    ratings_temp_b = ratings_temp.sort_values(by=['rating_score'], ascending = False)

    #making dfs with only first 'size' number of movie ids
    ratings_temp_best = ratings_temp_b.iloc[:size]
    ratings_temp_worst = ratings_temp.iloc[:size]


    best_movie_id =ratings_temp_best['movie_id'].to_list()
    worst_movie_id = ratings_temp_worst['movie_id'].to_list()
 
    
    return best_movie_id, worst_movie_id
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
23187,6,"    worst_movie_id = ratings_temp_worst['movie_id'].to_list()
 
    
    return best_movie_id, worst_movie_id",TODO,worst_movie_id,"def best_worst_movies(ratings, threshold = 50, size = 10):   

    ratings_temp = ratings.drop(['user_id', 'timestamp'], axis=1)
    
    #adding number of ratings per movie and average rating columns
    ratings_temp = ratings_temp.groupby(""item_id"").agg(rating_count = (""rating"", ""count""), rating_score = (""rating"", ""mean""))

    #filtering for only more than threshold ratings
    ratings_temp = ratings_temp[ratings_temp['rating_count'] > 50]
    
    ratings_temp['movie_id'] = ratings_temp.index

    
    #temp df for best movies sorted descending order
    ratings_temp = ratings_temp.sort_values(by=['rating_score'])

    #temp df for best movies sorted descending order
    ratings_temp_b = ratings_temp.sort_values(by=['rating_score'], ascending = False)

    #making dfs with only first 'size' number of movie ids
    ratings_temp_best = ratings_temp_b.iloc[:size]
    ratings_temp_worst = ratings_temp.iloc[:size]


    best_movie_id =ratings_temp_best['movie_id'].to_list()
    worst_movie_id = ratings_temp_worst['movie_id'].to_list()
 
    
    return best_movie_id, worst_movie_id
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):   

    ratings_temp = ratings.drop(['user_id', 'timestamp'], axis=1)
    
    #adding number of ratings per movie and average rating columns
    ratings_temp = ratings_temp.groupby(""item_id"").agg(rating_count = (""rating"", ""count""), rating_score = (""rating"", ""mean""))

    #filtering for only more than threshold ratings
    ratings_temp = ratings_temp[ratings_temp['rating_count'] > 50]
    
    ratings_temp['movie_id'] = ratings_temp.index

    
    #temp df for best movies sorted descending order
    ratings_temp = ratings_temp.sort_values(by=['rating_score'])

    #temp df for best movies sorted descending order
    ratings_temp_b = ratings_temp.sort_values(by=['rating_score'], ascending = False)

    #making dfs with only first 'size' number of movie ids
    ratings_temp_best = ratings_temp_b.iloc[:size]
    ratings_temp_worst = ratings_temp.iloc[:size]


    best_movie_id =ratings_temp_best['movie_id'].to_list()
    worst_movie_id = ratings_temp_worst['movie_id'].to_list()
 
    
    return best_movie_id, worst_movie_id
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
22152,1,"    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")",TODO,df,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
22152,2,"    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")",TODO,df1,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
22152,3,"    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]",TODO,df2,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
22152,4,"    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")",TODO,df3,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
22152,5,"    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()",TODO,df4,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
22152,6,"    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")",TODO,df5,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
22152,7,"    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()",TODO,df6,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
22152,8,"    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]",TODO,list0,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
22152,9,"    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1",TODO,list1,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
22152,10,"    list2 = list0[-size:]
    list2.reverse()
    return list2, list1",TODO,list2,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    df = ratings[[""item_id"",""rating""]].groupby(""item_id"").count()
    df1 = pd.DataFrame()
    df1[[""item_id"",""rating0""]] = ratings[[""item_id"",""rating""]]
    df2 = df1.merge(df, left_on = ""item_id"", right_on = ""item_id"", how = ""left"")
    df3 = df2[df2['rating'] >= threshold]
    df4 = df3.groupby(""item_id"").mean(""rating0"")
    df5=df4.reset_index()
    df6 = df5.sort_values(""rating0"")
    list0 = df6[""item_id""].tolist()
    list1 = list0[:size]
    list2 = list0[-size:]
    list2.reverse()
    return list2, list1
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
1355,1,"    counts_df = ratings.groupby('item_id').count().reset_index()[['item_id',  'rating']].rename(columns={""rating"": ""counts""})
    counts_df_filtered = counts_df[counts_df['counts'] > threshold]",TODO,counts_df,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    counts_df = ratings.groupby('item_id').count().reset_index()[['item_id',  'rating']].rename(columns={""rating"": ""counts""})
    counts_df_filtered = counts_df[counts_df['counts'] > threshold]
    ratings = counts_df_filtered.merge(ratings, on='item_id')
    ratings_trf = ratings.groupby('item_id').mean().reset_index()[['item_id', 'rating']].sort_values(by=['rating'])
    worst_df = ratings_trf.head(size)['item_id'].tolist()
    best_df = ratings_trf.tail(size)['item_id'].tolist()
    return (best_df[::-1], worst_df)        
    ","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    counts_df = ratings.groupby('item_id').count().reset_index()[['item_id',  'rating']].rename(columns={""rating"": ""counts""})
    counts_df_filtered = counts_df[counts_df['counts'] > threshold]
    ratings = counts_df_filtered.merge(ratings, on='item_id')
    ratings_trf = ratings.groupby('item_id').mean().reset_index()[['item_id', 'rating']].sort_values(by=['rating'])
    worst_df = ratings_trf.head(size)['item_id'].tolist()
    best_df = ratings_trf.tail(size)['item_id'].tolist()
    return (best_df[::-1], worst_df)        
    
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
1355,2,"    counts_df_filtered = counts_df[counts_df['counts'] > threshold]
    ratings = counts_df_filtered.merge(ratings, on='item_id')",TODO,counts_df_filtered,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    counts_df = ratings.groupby('item_id').count().reset_index()[['item_id',  'rating']].rename(columns={""rating"": ""counts""})
    counts_df_filtered = counts_df[counts_df['counts'] > threshold]
    ratings = counts_df_filtered.merge(ratings, on='item_id')
    ratings_trf = ratings.groupby('item_id').mean().reset_index()[['item_id', 'rating']].sort_values(by=['rating'])
    worst_df = ratings_trf.head(size)['item_id'].tolist()
    best_df = ratings_trf.tail(size)['item_id'].tolist()
    return (best_df[::-1], worst_df)        
    ","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    counts_df = ratings.groupby('item_id').count().reset_index()[['item_id',  'rating']].rename(columns={""rating"": ""counts""})
    counts_df_filtered = counts_df[counts_df['counts'] > threshold]
    ratings = counts_df_filtered.merge(ratings, on='item_id')
    ratings_trf = ratings.groupby('item_id').mean().reset_index()[['item_id', 'rating']].sort_values(by=['rating'])
    worst_df = ratings_trf.head(size)['item_id'].tolist()
    best_df = ratings_trf.tail(size)['item_id'].tolist()
    return (best_df[::-1], worst_df)        
    
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
1355,3,"    counts_df = ratings.groupby('item_id').count().reset_index()[['item_id',  'rating']].rename(columns={""rating"": ""counts""})
    counts_df_filtered = counts_df[counts_df['counts'] > threshold]
    ratings = counts_df_filtered.merge(ratings, on='item_id')
    ratings_trf = ratings.groupby('item_id').mean().reset_index()[['item_id', 'rating']].sort_values(by=['rating'])",TODO,ratings,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    counts_df = ratings.groupby('item_id').count().reset_index()[['item_id',  'rating']].rename(columns={""rating"": ""counts""})
    counts_df_filtered = counts_df[counts_df['counts'] > threshold]
    ratings = counts_df_filtered.merge(ratings, on='item_id')
    ratings_trf = ratings.groupby('item_id').mean().reset_index()[['item_id', 'rating']].sort_values(by=['rating'])
    worst_df = ratings_trf.head(size)['item_id'].tolist()
    best_df = ratings_trf.tail(size)['item_id'].tolist()
    return (best_df[::-1], worst_df)        
    ","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    counts_df = ratings.groupby('item_id').count().reset_index()[['item_id',  'rating']].rename(columns={""rating"": ""counts""})
    counts_df_filtered = counts_df[counts_df['counts'] > threshold]
    ratings = counts_df_filtered.merge(ratings, on='item_id')
    ratings_trf = ratings.groupby('item_id').mean().reset_index()[['item_id', 'rating']].sort_values(by=['rating'])
    worst_df = ratings_trf.head(size)['item_id'].tolist()
    best_df = ratings_trf.tail(size)['item_id'].tolist()
    return (best_df[::-1], worst_df)        
    
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
1355,4,"    ratings_trf = ratings.groupby('item_id').mean().reset_index()[['item_id', 'rating']].sort_values(by=['rating'])
    worst_df = ratings_trf.head(size)['item_id'].tolist()
    best_df = ratings_trf.tail(size)['item_id'].tolist()",TODO,ratings_trf,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    counts_df = ratings.groupby('item_id').count().reset_index()[['item_id',  'rating']].rename(columns={""rating"": ""counts""})
    counts_df_filtered = counts_df[counts_df['counts'] > threshold]
    ratings = counts_df_filtered.merge(ratings, on='item_id')
    ratings_trf = ratings.groupby('item_id').mean().reset_index()[['item_id', 'rating']].sort_values(by=['rating'])
    worst_df = ratings_trf.head(size)['item_id'].tolist()
    best_df = ratings_trf.tail(size)['item_id'].tolist()
    return (best_df[::-1], worst_df)        
    ","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    counts_df = ratings.groupby('item_id').count().reset_index()[['item_id',  'rating']].rename(columns={""rating"": ""counts""})
    counts_df_filtered = counts_df[counts_df['counts'] > threshold]
    ratings = counts_df_filtered.merge(ratings, on='item_id')
    ratings_trf = ratings.groupby('item_id').mean().reset_index()[['item_id', 'rating']].sort_values(by=['rating'])
    worst_df = ratings_trf.head(size)['item_id'].tolist()
    best_df = ratings_trf.tail(size)['item_id'].tolist()
    return (best_df[::-1], worst_df)        
    
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
1355,5,"    worst_df = ratings_trf.head(size)['item_id'].tolist()
    best_df = ratings_trf.tail(size)['item_id'].tolist()
    return (best_df[::-1], worst_df)",TODO,worst_df,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    counts_df = ratings.groupby('item_id').count().reset_index()[['item_id',  'rating']].rename(columns={""rating"": ""counts""})
    counts_df_filtered = counts_df[counts_df['counts'] > threshold]
    ratings = counts_df_filtered.merge(ratings, on='item_id')
    ratings_trf = ratings.groupby('item_id').mean().reset_index()[['item_id', 'rating']].sort_values(by=['rating'])
    worst_df = ratings_trf.head(size)['item_id'].tolist()
    best_df = ratings_trf.tail(size)['item_id'].tolist()
    return (best_df[::-1], worst_df)        
    ","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    counts_df = ratings.groupby('item_id').count().reset_index()[['item_id',  'rating']].rename(columns={""rating"": ""counts""})
    counts_df_filtered = counts_df[counts_df['counts'] > threshold]
    ratings = counts_df_filtered.merge(ratings, on='item_id')
    ratings_trf = ratings.groupby('item_id').mean().reset_index()[['item_id', 'rating']].sort_values(by=['rating'])
    worst_df = ratings_trf.head(size)['item_id'].tolist()
    best_df = ratings_trf.tail(size)['item_id'].tolist()
    return (best_df[::-1], worst_df)        
    
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
1355,6,"    best_df = ratings_trf.tail(size)['item_id'].tolist()
    return (best_df[::-1], worst_df)",TODO,best_df,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    counts_df = ratings.groupby('item_id').count().reset_index()[['item_id',  'rating']].rename(columns={""rating"": ""counts""})
    counts_df_filtered = counts_df[counts_df['counts'] > threshold]
    ratings = counts_df_filtered.merge(ratings, on='item_id')
    ratings_trf = ratings.groupby('item_id').mean().reset_index()[['item_id', 'rating']].sort_values(by=['rating'])
    worst_df = ratings_trf.head(size)['item_id'].tolist()
    best_df = ratings_trf.tail(size)['item_id'].tolist()
    return (best_df[::-1], worst_df)        
    ","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    counts_df = ratings.groupby('item_id').count().reset_index()[['item_id',  'rating']].rename(columns={""rating"": ""counts""})
    counts_df_filtered = counts_df[counts_df['counts'] > threshold]
    ratings = counts_df_filtered.merge(ratings, on='item_id')
    ratings_trf = ratings.groupby('item_id').mean().reset_index()[['item_id', 'rating']].sort_values(by=['rating'])
    worst_df = ratings_trf.head(size)['item_id'].tolist()
    best_df = ratings_trf.tail(size)['item_id'].tolist()
    return (best_df[::-1], worst_df)        
    
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
2588,1,"    work_df = ratings.copy()
    
    result = work_df.groupby(['item_id'])['rating'].agg(['count','mean'])",TODO,work_df,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    work_df = ratings.copy()
    
    result = work_df.groupby(['item_id'])['rating'].agg(['count','mean'])
    result.reset_index(inplace = True)
#     result = result[result['count']>=threshold]
    
    best_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean'],ascending = False).head(size)['item_id'].values)
    worst_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean']).head(size)['item_id'].values)

    return best_movie_ids, worst_movie_ids
    
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    work_df = ratings.copy()
    
    result = work_df.groupby(['item_id'])['rating'].agg(['count','mean'])
    result.reset_index(inplace = True)
#     result = result[result['count']>=threshold]
    
    best_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean'],ascending = False).head(size)['item_id'].values)
    worst_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean']).head(size)['item_id'].values)

    return best_movie_ids, worst_movie_ids
    
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
2588,2,"    result = work_df.groupby(['item_id'])['rating'].agg(['count','mean'])
    result.reset_index(inplace = True)
#     result = result[result['count']>=threshold]
    
    best_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean'],ascending = False).head(size)['item_id'].values)
    worst_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean']).head(size)['item_id'].values)",TODO,result,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    work_df = ratings.copy()
    
    result = work_df.groupby(['item_id'])['rating'].agg(['count','mean'])
    result.reset_index(inplace = True)
#     result = result[result['count']>=threshold]
    
    best_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean'],ascending = False).head(size)['item_id'].values)
    worst_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean']).head(size)['item_id'].values)

    return best_movie_ids, worst_movie_ids
    
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    work_df = ratings.copy()
    
    result = work_df.groupby(['item_id'])['rating'].agg(['count','mean'])
    result.reset_index(inplace = True)
#     result = result[result['count']>=threshold]
    
    best_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean'],ascending = False).head(size)['item_id'].values)
    worst_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean']).head(size)['item_id'].values)

    return best_movie_ids, worst_movie_ids
    
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
2588,3,"    best_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean'],ascending = False).head(size)['item_id'].values)
    worst_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean']).head(size)['item_id'].values)

    return best_movie_ids, worst_movie_ids",TODO,best_movie_ids,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    work_df = ratings.copy()
    
    result = work_df.groupby(['item_id'])['rating'].agg(['count','mean'])
    result.reset_index(inplace = True)
#     result = result[result['count']>=threshold]
    
    best_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean'],ascending = False).head(size)['item_id'].values)
    worst_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean']).head(size)['item_id'].values)

    return best_movie_ids, worst_movie_ids
    
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    work_df = ratings.copy()
    
    result = work_df.groupby(['item_id'])['rating'].agg(['count','mean'])
    result.reset_index(inplace = True)
#     result = result[result['count']>=threshold]
    
    best_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean'],ascending = False).head(size)['item_id'].values)
    worst_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean']).head(size)['item_id'].values)

    return best_movie_ids, worst_movie_ids
    
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
2588,4,"    worst_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean']).head(size)['item_id'].values)

    return best_movie_ids, worst_movie_ids",TODO,worst_movie_ids,"def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    work_df = ratings.copy()
    
    result = work_df.groupby(['item_id'])['rating'].agg(['count','mean'])
    result.reset_index(inplace = True)
#     result = result[result['count']>=threshold]
    
    best_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean'],ascending = False).head(size)['item_id'].values)
    worst_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean']).head(size)['item_id'].values)

    return best_movie_ids, worst_movie_ids
    
    pass","def best_worst_movies(ratings, threshold = 50, size = 10):
    """"""
    Get the top movies with highest average ratings and top movies with lowest average ratings

    args:
        ratings (pd.DataFrame)  : Dataframe containing user ratings

    kwargs:
        threshold (int) : movies that are considered should have more ratings than this threshold
        size (int) : the number of movies with lowest / highest average ratings to get

    return: Tuple (best_movie_ids, worst_movie_ids)
            best_movie_ids (List[int])  : a list of ids for movies with the highest average ratings, sorted from highest to lowest average rating.
            worst_movie_ids [List[int]] : a list of ids for movies with the lowest average ratings, sorted from lowest to highest average rating.
    """"""
    work_df = ratings.copy()
    
    result = work_df.groupby(['item_id'])['rating'].agg(['count','mean'])
    result.reset_index(inplace = True)
#     result = result[result['count']>=threshold]
    
    best_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean'],ascending = False).head(size)['item_id'].values)
    worst_movie_ids = list(result[(result['count']>threshold)].sort_values(by = ['mean']).head(size)['item_id'].values)

    return best_movie_ids, worst_movie_ids
    
    pass
# local test
def test_best_worst_movies():
    best_movie_ids, worst_movie_ids = best_worst_movies(ratings)
    assert isinstance(best_movie_ids, list) and isinstance(worst_movie_ids, list)
    assert len(best_movie_ids) == len(worst_movie_ids) == 10
    assert best_movie_ids[0] == 408
    assert worst_movie_ids[0] == 931
    print(""All tests passed!"")

test_best_worst_movies()"
18597,1,"    user_movie_ratings = ratings.groupby([""user_id"", ""item_id""]).agg(rating = (""rating"", ""max""))
    matrix = user_movie_ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").values",TODO,user_movie_ratings,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    user_movie_ratings = ratings.groupby([""user_id"", ""item_id""]).agg(rating = (""rating"", ""max""))
    matrix = user_movie_ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").values
    return (np.nan_to_num(matrix, nan = 0).astype(int), np.nanmean(matrix, axis = 1), np.nanmean(matrix, axis = 0))","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    user_movie_ratings = ratings.groupby([""user_id"", ""item_id""]).agg(rating = (""rating"", ""max""))
    matrix = user_movie_ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").values
    return (np.nan_to_num(matrix, nan = 0).astype(int), np.nanmean(matrix, axis = 1), np.nanmean(matrix, axis = 0))
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
18597,2,"    matrix = user_movie_ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").values
    return (np.nan_to_num(matrix, nan = 0).astype(int), np.nanmean(matrix, axis = 1), np.nanmean(matrix, axis = 0))",TODO,matrix,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    user_movie_ratings = ratings.groupby([""user_id"", ""item_id""]).agg(rating = (""rating"", ""max""))
    matrix = user_movie_ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").values
    return (np.nan_to_num(matrix, nan = 0).astype(int), np.nanmean(matrix, axis = 1), np.nanmean(matrix, axis = 0))","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    user_movie_ratings = ratings.groupby([""user_id"", ""item_id""]).agg(rating = (""rating"", ""max""))
    matrix = user_movie_ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"").values
    return (np.nan_to_num(matrix, nan = 0).astype(int), np.nanmean(matrix, axis = 1), np.nanmean(matrix, axis = 0))
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
23360,1,"    b = ratings.groupby(['user_id','item_id']).agg({'rating':'max'}).unstack().fillna(0).to_numpy().astype('int64')
    
    return(b,(b.sum(axis = 1)/(b>0).sum(axis = 1)),(b.sum(axis = 0)/(b>0).sum(axis = 0)))",TODO,b,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    
    b = ratings.groupby(['user_id','item_id']).agg({'rating':'max'}).unstack().fillna(0).to_numpy().astype('int64')
    
    return(b,(b.sum(axis = 1)/(b>0).sum(axis = 1)),(b.sum(axis = 0)/(b>0).sum(axis = 0)))
    ","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    
    b = ratings.groupby(['user_id','item_id']).agg({'rating':'max'}).unstack().fillna(0).to_numpy().astype('int64')
    
    return(b,(b.sum(axis = 1)/(b>0).sum(axis = 1)),(b.sum(axis = 0)/(b>0).sum(axis = 0)))
    
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
23098,1,"    X = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"", aggfunc='max').to_numpy()
    user_means = np.nanmean(X, axis=1)
    movie_means = np.nanmean(X, axis=0)
    np.nan_to_num(X, copy=False)
    X = X.astype(np.int64)
    return (X, user_means, movie_means)",TODO,X,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    X = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"", aggfunc='max').to_numpy()
    user_means = np.nanmean(X, axis=1)
    movie_means = np.nanmean(X, axis=0)
    np.nan_to_num(X, copy=False)
    X = X.astype(np.int64)
    return (X, user_means, movie_means)","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    X = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"", aggfunc='max').to_numpy()
    user_means = np.nanmean(X, axis=1)
    movie_means = np.nanmean(X, axis=0)
    np.nan_to_num(X, copy=False)
    X = X.astype(np.int64)
    return (X, user_means, movie_means)
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    # print(movie_means[0])
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
23098,2,"    user_means = np.nanmean(X, axis=1)
    movie_means = np.nanmean(X, axis=0)
    np.nan_to_num(X, copy=False)
    X = X.astype(np.int64)
    return (X, user_means, movie_means)",TODO,user_means,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    X = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"", aggfunc='max').to_numpy()
    user_means = np.nanmean(X, axis=1)
    movie_means = np.nanmean(X, axis=0)
    np.nan_to_num(X, copy=False)
    X = X.astype(np.int64)
    return (X, user_means, movie_means)","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    X = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"", aggfunc='max').to_numpy()
    user_means = np.nanmean(X, axis=1)
    movie_means = np.nanmean(X, axis=0)
    np.nan_to_num(X, copy=False)
    X = X.astype(np.int64)
    return (X, user_means, movie_means)
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    # print(movie_means[0])
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
23098,3,"    movie_means = np.nanmean(X, axis=0)
    np.nan_to_num(X, copy=False)
    X = X.astype(np.int64)
    return (X, user_means, movie_means)",TODO,movie_means,"def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    X = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"", aggfunc='max').to_numpy()
    user_means = np.nanmean(X, axis=1)
    movie_means = np.nanmean(X, axis=0)
    np.nan_to_num(X, copy=False)
    X = X.astype(np.int64)
    return (X, user_means, movie_means)","def matrix_data(ratings):
    """"""
    Represent the user-movie ratings data in a matrix format

    args:
        ratings (pd.DataFrame)  : raw ratings data represented in a pandas DataFrame

    return :
        Tuple[X, user_means, movie_means]
            X (np.array[num_users, num_movies]) : the actual ratings matrix
            user_means (np.array[num_users, ])  : mean user rating array over the observed ratings
            movie_means (np.array[num_movies, ])  : mean movie rating array over the obsevered ratings
    """"""
    X = ratings.pivot_table(index = ""user_id"", columns = ""item_id"", values = ""rating"", aggfunc='max').to_numpy()
    user_means = np.nanmean(X, axis=1)
    movie_means = np.nanmean(X, axis=0)
    np.nan_to_num(X, copy=False)
    X = X.astype(np.int64)
    return (X, user_means, movie_means)
# local test
def test_matrix_data():
    X, user_means, movie_means = matrix_data(ratings)
    assert X.shape == (943, 1682)
    assert X.dtype == np.int64
    assert user_means.shape == (943,)
    assert movie_means.shape == (1682,)
    assert np.array_equal(X[5,:10], np.array([4, 0, 0, 0, 0, 0, 2, 4, 4, 0]))
    # print(movie_means[0])
    assert np.allclose(movie_means[0], 3.8783185840707963)
    assert np.allclose(user_means[0], 3.610294117647059)
    print(""All tests passed!"")

test_matrix_data()"
23126,1,"    num = X @ X.T
    root_sum_square = np.sqrt((X**2).sum(axis = 1))
    denom = np.outer(root_sum_square, root_sum_square)
    
    return num/denom",TODO,num,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""

    num = X @ X.T
    root_sum_square = np.sqrt((X**2).sum(axis = 1))
    denom = np.outer(root_sum_square, root_sum_square)
    
    return num/denom","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""

    num = X @ X.T
    root_sum_square = np.sqrt((X**2).sum(axis = 1))
    denom = np.outer(root_sum_square, root_sum_square)
    
    return num/denom
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
23126,2,"    root_sum_square = np.sqrt((X**2).sum(axis = 1))
    denom = np.outer(root_sum_square, root_sum_square)",TODO,root_sum_square,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""

    num = X @ X.T
    root_sum_square = np.sqrt((X**2).sum(axis = 1))
    denom = np.outer(root_sum_square, root_sum_square)
    
    return num/denom","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""

    num = X @ X.T
    root_sum_square = np.sqrt((X**2).sum(axis = 1))
    denom = np.outer(root_sum_square, root_sum_square)
    
    return num/denom
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
23126,3,"    denom = np.outer(root_sum_square, root_sum_square)
    
    return num/denom",TODO,denom,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""

    num = X @ X.T
    root_sum_square = np.sqrt((X**2).sum(axis = 1))
    denom = np.outer(root_sum_square, root_sum_square)
    
    return num/denom","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""

    num = X @ X.T
    root_sum_square = np.sqrt((X**2).sum(axis = 1))
    denom = np.outer(root_sum_square, root_sum_square)
    
    return num/denom
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
22055,1,"    norm_X = np.sqrt(np.sum(X**2, axis=1)).reshape(-1, 1)
    W = X @ X.T / np.outer(norm_X, norm_X)",TODO,norm_X,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    norm_X = np.sqrt(np.sum(X**2, axis=1)).reshape(-1, 1)
    W = X @ X.T / np.outer(norm_X, norm_X)
    return W
    ","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    norm_X = np.sqrt(np.sum(X**2, axis=1)).reshape(-1, 1)
    W = X @ X.T / np.outer(norm_X, norm_X)
    return W
    
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
22055,2,"    W = X @ X.T / np.outer(norm_X, norm_X)
    return W",TODO,W,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    norm_X = np.sqrt(np.sum(X**2, axis=1)).reshape(-1, 1)
    W = X @ X.T / np.outer(norm_X, norm_X)
    return W
    ","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    norm_X = np.sqrt(np.sum(X**2, axis=1)).reshape(-1, 1)
    W = X @ X.T / np.outer(norm_X, norm_X)
    return W
    
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
18346,1,"    p = X / np.linalg.norm(X, 2, axis=1).reshape(-1,1)
    cosine_sim = np.dot(p, p.T)",TODO,p,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    # cosine_sim = np.dot(X, X.T)/(np.linalg.norm(X, axis = 1) * np.linalg.norm(X, axis = 1))
    p = X / np.linalg.norm(X, 2, axis=1).reshape(-1,1)
    cosine_sim = np.dot(p, p.T)
    return cosine_sim","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    # cosine_sim = np.dot(X, X.T)/(np.linalg.norm(X, axis = 1) * np.linalg.norm(X, axis = 1))
    p = X / np.linalg.norm(X, 2, axis=1).reshape(-1,1)
    cosine_sim = np.dot(p, p.T)
    return cosine_sim
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()
# note: this cell has been tagged with excluded_from_script.
# you do not need to remove it when submitting your notebook.
# print(""Ratings data"")
# display(ratings.head())

# print(""Movie data"")
# display(movies.head())
X = np.arange(20).reshape([4,5])
display(X)

M = np.where(X % 3 == 0, 1, 0)
display(M)"
18346,2,"    cosine_sim = np.dot(p, p.T)
    return cosine_sim",TODO,cosine_sim,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    # cosine_sim = np.dot(X, X.T)/(np.linalg.norm(X, axis = 1) * np.linalg.norm(X, axis = 1))
    p = X / np.linalg.norm(X, 2, axis=1).reshape(-1,1)
    cosine_sim = np.dot(p, p.T)
    return cosine_sim","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    # cosine_sim = np.dot(X, X.T)/(np.linalg.norm(X, axis = 1) * np.linalg.norm(X, axis = 1))
    p = X / np.linalg.norm(X, 2, axis=1).reshape(-1,1)
    cosine_sim = np.dot(p, p.T)
    return cosine_sim
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()
# note: this cell has been tagged with excluded_from_script.
# you do not need to remove it when submitting your notebook.
# print(""Ratings data"")
# display(ratings.head())

# print(""Movie data"")
# display(movies.head())
X = np.arange(20).reshape([4,5])
display(X)

M = np.where(X % 3 == 0, 1, 0)
display(M)"
16870,1,"    denom = np.sum(X**2, axis = 1)[:,None]
    sqr = np.sqrt(denom)",TODO,denom,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    denom = np.sum(X**2, axis = 1)[:,None]
    sqr = np.sqrt(denom)

    num = np.dot(X,X.T)
    den = sqr * sqr.T

    result = num/den
    return result","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    denom = np.sum(X**2, axis = 1)[:,None]
    sqr = np.sqrt(denom)

    num = np.dot(X,X.T)
    den = sqr * sqr.T

    result = num/den
    return result
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
16870,2,"    sqr = np.sqrt(denom)

    num = np.dot(X,X.T)
    den = sqr * sqr.T",TODO,sqr,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    denom = np.sum(X**2, axis = 1)[:,None]
    sqr = np.sqrt(denom)

    num = np.dot(X,X.T)
    den = sqr * sqr.T

    result = num/den
    return result","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    denom = np.sum(X**2, axis = 1)[:,None]
    sqr = np.sqrt(denom)

    num = np.dot(X,X.T)
    den = sqr * sqr.T

    result = num/den
    return result
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
16870,3,"    num = np.dot(X,X.T)
    den = sqr * sqr.T

    result = num/den",TODO,num,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    denom = np.sum(X**2, axis = 1)[:,None]
    sqr = np.sqrt(denom)

    num = np.dot(X,X.T)
    den = sqr * sqr.T

    result = num/den
    return result","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    denom = np.sum(X**2, axis = 1)[:,None]
    sqr = np.sqrt(denom)

    num = np.dot(X,X.T)
    den = sqr * sqr.T

    result = num/den
    return result
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
16870,4,"    den = sqr * sqr.T

    result = num/den",TODO,den,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    denom = np.sum(X**2, axis = 1)[:,None]
    sqr = np.sqrt(denom)

    num = np.dot(X,X.T)
    den = sqr * sqr.T

    result = num/den
    return result","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    denom = np.sum(X**2, axis = 1)[:,None]
    sqr = np.sqrt(denom)

    num = np.dot(X,X.T)
    den = sqr * sqr.T

    result = num/den
    return result
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
16870,5,"    result = num/den
    return result",TODO,result,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    denom = np.sum(X**2, axis = 1)[:,None]
    sqr = np.sqrt(denom)

    num = np.dot(X,X.T)
    den = sqr * sqr.T

    result = num/den
    return result","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    denom = np.sum(X**2, axis = 1)[:,None]
    sqr = np.sqrt(denom)

    num = np.dot(X,X.T)
    den = sqr * sqr.T

    result = num/den
    return result
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
1618,1,"    dot_product = np.dot(X, X.T)
    normalized_matrix = np.sqrt(np.sum(np.square(X), axis=1))    
    cos_sim = dot_product / np.dot(normalized_matrix.reshape(-1, 1), normalized_matrix.reshape(-1, 1).T)",TODO,dot_product,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    
#     #print(X.shape)
#     dim = X.shape
#     m = dim[0]
#     n = dim[1]
#     # calculate the dot product of the ratings matrix
#     dot_product = np.dot(X, X.T)
    
    dot_product = np.dot(X, X.T)
    normalized_matrix = np.sqrt(np.sum(np.square(X), axis=1))    
    cos_sim = dot_product / np.dot(normalized_matrix.reshape(-1, 1), normalized_matrix.reshape(-1, 1).T)
    return cos_sim    

    
    pass ","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    
#     #print(X.shape)
#     dim = X.shape
#     m = dim[0]
#     n = dim[1]
#     # calculate the dot product of the ratings matrix
#     dot_product = np.dot(X, X.T)
    
    dot_product = np.dot(X, X.T)
    normalized_matrix = np.sqrt(np.sum(np.square(X), axis=1))    
    cos_sim = dot_product / np.dot(normalized_matrix.reshape(-1, 1), normalized_matrix.reshape(-1, 1).T)
    return cos_sim    

    
    pass 
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
1618,2,"    normalized_matrix = np.sqrt(np.sum(np.square(X), axis=1))    
    cos_sim = dot_product / np.dot(normalized_matrix.reshape(-1, 1), normalized_matrix.reshape(-1, 1).T)",TODO,normalized_matrix,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    
#     #print(X.shape)
#     dim = X.shape
#     m = dim[0]
#     n = dim[1]
#     # calculate the dot product of the ratings matrix
#     dot_product = np.dot(X, X.T)
    
    dot_product = np.dot(X, X.T)
    normalized_matrix = np.sqrt(np.sum(np.square(X), axis=1))    
    cos_sim = dot_product / np.dot(normalized_matrix.reshape(-1, 1), normalized_matrix.reshape(-1, 1).T)
    return cos_sim    

    
    pass ","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    
#     #print(X.shape)
#     dim = X.shape
#     m = dim[0]
#     n = dim[1]
#     # calculate the dot product of the ratings matrix
#     dot_product = np.dot(X, X.T)
    
    dot_product = np.dot(X, X.T)
    normalized_matrix = np.sqrt(np.sum(np.square(X), axis=1))    
    cos_sim = dot_product / np.dot(normalized_matrix.reshape(-1, 1), normalized_matrix.reshape(-1, 1).T)
    return cos_sim    

    
    pass 
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
1618,3,"    cos_sim = dot_product / np.dot(normalized_matrix.reshape(-1, 1), normalized_matrix.reshape(-1, 1).T)
    return cos_sim    ",TODO,cos_sim,"def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    
#     #print(X.shape)
#     dim = X.shape
#     m = dim[0]
#     n = dim[1]
#     # calculate the dot product of the ratings matrix
#     dot_product = np.dot(X, X.T)
    
    dot_product = np.dot(X, X.T)
    normalized_matrix = np.sqrt(np.sum(np.square(X), axis=1))    
    cos_sim = dot_product / np.dot(normalized_matrix.reshape(-1, 1), normalized_matrix.reshape(-1, 1).T)
    return cos_sim    

    
    pass ","def cosine_similarity(X):
    """"""
        Compute the user-user cosine-similarity matrix.

    args: 
        X (np.array[num_users, num_movies]) : the user-movie rating matrix

    return:
        np.array[num_users, num_users]  : the cosine-similarity matrix
    """"""
    
#     #print(X.shape)
#     dim = X.shape
#     m = dim[0]
#     n = dim[1]
#     # calculate the dot product of the ratings matrix
#     dot_product = np.dot(X, X.T)
    
    dot_product = np.dot(X, X.T)
    normalized_matrix = np.sqrt(np.sum(np.square(X), axis=1))    
    cos_sim = dot_product / np.dot(normalized_matrix.reshape(-1, 1), normalized_matrix.reshape(-1, 1).T)
    return cos_sim    

    
    pass 
# local test
def test_cosine_similarity():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (3, 3)
    assert np.allclose(W_user_cosine[0, 2], 0.8320502943378437)

    X = matrix_data(ratings)[0]
    W_user_cosine = cosine_similarity(X)
    assert W_user_cosine.shape == (943, 943)
    assert np.allclose(W_user_cosine[20, 37], 0.16907916618206115)
    assert np.allclose(W_user_cosine[100, 101], 0.1537501042084208)
    print(""All tests passed!"")

test_cosine_similarity()"
20750,1,"    diff_sqr = (X_true != 0) * (X_true - X_pred) ** 2
    return diff_sqr[X_true != 0].sum() / (X_true != 0).sum()",TODO,diff_sqr,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.

    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    diff_sqr = (X_true != 0) * (X_true - X_pred) ** 2
    return diff_sqr[X_true != 0].sum() / (X_true != 0).sum()","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.

    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    diff_sqr = (X_true != 0) * (X_true - X_pred) ** 2
    return diff_sqr[X_true != 0].sum() / (X_true != 0).sum()
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)

    print(""All tests passed!"")


test_mean_rating_diff()"
16672,1,"    X_bool = (X_true!=0).astype(int)
    return np.sum(X_bool*((X_true - X_pred)**2)) / np.sum(X_bool)",TODO,X_bool,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    X_bool = (X_true!=0).astype(int)
    return np.sum(X_bool*((X_true - X_pred)**2)) / np.sum(X_bool)","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    X_bool = (X_true!=0).astype(int)
    return np.sum(X_bool*((X_true - X_pred)**2)) / np.sum(X_bool)
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
2492,1,"    mask = X_true != 0
    X_pred_masked = np.multiply(X_pred, mask)",TODO,mask,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    mask = X_true != 0
    X_pred_masked = np.multiply(X_pred, mask)
    return np.sum(np.square(X_true - X_pred_masked)) / np.count_nonzero(X_true)","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    mask = X_true != 0
    X_pred_masked = np.multiply(X_pred, mask)
    return np.sum(np.square(X_true - X_pred_masked)) / np.count_nonzero(X_true)
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
2492,2,"    X_pred_masked = np.multiply(X_pred, mask)
    return np.sum(np.square(X_true - X_pred_masked)) / np.count_nonzero(X_true)",TODO,X_pred_masked,"def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    mask = X_true != 0
    X_pred_masked = np.multiply(X_pred, mask)
    return np.sum(np.square(X_true - X_pred_masked)) / np.count_nonzero(X_true)","def mean_rating_diff(X_true, X_pred):
    """"""
    Return the mean squared difference between the predicted and the actual ratings matrix.
    
    args: 
        X_true (np.array[num_users, num_movies]) : the actual ratings matrix, where missing entries are 0
        X_pred (np.array[num_users, num_movies]) : the predicted ratings matrix

    return
        np.float64 : mean squared difference between the predicted and the actual ratings matrix
    """"""
    mask = X_true != 0
    X_pred_masked = np.multiply(X_pred, mask)
    return np.sum(np.square(X_true - X_pred_masked)) / np.count_nonzero(X_true)
# local test
def test_mean_rating_diff():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    user_means = np.array([3, 1, 2.5])
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64), type(diff)
    assert np.allclose(diff, 0.035175498983549575)

    X, user_means, _ = matrix_data(ratings)
    W_user_cosine = cosine_similarity(X)
    X_pred = predict_user_user(X, W_user_cosine, user_means)
    diff = mean_rating_diff(X, X_pred)
    assert isinstance(diff, np.float64)
    assert np.allclose(diff, 0.7992514466097069)
    
    print(""All tests passed!"")

test_mean_rating_diff()"
19545,1,"    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):",TODO,m,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,2,"    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):",TODO,n,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,3,"    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())",TODO,U,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,4,"    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())",TODO,V,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,5,"    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()",TODO,X_csr,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,6,"    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()",TODO,X_csc,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,7,"            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) ",TODO,Xcol,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,8,"            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T",TODO,Xcol_mask,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,9,"            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) ",TODO,U1,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,10,"            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) ",TODO,A,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,11,"            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) ",TODO,B,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,12,"            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)",TODO,a,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,13,"            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)",TODO,b,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,14,"            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) ",TODO,I,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,15,"            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) ",TODO,Xrow,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,16,"            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T",TODO,Xrow_mask,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
19545,17,"            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) ",TODO,V1,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  ","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    m,n=X_sparse.shape 
    
    U=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(m,k)))
    V=sp.csr_matrix(np.random.normal(loc=0,scale=0.1,size=(n,k)))

    X_csr=X_sparse.tocsr()
    X_csc=X_sparse.tocsc()
    for iter in range(niters): 
        for i in range(n):
            Xcol=X_csc.getcol(i).T.toarray()
            Xcol_mask=Xcol.copy()
            Xcol_mask[Xcol_mask!=0]=1
            #print(U)
            #print(Xcol_mask.T)
            U1=U.A*Xcol_mask.T
            #print(U1) 
            A=U1.T.dot(U1)
            #print(A.shape)
            B=U1.T.dot(Xcol.T) 
            a,b=A.shape
            I=np.eye(a,b)
            V[i]=la.solve(A + lam*I, B)

        for j in range(m):
            Xrow=X_csr.getrow(j).toarray()
            Xrow_mask=Xrow.copy() 
            Xrow_mask[Xrow_mask!=0]=1
            #print(V.shape)
            #print(pref.T.shape)
            V1=V.A*Xrow_mask.T
            #print(V1.shape)
            A=V1.T.dot(V1)
            #print(V1.shape)
            #print(Xrow.shape)
            B=V1.T.dot(Xrow.T) 
            a,b=A.shape
            I=np.eye(a,b)
            U[j]=la.solve(A + lam*I, B) 

    return (U.todense(), V.T.todense())  
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
21760,1,"    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))",TODO,m,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")


test_low_rank_matrix_factorization()"
21760,2,"    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))",TODO,n,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")


test_low_rank_matrix_factorization()"
21760,3,"    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V",TODO,matrix_U,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")


test_low_rank_matrix_factorization()"
21760,4,"    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V",TODO,matrix_V,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")


test_low_rank_matrix_factorization()"
21760,5,"    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)",TODO,I,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")


test_low_rank_matrix_factorization()"
21760,6,"        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,",TODO,sum_u_X,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")


test_low_rank_matrix_factorization()"
21760,7,"        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)",TODO,sum_u_uT,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")


test_low_rank_matrix_factorization()"
21760,8,"        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,",TODO,inv_sum_u_uT,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")


test_low_rank_matrix_factorization()"
21760,9,"        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,",TODO,sum_vT_X,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")


test_low_rank_matrix_factorization()"
21760,10,"        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)",TODO,sum_v_vT,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")


test_low_rank_matrix_factorization()"
21760,11,"        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,",TODO,inv_sum_v_vT,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed=0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)

    m, n = X_sparse.shape
    matrix_U = np.random.normal(loc=0, scale=0.1, size=(m, k))
    matrix_V = np.random.normal(loc=0, scale=0.1, size=(k, n))
    I = np.identity(k)
    for iter in range(niters):
        # Update the columns v_j's of V based on the most recent U
        sum_u_X = X_sparse.T.dot(matrix_U).T  # (n, m) @ (m, k) .T -> (k, n)
        sum_u_uT = np.einsum('ij, ik, li -> jkl',
                             (X_sparse != 0).A,
                             matrix_U,
                             matrix_U.T)  # (m, n), (m, k), (k, m) -> (n, k, k)
        inv_sum_u_uT = np.linalg.inv(sum_u_uT + lam * I)
        matrix_V = np.diagonal(inv_sum_u_uT @ sum_u_X,
                               axis1=0,
                               axis2=2)
        # Update the rows u_i's of U based on the most recent V
        sum_vT_X = X_sparse.dot(matrix_V.T)  # (m, n) @ (k, n).T -> (m, k)
        sum_v_vT = np.einsum('ij, kj, jl -> ikl',
                             (X_sparse != 0).A,
                             matrix_V,
                             matrix_V.T)  # (m, n), (k, n), (n, k) -> (m, k, k)
        inv_sum_v_vT = np.linalg.inv(sum_v_vT + lam * I)
        matrix_U = np.diagonal(inv_sum_v_vT @ sum_vT_X.T,
                               axis1=0,
                               axis2=2).T
    return matrix_U, matrix_V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")


test_low_rank_matrix_factorization()"
15637,1,"    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)",TODO,U,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

def my_test():
    X_check = sp.coo_matrix([[0., 3.], [0., 1.], [2., 3.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

def my_test_2():
    X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

test_low_rank_matrix_factorization()"
15637,2,"    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)",TODO,V,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

def my_test():
    X_check = sp.coo_matrix([[0., 3.], [0., 1.], [2., 3.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

def my_test_2():
    X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

test_low_rank_matrix_factorization()"
15637,3,"    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI",TODO,lamI,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

def my_test():
    X_check = sp.coo_matrix([[0., 3.], [0., 1.], [2., 3.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

def my_test_2():
    X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

test_low_rank_matrix_factorization()"
15637,4,"    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]",TODO,csc,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

def my_test():
    X_check = sp.coo_matrix([[0., 3.], [0., 1.], [2., 3.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

def my_test_2():
    X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

test_low_rank_matrix_factorization()"
15637,5,"    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]",TODO,csr,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

def my_test():
    X_check = sp.coo_matrix([[0., 3.], [0., 1.], [2., 3.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

def my_test_2():
    X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

test_low_rank_matrix_factorization()"
15637,6,"            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) ",TODO,col,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

def my_test():
    X_check = sp.coo_matrix([[0., 3.], [0., 1.], [2., 3.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

def my_test_2():
    X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

test_low_rank_matrix_factorization()"
15637,7,"            U1 = U * col

            firstTerm = U1.T @ U1",TODO,U1,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

def my_test():
    X_check = sp.coo_matrix([[0., 3.], [0., 1.], [2., 3.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

def my_test_2():
    X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

test_low_rank_matrix_factorization()"
15637,8,"            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)",TODO,firstTerm,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

def my_test():
    X_check = sp.coo_matrix([[0., 3.], [0., 1.], [2., 3.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

def my_test_2():
    X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

test_low_rank_matrix_factorization()"
15637,9,"            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)",TODO,secondTerm,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

def my_test():
    X_check = sp.coo_matrix([[0., 3.], [0., 1.], [2., 3.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

def my_test_2():
    X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

test_low_rank_matrix_factorization()"
15637,10,"            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec",TODO,resultVec,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

def my_test():
    X_check = sp.coo_matrix([[0., 3.], [0., 1.], [2., 3.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

def my_test_2():
    X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

test_low_rank_matrix_factorization()"
15637,11,"            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) ",TODO,row,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

def my_test():
    X_check = sp.coo_matrix([[0., 3.], [0., 1.], [2., 3.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

def my_test_2():
    X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

test_low_rank_matrix_factorization()"
15637,12,"            V1 = V * row

            firstTerm = V1 @ V1.T",TODO,V1,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    
    # your implementation here

    U = np.random.normal(loc=0, scale=0.1, size=(X_sparse.shape[0], k))
    V = np.random.normal(loc=0, scale=0.1, size=(k, X_sparse.shape[1]))

    lamI = np.eye(k)*lam
    
    csc = X_sparse.tocsc()
    csr = X_sparse.tocsr()

    for i in range(niters):

        for j in range(V.shape[1]): 

            col = csc.getcol(j).A
            col = col != 0

            U1 = U * col

            firstTerm = U1.T @ U1

            firstTerm = firstTerm + lamI

            col = csc.getcol(j).A.ravel()[:, None]
            col = col + np.zeros((col.shape[0], k))

            secondTerm = np.sum(U*col, axis = 0) 

            resultVec = la.solve(firstTerm, secondTerm)
            V[:, j] = resultVec


        for j in range(U.shape[0]):
            row = csr.getrow(j).A
            row = row != 0

            V1 = V * row

            firstTerm = V1 @ V1.T

            firstTerm = firstTerm + lamI
            
            row = csr.getrow(j).A.ravel()[None, :]
            row = row + np.zeros((k, row.shape[1]))

            secondTerm = np.sum(V*row, axis = 1) 

            resultVec = la.solve(firstTerm, secondTerm)
            U[j, :] = resultVec

        # print(U)

    return (U, V)
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

def my_test():
    X_check = sp.coo_matrix([[0., 3.], [0., 1.], [2., 3.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

def my_test_2():
    X_check = sp.coo_matrix([[0., 0., 1.], [3., 1., 0.], [0., 0., 3.], [4., 0., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)

test_low_rank_matrix_factorization()"
2673,1,"    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]",TODO,X,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,2,"    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):",TODO,m,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,3,"    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):",TODO,n,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,4,"    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]",TODO,X_bi,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,5,"    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))",TODO,loc,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,6,"    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))",TODO,scale,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,7,"    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V",TODO,U,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,8,"    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V",TODO,V,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,9,"            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))",TODO,X_bi_col,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,10,"            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U",TODO,X_bi_col_broadcast,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,11,"            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)",TODO,U_bi,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,12,"            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum",TODO,outer_sum,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,13,"            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)",TODO,A,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,14,"            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)",TODO,X_col,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,15,"            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)",TODO,B,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,16,"            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))",TODO,X_bi_row,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,17,"            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V",TODO,X_bi_row_broadcast,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,18,"            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)",TODO,V_bi_T,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2673,19,"            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)",TODO,X_row,"def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V","def low_rank_matrix_factorization(X_sparse, k, niters=5, lam=10., seed = 0):
    """"""
    Factor a rating matrix into user-features and movie-features.

    args: 
        X_sparse (sp.coo_matrix[num_users, num_movies]) : the ratings matrix, assumed sparse in COO format
        k (int) : the number of features in the lower-rank matrices U and V
        niters (int) : number of iterations to run
        lam (float) : regularization parameter, shown as lambda
        seed (int) : the seed for numpy random generator

    return : Tuple(U, V)
        U : np.array[num_users,  k] -- the user-feature matrix
        V : np.array[k, num_movies] -- the movie-feature matrix
    """"""
    # do not modify this line
    np.random.seed(seed)
    X = X_sparse.A
    m, n = X_sparse.shape
    
    # generate a n
    X_bi = X_sparse != 0
    X_bi = X_bi.A
    
    # initialize U, V
    loc, scale = 0, 0.1
    U = np.random.normal(loc, scale, (m, k))
    V = np.random.normal(loc, scale, (k, n))
    
    # update U, V
    for i in range(niters):
        # update V
        for col in range(n):
            X_bi_col = X_bi[:, col]
            X_bi_col_broadcast = np.tile(X_bi_col, (k, 1))
            U_bi = X_bi_col_broadcast.T*U
            
            # A
            outer_sum = sum(np.outer(row, row) for row in U_bi)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_col = X[:, col]
            B = np.dot(U_bi.T, X_col)
            
            V[:, col] = la.solve(A, B)
        
        # update U
        for row in range(m):
            X_bi_row = X_bi[row, :]
            X_bi_row_broadcast = np.tile(X_bi_row, (k, 1))
            V_bi_T = X_bi_row_broadcast * V
            
            # A
            outer_sum = sum(np.outer(row, row) for row in V_bi_T.T)
            A = lam*np.eye(k) + outer_sum
            
            # B
            X_row = X[row, :]
            B = np.dot(V_bi_T, X_row)
            
            U[row, :] = la.solve(A, B)
        
    # your implementation here
    return U, V
# local test
def test_low_rank_matrix_factorization():
    X_check = sp.coo_matrix([[5., 0., 0.], [1., 1., 0.]])
    U_check, V_check = low_rank_matrix_factorization(X_check, 3)
    assert np.allclose([
        [2.52975137e-04, 8.88862671e-05, 8.88927380e-05],
        [5.26156164e-05, 1.84872240e-05, 1.84885459e-05],
    ], U_check), U_check
    assert np.allclose([
        [5.05950291e-04, 1.77772540e-04, 1.77785482e-04],
        [2.02058899e-05, 7.09970620e-06, 7.09998333e-06],
        [0, 0, 0]
    ], V_check.T), V_check.T
    print(""All tests passed!"")

test_low_rank_matrix_factorization()"
2731,1,"    common_div = min(a,b)
    for k in range(1, common_div+1):
        if a % k == b % k and a % k == 0:
            common_div = k 
    if common_div != 0:
        return common_div",TODO,common_div,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    common_div = min(a,b)
    for k in range(1, common_div+1):
        if a % k == b % k and a % k == 0:
            common_div = k 
    if common_div != 0:
        return common_div
    else:
        return max(a,b)","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    common_div = min(a,b)
    for k in range(1, common_div+1):
        if a % k == b % k and a % k == 0:
            common_div = k 
    if common_div != 0:
        return common_div
    else:
        return max(a,b)"
2706,1,"    gcd = 1
    minm = min(a,b)
    for i in range(1,minm+1):
        if a % i == 0 and b % i == 0:
            gcd = i

    return gcd",TODO,gcd,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a == 0:
        return b
    if b == 0:
        return a
    gcd = 1
    minm = min(a,b)
    for i in range(1,minm+1):
        if a % i == 0 and b % i == 0:
            gcd = i

    return gcd","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a == 0:
        return b
    if b == 0:
        return a
    gcd = 1
    minm = min(a,b)
    for i in range(1,minm+1):
        if a % i == 0 and b % i == 0:
            gcd = i

    return gcd"
2706,2,"    minm = min(a,b)
    for i in range(1,minm+1):",TODO,minm,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a == 0:
        return b
    if b == 0:
        return a
    gcd = 1
    minm = min(a,b)
    for i in range(1,minm+1):
        if a % i == 0 and b % i == 0:
            gcd = i

    return gcd","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a == 0:
        return b
    if b == 0:
        return a
    gcd = 1
    minm = min(a,b)
    for i in range(1,minm+1):
        if a % i == 0 and b % i == 0:
            gcd = i

    return gcd"
13454,1,"        a, b = b, a % b
    return a",TODO,a,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while b != 0:
        a, b = b, a % b
    return a","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while b != 0:
        a, b = b, a % b
    return a"
13454,2,"    while b != 0:
        a, b = b, a % b",TODO,b,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while b != 0:
        a, b = b, a % b
    return a","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while b != 0:
        a, b = b, a % b
    return a"
13429,1,"        a, b = b, a % b
    return a",TODO,a,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while b:
        a, b = b, a % b
    return a
    pass","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while b:
        a, b = b, a % b
    return a
    pass"
13429,2,"    while b:
        a, b = b, a % b",TODO,b,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while b:
        a, b = b, a % b
    return a
    pass","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    while b:
        a, b = b, a % b
    return a
    pass"
13456,1,"    gcd = 1
    for divisor in range(2,smaller+1):
      if (a%divisor == 0 and b%divisor == 0 and divisor>gcd):
        gcd = divisor
    return gcd",TODO,gcd,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if (a == 0):
      return b
    if (b == 0):
      return a
    if (a > b):
      smaller = b
    else:
      smaller = a
    gcd = 1
    for divisor in range(2,smaller+1):
      if (a%divisor == 0 and b%divisor == 0 and divisor>gcd):
        gcd = divisor
    return gcd","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if (a == 0):
      return b
    if (b == 0):
      return a
    if (a > b):
      smaller = b
    else:
      smaller = a
    gcd = 1
    for divisor in range(2,smaller+1):
      if (a%divisor == 0 and b%divisor == 0 and divisor>gcd):
        gcd = divisor
    return gcd"
13456,2,"      smaller = b
    else:
      smaller = a
    gcd = 1
    for divisor in range(2,smaller+1):",TODO,smaller,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if (a == 0):
      return b
    if (b == 0):
      return a
    if (a > b):
      smaller = b
    else:
      smaller = a
    gcd = 1
    for divisor in range(2,smaller+1):
      if (a%divisor == 0 and b%divisor == 0 and divisor>gcd):
        gcd = divisor
    return gcd","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if (a == 0):
      return b
    if (b == 0):
      return a
    if (a > b):
      smaller = b
    else:
      smaller = a
    gcd = 1
    for divisor in range(2,smaller+1):
      if (a%divisor == 0 and b%divisor == 0 and divisor>gcd):
        gcd = divisor
    return gcd"
13427,1,"      c = a%b
      if c == 0:
        return b
      return gcd(b,c)
    else:
      c = b%a
      if c == 0:
        return a
      return gcd(a,c)",TODO,c,"def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a == 0:
      return b
    if b == 0:
      return a
    if a > b:
      c = a%b
      if c == 0:
        return b
      return gcd(b,c)
    else:
      c = b%a
      if c == 0:
        return a
      return gcd(a,c)
    ","def gcd(a, b):
    """"""
    Compute the greatest common divisor of two integers.
    
    args:
        a (int) : the first integer
        b (int) : the second integer
    
    return:
        int : the greatest common divisor of a and b.
    """"""
    if a == 0:
      return b
    if b == 0:
      return a
    if a > b:
      c = a%b
      if c == 0:
        return b
      return gcd(b,c)
    else:
      c = b%a
      if c == 0:
        return a
      return gcd(a,c)
    "
8965,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8965,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8965,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8965,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8965,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8965,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8965,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8965,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8965,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8965,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8965,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8965,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8965,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8965,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8965,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8965,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
10050,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12783,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
4083,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8818,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13105,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
9756,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
12692,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
8881,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,1,"    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]",TODO,data,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,2,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model",TODO,X,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,3,"    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model",TODO,y,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,4,"    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w",TODO,w,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,5,"    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N",TODO,total_time,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,6,"    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)",TODO,y_predicted,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,7,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,X_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,8,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,X_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,9,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)",TODO,y_train,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,10,"    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,y_test,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,11,"    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)",TODO,model,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,12,"    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,memory_usage,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,13,"    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_runtime,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,14,"    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error",TODO,prediction_error,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,15,"        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,start,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
3999,16,"        end = datetime.now()
        total_time += (end - start).total_seconds()",TODO,end,"def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error","def construct_dataset():
    """"""
    Generate the input matrix data (X) and output label vector (y) for a regression problem
    See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html for more details
    """"""
    print(""I am now executing construct_dataset"")
    data = make_regression(n_samples=10000, random_state=42)
    X, y = data[0], data[1]
    X = np.concatenate((np.ones((len(X),1)), X), axis=1)
    return X, y

def train_linear_regression_model(X, y):
    """"""
    Train a linear regression model to predict the output label vector y based on the input data matrix X
    The output is a weight vector w such that Xw is closest to y, in terms of Mean Squared Erroor
    """"""
    w = np.linalg.inv(X.T @ X) @ X.T @ y
    return w

def compute_memory_usage(model):
    """"""
    Compute the number of bytes needed to store the model vector
    """"""
    return model.nbytes

def compute_prediction_runtime(X, model, N=1000):
    """"""
    Record the average time taken to perform prediction, sampled from 1000 runs
    """"""
    total_time = 0
    for _ in range(N):
        start = datetime.now()
        y = X @ model
        end = datetime.now()
        total_time += (end - start).total_seconds()
    return total_time / N

def compute_prediction_error(X, model, y_true):
    """"""
    Compute the Mean Squared Error from the vector of predicted labels and the vector of true labels
    """"""
    y_predicted = X @ model
    return np.mean((y_predicted - y_true)**2)

def evaluate_dtype(dtype):
    """"""
    Return the memory usage, prediction runtime and prediction error when training a linear regression model with the specified data type
    """"""
    # train the model on 60% of the data
    X_train, X_test, y_train, y_test = train_test_split(*construct_dataset(), test_size = 0.4)
    model = train_linear_regression_model(X_train, y_train)
    
    # convert the test data and trained model vector to the specified dtype
    X_test, model = X_test.astype(dtype), model.astype(dtype)
    
    # perform evaluation
    memory_usage = compute_memory_usage(model)
    prediction_runtime = compute_prediction_runtime(X_test, model)
    prediction_error = compute_prediction_error(X_test, model, y_test)
    print(f""A model stored in data type {dtype} consumes {memory_usage} bytes, takes {prediction_runtime} seconds to perform prediction on average, and has a prediction error of {prediction_error}"")
    return memory_usage, prediction_runtime, prediction_error"
13197,1,"        blacklist = set()
        for x in f.readlines():
            blacklist.add(int(x.strip()))
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set()
        for x in f.readlines():
            whitelist.add(int(x.strip()))
    
    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):

    with open(blacklist_file, ""r"") as f:
        blacklist = set()
        for x in f.readlines():
            blacklist.add(int(x.strip()))
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set()
        for x in f.readlines():
            whitelist.add(int(x.strip()))
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):

    with open(blacklist_file, ""r"") as f:
        blacklist = set()
        for x in f.readlines():
            blacklist.add(int(x.strip()))
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set()
        for x in f.readlines():
            whitelist.add(int(x.strip()))
    
    return blacklist, whitelist"
13197,2,"        whitelist = set()
        for x in f.readlines():
            whitelist.add(int(x.strip()))
    
    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):

    with open(blacklist_file, ""r"") as f:
        blacklist = set()
        for x in f.readlines():
            blacklist.add(int(x.strip()))
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set()
        for x in f.readlines():
            whitelist.add(int(x.strip()))
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):

    with open(blacklist_file, ""r"") as f:
        blacklist = set()
        for x in f.readlines():
            blacklist.add(int(x.strip()))
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set()
        for x in f.readlines():
            whitelist.add(int(x.strip()))
    
    return blacklist, whitelist"
12084,1,"    blacklist= np.loadtxt(blacklist_file, dtype=int)
    whitelist= np.loadtxt(whitelist_file, dtype=int)
    blacklist=set(blacklist)
    whitelist=set(whitelist)
    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    blacklist= np.loadtxt(blacklist_file, dtype=int)
    whitelist= np.loadtxt(whitelist_file, dtype=int)
    blacklist=set(blacklist)
    whitelist=set(whitelist)
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    blacklist= np.loadtxt(blacklist_file, dtype=int)
    whitelist= np.loadtxt(whitelist_file, dtype=int)
    blacklist=set(blacklist)
    whitelist=set(whitelist)
    return blacklist, whitelist"
12084,2,"    whitelist= np.loadtxt(whitelist_file, dtype=int)
    blacklist=set(blacklist)
    whitelist=set(whitelist)
    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    blacklist= np.loadtxt(blacklist_file, dtype=int)
    whitelist= np.loadtxt(whitelist_file, dtype=int)
    blacklist=set(blacklist)
    whitelist=set(whitelist)
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    blacklist= np.loadtxt(blacklist_file, dtype=int)
    whitelist= np.loadtxt(whitelist_file, dtype=int)
    blacklist=set(blacklist)
    whitelist=set(whitelist)
    return blacklist, whitelist"
7242,1,"        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist"
7242,2,"        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist"
7494,1,"        blacklist = set([int(x.strip()) for x in f.readlines()])
#         blacklist.add(int(x.strip()) for x in f.readlines())
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()]) 
#         whitelist.add(int(x.strip()) for x in f.readlines())
    
    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
#     blacklist = set()
#     whitelist = set()
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
#         blacklist.add(int(x.strip()) for x in f.readlines())
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()]) 
#         whitelist.add(int(x.strip()) for x in f.readlines())
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
#     blacklist = set()
#     whitelist = set()
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
#         blacklist.add(int(x.strip()) for x in f.readlines())
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()]) 
#         whitelist.add(int(x.strip()) for x in f.readlines())
    
    return blacklist, whitelist"
7494,2,"        whitelist = set([int(x.strip()) for x in f.readlines()]) 
#         whitelist.add(int(x.strip()) for x in f.readlines())
    
    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
#     blacklist = set()
#     whitelist = set()
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
#         blacklist.add(int(x.strip()) for x in f.readlines())
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()]) 
#         whitelist.add(int(x.strip()) for x in f.readlines())
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
#     blacklist = set()
#     whitelist = set()
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
#         blacklist.add(int(x.strip()) for x in f.readlines())
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()]) 
#         whitelist.add(int(x.strip()) for x in f.readlines())
    
    return blacklist, whitelist"
4413,1,"        blacklist = set([int(x.strip()) for x in f.readlines()])
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()])
    
    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()])
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()])
    
    return blacklist, whitelist"
4413,2,"        whitelist = set([int(x.strip()) for x in f.readlines()])
    
    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()])
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()])
    
    return blacklist, whitelist"
3706,1,"        blacklist = {int(x.strip()) for x in f.readlines()} 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()}
    
    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()} 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()}
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()} 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()}
    
    return blacklist, whitelist"
3706,2,"        whitelist = {int(x.strip()) for x in f.readlines()}
    
    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()} 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()}
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = {int(x.strip()) for x in f.readlines()} 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = {int(x.strip()) for x in f.readlines()}
    
    return blacklist, whitelist"
9708,1,"        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist"
9708,2,"        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist"
9582,1,"        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return set(blacklist), set(whitelist)",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return set(blacklist), set(whitelist)","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return set(blacklist), set(whitelist)"
9582,2,"        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return set(blacklist), set(whitelist)",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return set(blacklist), set(whitelist)","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return set(blacklist), set(whitelist)"
9757,1,"        blacklist = set([int(x.strip()) for x in f.readlines()])
#         blacklist.add(int(x.strip()) for x in f.readlines())
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()]) 
#         whitelist.add(int(x.strip()) for x in f.readlines())
    
    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
#     blacklist = set()
#     whitelist = set()
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
#         blacklist.add(int(x.strip()) for x in f.readlines())
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()]) 
#         whitelist.add(int(x.strip()) for x in f.readlines())
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
#     blacklist = set()
#     whitelist = set()
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
#         blacklist.add(int(x.strip()) for x in f.readlines())
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()]) 
#         whitelist.add(int(x.strip()) for x in f.readlines())
    
    return blacklist, whitelist"
9757,2,"        whitelist = set([int(x.strip()) for x in f.readlines()]) 
#         whitelist.add(int(x.strip()) for x in f.readlines())
    
    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
#     blacklist = set()
#     whitelist = set()
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
#         blacklist.add(int(x.strip()) for x in f.readlines())
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()]) 
#         whitelist.add(int(x.strip()) for x in f.readlines())
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
#     blacklist = set()
#     whitelist = set()
    with open(blacklist_file, ""r"") as f:
        blacklist = set([int(x.strip()) for x in f.readlines()])
#         blacklist.add(int(x.strip()) for x in f.readlines())
    
    with open(whitelist_file, ""r"") as f:
        whitelist = set([int(x.strip()) for x in f.readlines()]) 
#         whitelist.add(int(x.strip()) for x in f.readlines())
    
    return blacklist, whitelist"
3825,1,"        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist",TODO,blacklist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist"
3825,2,"        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist",TODO,whitelist,"def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist","def read_blacklist_and_whitelist(blacklist_file, whitelist_file):
    '''
    NOTE: you should change the data structure used for storing the blacklisted and whitelisted IDs

    Reads the blacklist and whitelist from input files and store them into appropriate data structures
    
    args:
        blacklist_file (str) : file path of blacklist, each line is separate id
        whitelist_file (str) : file path of whitelist, each line is separate id
        
    returns: Tuple(blacklist, whitelist)
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    '''
    with open(blacklist_file, ""r"") as f:
        blacklist = [int(x.strip()) for x in f.readlines()] 
    
    with open(whitelist_file, ""r"") as f:
        whitelist = [int(x.strip()) for x in f.readlines()] 
    
    return blacklist, whitelist"
8897,1,"      id_state = True
    else:
      id_state = False
    return id_state",TODO,id_state,"def check_single_id(id_to_check, blacklist, whitelist):
    if id_to_check in blacklist and id_to_check not in whitelist:
      id_state = True
    else:
      id_state = False
    return id_state
    raise NotImplementedError(""Complete this function!"")","def check_single_id(id_to_check, blacklist, whitelist):
    if id_to_check in blacklist and id_to_check not in whitelist:
      id_state = True
    else:
      id_state = False
    return id_state
    raise NotImplementedError(""Complete this function!"")
def test_check_single_id():
    blacklist, whitelist = read_blacklist_and_whitelist(""blacklist.txt"", ""whitelist.txt"")
    assert check_single_id(59735, blacklist, whitelist) == False, ""Check that you handle cases when id is in both blacklist and whitelist!""
    assert check_single_id(5935, blacklist, whitelist) == True, ""Check that you handle cases when id is in blacklist and not in whitelist!""
    print(""All tests passed!"")
    
test_check_single_id()

# let's also see how long it takes to run this function
%timeit check_single_id(59735, blacklist, whitelist)"
11161,1,"      id_state = True
    else:
      id_state = False
    return id_state",TODO,id_state,"def check_single_id(id_to_check, blacklist, whitelist):
    if id_to_check in blacklist and id_to_check not in whitelist:
      id_state = True
    else:
      id_state = False
    return id_state
    raise NotImplementedError(""Complete this function!"")","def check_single_id(id_to_check, blacklist, whitelist):
    if id_to_check in blacklist and id_to_check not in whitelist:
      id_state = True
    else:
      id_state = False
    return id_state
    raise NotImplementedError(""Complete this function!"")
def test_check_single_id():
    blacklist, whitelist = read_blacklist_and_whitelist(""blacklist.txt"", ""whitelist.txt"")
    assert check_single_id(59735, blacklist, whitelist) == False, ""Check that you handle cases when id is in both blacklist and whitelist!""
    assert check_single_id(5935, blacklist, whitelist) == True, ""Check that you handle cases when id is in blacklist and not in whitelist!""
    print(""All tests passed!"")
    
test_check_single_id()

# let's also see how long it takes to run this function
%timeit check_single_id(59735, blacklist, whitelist)"
5618,1,"    banned_set = blacklist - whitelist
    return id_to_check in banned_set",TODO,banned_set,"def check_single_id(id_to_check, blacklist, whitelist):
    '''
    Checks whether an input ID is banned, based on the stored blacklist and whiteliist
    
    args:
        id_to_check (int) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        id_state (bool) : True if id_to_check belongs to a banned user, and False otherwise
    '''
    # raise NotImplementedError(""Complete this function!"")
    banned_set = blacklist - whitelist
    return id_to_check in banned_set","def check_single_id(id_to_check, blacklist, whitelist):
    '''
    Checks whether an input ID is banned, based on the stored blacklist and whiteliist
    
    args:
        id_to_check (int) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        id_state (bool) : True if id_to_check belongs to a banned user, and False otherwise
    '''
    # raise NotImplementedError(""Complete this function!"")
    banned_set = blacklist - whitelist
    return id_to_check in banned_set
def test_check_single_id():
    blacklist, whitelist = read_blacklist_and_whitelist(""blacklist.txt"", ""whitelist.txt"")
    assert check_single_id(59735, blacklist, whitelist) == False, ""Check that you handle cases when id is in both blacklist and whitelist!""
    assert check_single_id(5935, blacklist, whitelist) == True, ""Check that you handle cases when id is in blacklist and not in whitelist!""
    print(""All tests passed!"")
    
test_check_single_id()

# let's also see how long it takes to run this function
%timeit check_single_id(59735, blacklist, whitelist)"
7778,1,"    fifo_storage = deque(initial_data)
    
    # item = next(data_socket)
    # print(item)
    # while item:
    #   if item == ""generate"":
    #     if len(fifo_storage) > 0:
    #       yield fifo_storage.pop(0)
    #   else:
    #     fifo_storage.append(item)
    #   item = next(data_socket)
    # return fifo_storage
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    from collections import deque
    fifo_storage = deque(initial_data)
    
    # item = next(data_socket)
    # print(item)
    # while item:
    #   if item == ""generate"":
    #     if len(fifo_storage) > 0:
    #       yield fifo_storage.pop(0)
    #   else:
    #     fifo_storage.append(item)
    #   item = next(data_socket)
    # return fifo_storage
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)","def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    from collections import deque
    fifo_storage = deque(initial_data)
    
    # item = next(data_socket)
    # print(item)
    # while item:
    #   if item == ""generate"":
    #     if len(fifo_storage) > 0:
    #       yield fifo_storage.pop(0)
    #   else:
    #     fifo_storage.append(item)
    #   item = next(data_socket)
    # return fifo_storage
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
3239,1,"    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    from collections import deque
    
    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
#    raise NotImplementedError(""Complete this function!"")","def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    from collections import deque
    
    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
#    raise NotImplementedError(""Complete this function!"")
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
5514,1,"    fifo_storage = deque(initial_data)
    #fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            if fifo_storage:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"from collections import deque

def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = deque(initial_data)
    #fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            if fifo_storage:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)","from collections import deque

def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = deque(initial_data)
    #fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            if fifo_storage:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
4142,1,"    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.pop(0)
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.pop(0)
        else:
            fifo_storage.append(item)","def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = initial_data
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.pop(0)
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
3743,1,"    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)","def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
12170,1,"    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"from collections import deque
def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)","from collections import deque
def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
6578,1,"    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"from collections import deque
def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)","from collections import deque
def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
5304,1,"    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)","def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
6711,1,"    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"from collections import deque
def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    
    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)","from collections import deque
def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''
    
    fifo_storage = deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
5717,1,"    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)",TODO,fifo_storage,"# class Queue: 
#     def __init__(self):
#         self.length = 0
#         self.head = None
#         self.tail = None
#     class Cell:
#         def __init__(self,element,next):
#             self.element = element
#             self.next = next
#     def dequeue(self):
#         self.length -=1
#         head_value = self.head.element
#         self.head = self.head.next
#         return head_value
#     def enqueue(self,element):
#         self.length +=1
#         new_cell = self.Cell(element, None)
#         if (self.head == None):
#             self.head = new_cell
#         else:
#             self.tail.next = new_cell
#         self.tail = new_cell

def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''

    # for each in initial_data:
    #     fifo_storage.enqueue(each)
    # for item in data_socket():
    #     if item == ""generate"":
    #         if fifo_storage.length > 0:
    #             yield fifo_storage.dequeue()
    #     else:
    #         fifo_storage.enqueue(item)
    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
    
    ","# class Queue: 
#     def __init__(self):
#         self.length = 0
#         self.head = None
#         self.tail = None
#     class Cell:
#         def __init__(self,element,next):
#             self.element = element
#             self.next = next
#     def dequeue(self):
#         self.length -=1
#         head_value = self.head.element
#         self.head = self.head.next
#         return head_value
#     def enqueue(self,element):
#         self.length +=1
#         new_cell = self.Cell(element, None)
#         if (self.head == None):
#             self.head = new_cell
#         else:
#             self.tail.next = new_cell
#         self.tail = new_cell

def Storage(initial_data, data_socket):
    '''
    NOTE: you should modify this function to be more efficient
    
    Dynamically update the collection of search results based on the generator data_socket and the initial result collection initial_data
    
    args:
        initial_data (List[Object]) - the initial collection of search results
        data_socket (generator [Object|String]) - a generator that yields either some object or the string ""generate""
        
    yields:
        Object from initial_data or data_socket
    '''

    # for each in initial_data:
    #     fifo_storage.enqueue(each)
    # for item in data_socket():
    #     if item == ""generate"":
    #         if fifo_storage.length > 0:
    #             yield fifo_storage.dequeue()
    #     else:
    #         fifo_storage.enqueue(item)
    fifo_storage = collections.deque(initial_data)
    for item in data_socket():
        if item == ""generate"":
            if len(fifo_storage) > 0:
                yield fifo_storage.popleft()
        else:
            fifo_storage.append(item)
    
    
def test_storage():
    def test_socket():
        for x in [3,""generate"",""generate"",""generate"",""generate"",4,""generate"",5]:
            yield x

    assert list(Storage([1,2], test_socket)) == [1, 2, 3, 4]
    print(""All tests passed!"")
    
test_storage()

# let's also see how long it takes to run this function
def data_socket():
    # a sample data_socket that yields ""generate"" once in every 10 elements
    for i in range(10000):
        if i % 10: 
            yield ""generate""
        yield airbnb_data[""description""].iloc[i % len(airbnb_data)]
        
%timeit list(Storage(list(airbnb_data[""description""].values), data_socket));"
12885,1,"      chain_of_ancestors.clear()
    
    history.append(item)
    chain_of_ancestors.append(item)

    if item == target:
      #print(item)
      #print(chain_of_ancestors)
      return True

    if graph[item] != {}:
      if dfs_helper(history,chain_of_ancestors,graph[item],target,level+1):
        return True
  return False

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    #raise NotImplementedError(""Complete this function!"")

    chain_of_ancestors = []
    history = []
    
    result = dfs_helper(history,chain_of_ancestors, tree, target, 0)

    if result == False:
      chain_of_ancestors.clear()
    
    return chain_of_ancestors, history",TODO,chain_of_ancestors,"def dfs_helper(history, chain_of_ancestors, graph, target, level):
  for item in graph:
    if level == 0:
      #In top level, we clear the old ancestors
      chain_of_ancestors.clear()
    
    history.append(item)
    chain_of_ancestors.append(item)

    if item == target:
      #print(item)
      #print(chain_of_ancestors)
      return True

    if graph[item] != {}:
      if dfs_helper(history,chain_of_ancestors,graph[item],target,level+1):
        return True
  return False

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    #raise NotImplementedError(""Complete this function!"")

    chain_of_ancestors = []
    history = []
    
    result = dfs_helper(history,chain_of_ancestors, tree, target, 0)

    if result == False:
      chain_of_ancestors.clear()
    
    return chain_of_ancestors, history
    

","def dfs_helper(history, chain_of_ancestors, graph, target, level):
  for item in graph:
    if level == 0:
      #In top level, we clear the old ancestors
      chain_of_ancestors.clear()
    
    history.append(item)
    chain_of_ancestors.append(item)

    if item == target:
      #print(item)
      #print(chain_of_ancestors)
      return True

    if graph[item] != {}:
      if dfs_helper(history,chain_of_ancestors,graph[item],target,level+1):
        return True
  return False

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    #raise NotImplementedError(""Complete this function!"")

    chain_of_ancestors = []
    history = []
    
    result = dfs_helper(history,chain_of_ancestors, tree, target, 0)

    if result == False:
      chain_of_ancestors.clear()
    
    return chain_of_ancestors, history
    


def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
12885,2,"    history.append(item)
    chain_of_ancestors.append(item)

    if item == target:
      #print(item)
      #print(chain_of_ancestors)
      return True

    if graph[item] != {}:
      if dfs_helper(history,chain_of_ancestors,graph[item],target,level+1):
        return True
  return False

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    #raise NotImplementedError(""Complete this function!"")

    chain_of_ancestors = []
    history = []
    
    result = dfs_helper(history,chain_of_ancestors, tree, target, 0)

    if result == False:
      chain_of_ancestors.clear()
    
    return chain_of_ancestors, history",TODO,history,"def dfs_helper(history, chain_of_ancestors, graph, target, level):
  for item in graph:
    if level == 0:
      #In top level, we clear the old ancestors
      chain_of_ancestors.clear()
    
    history.append(item)
    chain_of_ancestors.append(item)

    if item == target:
      #print(item)
      #print(chain_of_ancestors)
      return True

    if graph[item] != {}:
      if dfs_helper(history,chain_of_ancestors,graph[item],target,level+1):
        return True
  return False

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    #raise NotImplementedError(""Complete this function!"")

    chain_of_ancestors = []
    history = []
    
    result = dfs_helper(history,chain_of_ancestors, tree, target, 0)

    if result == False:
      chain_of_ancestors.clear()
    
    return chain_of_ancestors, history
    

","def dfs_helper(history, chain_of_ancestors, graph, target, level):
  for item in graph:
    if level == 0:
      #In top level, we clear the old ancestors
      chain_of_ancestors.clear()
    
    history.append(item)
    chain_of_ancestors.append(item)

    if item == target:
      #print(item)
      #print(chain_of_ancestors)
      return True

    if graph[item] != {}:
      if dfs_helper(history,chain_of_ancestors,graph[item],target,level+1):
        return True
  return False

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    #raise NotImplementedError(""Complete this function!"")

    chain_of_ancestors = []
    history = []
    
    result = dfs_helper(history,chain_of_ancestors, tree, target, 0)

    if result == False:
      chain_of_ancestors.clear()
    
    return chain_of_ancestors, history
    


def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
12885,3,"    result = dfs_helper(history,chain_of_ancestors, tree, target, 0)

    if result == False:",TODO,result,"def dfs_helper(history, chain_of_ancestors, graph, target, level):
  for item in graph:
    if level == 0:
      #In top level, we clear the old ancestors
      chain_of_ancestors.clear()
    
    history.append(item)
    chain_of_ancestors.append(item)

    if item == target:
      #print(item)
      #print(chain_of_ancestors)
      return True

    if graph[item] != {}:
      if dfs_helper(history,chain_of_ancestors,graph[item],target,level+1):
        return True
  return False

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    #raise NotImplementedError(""Complete this function!"")

    chain_of_ancestors = []
    history = []
    
    result = dfs_helper(history,chain_of_ancestors, tree, target, 0)

    if result == False:
      chain_of_ancestors.clear()
    
    return chain_of_ancestors, history
    

","def dfs_helper(history, chain_of_ancestors, graph, target, level):
  for item in graph:
    if level == 0:
      #In top level, we clear the old ancestors
      chain_of_ancestors.clear()
    
    history.append(item)
    chain_of_ancestors.append(item)

    if item == target:
      #print(item)
      #print(chain_of_ancestors)
      return True

    if graph[item] != {}:
      if dfs_helper(history,chain_of_ancestors,graph[item],target,level+1):
        return True
  return False

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    #raise NotImplementedError(""Complete this function!"")

    chain_of_ancestors = []
    history = []
    
    result = dfs_helper(history,chain_of_ancestors, tree, target, 0)

    if result == False:
      chain_of_ancestors.clear()
    
    return chain_of_ancestors, history
    


def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
11352,1,"    parents = []
    history = []
    def find(target, tree, flag=0):
        for child in tree:
            history.append(child)
            parents.append(child)
            if target==child:
                flag+=1
                return flag
            else:
                if not tree[child]=={}:
                    if find(target, tree[child])==1:
                        return 1
                else: 
                    parents.pop()
        if len(parents)>0: parents.pop()

    find(target, tree, 0)
    return parents, history",TODO,parents,"def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    parents = []
    history = []
    def find(target, tree, flag=0):
        for child in tree:
            history.append(child)
            parents.append(child)
            if target==child:
                flag+=1
                return flag
            else:
                if not tree[child]=={}:
                    if find(target, tree[child])==1:
                        return 1
                else: 
                    parents.pop()
        if len(parents)>0: parents.pop()

    find(target, tree, 0)
    return parents, history
            
    raise NotImplementedError(""Complete this function!"")","def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    parents = []
    history = []
    def find(target, tree, flag=0):
        for child in tree:
            history.append(child)
            parents.append(child)
            if target==child:
                flag+=1
                return flag
            else:
                if not tree[child]=={}:
                    if find(target, tree[child])==1:
                        return 1
                else: 
                    parents.pop()
        if len(parents)>0: parents.pop()

    find(target, tree, 0)
    return parents, history
            
    raise NotImplementedError(""Complete this function!"")
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
11352,2,"    history = []
    def find(target, tree, flag=0):
        for child in tree:
            history.append(child)
            parents.append(child)
            if target==child:
                flag+=1
                return flag
            else:
                if not tree[child]=={}:
                    if find(target, tree[child])==1:
                        return 1
                else: 
                    parents.pop()
        if len(parents)>0: parents.pop()

    find(target, tree, 0)
    return parents, history",TODO,history,"def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    parents = []
    history = []
    def find(target, tree, flag=0):
        for child in tree:
            history.append(child)
            parents.append(child)
            if target==child:
                flag+=1
                return flag
            else:
                if not tree[child]=={}:
                    if find(target, tree[child])==1:
                        return 1
                else: 
                    parents.pop()
        if len(parents)>0: parents.pop()

    find(target, tree, 0)
    return parents, history
            
    raise NotImplementedError(""Complete this function!"")","def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    parents = []
    history = []
    def find(target, tree, flag=0):
        for child in tree:
            history.append(child)
            parents.append(child)
            if target==child:
                flag+=1
                return flag
            else:
                if not tree[child]=={}:
                    if find(target, tree[child])==1:
                        return 1
                else: 
                    parents.pop()
        if len(parents)>0: parents.pop()

    find(target, tree, 0)
    return parents, history
            
    raise NotImplementedError(""Complete this function!"")
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
12528,1,"    pop = True
    for i in tree[node].keys():
        pop &= dfs_ancestors(target, tree[node], i, vis, ancestors)
    if pop:
        ancestors.pop()
    return pop",TODO,pop,"def dfs_history(target, tree, node, vis, history):
    if node in vis:
        return
    if target in history:
        return
    history.append(node);
    vis.add(node);
    for i in tree[node].keys():
        dfs_history(target, tree[node], i, vis, history)

        
def dfs_ancestors(target, tree, node, vis, ancestors):
    if node in vis:
        return
    ancestors.append(node)
    vis.add(node);
    if node == target:
        return False
    pop = True
    for i in tree[node].keys():
        pop &= dfs_ancestors(target, tree[node], i, vis, ancestors)
    if pop:
        ancestors.pop()
    return pop
        

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    vis = set()
    history = []
    for node in tree.keys():
        dfs_history(target, tree, node, vis, history)
        
    vis = set()
    ancestors = []
    for node in tree.keys():
        dfs_ancestors(target, tree, node, vis, ancestors)
            
    return ancestors, history","def dfs_history(target, tree, node, vis, history):
    if node in vis:
        return
    if target in history:
        return
    history.append(node);
    vis.add(node);
    for i in tree[node].keys():
        dfs_history(target, tree[node], i, vis, history)

        
def dfs_ancestors(target, tree, node, vis, ancestors):
    if node in vis:
        return
    ancestors.append(node)
    vis.add(node);
    if node == target:
        return False
    pop = True
    for i in tree[node].keys():
        pop &= dfs_ancestors(target, tree[node], i, vis, ancestors)
    if pop:
        ancestors.pop()
    return pop
        

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    vis = set()
    history = []
    for node in tree.keys():
        dfs_history(target, tree, node, vis, history)
        
    vis = set()
    ancestors = []
    for node in tree.keys():
        dfs_ancestors(target, tree, node, vis, ancestors)
            
    return ancestors, history
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
12528,2,"    if node in vis:
        return
    if target in history:
        return
    history.append(node);
    vis.add(node);
    for i in tree[node].keys():
        dfs_history(target, tree[node], i, vis, history)

        
def dfs_ancestors(target, tree, node, vis, ancestors):
    if node in vis:
        return
    ancestors.append(node)
    vis.add(node);
    if node == target:
        return False
    pop = True
    for i in tree[node].keys():
        pop &= dfs_ancestors(target, tree[node], i, vis, ancestors)
    if pop:
        ancestors.pop()
    return pop
        

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    vis = set()
    history = []
    for node in tree.keys():
        dfs_history(target, tree, node, vis, history)
        
    vis = set()
    ancestors = []
    for node in tree.keys():
        dfs_ancestors(target, tree, node, vis, ancestors)",TODO,vis,"def dfs_history(target, tree, node, vis, history):
    if node in vis:
        return
    if target in history:
        return
    history.append(node);
    vis.add(node);
    for i in tree[node].keys():
        dfs_history(target, tree[node], i, vis, history)

        
def dfs_ancestors(target, tree, node, vis, ancestors):
    if node in vis:
        return
    ancestors.append(node)
    vis.add(node);
    if node == target:
        return False
    pop = True
    for i in tree[node].keys():
        pop &= dfs_ancestors(target, tree[node], i, vis, ancestors)
    if pop:
        ancestors.pop()
    return pop
        

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    vis = set()
    history = []
    for node in tree.keys():
        dfs_history(target, tree, node, vis, history)
        
    vis = set()
    ancestors = []
    for node in tree.keys():
        dfs_ancestors(target, tree, node, vis, ancestors)
            
    return ancestors, history","def dfs_history(target, tree, node, vis, history):
    if node in vis:
        return
    if target in history:
        return
    history.append(node);
    vis.add(node);
    for i in tree[node].keys():
        dfs_history(target, tree[node], i, vis, history)

        
def dfs_ancestors(target, tree, node, vis, ancestors):
    if node in vis:
        return
    ancestors.append(node)
    vis.add(node);
    if node == target:
        return False
    pop = True
    for i in tree[node].keys():
        pop &= dfs_ancestors(target, tree[node], i, vis, ancestors)
    if pop:
        ancestors.pop()
    return pop
        

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    vis = set()
    history = []
    for node in tree.keys():
        dfs_history(target, tree, node, vis, history)
        
    vis = set()
    ancestors = []
    for node in tree.keys():
        dfs_ancestors(target, tree, node, vis, ancestors)
            
    return ancestors, history
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
12528,3,"    if target in history:
        return
    history.append(node);
    vis.add(node);
    for i in tree[node].keys():
        dfs_history(target, tree[node], i, vis, history)

        
def dfs_ancestors(target, tree, node, vis, ancestors):
    if node in vis:
        return
    ancestors.append(node)
    vis.add(node);
    if node == target:
        return False
    pop = True
    for i in tree[node].keys():
        pop &= dfs_ancestors(target, tree[node], i, vis, ancestors)
    if pop:
        ancestors.pop()
    return pop
        

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    vis = set()
    history = []
    for node in tree.keys():
        dfs_history(target, tree, node, vis, history)
        
    vis = set()
    ancestors = []
    for node in tree.keys():
        dfs_ancestors(target, tree, node, vis, ancestors)
            
    return ancestors, history",TODO,history,"def dfs_history(target, tree, node, vis, history):
    if node in vis:
        return
    if target in history:
        return
    history.append(node);
    vis.add(node);
    for i in tree[node].keys():
        dfs_history(target, tree[node], i, vis, history)

        
def dfs_ancestors(target, tree, node, vis, ancestors):
    if node in vis:
        return
    ancestors.append(node)
    vis.add(node);
    if node == target:
        return False
    pop = True
    for i in tree[node].keys():
        pop &= dfs_ancestors(target, tree[node], i, vis, ancestors)
    if pop:
        ancestors.pop()
    return pop
        

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    vis = set()
    history = []
    for node in tree.keys():
        dfs_history(target, tree, node, vis, history)
        
    vis = set()
    ancestors = []
    for node in tree.keys():
        dfs_ancestors(target, tree, node, vis, ancestors)
            
    return ancestors, history","def dfs_history(target, tree, node, vis, history):
    if node in vis:
        return
    if target in history:
        return
    history.append(node);
    vis.add(node);
    for i in tree[node].keys():
        dfs_history(target, tree[node], i, vis, history)

        
def dfs_ancestors(target, tree, node, vis, ancestors):
    if node in vis:
        return
    ancestors.append(node)
    vis.add(node);
    if node == target:
        return False
    pop = True
    for i in tree[node].keys():
        pop &= dfs_ancestors(target, tree[node], i, vis, ancestors)
    if pop:
        ancestors.pop()
    return pop
        

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    vis = set()
    history = []
    for node in tree.keys():
        dfs_history(target, tree, node, vis, history)
        
    vis = set()
    ancestors = []
    for node in tree.keys():
        dfs_ancestors(target, tree, node, vis, ancestors)
            
    return ancestors, history
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
12528,4,"    ancestors.append(node)
    vis.add(node);
    if node == target:
        return False
    pop = True
    for i in tree[node].keys():
        pop &= dfs_ancestors(target, tree[node], i, vis, ancestors)
    if pop:
        ancestors.pop()
    return pop
        

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    vis = set()
    history = []
    for node in tree.keys():
        dfs_history(target, tree, node, vis, history)
        
    vis = set()
    ancestors = []
    for node in tree.keys():
        dfs_ancestors(target, tree, node, vis, ancestors)
            
    return ancestors, history",TODO,ancestors,"def dfs_history(target, tree, node, vis, history):
    if node in vis:
        return
    if target in history:
        return
    history.append(node);
    vis.add(node);
    for i in tree[node].keys():
        dfs_history(target, tree[node], i, vis, history)

        
def dfs_ancestors(target, tree, node, vis, ancestors):
    if node in vis:
        return
    ancestors.append(node)
    vis.add(node);
    if node == target:
        return False
    pop = True
    for i in tree[node].keys():
        pop &= dfs_ancestors(target, tree[node], i, vis, ancestors)
    if pop:
        ancestors.pop()
    return pop
        

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    vis = set()
    history = []
    for node in tree.keys():
        dfs_history(target, tree, node, vis, history)
        
    vis = set()
    ancestors = []
    for node in tree.keys():
        dfs_ancestors(target, tree, node, vis, ancestors)
            
    return ancestors, history","def dfs_history(target, tree, node, vis, history):
    if node in vis:
        return
    if target in history:
        return
    history.append(node);
    vis.add(node);
    for i in tree[node].keys():
        dfs_history(target, tree[node], i, vis, history)

        
def dfs_ancestors(target, tree, node, vis, ancestors):
    if node in vis:
        return
    ancestors.append(node)
    vis.add(node);
    if node == target:
        return False
    pop = True
    for i in tree[node].keys():
        pop &= dfs_ancestors(target, tree[node], i, vis, ancestors)
    if pop:
        ancestors.pop()
    return pop
        

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    vis = set()
    history = []
    for node in tree.keys():
        dfs_history(target, tree, node, vis, history)
        
    vis = set()
    ancestors = []
    for node in tree.keys():
        dfs_ancestors(target, tree, node, vis, ancestors)
            
    return ancestors, history
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
3352,1,"    found_t = False

    for key, value in tree.items():
#         print('KEYYYY ', key, 'VALUEEEE', value)
        if key == target:
            parents_ref.append(key)
            history_ref.append(key)
            found_t = True
            break
        parents_ref.append(key)
        history_ref.append(key)
        if found_t == False:
            _, _, found_t = dfs_helper(value, target, parents_ref, history_ref)
            if not found_t:
                parents_ref.pop()
            if found_t:
                break
    return parents_ref, history_ref,found_t",TODO,found_t,"
def dfs_helper(tree, target,parents_ref=[],history_ref=[]):
    
    found_t = False

    for key, value in tree.items():
#         print('KEYYYY ', key, 'VALUEEEE', value)
        if key == target:
            parents_ref.append(key)
            history_ref.append(key)
            found_t = True
            break
        parents_ref.append(key)
        history_ref.append(key)
        if found_t == False:
            _, _, found_t = dfs_helper(value, target, parents_ref, history_ref)
            if not found_t:
                parents_ref.pop()
            if found_t:
                break
    return parents_ref, history_ref,found_t

def dfs(target, tree,  parents_ref=[], history_ref=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the tar   get user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    parents_ref, history_ref = [], []
    parents_ref, history_ref, _ = dfs_helper(tree, target,parents_ref,history_ref)
#     print(parents_ref, history_ref)
    
    return parents_ref, history_ref
    
    
#     raise NotImplementedError(""Complete this function!"")","
def dfs_helper(tree, target,parents_ref=[],history_ref=[]):
    
    found_t = False

    for key, value in tree.items():
#         print('KEYYYY ', key, 'VALUEEEE', value)
        if key == target:
            parents_ref.append(key)
            history_ref.append(key)
            found_t = True
            break
        parents_ref.append(key)
        history_ref.append(key)
        if found_t == False:
            _, _, found_t = dfs_helper(value, target, parents_ref, history_ref)
            if not found_t:
                parents_ref.pop()
            if found_t:
                break
    return parents_ref, history_ref,found_t

def dfs(target, tree,  parents_ref=[], history_ref=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the tar   get user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    parents_ref, history_ref = [], []
    parents_ref, history_ref, _ = dfs_helper(tree, target,parents_ref,history_ref)
#     print(parents_ref, history_ref)
    
    return parents_ref, history_ref
    
    
#     raise NotImplementedError(""Complete this function!"")
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()
# import collections

def my_dfs(tree, target,parents_ref=[],history_ref=[]):
    found_t = False
    
#     if tree is None:
#         return parents_ref, history_ref
    
#     while found_t != True:
    for key, value in tree.items():
        print('KEYYYY ', key,'& VALUEE ', value)
        if key == target:
            parents_ref.append(key)
            history_ref.append(key)
            found_t = True
#             return parents_ref, history_ref
            break
        history_ref.append(key)
#         if value is {} and found_t is False:
#             parents_ref = [parents_ref.popleft() for i in parents_ref]
        if found_t == False:
#             history_ref.append(key)
            print(value)
            my_dfs(value, target, parents_ref, history_ref)

    return parents_ref, history_ref


tree = {
  '1' : {'2':{}},
  '3' : {'4':{}, '5':{}},
  '6' : {'7': {'8': {'9':{}, '10': {}}}},
  '11' : {} 
}
parents_ref, history_ref = [], []
my_dfs(tree,'8')"
3352,2,"            parents_ref.append(key)
            history_ref.append(key)
            found_t = True
            break
        parents_ref.append(key)
        history_ref.append(key)
        if found_t == False:
            _, _, found_t = dfs_helper(value, target, parents_ref, history_ref)
            if not found_t:
                parents_ref.pop()
            if found_t:
                break
    return parents_ref, history_ref,found_t

def dfs(target, tree,  parents_ref=[], history_ref=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the tar   get user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    parents_ref, history_ref = [], []
    parents_ref, history_ref, _ = dfs_helper(tree, target,parents_ref,history_ref)
#     print(parents_ref, history_ref)
    
    return parents_ref, history_ref",TODO,parents_ref,"
def dfs_helper(tree, target,parents_ref=[],history_ref=[]):
    
    found_t = False

    for key, value in tree.items():
#         print('KEYYYY ', key, 'VALUEEEE', value)
        if key == target:
            parents_ref.append(key)
            history_ref.append(key)
            found_t = True
            break
        parents_ref.append(key)
        history_ref.append(key)
        if found_t == False:
            _, _, found_t = dfs_helper(value, target, parents_ref, history_ref)
            if not found_t:
                parents_ref.pop()
            if found_t:
                break
    return parents_ref, history_ref,found_t

def dfs(target, tree,  parents_ref=[], history_ref=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the tar   get user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    parents_ref, history_ref = [], []
    parents_ref, history_ref, _ = dfs_helper(tree, target,parents_ref,history_ref)
#     print(parents_ref, history_ref)
    
    return parents_ref, history_ref
    
    
#     raise NotImplementedError(""Complete this function!"")","
def dfs_helper(tree, target,parents_ref=[],history_ref=[]):
    
    found_t = False

    for key, value in tree.items():
#         print('KEYYYY ', key, 'VALUEEEE', value)
        if key == target:
            parents_ref.append(key)
            history_ref.append(key)
            found_t = True
            break
        parents_ref.append(key)
        history_ref.append(key)
        if found_t == False:
            _, _, found_t = dfs_helper(value, target, parents_ref, history_ref)
            if not found_t:
                parents_ref.pop()
            if found_t:
                break
    return parents_ref, history_ref,found_t

def dfs(target, tree,  parents_ref=[], history_ref=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the tar   get user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    parents_ref, history_ref = [], []
    parents_ref, history_ref, _ = dfs_helper(tree, target,parents_ref,history_ref)
#     print(parents_ref, history_ref)
    
    return parents_ref, history_ref
    
    
#     raise NotImplementedError(""Complete this function!"")
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()
# import collections

def my_dfs(tree, target,parents_ref=[],history_ref=[]):
    found_t = False
    
#     if tree is None:
#         return parents_ref, history_ref
    
#     while found_t != True:
    for key, value in tree.items():
        print('KEYYYY ', key,'& VALUEE ', value)
        if key == target:
            parents_ref.append(key)
            history_ref.append(key)
            found_t = True
#             return parents_ref, history_ref
            break
        history_ref.append(key)
#         if value is {} and found_t is False:
#             parents_ref = [parents_ref.popleft() for i in parents_ref]
        if found_t == False:
#             history_ref.append(key)
            print(value)
            my_dfs(value, target, parents_ref, history_ref)

    return parents_ref, history_ref


tree = {
  '1' : {'2':{}},
  '3' : {'4':{}, '5':{}},
  '6' : {'7': {'8': {'9':{}, '10': {}}}},
  '11' : {} 
}
parents_ref, history_ref = [], []
my_dfs(tree,'8')"
3352,3,"            history_ref.append(key)
            found_t = True
            break
        parents_ref.append(key)
        history_ref.append(key)
        if found_t == False:
            _, _, found_t = dfs_helper(value, target, parents_ref, history_ref)
            if not found_t:
                parents_ref.pop()
            if found_t:
                break
    return parents_ref, history_ref,found_t

def dfs(target, tree,  parents_ref=[], history_ref=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the tar   get user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    parents_ref, history_ref = [], []
    parents_ref, history_ref, _ = dfs_helper(tree, target,parents_ref,history_ref)
#     print(parents_ref, history_ref)
    
    return parents_ref, history_ref",TODO,history_ref,"
def dfs_helper(tree, target,parents_ref=[],history_ref=[]):
    
    found_t = False

    for key, value in tree.items():
#         print('KEYYYY ', key, 'VALUEEEE', value)
        if key == target:
            parents_ref.append(key)
            history_ref.append(key)
            found_t = True
            break
        parents_ref.append(key)
        history_ref.append(key)
        if found_t == False:
            _, _, found_t = dfs_helper(value, target, parents_ref, history_ref)
            if not found_t:
                parents_ref.pop()
            if found_t:
                break
    return parents_ref, history_ref,found_t

def dfs(target, tree,  parents_ref=[], history_ref=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the tar   get user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    parents_ref, history_ref = [], []
    parents_ref, history_ref, _ = dfs_helper(tree, target,parents_ref,history_ref)
#     print(parents_ref, history_ref)
    
    return parents_ref, history_ref
    
    
#     raise NotImplementedError(""Complete this function!"")","
def dfs_helper(tree, target,parents_ref=[],history_ref=[]):
    
    found_t = False

    for key, value in tree.items():
#         print('KEYYYY ', key, 'VALUEEEE', value)
        if key == target:
            parents_ref.append(key)
            history_ref.append(key)
            found_t = True
            break
        parents_ref.append(key)
        history_ref.append(key)
        if found_t == False:
            _, _, found_t = dfs_helper(value, target, parents_ref, history_ref)
            if not found_t:
                parents_ref.pop()
            if found_t:
                break
    return parents_ref, history_ref,found_t

def dfs(target, tree,  parents_ref=[], history_ref=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the tar   get user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    parents_ref, history_ref = [], []
    parents_ref, history_ref, _ = dfs_helper(tree, target,parents_ref,history_ref)
#     print(parents_ref, history_ref)
    
    return parents_ref, history_ref
    
    
#     raise NotImplementedError(""Complete this function!"")
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()
# import collections

def my_dfs(tree, target,parents_ref=[],history_ref=[]):
    found_t = False
    
#     if tree is None:
#         return parents_ref, history_ref
    
#     while found_t != True:
    for key, value in tree.items():
        print('KEYYYY ', key,'& VALUEE ', value)
        if key == target:
            parents_ref.append(key)
            history_ref.append(key)
            found_t = True
#             return parents_ref, history_ref
            break
        history_ref.append(key)
#         if value is {} and found_t is False:
#             parents_ref = [parents_ref.popleft() for i in parents_ref]
        if found_t == False:
#             history_ref.append(key)
            print(value)
            my_dfs(value, target, parents_ref, history_ref)

    return parents_ref, history_ref


tree = {
  '1' : {'2':{}},
  '3' : {'4':{}, '5':{}},
  '6' : {'7': {'8': {'9':{}, '10': {}}}},
  '11' : {} 
}
parents_ref, history_ref = [], []
my_dfs(tree,'8')"
3352,4,"            _, _, found_t = dfs_helper(value, target, parents_ref, history_ref)
            if not found_t:
                parents_ref.pop()
            if found_t:
                break
    return parents_ref, history_ref,found_t

def dfs(target, tree,  parents_ref=[], history_ref=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the tar   get user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    parents_ref, history_ref = [], []
    parents_ref, history_ref, _ = dfs_helper(tree, target,parents_ref,history_ref)",TODO,_,"
def dfs_helper(tree, target,parents_ref=[],history_ref=[]):
    
    found_t = False

    for key, value in tree.items():
#         print('KEYYYY ', key, 'VALUEEEE', value)
        if key == target:
            parents_ref.append(key)
            history_ref.append(key)
            found_t = True
            break
        parents_ref.append(key)
        history_ref.append(key)
        if found_t == False:
            _, _, found_t = dfs_helper(value, target, parents_ref, history_ref)
            if not found_t:
                parents_ref.pop()
            if found_t:
                break
    return parents_ref, history_ref,found_t

def dfs(target, tree,  parents_ref=[], history_ref=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the tar   get user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    parents_ref, history_ref = [], []
    parents_ref, history_ref, _ = dfs_helper(tree, target,parents_ref,history_ref)
#     print(parents_ref, history_ref)
    
    return parents_ref, history_ref
    
    
#     raise NotImplementedError(""Complete this function!"")","
def dfs_helper(tree, target,parents_ref=[],history_ref=[]):
    
    found_t = False

    for key, value in tree.items():
#         print('KEYYYY ', key, 'VALUEEEE', value)
        if key == target:
            parents_ref.append(key)
            history_ref.append(key)
            found_t = True
            break
        parents_ref.append(key)
        history_ref.append(key)
        if found_t == False:
            _, _, found_t = dfs_helper(value, target, parents_ref, history_ref)
            if not found_t:
                parents_ref.pop()
            if found_t:
                break
    return parents_ref, history_ref,found_t

def dfs(target, tree,  parents_ref=[], history_ref=[]):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the tar   get user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    parents_ref, history_ref = [], []
    parents_ref, history_ref, _ = dfs_helper(tree, target,parents_ref,history_ref)
#     print(parents_ref, history_ref)
    
    return parents_ref, history_ref
    
    
#     raise NotImplementedError(""Complete this function!"")
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()
# import collections

def my_dfs(tree, target,parents_ref=[],history_ref=[]):
    found_t = False
    
#     if tree is None:
#         return parents_ref, history_ref
    
#     while found_t != True:
    for key, value in tree.items():
        print('KEYYYY ', key,'& VALUEE ', value)
        if key == target:
            parents_ref.append(key)
            history_ref.append(key)
            found_t = True
#             return parents_ref, history_ref
            break
        history_ref.append(key)
#         if value is {} and found_t is False:
#             parents_ref = [parents_ref.popleft() for i in parents_ref]
        if found_t == False:
#             history_ref.append(key)
            print(value)
            my_dfs(value, target, parents_ref, history_ref)

    return parents_ref, history_ref


tree = {
  '1' : {'2':{}},
  '3' : {'4':{}, '5':{}},
  '6' : {'7': {'8': {'9':{}, '10': {}}}},
  '11' : {} 
}
parents_ref, history_ref = [], []
my_dfs(tree,'8')"
3429,1,"    new_dict = tree
    history = []
    chain_of_ancestors = []
    
    for item in new_dict:  # Checks each of the items in the dictionary
        temp_dict = new_dict[item]",TODO,new_dict,"def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    new_dict = tree
    history = []
    chain_of_ancestors = []
    
    for item in new_dict:  # Checks each of the items in the dictionary
        temp_dict = new_dict[item]

        history.append(item) # tracks every item traversed in the tree
        chain_of_ancestors = [item]  # reset for every branch of the tree
        
        # check if present in first layer
        if target == item:  # Do not want to continue the for loop once the node with the target is identified
            chain_of_ancestors = []
            break
            
            
        temp_dict_2 = temp_dict # initialize
        while target not in temp_dict_2: # while we are not returning the target, we will keep indexing deeper into the tree 


            # check each item in the current layer of the tree
            for item in temp_dict:
                temp_dict_2 = {}  # reset this is a dictionary to be used for check the various nodes in a branch
                history.append(item)
                chain_of_ancestors.append(item)
                temp_dict_2.update(temp_dict[item])  # add the new terms to the dictionary
                if target in temp_dict_2:  # check if the target is at these nodes
                    break

            temp_dict = temp_dict_2  # this will be a dictionary containing all of the items in a branch
            # break when the temp dictionary is empty, check after updating the temp_dict
            if temp_dict_2 == {}:
                break

        if target in temp_dict:  # Do not want to continue the for loop once the node with the target is identified
            for items in temp_dict:
                if items != target:
                    chain_of_ancestors.append(items)
                    history.append(items)
                else:
                    break
            chain_of_ancestors.append(target)
            history.append(target)
            break

        if target not in temp_dict_2:
            chain_of_ancestors = []
        
   
    return chain_of_ancestors, history # lists of strings


#     raise NotImplementedError(""Complete this function!"")","def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    new_dict = tree
    history = []
    chain_of_ancestors = []
    
    for item in new_dict:  # Checks each of the items in the dictionary
        temp_dict = new_dict[item]

        history.append(item) # tracks every item traversed in the tree
        chain_of_ancestors = [item]  # reset for every branch of the tree
        
        # check if present in first layer
        if target == item:  # Do not want to continue the for loop once the node with the target is identified
            chain_of_ancestors = []
            break
            
            
        temp_dict_2 = temp_dict # initialize
        while target not in temp_dict_2: # while we are not returning the target, we will keep indexing deeper into the tree 


            # check each item in the current layer of the tree
            for item in temp_dict:
                temp_dict_2 = {}  # reset this is a dictionary to be used for check the various nodes in a branch
                history.append(item)
                chain_of_ancestors.append(item)
                temp_dict_2.update(temp_dict[item])  # add the new terms to the dictionary
                if target in temp_dict_2:  # check if the target is at these nodes
                    break

            temp_dict = temp_dict_2  # this will be a dictionary containing all of the items in a branch
            # break when the temp dictionary is empty, check after updating the temp_dict
            if temp_dict_2 == {}:
                break

        if target in temp_dict:  # Do not want to continue the for loop once the node with the target is identified
            for items in temp_dict:
                if items != target:
                    chain_of_ancestors.append(items)
                    history.append(items)
                else:
                    break
            chain_of_ancestors.append(target)
            history.append(target)
            break

        if target not in temp_dict_2:
            chain_of_ancestors = []
        
   
    return chain_of_ancestors, history # lists of strings


#     raise NotImplementedError(""Complete this function!"")
# tree = {
#       '1' : {'2':{}},
#       '3' : {'4':{}, '5':{}},
#       '6' : {'7': {'8': {'9':{}, '10': {}}}},
#       '11' :{} 
#     }

# # print(list(tree['3'])[0])
# target = '3'

# print(dfs(target,tree))
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
3429,2,"    history = []
    chain_of_ancestors = []
    
    for item in new_dict:  # Checks each of the items in the dictionary
        temp_dict = new_dict[item]

        history.append(item) # tracks every item traversed in the tree
        chain_of_ancestors = [item]  # reset for every branch of the tree
        
        # check if present in first layer
        if target == item:  # Do not want to continue the for loop once the node with the target is identified
            chain_of_ancestors = []
            break
            
            
        temp_dict_2 = temp_dict # initialize
        while target not in temp_dict_2: # while we are not returning the target, we will keep indexing deeper into the tree 


            # check each item in the current layer of the tree
            for item in temp_dict:
                temp_dict_2 = {}  # reset this is a dictionary to be used for check the various nodes in a branch
                history.append(item)
                chain_of_ancestors.append(item)
                temp_dict_2.update(temp_dict[item])  # add the new terms to the dictionary
                if target in temp_dict_2:  # check if the target is at these nodes
                    break

            temp_dict = temp_dict_2  # this will be a dictionary containing all of the items in a branch
            # break when the temp dictionary is empty, check after updating the temp_dict
            if temp_dict_2 == {}:
                break

        if target in temp_dict:  # Do not want to continue the for loop once the node with the target is identified
            for items in temp_dict:
                if items != target:
                    chain_of_ancestors.append(items)
                    history.append(items)
                else:
                    break
            chain_of_ancestors.append(target)
            history.append(target)
            break

        if target not in temp_dict_2:
            chain_of_ancestors = []
        
   
    return chain_of_ancestors, history # lists of strings",TODO,history,"def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    new_dict = tree
    history = []
    chain_of_ancestors = []
    
    for item in new_dict:  # Checks each of the items in the dictionary
        temp_dict = new_dict[item]

        history.append(item) # tracks every item traversed in the tree
        chain_of_ancestors = [item]  # reset for every branch of the tree
        
        # check if present in first layer
        if target == item:  # Do not want to continue the for loop once the node with the target is identified
            chain_of_ancestors = []
            break
            
            
        temp_dict_2 = temp_dict # initialize
        while target not in temp_dict_2: # while we are not returning the target, we will keep indexing deeper into the tree 


            # check each item in the current layer of the tree
            for item in temp_dict:
                temp_dict_2 = {}  # reset this is a dictionary to be used for check the various nodes in a branch
                history.append(item)
                chain_of_ancestors.append(item)
                temp_dict_2.update(temp_dict[item])  # add the new terms to the dictionary
                if target in temp_dict_2:  # check if the target is at these nodes
                    break

            temp_dict = temp_dict_2  # this will be a dictionary containing all of the items in a branch
            # break when the temp dictionary is empty, check after updating the temp_dict
            if temp_dict_2 == {}:
                break

        if target in temp_dict:  # Do not want to continue the for loop once the node with the target is identified
            for items in temp_dict:
                if items != target:
                    chain_of_ancestors.append(items)
                    history.append(items)
                else:
                    break
            chain_of_ancestors.append(target)
            history.append(target)
            break

        if target not in temp_dict_2:
            chain_of_ancestors = []
        
   
    return chain_of_ancestors, history # lists of strings


#     raise NotImplementedError(""Complete this function!"")","def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    new_dict = tree
    history = []
    chain_of_ancestors = []
    
    for item in new_dict:  # Checks each of the items in the dictionary
        temp_dict = new_dict[item]

        history.append(item) # tracks every item traversed in the tree
        chain_of_ancestors = [item]  # reset for every branch of the tree
        
        # check if present in first layer
        if target == item:  # Do not want to continue the for loop once the node with the target is identified
            chain_of_ancestors = []
            break
            
            
        temp_dict_2 = temp_dict # initialize
        while target not in temp_dict_2: # while we are not returning the target, we will keep indexing deeper into the tree 


            # check each item in the current layer of the tree
            for item in temp_dict:
                temp_dict_2 = {}  # reset this is a dictionary to be used for check the various nodes in a branch
                history.append(item)
                chain_of_ancestors.append(item)
                temp_dict_2.update(temp_dict[item])  # add the new terms to the dictionary
                if target in temp_dict_2:  # check if the target is at these nodes
                    break

            temp_dict = temp_dict_2  # this will be a dictionary containing all of the items in a branch
            # break when the temp dictionary is empty, check after updating the temp_dict
            if temp_dict_2 == {}:
                break

        if target in temp_dict:  # Do not want to continue the for loop once the node with the target is identified
            for items in temp_dict:
                if items != target:
                    chain_of_ancestors.append(items)
                    history.append(items)
                else:
                    break
            chain_of_ancestors.append(target)
            history.append(target)
            break

        if target not in temp_dict_2:
            chain_of_ancestors = []
        
   
    return chain_of_ancestors, history # lists of strings


#     raise NotImplementedError(""Complete this function!"")
# tree = {
#       '1' : {'2':{}},
#       '3' : {'4':{}, '5':{}},
#       '6' : {'7': {'8': {'9':{}, '10': {}}}},
#       '11' :{} 
#     }

# # print(list(tree['3'])[0])
# target = '3'

# print(dfs(target,tree))
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
3429,3,"    chain_of_ancestors = []
    
    for item in new_dict:  # Checks each of the items in the dictionary
        temp_dict = new_dict[item]

        history.append(item) # tracks every item traversed in the tree
        chain_of_ancestors = [item]  # reset for every branch of the tree
        
        # check if present in first layer
        if target == item:  # Do not want to continue the for loop once the node with the target is identified
            chain_of_ancestors = []
            break
            
            
        temp_dict_2 = temp_dict # initialize
        while target not in temp_dict_2: # while we are not returning the target, we will keep indexing deeper into the tree 


            # check each item in the current layer of the tree
            for item in temp_dict:
                temp_dict_2 = {}  # reset this is a dictionary to be used for check the various nodes in a branch
                history.append(item)
                chain_of_ancestors.append(item)
                temp_dict_2.update(temp_dict[item])  # add the new terms to the dictionary
                if target in temp_dict_2:  # check if the target is at these nodes
                    break

            temp_dict = temp_dict_2  # this will be a dictionary containing all of the items in a branch
            # break when the temp dictionary is empty, check after updating the temp_dict
            if temp_dict_2 == {}:
                break

        if target in temp_dict:  # Do not want to continue the for loop once the node with the target is identified
            for items in temp_dict:
                if items != target:
                    chain_of_ancestors.append(items)
                    history.append(items)
                else:
                    break
            chain_of_ancestors.append(target)
            history.append(target)
            break

        if target not in temp_dict_2:
            chain_of_ancestors = []
        
   
    return chain_of_ancestors, history # lists of strings",TODO,chain_of_ancestors,"def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    new_dict = tree
    history = []
    chain_of_ancestors = []
    
    for item in new_dict:  # Checks each of the items in the dictionary
        temp_dict = new_dict[item]

        history.append(item) # tracks every item traversed in the tree
        chain_of_ancestors = [item]  # reset for every branch of the tree
        
        # check if present in first layer
        if target == item:  # Do not want to continue the for loop once the node with the target is identified
            chain_of_ancestors = []
            break
            
            
        temp_dict_2 = temp_dict # initialize
        while target not in temp_dict_2: # while we are not returning the target, we will keep indexing deeper into the tree 


            # check each item in the current layer of the tree
            for item in temp_dict:
                temp_dict_2 = {}  # reset this is a dictionary to be used for check the various nodes in a branch
                history.append(item)
                chain_of_ancestors.append(item)
                temp_dict_2.update(temp_dict[item])  # add the new terms to the dictionary
                if target in temp_dict_2:  # check if the target is at these nodes
                    break

            temp_dict = temp_dict_2  # this will be a dictionary containing all of the items in a branch
            # break when the temp dictionary is empty, check after updating the temp_dict
            if temp_dict_2 == {}:
                break

        if target in temp_dict:  # Do not want to continue the for loop once the node with the target is identified
            for items in temp_dict:
                if items != target:
                    chain_of_ancestors.append(items)
                    history.append(items)
                else:
                    break
            chain_of_ancestors.append(target)
            history.append(target)
            break

        if target not in temp_dict_2:
            chain_of_ancestors = []
        
   
    return chain_of_ancestors, history # lists of strings


#     raise NotImplementedError(""Complete this function!"")","def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    new_dict = tree
    history = []
    chain_of_ancestors = []
    
    for item in new_dict:  # Checks each of the items in the dictionary
        temp_dict = new_dict[item]

        history.append(item) # tracks every item traversed in the tree
        chain_of_ancestors = [item]  # reset for every branch of the tree
        
        # check if present in first layer
        if target == item:  # Do not want to continue the for loop once the node with the target is identified
            chain_of_ancestors = []
            break
            
            
        temp_dict_2 = temp_dict # initialize
        while target not in temp_dict_2: # while we are not returning the target, we will keep indexing deeper into the tree 


            # check each item in the current layer of the tree
            for item in temp_dict:
                temp_dict_2 = {}  # reset this is a dictionary to be used for check the various nodes in a branch
                history.append(item)
                chain_of_ancestors.append(item)
                temp_dict_2.update(temp_dict[item])  # add the new terms to the dictionary
                if target in temp_dict_2:  # check if the target is at these nodes
                    break

            temp_dict = temp_dict_2  # this will be a dictionary containing all of the items in a branch
            # break when the temp dictionary is empty, check after updating the temp_dict
            if temp_dict_2 == {}:
                break

        if target in temp_dict:  # Do not want to continue the for loop once the node with the target is identified
            for items in temp_dict:
                if items != target:
                    chain_of_ancestors.append(items)
                    history.append(items)
                else:
                    break
            chain_of_ancestors.append(target)
            history.append(target)
            break

        if target not in temp_dict_2:
            chain_of_ancestors = []
        
   
    return chain_of_ancestors, history # lists of strings


#     raise NotImplementedError(""Complete this function!"")
# tree = {
#       '1' : {'2':{}},
#       '3' : {'4':{}, '5':{}},
#       '6' : {'7': {'8': {'9':{}, '10': {}}}},
#       '11' :{} 
#     }

# # print(list(tree['3'])[0])
# target = '3'

# print(dfs(target,tree))
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
3429,4,"        temp_dict = new_dict[item]

        history.append(item) # tracks every item traversed in the tree
        chain_of_ancestors = [item]  # reset for every branch of the tree
        
        # check if present in first layer
        if target == item:  # Do not want to continue the for loop once the node with the target is identified
            chain_of_ancestors = []
            break
            
            
        temp_dict_2 = temp_dict # initialize
        while target not in temp_dict_2: # while we are not returning the target, we will keep indexing deeper into the tree 


            # check each item in the current layer of the tree
            for item in temp_dict:
                temp_dict_2 = {}  # reset this is a dictionary to be used for check the various nodes in a branch
                history.append(item)
                chain_of_ancestors.append(item)
                temp_dict_2.update(temp_dict[item])  # add the new terms to the dictionary
                if target in temp_dict_2:  # check if the target is at these nodes
                    break

            temp_dict = temp_dict_2  # this will be a dictionary containing all of the items in a branch
            # break when the temp dictionary is empty, check after updating the temp_dict
            if temp_dict_2 == {}:
                break

        if target in temp_dict:  # Do not want to continue the for loop once the node with the target is identified
            for items in temp_dict:",TODO,temp_dict,"def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    new_dict = tree
    history = []
    chain_of_ancestors = []
    
    for item in new_dict:  # Checks each of the items in the dictionary
        temp_dict = new_dict[item]

        history.append(item) # tracks every item traversed in the tree
        chain_of_ancestors = [item]  # reset for every branch of the tree
        
        # check if present in first layer
        if target == item:  # Do not want to continue the for loop once the node with the target is identified
            chain_of_ancestors = []
            break
            
            
        temp_dict_2 = temp_dict # initialize
        while target not in temp_dict_2: # while we are not returning the target, we will keep indexing deeper into the tree 


            # check each item in the current layer of the tree
            for item in temp_dict:
                temp_dict_2 = {}  # reset this is a dictionary to be used for check the various nodes in a branch
                history.append(item)
                chain_of_ancestors.append(item)
                temp_dict_2.update(temp_dict[item])  # add the new terms to the dictionary
                if target in temp_dict_2:  # check if the target is at these nodes
                    break

            temp_dict = temp_dict_2  # this will be a dictionary containing all of the items in a branch
            # break when the temp dictionary is empty, check after updating the temp_dict
            if temp_dict_2 == {}:
                break

        if target in temp_dict:  # Do not want to continue the for loop once the node with the target is identified
            for items in temp_dict:
                if items != target:
                    chain_of_ancestors.append(items)
                    history.append(items)
                else:
                    break
            chain_of_ancestors.append(target)
            history.append(target)
            break

        if target not in temp_dict_2:
            chain_of_ancestors = []
        
   
    return chain_of_ancestors, history # lists of strings


#     raise NotImplementedError(""Complete this function!"")","def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    new_dict = tree
    history = []
    chain_of_ancestors = []
    
    for item in new_dict:  # Checks each of the items in the dictionary
        temp_dict = new_dict[item]

        history.append(item) # tracks every item traversed in the tree
        chain_of_ancestors = [item]  # reset for every branch of the tree
        
        # check if present in first layer
        if target == item:  # Do not want to continue the for loop once the node with the target is identified
            chain_of_ancestors = []
            break
            
            
        temp_dict_2 = temp_dict # initialize
        while target not in temp_dict_2: # while we are not returning the target, we will keep indexing deeper into the tree 


            # check each item in the current layer of the tree
            for item in temp_dict:
                temp_dict_2 = {}  # reset this is a dictionary to be used for check the various nodes in a branch
                history.append(item)
                chain_of_ancestors.append(item)
                temp_dict_2.update(temp_dict[item])  # add the new terms to the dictionary
                if target in temp_dict_2:  # check if the target is at these nodes
                    break

            temp_dict = temp_dict_2  # this will be a dictionary containing all of the items in a branch
            # break when the temp dictionary is empty, check after updating the temp_dict
            if temp_dict_2 == {}:
                break

        if target in temp_dict:  # Do not want to continue the for loop once the node with the target is identified
            for items in temp_dict:
                if items != target:
                    chain_of_ancestors.append(items)
                    history.append(items)
                else:
                    break
            chain_of_ancestors.append(target)
            history.append(target)
            break

        if target not in temp_dict_2:
            chain_of_ancestors = []
        
   
    return chain_of_ancestors, history # lists of strings


#     raise NotImplementedError(""Complete this function!"")
# tree = {
#       '1' : {'2':{}},
#       '3' : {'4':{}, '5':{}},
#       '6' : {'7': {'8': {'9':{}, '10': {}}}},
#       '11' :{} 
#     }

# # print(list(tree['3'])[0])
# target = '3'

# print(dfs(target,tree))
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
3429,5,"        temp_dict_2 = temp_dict # initialize
        while target not in temp_dict_2: # while we are not returning the target, we will keep indexing deeper into the tree 


            # check each item in the current layer of the tree
            for item in temp_dict:
                temp_dict_2 = {}  # reset this is a dictionary to be used for check the various nodes in a branch
                history.append(item)
                chain_of_ancestors.append(item)
                temp_dict_2.update(temp_dict[item])  # add the new terms to the dictionary
                if target in temp_dict_2:  # check if the target is at these nodes
                    break

            temp_dict = temp_dict_2  # this will be a dictionary containing all of the items in a branch
            # break when the temp dictionary is empty, check after updating the temp_dict
            if temp_dict_2 == {}:
                break

        if target in temp_dict:  # Do not want to continue the for loop once the node with the target is identified
            for items in temp_dict:
                if items != target:
                    chain_of_ancestors.append(items)
                    history.append(items)
                else:
                    break
            chain_of_ancestors.append(target)
            history.append(target)
            break

        if target not in temp_dict_2:",TODO,temp_dict_2,"def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    new_dict = tree
    history = []
    chain_of_ancestors = []
    
    for item in new_dict:  # Checks each of the items in the dictionary
        temp_dict = new_dict[item]

        history.append(item) # tracks every item traversed in the tree
        chain_of_ancestors = [item]  # reset for every branch of the tree
        
        # check if present in first layer
        if target == item:  # Do not want to continue the for loop once the node with the target is identified
            chain_of_ancestors = []
            break
            
            
        temp_dict_2 = temp_dict # initialize
        while target not in temp_dict_2: # while we are not returning the target, we will keep indexing deeper into the tree 


            # check each item in the current layer of the tree
            for item in temp_dict:
                temp_dict_2 = {}  # reset this is a dictionary to be used for check the various nodes in a branch
                history.append(item)
                chain_of_ancestors.append(item)
                temp_dict_2.update(temp_dict[item])  # add the new terms to the dictionary
                if target in temp_dict_2:  # check if the target is at these nodes
                    break

            temp_dict = temp_dict_2  # this will be a dictionary containing all of the items in a branch
            # break when the temp dictionary is empty, check after updating the temp_dict
            if temp_dict_2 == {}:
                break

        if target in temp_dict:  # Do not want to continue the for loop once the node with the target is identified
            for items in temp_dict:
                if items != target:
                    chain_of_ancestors.append(items)
                    history.append(items)
                else:
                    break
            chain_of_ancestors.append(target)
            history.append(target)
            break

        if target not in temp_dict_2:
            chain_of_ancestors = []
        
   
    return chain_of_ancestors, history # lists of strings


#     raise NotImplementedError(""Complete this function!"")","def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    new_dict = tree
    history = []
    chain_of_ancestors = []
    
    for item in new_dict:  # Checks each of the items in the dictionary
        temp_dict = new_dict[item]

        history.append(item) # tracks every item traversed in the tree
        chain_of_ancestors = [item]  # reset for every branch of the tree
        
        # check if present in first layer
        if target == item:  # Do not want to continue the for loop once the node with the target is identified
            chain_of_ancestors = []
            break
            
            
        temp_dict_2 = temp_dict # initialize
        while target not in temp_dict_2: # while we are not returning the target, we will keep indexing deeper into the tree 


            # check each item in the current layer of the tree
            for item in temp_dict:
                temp_dict_2 = {}  # reset this is a dictionary to be used for check the various nodes in a branch
                history.append(item)
                chain_of_ancestors.append(item)
                temp_dict_2.update(temp_dict[item])  # add the new terms to the dictionary
                if target in temp_dict_2:  # check if the target is at these nodes
                    break

            temp_dict = temp_dict_2  # this will be a dictionary containing all of the items in a branch
            # break when the temp dictionary is empty, check after updating the temp_dict
            if temp_dict_2 == {}:
                break

        if target in temp_dict:  # Do not want to continue the for loop once the node with the target is identified
            for items in temp_dict:
                if items != target:
                    chain_of_ancestors.append(items)
                    history.append(items)
                else:
                    break
            chain_of_ancestors.append(target)
            history.append(target)
            break

        if target not in temp_dict_2:
            chain_of_ancestors = []
        
   
    return chain_of_ancestors, history # lists of strings


#     raise NotImplementedError(""Complete this function!"")
# tree = {
#       '1' : {'2':{}},
#       '3' : {'4':{}, '5':{}},
#       '6' : {'7': {'8': {'9':{}, '10': {}}}},
#       '11' :{} 
#     }

# # print(list(tree['3'])[0])
# target = '3'

# print(dfs(target,tree))
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
8199,1,"    chain_of_ancestors = []
    history = []
    def dfs_helper(target, temp_tree, chain_of_ancestors, history):
        nodes = temp_tree.keys()
        if len(nodes) == 0:
          return [], history
        if target in nodes:
          return chain_of_ancestors + [target], history + [target]
        
        for x in nodes:
          if x not in history:
            chain_of_ancestors = chain_of_ancestors + [x]
            history = history + [x]
            chain_of_ancestors, history = dfs_helper(target, temp_tree[x], chain_of_ancestors, history)
            if target in history:
              return chain_of_ancestors, history
        return [], history
        
    return dfs_helper(target, tree, chain_of_ancestors, history)",TODO,chain_of_ancestors,"from pandas.core.window.expanding import template_returns

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    chain_of_ancestors = []
    history = []
    def dfs_helper(target, temp_tree, chain_of_ancestors, history):
        nodes = temp_tree.keys()
        if len(nodes) == 0:
          return [], history
        if target in nodes:
          return chain_of_ancestors + [target], history + [target]
        
        for x in nodes:
          if x not in history:
            chain_of_ancestors = chain_of_ancestors + [x]
            history = history + [x]
            chain_of_ancestors, history = dfs_helper(target, temp_tree[x], chain_of_ancestors, history)
            if target in history:
              return chain_of_ancestors, history
        return [], history
        
    return dfs_helper(target, tree, chain_of_ancestors, history)","from pandas.core.window.expanding import template_returns

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    chain_of_ancestors = []
    history = []
    def dfs_helper(target, temp_tree, chain_of_ancestors, history):
        nodes = temp_tree.keys()
        if len(nodes) == 0:
          return [], history
        if target in nodes:
          return chain_of_ancestors + [target], history + [target]
        
        for x in nodes:
          if x not in history:
            chain_of_ancestors = chain_of_ancestors + [x]
            history = history + [x]
            chain_of_ancestors, history = dfs_helper(target, temp_tree[x], chain_of_ancestors, history)
            if target in history:
              return chain_of_ancestors, history
        return [], history
        
    return dfs_helper(target, tree, chain_of_ancestors, history)
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']

    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
8199,2,"    history = []
    def dfs_helper(target, temp_tree, chain_of_ancestors, history):
        nodes = temp_tree.keys()
        if len(nodes) == 0:
          return [], history
        if target in nodes:
          return chain_of_ancestors + [target], history + [target]
        
        for x in nodes:
          if x not in history:
            chain_of_ancestors = chain_of_ancestors + [x]
            history = history + [x]
            chain_of_ancestors, history = dfs_helper(target, temp_tree[x], chain_of_ancestors, history)
            if target in history:
              return chain_of_ancestors, history
        return [], history
        
    return dfs_helper(target, tree, chain_of_ancestors, history)",TODO,history,"from pandas.core.window.expanding import template_returns

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    chain_of_ancestors = []
    history = []
    def dfs_helper(target, temp_tree, chain_of_ancestors, history):
        nodes = temp_tree.keys()
        if len(nodes) == 0:
          return [], history
        if target in nodes:
          return chain_of_ancestors + [target], history + [target]
        
        for x in nodes:
          if x not in history:
            chain_of_ancestors = chain_of_ancestors + [x]
            history = history + [x]
            chain_of_ancestors, history = dfs_helper(target, temp_tree[x], chain_of_ancestors, history)
            if target in history:
              return chain_of_ancestors, history
        return [], history
        
    return dfs_helper(target, tree, chain_of_ancestors, history)","from pandas.core.window.expanding import template_returns

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    chain_of_ancestors = []
    history = []
    def dfs_helper(target, temp_tree, chain_of_ancestors, history):
        nodes = temp_tree.keys()
        if len(nodes) == 0:
          return [], history
        if target in nodes:
          return chain_of_ancestors + [target], history + [target]
        
        for x in nodes:
          if x not in history:
            chain_of_ancestors = chain_of_ancestors + [x]
            history = history + [x]
            chain_of_ancestors, history = dfs_helper(target, temp_tree[x], chain_of_ancestors, history)
            if target in history:
              return chain_of_ancestors, history
        return [], history
        
    return dfs_helper(target, tree, chain_of_ancestors, history)
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']

    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
8199,3,"        nodes = temp_tree.keys()
        if len(nodes) == 0:
          return [], history
        if target in nodes:
          return chain_of_ancestors + [target], history + [target]
        
        for x in nodes:",TODO,nodes,"from pandas.core.window.expanding import template_returns

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    chain_of_ancestors = []
    history = []
    def dfs_helper(target, temp_tree, chain_of_ancestors, history):
        nodes = temp_tree.keys()
        if len(nodes) == 0:
          return [], history
        if target in nodes:
          return chain_of_ancestors + [target], history + [target]
        
        for x in nodes:
          if x not in history:
            chain_of_ancestors = chain_of_ancestors + [x]
            history = history + [x]
            chain_of_ancestors, history = dfs_helper(target, temp_tree[x], chain_of_ancestors, history)
            if target in history:
              return chain_of_ancestors, history
        return [], history
        
    return dfs_helper(target, tree, chain_of_ancestors, history)","from pandas.core.window.expanding import template_returns

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''

    chain_of_ancestors = []
    history = []
    def dfs_helper(target, temp_tree, chain_of_ancestors, history):
        nodes = temp_tree.keys()
        if len(nodes) == 0:
          return [], history
        if target in nodes:
          return chain_of_ancestors + [target], history + [target]
        
        for x in nodes:
          if x not in history:
            chain_of_ancestors = chain_of_ancestors + [x]
            history = history + [x]
            chain_of_ancestors, history = dfs_helper(target, temp_tree[x], chain_of_ancestors, history)
            if target in history:
              return chain_of_ancestors, history
        return [], history
        
    return dfs_helper(target, tree, chain_of_ancestors, history)
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']

    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
8045,1,"      indices.append(history.index(root))
    indices, history = full_depth(tree[root],history, keys, indices)
  return indices,history",TODO,indices,"def full_depth(tree, history, keys, indices):
  for root in list(tree.keys()):
    history.append(root)
    if root in keys:
      indices.append(history.index(root))
    indices, history = full_depth(tree[root],history, keys, indices)
  return indices,history","def full_depth(tree, history, keys, indices):
  for root in list(tree.keys()):
    history.append(root)
    if root in keys:
      indices.append(history.index(root))
    indices, history = full_depth(tree[root],history, keys, indices)
  return indices,history
def dfs(target, tree):
    indices,complete_path = full_depth(tree, [], list(tree.keys()),[])
    if target in complete_path:
      history_ref = complete_path[ : complete_path.index(target)+1]
      for i in indices:
        if i<= complete_path.index(target):
          start = i
      parents_ref = complete_path[start:complete_path.index(target)+1]
    else:
      parents_ref = []
      history_ref = complete_path

    return parents_ref,history_ref

    raise NotImplementedError(""Complete this function!"")

dfs('9',{
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    )
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
8045,2,"    history.append(root)
    if root in keys:
      indices.append(history.index(root))
    indices, history = full_depth(tree[root],history, keys, indices)
  return indices,history",TODO,history,"def full_depth(tree, history, keys, indices):
  for root in list(tree.keys()):
    history.append(root)
    if root in keys:
      indices.append(history.index(root))
    indices, history = full_depth(tree[root],history, keys, indices)
  return indices,history","def full_depth(tree, history, keys, indices):
  for root in list(tree.keys()):
    history.append(root)
    if root in keys:
      indices.append(history.index(root))
    indices, history = full_depth(tree[root],history, keys, indices)
  return indices,history
def dfs(target, tree):
    indices,complete_path = full_depth(tree, [], list(tree.keys()),[])
    if target in complete_path:
      history_ref = complete_path[ : complete_path.index(target)+1]
      for i in indices:
        if i<= complete_path.index(target):
          start = i
      parents_ref = complete_path[start:complete_path.index(target)+1]
    else:
      parents_ref = []
      history_ref = complete_path

    return parents_ref,history_ref

    raise NotImplementedError(""Complete this function!"")

dfs('9',{
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    )
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
8500,1,"    chain_of_ancestors = []
    history = []

    for key in tree.keys():
        history.append(key)
        if key == target:
            chain_of_ancestors.append(key)
            return chain_of_ancestors, history
        sub_chain_of_ancestors, sub_history = dfs(target, tree[key])
        history = history + sub_history
        if len(sub_chain_of_ancestors) != 0:
            chain_of_ancestors = [key] + sub_chain_of_ancestors
            break
    
    return chain_of_ancestors, history",TODO,chain_of_ancestors,"def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    chain_of_ancestors = []
    history = []

    for key in tree.keys():
        history.append(key)
        if key == target:
            chain_of_ancestors.append(key)
            return chain_of_ancestors, history
        sub_chain_of_ancestors, sub_history = dfs(target, tree[key])
        history = history + sub_history
        if len(sub_chain_of_ancestors) != 0:
            chain_of_ancestors = [key] + sub_chain_of_ancestors
            break
    
    return chain_of_ancestors, history","def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    chain_of_ancestors = []
    history = []

    for key in tree.keys():
        history.append(key)
        if key == target:
            chain_of_ancestors.append(key)
            return chain_of_ancestors, history
        sub_chain_of_ancestors, sub_history = dfs(target, tree[key])
        history = history + sub_history
        if len(sub_chain_of_ancestors) != 0:
            chain_of_ancestors = [key] + sub_chain_of_ancestors
            break
    
    return chain_of_ancestors, history
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
8500,2,"    history = []

    for key in tree.keys():
        history.append(key)
        if key == target:
            chain_of_ancestors.append(key)
            return chain_of_ancestors, history
        sub_chain_of_ancestors, sub_history = dfs(target, tree[key])
        history = history + sub_history
        if len(sub_chain_of_ancestors) != 0:
            chain_of_ancestors = [key] + sub_chain_of_ancestors
            break
    
    return chain_of_ancestors, history",TODO,history,"def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    chain_of_ancestors = []
    history = []

    for key in tree.keys():
        history.append(key)
        if key == target:
            chain_of_ancestors.append(key)
            return chain_of_ancestors, history
        sub_chain_of_ancestors, sub_history = dfs(target, tree[key])
        history = history + sub_history
        if len(sub_chain_of_ancestors) != 0:
            chain_of_ancestors = [key] + sub_chain_of_ancestors
            break
    
    return chain_of_ancestors, history","def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    chain_of_ancestors = []
    history = []

    for key in tree.keys():
        history.append(key)
        if key == target:
            chain_of_ancestors.append(key)
            return chain_of_ancestors, history
        sub_chain_of_ancestors, sub_history = dfs(target, tree[key])
        history = history + sub_history
        if len(sub_chain_of_ancestors) != 0:
            chain_of_ancestors = [key] + sub_chain_of_ancestors
            break
    
    return chain_of_ancestors, history
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
8500,3,"        sub_chain_of_ancestors, sub_history = dfs(target, tree[key])
        history = history + sub_history
        if len(sub_chain_of_ancestors) != 0:
            chain_of_ancestors = [key] + sub_chain_of_ancestors",TODO,sub_chain_of_ancestors,"def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    chain_of_ancestors = []
    history = []

    for key in tree.keys():
        history.append(key)
        if key == target:
            chain_of_ancestors.append(key)
            return chain_of_ancestors, history
        sub_chain_of_ancestors, sub_history = dfs(target, tree[key])
        history = history + sub_history
        if len(sub_chain_of_ancestors) != 0:
            chain_of_ancestors = [key] + sub_chain_of_ancestors
            break
    
    return chain_of_ancestors, history","def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    chain_of_ancestors = []
    history = []

    for key in tree.keys():
        history.append(key)
        if key == target:
            chain_of_ancestors.append(key)
            return chain_of_ancestors, history
        sub_chain_of_ancestors, sub_history = dfs(target, tree[key])
        history = history + sub_history
        if len(sub_chain_of_ancestors) != 0:
            chain_of_ancestors = [key] + sub_chain_of_ancestors
            break
    
    return chain_of_ancestors, history
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
8500,4,"        sub_chain_of_ancestors, sub_history = dfs(target, tree[key])
        history = history + sub_history",TODO,sub_history,"def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    chain_of_ancestors = []
    history = []

    for key in tree.keys():
        history.append(key)
        if key == target:
            chain_of_ancestors.append(key)
            return chain_of_ancestors, history
        sub_chain_of_ancestors, sub_history = dfs(target, tree[key])
        history = history + sub_history
        if len(sub_chain_of_ancestors) != 0:
            chain_of_ancestors = [key] + sub_chain_of_ancestors
            break
    
    return chain_of_ancestors, history","def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    chain_of_ancestors = []
    history = []

    for key in tree.keys():
        history.append(key)
        if key == target:
            chain_of_ancestors.append(key)
            return chain_of_ancestors, history
        sub_chain_of_ancestors, sub_history = dfs(target, tree[key])
        history = history + sub_history
        if len(sub_chain_of_ancestors) != 0:
            chain_of_ancestors = [key] + sub_chain_of_ancestors
            break
    
    return chain_of_ancestors, history
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
3135,1,"parents = deque([])
history = []

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    
    
    #     #put the children of the current node in the que to be visited
    #     for key, val in list(tree.items()):
    #         parents.append(key)

    #     dfs(target, tree[curnode])

    #subproblem: take the current node, see if it is the target
      
    def traverse(target, tree):
        curnode = parents.popleft()
        history.append(curnode)
        if curnode == target: return parents, history
        else:
            for key, val in list(tree.items()):
                parents.append(key)      ",TODO,parents,"from collections import deque#remove

parents = deque([])
history = []

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    
    
    #     #put the children of the current node in the que to be visited
    #     for key, val in list(tree.items()):
    #         parents.append(key)

    #     dfs(target, tree[curnode])

    #subproblem: take the current node, see if it is the target
      
    def traverse(target, tree):
        curnode = parents.popleft()
        history.append(curnode)
        if curnode == target: return parents, history
        else:
            for key, val in list(tree.items()):
                parents.append(key)      
            traverse(target, tree[curnode])
            print(tree[curnode])

    traverse(target, tree)



tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
dfs(9, tree)","from collections import deque#remove

parents = deque([])
history = []

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    
    
    #     #put the children of the current node in the que to be visited
    #     for key, val in list(tree.items()):
    #         parents.append(key)

    #     dfs(target, tree[curnode])

    #subproblem: take the current node, see if it is the target
      
    def traverse(target, tree):
        curnode = parents.popleft()
        history.append(curnode)
        if curnode == target: return parents, history
        else:
            for key, val in list(tree.items()):
                parents.append(key)      
            traverse(target, tree[curnode])
            print(tree[curnode])

    traverse(target, tree)



tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
dfs(9, tree)
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
3135,2,"history = []

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    
    
    #     #put the children of the current node in the que to be visited
    #     for key, val in list(tree.items()):
    #         parents.append(key)

    #     dfs(target, tree[curnode])

    #subproblem: take the current node, see if it is the target
      
    def traverse(target, tree):
        curnode = parents.popleft()
        history.append(curnode)
        if curnode == target: return parents, history",TODO,history,"from collections import deque#remove

parents = deque([])
history = []

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    
    
    #     #put the children of the current node in the que to be visited
    #     for key, val in list(tree.items()):
    #         parents.append(key)

    #     dfs(target, tree[curnode])

    #subproblem: take the current node, see if it is the target
      
    def traverse(target, tree):
        curnode = parents.popleft()
        history.append(curnode)
        if curnode == target: return parents, history
        else:
            for key, val in list(tree.items()):
                parents.append(key)      
            traverse(target, tree[curnode])
            print(tree[curnode])

    traverse(target, tree)



tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
dfs(9, tree)","from collections import deque#remove

parents = deque([])
history = []

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    
    
    #     #put the children of the current node in the que to be visited
    #     for key, val in list(tree.items()):
    #         parents.append(key)

    #     dfs(target, tree[curnode])

    #subproblem: take the current node, see if it is the target
      
    def traverse(target, tree):
        curnode = parents.popleft()
        history.append(curnode)
        if curnode == target: return parents, history
        else:
            for key, val in list(tree.items()):
                parents.append(key)      
            traverse(target, tree[curnode])
            print(tree[curnode])

    traverse(target, tree)



tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
dfs(9, tree)
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
3135,3,"            for key, val in list(tree.items()):
                parents.append(key)      
            traverse(target, tree[curnode])
            print(tree[curnode])

    traverse(target, tree)



tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
dfs(9, tree)",TODO,tree,"from collections import deque#remove

parents = deque([])
history = []

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    
    
    #     #put the children of the current node in the que to be visited
    #     for key, val in list(tree.items()):
    #         parents.append(key)

    #     dfs(target, tree[curnode])

    #subproblem: take the current node, see if it is the target
      
    def traverse(target, tree):
        curnode = parents.popleft()
        history.append(curnode)
        if curnode == target: return parents, history
        else:
            for key, val in list(tree.items()):
                parents.append(key)      
            traverse(target, tree[curnode])
            print(tree[curnode])

    traverse(target, tree)



tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
dfs(9, tree)","from collections import deque#remove

parents = deque([])
history = []

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    
    
    #     #put the children of the current node in the que to be visited
    #     for key, val in list(tree.items()):
    #         parents.append(key)

    #     dfs(target, tree[curnode])

    #subproblem: take the current node, see if it is the target
      
    def traverse(target, tree):
        curnode = parents.popleft()
        history.append(curnode)
        if curnode == target: return parents, history
        else:
            for key, val in list(tree.items()):
                parents.append(key)      
            traverse(target, tree[curnode])
            print(tree[curnode])

    traverse(target, tree)



tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
dfs(9, tree)
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
3135,4,"        curnode = parents.popleft()
        history.append(curnode)
        if curnode == target: return parents, history
        else:
            for key, val in list(tree.items()):
                parents.append(key)      
            traverse(target, tree[curnode])
            print(tree[curnode])",TODO,curnode,"from collections import deque#remove

parents = deque([])
history = []

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    
    
    #     #put the children of the current node in the que to be visited
    #     for key, val in list(tree.items()):
    #         parents.append(key)

    #     dfs(target, tree[curnode])

    #subproblem: take the current node, see if it is the target
      
    def traverse(target, tree):
        curnode = parents.popleft()
        history.append(curnode)
        if curnode == target: return parents, history
        else:
            for key, val in list(tree.items()):
                parents.append(key)      
            traverse(target, tree[curnode])
            print(tree[curnode])

    traverse(target, tree)



tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
dfs(9, tree)","from collections import deque#remove

parents = deque([])
history = []

def dfs(target, tree):
    '''
    Find the chain of referrals leading to a target user ID
    
    args:
        target (String) : the target user id
        tree (dict) - the referral poly-tree represented in nested dictionary format

    returns:
        chain_of_ancestors (List[String]) - chain of parents of the target node, or empty list if the target node has no parent
        history (List[String]) - the full DFS traversal sequence leading to the target node
    '''
    
    
    #     #put the children of the current node in the que to be visited
    #     for key, val in list(tree.items()):
    #         parents.append(key)

    #     dfs(target, tree[curnode])

    #subproblem: take the current node, see if it is the target
      
    def traverse(target, tree):
        curnode = parents.popleft()
        history.append(curnode)
        if curnode == target: return parents, history
        else:
            for key, val in list(tree.items()):
                parents.append(key)      
            traverse(target, tree[curnode])
            print(tree[curnode])

    traverse(target, tree)



tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
dfs(9, tree)
def test_dfs():
    tree = {
      '1' : {'2':{}},
      '3' : {'4':{}, '5':{}},
      '6' : {'7': {'8': {'9':{}, '10': {}}}},
      '11' : {} 
    }
    
    target = '9'
    parents_ref = ['6', '7', '8', '9']
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node is in a tree!""
    
    target = '111'
    parents_ref = []
    history_ref = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']
    
    assert dfs(target, tree) == (parents_ref, history_ref), ""Check you implementation for when node can not be found!""
    print(""All tests passed!"")

test_dfs()"
12823,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
12823,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
11213,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
11213,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
7519,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
7519,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
12781,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
12781,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
6335,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
6335,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
12186,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
12186,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
12494,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
12494,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
4480,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
4480,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
12648,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
12648,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
7675,1,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)",TODO,test_string,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
7675,2,"predicted_rating = predict(test_string)
print(predicted_rating)",TODO,predicted_rating,"test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)","test_string = 'This course is great and I feel like I am learning a lot!'
predicted_rating = predict(test_string)
print(predicted_rating)"
4495,1,"            lines = f.readlines()
        
        for line in lines:
            if self.most_recent_disk.isspace():
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break
            elif line.replace('\n', '').split(' ')[0] != self.most_recent_disk:
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break  
        with open(self.cache_disk, ""w"") as f:
            for line in lines:",TODO,lines,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.most_recent_ram = ''
        self.most_recent_disk = ''
    
    def get_least_recent_ram(self):
        '''
        Return least recent ram value. 
        return: key (Integer) - least recent RAM value
        '''
        for key in self.cache_ram.keys():
            if key != self.most_recent_ram:
                return key
    
    def remove_least_recent_disk(self):
        '''
        Remove least recent value from disk cache. 
        '''
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
        
        for line in lines:
            if self.most_recent_disk.isspace():
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break
            elif line.replace('\n', '').split(' ')[0] != self.most_recent_disk:
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break  
        with open(self.cache_disk, ""w"") as f:
            for line in lines:
                review_rating_pair = line.replace('\n', '').split(' ')
                if review_rating_pair[0] != least_recent_disk:
                    f.write(line)
        
        
    def get(self, new_text):
        # implement the caching algorithm
        
        if self.get_from_ram_cache(new_text) != None:
            self.most_recent_ram = new_text
            return self.get_from_ram_cache(new_text)
        elif self.get_from_disk_cache(new_text) != None:
            self.most_recent_disk = new_text
            return self.get_from_disk_cache(new_text)
        else:
            #print(""Input"", new_text)
            predicted_rating = super().get(new_text)
            if self.ram_size() < self.ram_max:
                #print(new_text, ""Added to ram"")
                self.add_to_ram_cache(new_text, predicted_rating)
            elif self.disk_size() < self.disk_max:
                #print(new_text, ""Added to disk"")
                least_recent_ram = self.get_least_recent_ram()
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            else:
                self.remove_least_recent_disk()
                least_recent_ram = self.get_least_recent_ram()
                #print(""Least Recent RAM moved to disk"", least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            
            self.most_recent_ram = new_text
            return predicted_rating
                    
                    
                    
            
            
        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.most_recent_ram = ''
        self.most_recent_disk = ''
    
    def get_least_recent_ram(self):
        '''
        Return least recent ram value. 
        return: key (Integer) - least recent RAM value
        '''
        for key in self.cache_ram.keys():
            if key != self.most_recent_ram:
                return key
    
    def remove_least_recent_disk(self):
        '''
        Remove least recent value from disk cache. 
        '''
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
        
        for line in lines:
            if self.most_recent_disk.isspace():
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break
            elif line.replace('\n', '').split(' ')[0] != self.most_recent_disk:
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break  
        with open(self.cache_disk, ""w"") as f:
            for line in lines:
                review_rating_pair = line.replace('\n', '').split(' ')
                if review_rating_pair[0] != least_recent_disk:
                    f.write(line)
        
        
    def get(self, new_text):
        # implement the caching algorithm
        
        if self.get_from_ram_cache(new_text) != None:
            self.most_recent_ram = new_text
            return self.get_from_ram_cache(new_text)
        elif self.get_from_disk_cache(new_text) != None:
            self.most_recent_disk = new_text
            return self.get_from_disk_cache(new_text)
        else:
            #print(""Input"", new_text)
            predicted_rating = super().get(new_text)
            if self.ram_size() < self.ram_max:
                #print(new_text, ""Added to ram"")
                self.add_to_ram_cache(new_text, predicted_rating)
            elif self.disk_size() < self.disk_max:
                #print(new_text, ""Added to disk"")
                least_recent_ram = self.get_least_recent_ram()
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            else:
                self.remove_least_recent_disk()
                least_recent_ram = self.get_least_recent_ram()
                #print(""Least Recent RAM moved to disk"", least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            
            self.most_recent_ram = new_text
            return predicted_rating
                    
                    
                    
            
            
        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
    
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
4495,2,"                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break
            elif line.replace('\n', '').split(' ')[0] != self.most_recent_disk:
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break  
        with open(self.cache_disk, ""w"") as f:
            for line in lines:
                review_rating_pair = line.replace('\n', '').split(' ')
                if review_rating_pair[0] != least_recent_disk:",TODO,least_recent_disk,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.most_recent_ram = ''
        self.most_recent_disk = ''
    
    def get_least_recent_ram(self):
        '''
        Return least recent ram value. 
        return: key (Integer) - least recent RAM value
        '''
        for key in self.cache_ram.keys():
            if key != self.most_recent_ram:
                return key
    
    def remove_least_recent_disk(self):
        '''
        Remove least recent value from disk cache. 
        '''
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
        
        for line in lines:
            if self.most_recent_disk.isspace():
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break
            elif line.replace('\n', '').split(' ')[0] != self.most_recent_disk:
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break  
        with open(self.cache_disk, ""w"") as f:
            for line in lines:
                review_rating_pair = line.replace('\n', '').split(' ')
                if review_rating_pair[0] != least_recent_disk:
                    f.write(line)
        
        
    def get(self, new_text):
        # implement the caching algorithm
        
        if self.get_from_ram_cache(new_text) != None:
            self.most_recent_ram = new_text
            return self.get_from_ram_cache(new_text)
        elif self.get_from_disk_cache(new_text) != None:
            self.most_recent_disk = new_text
            return self.get_from_disk_cache(new_text)
        else:
            #print(""Input"", new_text)
            predicted_rating = super().get(new_text)
            if self.ram_size() < self.ram_max:
                #print(new_text, ""Added to ram"")
                self.add_to_ram_cache(new_text, predicted_rating)
            elif self.disk_size() < self.disk_max:
                #print(new_text, ""Added to disk"")
                least_recent_ram = self.get_least_recent_ram()
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            else:
                self.remove_least_recent_disk()
                least_recent_ram = self.get_least_recent_ram()
                #print(""Least Recent RAM moved to disk"", least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            
            self.most_recent_ram = new_text
            return predicted_rating
                    
                    
                    
            
            
        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.most_recent_ram = ''
        self.most_recent_disk = ''
    
    def get_least_recent_ram(self):
        '''
        Return least recent ram value. 
        return: key (Integer) - least recent RAM value
        '''
        for key in self.cache_ram.keys():
            if key != self.most_recent_ram:
                return key
    
    def remove_least_recent_disk(self):
        '''
        Remove least recent value from disk cache. 
        '''
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
        
        for line in lines:
            if self.most_recent_disk.isspace():
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break
            elif line.replace('\n', '').split(' ')[0] != self.most_recent_disk:
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break  
        with open(self.cache_disk, ""w"") as f:
            for line in lines:
                review_rating_pair = line.replace('\n', '').split(' ')
                if review_rating_pair[0] != least_recent_disk:
                    f.write(line)
        
        
    def get(self, new_text):
        # implement the caching algorithm
        
        if self.get_from_ram_cache(new_text) != None:
            self.most_recent_ram = new_text
            return self.get_from_ram_cache(new_text)
        elif self.get_from_disk_cache(new_text) != None:
            self.most_recent_disk = new_text
            return self.get_from_disk_cache(new_text)
        else:
            #print(""Input"", new_text)
            predicted_rating = super().get(new_text)
            if self.ram_size() < self.ram_max:
                #print(new_text, ""Added to ram"")
                self.add_to_ram_cache(new_text, predicted_rating)
            elif self.disk_size() < self.disk_max:
                #print(new_text, ""Added to disk"")
                least_recent_ram = self.get_least_recent_ram()
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            else:
                self.remove_least_recent_disk()
                least_recent_ram = self.get_least_recent_ram()
                #print(""Least Recent RAM moved to disk"", least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            
            self.most_recent_ram = new_text
            return predicted_rating
                    
                    
                    
            
            
        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
    
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
4495,3,"                review_rating_pair = line.replace('\n', '').split(' ')
                if review_rating_pair[0] != least_recent_disk:",TODO,review_rating_pair,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.most_recent_ram = ''
        self.most_recent_disk = ''
    
    def get_least_recent_ram(self):
        '''
        Return least recent ram value. 
        return: key (Integer) - least recent RAM value
        '''
        for key in self.cache_ram.keys():
            if key != self.most_recent_ram:
                return key
    
    def remove_least_recent_disk(self):
        '''
        Remove least recent value from disk cache. 
        '''
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
        
        for line in lines:
            if self.most_recent_disk.isspace():
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break
            elif line.replace('\n', '').split(' ')[0] != self.most_recent_disk:
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break  
        with open(self.cache_disk, ""w"") as f:
            for line in lines:
                review_rating_pair = line.replace('\n', '').split(' ')
                if review_rating_pair[0] != least_recent_disk:
                    f.write(line)
        
        
    def get(self, new_text):
        # implement the caching algorithm
        
        if self.get_from_ram_cache(new_text) != None:
            self.most_recent_ram = new_text
            return self.get_from_ram_cache(new_text)
        elif self.get_from_disk_cache(new_text) != None:
            self.most_recent_disk = new_text
            return self.get_from_disk_cache(new_text)
        else:
            #print(""Input"", new_text)
            predicted_rating = super().get(new_text)
            if self.ram_size() < self.ram_max:
                #print(new_text, ""Added to ram"")
                self.add_to_ram_cache(new_text, predicted_rating)
            elif self.disk_size() < self.disk_max:
                #print(new_text, ""Added to disk"")
                least_recent_ram = self.get_least_recent_ram()
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            else:
                self.remove_least_recent_disk()
                least_recent_ram = self.get_least_recent_ram()
                #print(""Least Recent RAM moved to disk"", least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            
            self.most_recent_ram = new_text
            return predicted_rating
                    
                    
                    
            
            
        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.most_recent_ram = ''
        self.most_recent_disk = ''
    
    def get_least_recent_ram(self):
        '''
        Return least recent ram value. 
        return: key (Integer) - least recent RAM value
        '''
        for key in self.cache_ram.keys():
            if key != self.most_recent_ram:
                return key
    
    def remove_least_recent_disk(self):
        '''
        Remove least recent value from disk cache. 
        '''
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
        
        for line in lines:
            if self.most_recent_disk.isspace():
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break
            elif line.replace('\n', '').split(' ')[0] != self.most_recent_disk:
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break  
        with open(self.cache_disk, ""w"") as f:
            for line in lines:
                review_rating_pair = line.replace('\n', '').split(' ')
                if review_rating_pair[0] != least_recent_disk:
                    f.write(line)
        
        
    def get(self, new_text):
        # implement the caching algorithm
        
        if self.get_from_ram_cache(new_text) != None:
            self.most_recent_ram = new_text
            return self.get_from_ram_cache(new_text)
        elif self.get_from_disk_cache(new_text) != None:
            self.most_recent_disk = new_text
            return self.get_from_disk_cache(new_text)
        else:
            #print(""Input"", new_text)
            predicted_rating = super().get(new_text)
            if self.ram_size() < self.ram_max:
                #print(new_text, ""Added to ram"")
                self.add_to_ram_cache(new_text, predicted_rating)
            elif self.disk_size() < self.disk_max:
                #print(new_text, ""Added to disk"")
                least_recent_ram = self.get_least_recent_ram()
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            else:
                self.remove_least_recent_disk()
                least_recent_ram = self.get_least_recent_ram()
                #print(""Least Recent RAM moved to disk"", least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            
            self.most_recent_ram = new_text
            return predicted_rating
                    
                    
                    
            
            
        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
    
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
4495,4,"            predicted_rating = super().get(new_text)
            if self.ram_size() < self.ram_max:
                #print(new_text, ""Added to ram"")
                self.add_to_ram_cache(new_text, predicted_rating)
            elif self.disk_size() < self.disk_max:
                #print(new_text, ""Added to disk"")
                least_recent_ram = self.get_least_recent_ram()
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            else:
                self.remove_least_recent_disk()
                least_recent_ram = self.get_least_recent_ram()
                #print(""Least Recent RAM moved to disk"", least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            
            self.most_recent_ram = new_text
            return predicted_rating",TODO,predicted_rating,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.most_recent_ram = ''
        self.most_recent_disk = ''
    
    def get_least_recent_ram(self):
        '''
        Return least recent ram value. 
        return: key (Integer) - least recent RAM value
        '''
        for key in self.cache_ram.keys():
            if key != self.most_recent_ram:
                return key
    
    def remove_least_recent_disk(self):
        '''
        Remove least recent value from disk cache. 
        '''
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
        
        for line in lines:
            if self.most_recent_disk.isspace():
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break
            elif line.replace('\n', '').split(' ')[0] != self.most_recent_disk:
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break  
        with open(self.cache_disk, ""w"") as f:
            for line in lines:
                review_rating_pair = line.replace('\n', '').split(' ')
                if review_rating_pair[0] != least_recent_disk:
                    f.write(line)
        
        
    def get(self, new_text):
        # implement the caching algorithm
        
        if self.get_from_ram_cache(new_text) != None:
            self.most_recent_ram = new_text
            return self.get_from_ram_cache(new_text)
        elif self.get_from_disk_cache(new_text) != None:
            self.most_recent_disk = new_text
            return self.get_from_disk_cache(new_text)
        else:
            #print(""Input"", new_text)
            predicted_rating = super().get(new_text)
            if self.ram_size() < self.ram_max:
                #print(new_text, ""Added to ram"")
                self.add_to_ram_cache(new_text, predicted_rating)
            elif self.disk_size() < self.disk_max:
                #print(new_text, ""Added to disk"")
                least_recent_ram = self.get_least_recent_ram()
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            else:
                self.remove_least_recent_disk()
                least_recent_ram = self.get_least_recent_ram()
                #print(""Least Recent RAM moved to disk"", least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            
            self.most_recent_ram = new_text
            return predicted_rating
                    
                    
                    
            
            
        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.most_recent_ram = ''
        self.most_recent_disk = ''
    
    def get_least_recent_ram(self):
        '''
        Return least recent ram value. 
        return: key (Integer) - least recent RAM value
        '''
        for key in self.cache_ram.keys():
            if key != self.most_recent_ram:
                return key
    
    def remove_least_recent_disk(self):
        '''
        Remove least recent value from disk cache. 
        '''
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
        
        for line in lines:
            if self.most_recent_disk.isspace():
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break
            elif line.replace('\n', '').split(' ')[0] != self.most_recent_disk:
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break  
        with open(self.cache_disk, ""w"") as f:
            for line in lines:
                review_rating_pair = line.replace('\n', '').split(' ')
                if review_rating_pair[0] != least_recent_disk:
                    f.write(line)
        
        
    def get(self, new_text):
        # implement the caching algorithm
        
        if self.get_from_ram_cache(new_text) != None:
            self.most_recent_ram = new_text
            return self.get_from_ram_cache(new_text)
        elif self.get_from_disk_cache(new_text) != None:
            self.most_recent_disk = new_text
            return self.get_from_disk_cache(new_text)
        else:
            #print(""Input"", new_text)
            predicted_rating = super().get(new_text)
            if self.ram_size() < self.ram_max:
                #print(new_text, ""Added to ram"")
                self.add_to_ram_cache(new_text, predicted_rating)
            elif self.disk_size() < self.disk_max:
                #print(new_text, ""Added to disk"")
                least_recent_ram = self.get_least_recent_ram()
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            else:
                self.remove_least_recent_disk()
                least_recent_ram = self.get_least_recent_ram()
                #print(""Least Recent RAM moved to disk"", least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            
            self.most_recent_ram = new_text
            return predicted_rating
                    
                    
                    
            
            
        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
    
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
4495,5,"                least_recent_ram = self.get_least_recent_ram()
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            else:
                self.remove_least_recent_disk()
                least_recent_ram = self.get_least_recent_ram()
                #print(""Least Recent RAM moved to disk"", least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)",TODO,least_recent_ram,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.most_recent_ram = ''
        self.most_recent_disk = ''
    
    def get_least_recent_ram(self):
        '''
        Return least recent ram value. 
        return: key (Integer) - least recent RAM value
        '''
        for key in self.cache_ram.keys():
            if key != self.most_recent_ram:
                return key
    
    def remove_least_recent_disk(self):
        '''
        Remove least recent value from disk cache. 
        '''
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
        
        for line in lines:
            if self.most_recent_disk.isspace():
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break
            elif line.replace('\n', '').split(' ')[0] != self.most_recent_disk:
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break  
        with open(self.cache_disk, ""w"") as f:
            for line in lines:
                review_rating_pair = line.replace('\n', '').split(' ')
                if review_rating_pair[0] != least_recent_disk:
                    f.write(line)
        
        
    def get(self, new_text):
        # implement the caching algorithm
        
        if self.get_from_ram_cache(new_text) != None:
            self.most_recent_ram = new_text
            return self.get_from_ram_cache(new_text)
        elif self.get_from_disk_cache(new_text) != None:
            self.most_recent_disk = new_text
            return self.get_from_disk_cache(new_text)
        else:
            #print(""Input"", new_text)
            predicted_rating = super().get(new_text)
            if self.ram_size() < self.ram_max:
                #print(new_text, ""Added to ram"")
                self.add_to_ram_cache(new_text, predicted_rating)
            elif self.disk_size() < self.disk_max:
                #print(new_text, ""Added to disk"")
                least_recent_ram = self.get_least_recent_ram()
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            else:
                self.remove_least_recent_disk()
                least_recent_ram = self.get_least_recent_ram()
                #print(""Least Recent RAM moved to disk"", least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            
            self.most_recent_ram = new_text
            return predicted_rating
                    
                    
                    
            
            
        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.most_recent_ram = ''
        self.most_recent_disk = ''
    
    def get_least_recent_ram(self):
        '''
        Return least recent ram value. 
        return: key (Integer) - least recent RAM value
        '''
        for key in self.cache_ram.keys():
            if key != self.most_recent_ram:
                return key
    
    def remove_least_recent_disk(self):
        '''
        Remove least recent value from disk cache. 
        '''
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
        
        for line in lines:
            if self.most_recent_disk.isspace():
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break
            elif line.replace('\n', '').split(' ')[0] != self.most_recent_disk:
                least_recent_disk = line.replace('\n', '').split(' ')[0]
                break  
        with open(self.cache_disk, ""w"") as f:
            for line in lines:
                review_rating_pair = line.replace('\n', '').split(' ')
                if review_rating_pair[0] != least_recent_disk:
                    f.write(line)
        
        
    def get(self, new_text):
        # implement the caching algorithm
        
        if self.get_from_ram_cache(new_text) != None:
            self.most_recent_ram = new_text
            return self.get_from_ram_cache(new_text)
        elif self.get_from_disk_cache(new_text) != None:
            self.most_recent_disk = new_text
            return self.get_from_disk_cache(new_text)
        else:
            #print(""Input"", new_text)
            predicted_rating = super().get(new_text)
            if self.ram_size() < self.ram_max:
                #print(new_text, ""Added to ram"")
                self.add_to_ram_cache(new_text, predicted_rating)
            elif self.disk_size() < self.disk_max:
                #print(new_text, ""Added to disk"")
                least_recent_ram = self.get_least_recent_ram()
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            else:
                self.remove_least_recent_disk()
                least_recent_ram = self.get_least_recent_ram()
                #print(""Least Recent RAM moved to disk"", least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.add_to_disk_cache(least_recent_ram, self.get_from_ram_cache(least_recent_ram))
                self.remove_from_ram_cache(least_recent_ram)
                self.add_to_ram_cache(new_text, predicted_rating)
            
            self.most_recent_ram = new_text
            return predicted_rating
                    
                    
                    
            
            
        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
    
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
6728,1,"            lines = f.readlines()
            for line in lines:",TODO,lines,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max

        # initialize additional fields here
        self.recent_ram = collections.deque()
        self.recent_disk = collections.deque()
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
            for line in lines:
                line.replace('\n', '').split(' ')

    def get(self, new_text):
        # implement the caching algorithm
        if new_text in self.cache_ram:
            if self.recent_ram[-1] != new_text:
                self.recent_ram.remove(new_text)
                self.recent_ram.append(new_text)
            return self.get_from_ram_cache(new_text)
        elif new_text in self.recent_disk:
            if self.recent_disk[-1] != new_text:
                self.recent_disk.remove(new_text)
                self.recent_disk.append(new_text)
            return self.get_from_disk_cache(new_text)
        else:
            if self.ram_size() == self.ram_max:
                if self.disk_size()==self.disk_max:
                    pop_disk = self.recent_disk.popleft()
                    self.remove_from_disk_cache(pop_disk)

                pop_ram = self.recent_ram.popleft()
                self.add_to_disk_cache(pop_ram, self.get_from_ram_cache(pop_ram))
                self.recent_disk.append(pop_ram)
                self.remove_from_ram_cache(pop_ram)
            
            prediction = predict(new_text)
            self.add_to_ram_cache(new_text,prediction)
            self.recent_ram.append(new_text)
            return prediction
        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max

        # initialize additional fields here
        self.recent_ram = collections.deque()
        self.recent_disk = collections.deque()
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
            for line in lines:
                line.replace('\n', '').split(' ')

    def get(self, new_text):
        # implement the caching algorithm
        if new_text in self.cache_ram:
            if self.recent_ram[-1] != new_text:
                self.recent_ram.remove(new_text)
                self.recent_ram.append(new_text)
            return self.get_from_ram_cache(new_text)
        elif new_text in self.recent_disk:
            if self.recent_disk[-1] != new_text:
                self.recent_disk.remove(new_text)
                self.recent_disk.append(new_text)
            return self.get_from_disk_cache(new_text)
        else:
            if self.ram_size() == self.ram_max:
                if self.disk_size()==self.disk_max:
                    pop_disk = self.recent_disk.popleft()
                    self.remove_from_disk_cache(pop_disk)

                pop_ram = self.recent_ram.popleft()
                self.add_to_disk_cache(pop_ram, self.get_from_ram_cache(pop_ram))
                self.recent_disk.append(pop_ram)
                self.remove_from_ram_cache(pop_ram)
            
            prediction = predict(new_text)
            self.add_to_ram_cache(new_text,prediction)
            self.recent_ram.append(new_text)
            return prediction
        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
6728,2,"            prediction = predict(new_text)
            self.add_to_ram_cache(new_text,prediction)
            self.recent_ram.append(new_text)
            return prediction",TODO,prediction,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max

        # initialize additional fields here
        self.recent_ram = collections.deque()
        self.recent_disk = collections.deque()
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
            for line in lines:
                line.replace('\n', '').split(' ')

    def get(self, new_text):
        # implement the caching algorithm
        if new_text in self.cache_ram:
            if self.recent_ram[-1] != new_text:
                self.recent_ram.remove(new_text)
                self.recent_ram.append(new_text)
            return self.get_from_ram_cache(new_text)
        elif new_text in self.recent_disk:
            if self.recent_disk[-1] != new_text:
                self.recent_disk.remove(new_text)
                self.recent_disk.append(new_text)
            return self.get_from_disk_cache(new_text)
        else:
            if self.ram_size() == self.ram_max:
                if self.disk_size()==self.disk_max:
                    pop_disk = self.recent_disk.popleft()
                    self.remove_from_disk_cache(pop_disk)

                pop_ram = self.recent_ram.popleft()
                self.add_to_disk_cache(pop_ram, self.get_from_ram_cache(pop_ram))
                self.recent_disk.append(pop_ram)
                self.remove_from_ram_cache(pop_ram)
            
            prediction = predict(new_text)
            self.add_to_ram_cache(new_text,prediction)
            self.recent_ram.append(new_text)
            return prediction
        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max

        # initialize additional fields here
        self.recent_ram = collections.deque()
        self.recent_disk = collections.deque()
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
            for line in lines:
                line.replace('\n', '').split(' ')

    def get(self, new_text):
        # implement the caching algorithm
        if new_text in self.cache_ram:
            if self.recent_ram[-1] != new_text:
                self.recent_ram.remove(new_text)
                self.recent_ram.append(new_text)
            return self.get_from_ram_cache(new_text)
        elif new_text in self.recent_disk:
            if self.recent_disk[-1] != new_text:
                self.recent_disk.remove(new_text)
                self.recent_disk.append(new_text)
            return self.get_from_disk_cache(new_text)
        else:
            if self.ram_size() == self.ram_max:
                if self.disk_size()==self.disk_max:
                    pop_disk = self.recent_disk.popleft()
                    self.remove_from_disk_cache(pop_disk)

                pop_ram = self.recent_ram.popleft()
                self.add_to_disk_cache(pop_ram, self.get_from_ram_cache(pop_ram))
                self.recent_disk.append(pop_ram)
                self.remove_from_ram_cache(pop_ram)
            
            prediction = predict(new_text)
            self.add_to_ram_cache(new_text,prediction)
            self.recent_ram.append(new_text)
            return prediction
        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
6728,3,"                pop_ram = self.recent_ram.popleft()
                self.add_to_disk_cache(pop_ram, self.get_from_ram_cache(pop_ram))
                self.recent_disk.append(pop_ram)
                self.remove_from_ram_cache(pop_ram)",TODO,pop_ram,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max

        # initialize additional fields here
        self.recent_ram = collections.deque()
        self.recent_disk = collections.deque()
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
            for line in lines:
                line.replace('\n', '').split(' ')

    def get(self, new_text):
        # implement the caching algorithm
        if new_text in self.cache_ram:
            if self.recent_ram[-1] != new_text:
                self.recent_ram.remove(new_text)
                self.recent_ram.append(new_text)
            return self.get_from_ram_cache(new_text)
        elif new_text in self.recent_disk:
            if self.recent_disk[-1] != new_text:
                self.recent_disk.remove(new_text)
                self.recent_disk.append(new_text)
            return self.get_from_disk_cache(new_text)
        else:
            if self.ram_size() == self.ram_max:
                if self.disk_size()==self.disk_max:
                    pop_disk = self.recent_disk.popleft()
                    self.remove_from_disk_cache(pop_disk)

                pop_ram = self.recent_ram.popleft()
                self.add_to_disk_cache(pop_ram, self.get_from_ram_cache(pop_ram))
                self.recent_disk.append(pop_ram)
                self.remove_from_ram_cache(pop_ram)
            
            prediction = predict(new_text)
            self.add_to_ram_cache(new_text,prediction)
            self.recent_ram.append(new_text)
            return prediction
        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max

        # initialize additional fields here
        self.recent_ram = collections.deque()
        self.recent_disk = collections.deque()
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
            for line in lines:
                line.replace('\n', '').split(' ')

    def get(self, new_text):
        # implement the caching algorithm
        if new_text in self.cache_ram:
            if self.recent_ram[-1] != new_text:
                self.recent_ram.remove(new_text)
                self.recent_ram.append(new_text)
            return self.get_from_ram_cache(new_text)
        elif new_text in self.recent_disk:
            if self.recent_disk[-1] != new_text:
                self.recent_disk.remove(new_text)
                self.recent_disk.append(new_text)
            return self.get_from_disk_cache(new_text)
        else:
            if self.ram_size() == self.ram_max:
                if self.disk_size()==self.disk_max:
                    pop_disk = self.recent_disk.popleft()
                    self.remove_from_disk_cache(pop_disk)

                pop_ram = self.recent_ram.popleft()
                self.add_to_disk_cache(pop_ram, self.get_from_ram_cache(pop_ram))
                self.recent_disk.append(pop_ram)
                self.remove_from_ram_cache(pop_ram)
            
            prediction = predict(new_text)
            self.add_to_ram_cache(new_text,prediction)
            self.recent_ram.append(new_text)
            return prediction
        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
6728,4,"                    pop_disk = self.recent_disk.popleft()
                    self.remove_from_disk_cache(pop_disk)",TODO,pop_disk,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max

        # initialize additional fields here
        self.recent_ram = collections.deque()
        self.recent_disk = collections.deque()
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
            for line in lines:
                line.replace('\n', '').split(' ')

    def get(self, new_text):
        # implement the caching algorithm
        if new_text in self.cache_ram:
            if self.recent_ram[-1] != new_text:
                self.recent_ram.remove(new_text)
                self.recent_ram.append(new_text)
            return self.get_from_ram_cache(new_text)
        elif new_text in self.recent_disk:
            if self.recent_disk[-1] != new_text:
                self.recent_disk.remove(new_text)
                self.recent_disk.append(new_text)
            return self.get_from_disk_cache(new_text)
        else:
            if self.ram_size() == self.ram_max:
                if self.disk_size()==self.disk_max:
                    pop_disk = self.recent_disk.popleft()
                    self.remove_from_disk_cache(pop_disk)

                pop_ram = self.recent_ram.popleft()
                self.add_to_disk_cache(pop_ram, self.get_from_ram_cache(pop_ram))
                self.recent_disk.append(pop_ram)
                self.remove_from_ram_cache(pop_ram)
            
            prediction = predict(new_text)
            self.add_to_ram_cache(new_text,prediction)
            self.recent_ram.append(new_text)
            return prediction
        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max

        # initialize additional fields here
        self.recent_ram = collections.deque()
        self.recent_disk = collections.deque()
        with open(self.cache_disk, ""r"") as f:
            lines = f.readlines()
            for line in lines:
                line.replace('\n', '').split(' ')

    def get(self, new_text):
        # implement the caching algorithm
        if new_text in self.cache_ram:
            if self.recent_ram[-1] != new_text:
                self.recent_ram.remove(new_text)
                self.recent_ram.append(new_text)
            return self.get_from_ram_cache(new_text)
        elif new_text in self.recent_disk:
            if self.recent_disk[-1] != new_text:
                self.recent_disk.remove(new_text)
                self.recent_disk.append(new_text)
            return self.get_from_disk_cache(new_text)
        else:
            if self.ram_size() == self.ram_max:
                if self.disk_size()==self.disk_max:
                    pop_disk = self.recent_disk.popleft()
                    self.remove_from_disk_cache(pop_disk)

                pop_ram = self.recent_ram.popleft()
                self.add_to_disk_cache(pop_ram, self.get_from_ram_cache(pop_ram))
                self.recent_disk.append(pop_ram)
                self.remove_from_ram_cache(pop_ram)
            
            prediction = predict(new_text)
            self.add_to_ram_cache(new_text,prediction)
            self.recent_ram.append(new_text)
            return prediction
        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
3319,1,"        hashed_new_text = hash_str(new_text)
        #check if new_text in RAM
        if new_text in self.ram_deqeue:
            predicted_rating = self.get_from_ram_cache(new_text)
            self.ram_deqeue.remove(new_text)
            self.remove_from_ram_cache(new_text)
        #check if new_text in Disk
        elif hashed_new_text in self.disk_deqeue:",TODO,hashed_new_text,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_deqeue = collections.deque()
        self.disk_deqeue = collections.deque()      
        
    def get(self, new_text):
        # implement the caching algorithm
        hashed_new_text = hash_str(new_text)
        #check if new_text in RAM
        if new_text in self.ram_deqeue:
            predicted_rating = self.get_from_ram_cache(new_text)
            self.ram_deqeue.remove(new_text)
            self.remove_from_ram_cache(new_text)
        #check if new_text in Disk
        elif hashed_new_text in self.disk_deqeue:
            predicted_rating = self.get_from_disk_cache(new_text)
            self.disk_deqeue.remove(new_text)
            self.remove_from_disk_cache(new_text)
            self.disk_deqeue.append(new_text)
            self.add_to_disk_cache(new_text, predicted_rating)
        else:
            predicted_rating = predict(new_text)
            if self.ram_size() == self.ram_max:
                if self.disk_size() == self.disk_max:
                    popped_disk_element = self.disk_deqeue.popleft()
                    self.remove_from_disk_cache(popped_disk_element)
                popped_ram_element = self.ram_deqeue.popleft()
                popped_element_rating = self.get_from_ram_cache(popped_ram_element)
                self.remove_from_ram_cache(popped_ram_element)
                self.disk_deqeue.append(popped_ram_element)
                self.add_to_disk_cache(popped_ram_element, popped_element_rating)
        self.ram_deqeue.append(new_text)
        self.add_to_ram_cache(new_text, predicted_rating)
        return predicted_rating
        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_deqeue = collections.deque()
        self.disk_deqeue = collections.deque()      
        
    def get(self, new_text):
        # implement the caching algorithm
        hashed_new_text = hash_str(new_text)
        #check if new_text in RAM
        if new_text in self.ram_deqeue:
            predicted_rating = self.get_from_ram_cache(new_text)
            self.ram_deqeue.remove(new_text)
            self.remove_from_ram_cache(new_text)
        #check if new_text in Disk
        elif hashed_new_text in self.disk_deqeue:
            predicted_rating = self.get_from_disk_cache(new_text)
            self.disk_deqeue.remove(new_text)
            self.remove_from_disk_cache(new_text)
            self.disk_deqeue.append(new_text)
            self.add_to_disk_cache(new_text, predicted_rating)
        else:
            predicted_rating = predict(new_text)
            if self.ram_size() == self.ram_max:
                if self.disk_size() == self.disk_max:
                    popped_disk_element = self.disk_deqeue.popleft()
                    self.remove_from_disk_cache(popped_disk_element)
                popped_ram_element = self.ram_deqeue.popleft()
                popped_element_rating = self.get_from_ram_cache(popped_ram_element)
                self.remove_from_ram_cache(popped_ram_element)
                self.disk_deqeue.append(popped_ram_element)
                self.add_to_disk_cache(popped_ram_element, popped_element_rating)
        self.ram_deqeue.append(new_text)
        self.add_to_ram_cache(new_text, predicted_rating)
        return predicted_rating
        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
3319,2,"            predicted_rating = self.get_from_ram_cache(new_text)
            self.ram_deqeue.remove(new_text)
            self.remove_from_ram_cache(new_text)
        #check if new_text in Disk
        elif hashed_new_text in self.disk_deqeue:
            predicted_rating = self.get_from_disk_cache(new_text)
            self.disk_deqeue.remove(new_text)
            self.remove_from_disk_cache(new_text)
            self.disk_deqeue.append(new_text)
            self.add_to_disk_cache(new_text, predicted_rating)
        else:
            predicted_rating = predict(new_text)
            if self.ram_size() == self.ram_max:
                if self.disk_size() == self.disk_max:
                    popped_disk_element = self.disk_deqeue.popleft()
                    self.remove_from_disk_cache(popped_disk_element)
                popped_ram_element = self.ram_deqeue.popleft()
                popped_element_rating = self.get_from_ram_cache(popped_ram_element)
                self.remove_from_ram_cache(popped_ram_element)
                self.disk_deqeue.append(popped_ram_element)
                self.add_to_disk_cache(popped_ram_element, popped_element_rating)
        self.ram_deqeue.append(new_text)
        self.add_to_ram_cache(new_text, predicted_rating)
        return predicted_rating",TODO,predicted_rating,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_deqeue = collections.deque()
        self.disk_deqeue = collections.deque()      
        
    def get(self, new_text):
        # implement the caching algorithm
        hashed_new_text = hash_str(new_text)
        #check if new_text in RAM
        if new_text in self.ram_deqeue:
            predicted_rating = self.get_from_ram_cache(new_text)
            self.ram_deqeue.remove(new_text)
            self.remove_from_ram_cache(new_text)
        #check if new_text in Disk
        elif hashed_new_text in self.disk_deqeue:
            predicted_rating = self.get_from_disk_cache(new_text)
            self.disk_deqeue.remove(new_text)
            self.remove_from_disk_cache(new_text)
            self.disk_deqeue.append(new_text)
            self.add_to_disk_cache(new_text, predicted_rating)
        else:
            predicted_rating = predict(new_text)
            if self.ram_size() == self.ram_max:
                if self.disk_size() == self.disk_max:
                    popped_disk_element = self.disk_deqeue.popleft()
                    self.remove_from_disk_cache(popped_disk_element)
                popped_ram_element = self.ram_deqeue.popleft()
                popped_element_rating = self.get_from_ram_cache(popped_ram_element)
                self.remove_from_ram_cache(popped_ram_element)
                self.disk_deqeue.append(popped_ram_element)
                self.add_to_disk_cache(popped_ram_element, popped_element_rating)
        self.ram_deqeue.append(new_text)
        self.add_to_ram_cache(new_text, predicted_rating)
        return predicted_rating
        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_deqeue = collections.deque()
        self.disk_deqeue = collections.deque()      
        
    def get(self, new_text):
        # implement the caching algorithm
        hashed_new_text = hash_str(new_text)
        #check if new_text in RAM
        if new_text in self.ram_deqeue:
            predicted_rating = self.get_from_ram_cache(new_text)
            self.ram_deqeue.remove(new_text)
            self.remove_from_ram_cache(new_text)
        #check if new_text in Disk
        elif hashed_new_text in self.disk_deqeue:
            predicted_rating = self.get_from_disk_cache(new_text)
            self.disk_deqeue.remove(new_text)
            self.remove_from_disk_cache(new_text)
            self.disk_deqeue.append(new_text)
            self.add_to_disk_cache(new_text, predicted_rating)
        else:
            predicted_rating = predict(new_text)
            if self.ram_size() == self.ram_max:
                if self.disk_size() == self.disk_max:
                    popped_disk_element = self.disk_deqeue.popleft()
                    self.remove_from_disk_cache(popped_disk_element)
                popped_ram_element = self.ram_deqeue.popleft()
                popped_element_rating = self.get_from_ram_cache(popped_ram_element)
                self.remove_from_ram_cache(popped_ram_element)
                self.disk_deqeue.append(popped_ram_element)
                self.add_to_disk_cache(popped_ram_element, popped_element_rating)
        self.ram_deqeue.append(new_text)
        self.add_to_ram_cache(new_text, predicted_rating)
        return predicted_rating
        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
3319,3,"                popped_ram_element = self.ram_deqeue.popleft()
                popped_element_rating = self.get_from_ram_cache(popped_ram_element)
                self.remove_from_ram_cache(popped_ram_element)
                self.disk_deqeue.append(popped_ram_element)
                self.add_to_disk_cache(popped_ram_element, popped_element_rating)",TODO,popped_ram_element,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_deqeue = collections.deque()
        self.disk_deqeue = collections.deque()      
        
    def get(self, new_text):
        # implement the caching algorithm
        hashed_new_text = hash_str(new_text)
        #check if new_text in RAM
        if new_text in self.ram_deqeue:
            predicted_rating = self.get_from_ram_cache(new_text)
            self.ram_deqeue.remove(new_text)
            self.remove_from_ram_cache(new_text)
        #check if new_text in Disk
        elif hashed_new_text in self.disk_deqeue:
            predicted_rating = self.get_from_disk_cache(new_text)
            self.disk_deqeue.remove(new_text)
            self.remove_from_disk_cache(new_text)
            self.disk_deqeue.append(new_text)
            self.add_to_disk_cache(new_text, predicted_rating)
        else:
            predicted_rating = predict(new_text)
            if self.ram_size() == self.ram_max:
                if self.disk_size() == self.disk_max:
                    popped_disk_element = self.disk_deqeue.popleft()
                    self.remove_from_disk_cache(popped_disk_element)
                popped_ram_element = self.ram_deqeue.popleft()
                popped_element_rating = self.get_from_ram_cache(popped_ram_element)
                self.remove_from_ram_cache(popped_ram_element)
                self.disk_deqeue.append(popped_ram_element)
                self.add_to_disk_cache(popped_ram_element, popped_element_rating)
        self.ram_deqeue.append(new_text)
        self.add_to_ram_cache(new_text, predicted_rating)
        return predicted_rating
        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_deqeue = collections.deque()
        self.disk_deqeue = collections.deque()      
        
    def get(self, new_text):
        # implement the caching algorithm
        hashed_new_text = hash_str(new_text)
        #check if new_text in RAM
        if new_text in self.ram_deqeue:
            predicted_rating = self.get_from_ram_cache(new_text)
            self.ram_deqeue.remove(new_text)
            self.remove_from_ram_cache(new_text)
        #check if new_text in Disk
        elif hashed_new_text in self.disk_deqeue:
            predicted_rating = self.get_from_disk_cache(new_text)
            self.disk_deqeue.remove(new_text)
            self.remove_from_disk_cache(new_text)
            self.disk_deqeue.append(new_text)
            self.add_to_disk_cache(new_text, predicted_rating)
        else:
            predicted_rating = predict(new_text)
            if self.ram_size() == self.ram_max:
                if self.disk_size() == self.disk_max:
                    popped_disk_element = self.disk_deqeue.popleft()
                    self.remove_from_disk_cache(popped_disk_element)
                popped_ram_element = self.ram_deqeue.popleft()
                popped_element_rating = self.get_from_ram_cache(popped_ram_element)
                self.remove_from_ram_cache(popped_ram_element)
                self.disk_deqeue.append(popped_ram_element)
                self.add_to_disk_cache(popped_ram_element, popped_element_rating)
        self.ram_deqeue.append(new_text)
        self.add_to_ram_cache(new_text, predicted_rating)
        return predicted_rating
        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
3319,4,"                popped_element_rating = self.get_from_ram_cache(popped_ram_element)
                self.remove_from_ram_cache(popped_ram_element)
                self.disk_deqeue.append(popped_ram_element)
                self.add_to_disk_cache(popped_ram_element, popped_element_rating)",TODO,popped_element_rating,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_deqeue = collections.deque()
        self.disk_deqeue = collections.deque()      
        
    def get(self, new_text):
        # implement the caching algorithm
        hashed_new_text = hash_str(new_text)
        #check if new_text in RAM
        if new_text in self.ram_deqeue:
            predicted_rating = self.get_from_ram_cache(new_text)
            self.ram_deqeue.remove(new_text)
            self.remove_from_ram_cache(new_text)
        #check if new_text in Disk
        elif hashed_new_text in self.disk_deqeue:
            predicted_rating = self.get_from_disk_cache(new_text)
            self.disk_deqeue.remove(new_text)
            self.remove_from_disk_cache(new_text)
            self.disk_deqeue.append(new_text)
            self.add_to_disk_cache(new_text, predicted_rating)
        else:
            predicted_rating = predict(new_text)
            if self.ram_size() == self.ram_max:
                if self.disk_size() == self.disk_max:
                    popped_disk_element = self.disk_deqeue.popleft()
                    self.remove_from_disk_cache(popped_disk_element)
                popped_ram_element = self.ram_deqeue.popleft()
                popped_element_rating = self.get_from_ram_cache(popped_ram_element)
                self.remove_from_ram_cache(popped_ram_element)
                self.disk_deqeue.append(popped_ram_element)
                self.add_to_disk_cache(popped_ram_element, popped_element_rating)
        self.ram_deqeue.append(new_text)
        self.add_to_ram_cache(new_text, predicted_rating)
        return predicted_rating
        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_deqeue = collections.deque()
        self.disk_deqeue = collections.deque()      
        
    def get(self, new_text):
        # implement the caching algorithm
        hashed_new_text = hash_str(new_text)
        #check if new_text in RAM
        if new_text in self.ram_deqeue:
            predicted_rating = self.get_from_ram_cache(new_text)
            self.ram_deqeue.remove(new_text)
            self.remove_from_ram_cache(new_text)
        #check if new_text in Disk
        elif hashed_new_text in self.disk_deqeue:
            predicted_rating = self.get_from_disk_cache(new_text)
            self.disk_deqeue.remove(new_text)
            self.remove_from_disk_cache(new_text)
            self.disk_deqeue.append(new_text)
            self.add_to_disk_cache(new_text, predicted_rating)
        else:
            predicted_rating = predict(new_text)
            if self.ram_size() == self.ram_max:
                if self.disk_size() == self.disk_max:
                    popped_disk_element = self.disk_deqeue.popleft()
                    self.remove_from_disk_cache(popped_disk_element)
                popped_ram_element = self.ram_deqeue.popleft()
                popped_element_rating = self.get_from_ram_cache(popped_ram_element)
                self.remove_from_ram_cache(popped_ram_element)
                self.disk_deqeue.append(popped_ram_element)
                self.add_to_disk_cache(popped_ram_element, popped_element_rating)
        self.ram_deqeue.append(new_text)
        self.add_to_ram_cache(new_text, predicted_rating)
        return predicted_rating
        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
3319,5,"                    popped_disk_element = self.disk_deqeue.popleft()
                    self.remove_from_disk_cache(popped_disk_element)",TODO,popped_disk_element,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_deqeue = collections.deque()
        self.disk_deqeue = collections.deque()      
        
    def get(self, new_text):
        # implement the caching algorithm
        hashed_new_text = hash_str(new_text)
        #check if new_text in RAM
        if new_text in self.ram_deqeue:
            predicted_rating = self.get_from_ram_cache(new_text)
            self.ram_deqeue.remove(new_text)
            self.remove_from_ram_cache(new_text)
        #check if new_text in Disk
        elif hashed_new_text in self.disk_deqeue:
            predicted_rating = self.get_from_disk_cache(new_text)
            self.disk_deqeue.remove(new_text)
            self.remove_from_disk_cache(new_text)
            self.disk_deqeue.append(new_text)
            self.add_to_disk_cache(new_text, predicted_rating)
        else:
            predicted_rating = predict(new_text)
            if self.ram_size() == self.ram_max:
                if self.disk_size() == self.disk_max:
                    popped_disk_element = self.disk_deqeue.popleft()
                    self.remove_from_disk_cache(popped_disk_element)
                popped_ram_element = self.ram_deqeue.popleft()
                popped_element_rating = self.get_from_ram_cache(popped_ram_element)
                self.remove_from_ram_cache(popped_ram_element)
                self.disk_deqeue.append(popped_ram_element)
                self.add_to_disk_cache(popped_ram_element, popped_element_rating)
        self.ram_deqeue.append(new_text)
        self.add_to_ram_cache(new_text, predicted_rating)
        return predicted_rating
        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_deqeue = collections.deque()
        self.disk_deqeue = collections.deque()      
        
    def get(self, new_text):
        # implement the caching algorithm
        hashed_new_text = hash_str(new_text)
        #check if new_text in RAM
        if new_text in self.ram_deqeue:
            predicted_rating = self.get_from_ram_cache(new_text)
            self.ram_deqeue.remove(new_text)
            self.remove_from_ram_cache(new_text)
        #check if new_text in Disk
        elif hashed_new_text in self.disk_deqeue:
            predicted_rating = self.get_from_disk_cache(new_text)
            self.disk_deqeue.remove(new_text)
            self.remove_from_disk_cache(new_text)
            self.disk_deqeue.append(new_text)
            self.add_to_disk_cache(new_text, predicted_rating)
        else:
            predicted_rating = predict(new_text)
            if self.ram_size() == self.ram_max:
                if self.disk_size() == self.disk_max:
                    popped_disk_element = self.disk_deqeue.popleft()
                    self.remove_from_disk_cache(popped_disk_element)
                popped_ram_element = self.ram_deqeue.popleft()
                popped_element_rating = self.get_from_ram_cache(popped_ram_element)
                self.remove_from_ram_cache(popped_ram_element)
                self.disk_deqeue.append(popped_ram_element)
                self.add_to_disk_cache(popped_ram_element, popped_element_rating)
        self.ram_deqeue.append(new_text)
        self.add_to_ram_cache(new_text, predicted_rating)
        return predicted_rating
        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
5902,1,"            list_ram_elemnts = list(self.cache_ram.keys())
            self.last_ram_element_text = list_ram_elemnts[0]",TODO,list_ram_elemnts,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_recent = """"
        self.disk_recent = """"
        self.new_predicted_rating = """"
        self.last_ram_element_text = """"
        self.last_ram_element_rating = 0
        
    def get(self, new_text):

        if Cache.get_from_ram_cache(self,new_text) is not None:
          self.ram_recent = new_text
          return Cache.get_from_ram_cache(self,new_text)
        
        elif Cache.get_from_disk_cache(self,new_text) is not None:
          self.disk_recent = new_text
          return Cache.get_from_disk_cache(self,new_text)

        else:
          self.new_predicted_rating=Cache.get(self,new_text)

          if len(self.cache_ram) == self.ram_max:
            with open(self.cache_disk, ""r"") as f:
              lines = f.readlines()
            if len(lines) == self.disk_max:
              lines.pop(0)
            with open(self.cache_disk, ""w"") as f:
              f.writelines(lines)

            list_ram_elemnts = list(self.cache_ram.keys())
            self.last_ram_element_text = list_ram_elemnts[0]
            self.last_ram_element_rating = Cache.get_from_ram_cache(self,self.last_ram_element_text)
            Cache.remove_from_ram_cache(self,self.last_ram_element_text)
            Cache.add_to_disk_cache(self,self.last_ram_element_text,self.last_ram_element_rating)

          Cache.add_to_ram_cache(self,new_text,self.new_predicted_rating)
          self.ram_recent=new_text
          self.disk_recent=self.last_ram_element_text
          return self.new_predicted_rating

        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_recent = """"
        self.disk_recent = """"
        self.new_predicted_rating = """"
        self.last_ram_element_text = """"
        self.last_ram_element_rating = 0
        
    def get(self, new_text):

        if Cache.get_from_ram_cache(self,new_text) is not None:
          self.ram_recent = new_text
          return Cache.get_from_ram_cache(self,new_text)
        
        elif Cache.get_from_disk_cache(self,new_text) is not None:
          self.disk_recent = new_text
          return Cache.get_from_disk_cache(self,new_text)

        else:
          self.new_predicted_rating=Cache.get(self,new_text)

          if len(self.cache_ram) == self.ram_max:
            with open(self.cache_disk, ""r"") as f:
              lines = f.readlines()
            if len(lines) == self.disk_max:
              lines.pop(0)
            with open(self.cache_disk, ""w"") as f:
              f.writelines(lines)

            list_ram_elemnts = list(self.cache_ram.keys())
            self.last_ram_element_text = list_ram_elemnts[0]
            self.last_ram_element_rating = Cache.get_from_ram_cache(self,self.last_ram_element_text)
            Cache.remove_from_ram_cache(self,self.last_ram_element_text)
            Cache.add_to_disk_cache(self,self.last_ram_element_text,self.last_ram_element_rating)

          Cache.add_to_ram_cache(self,new_text,self.new_predicted_rating)
          self.ram_recent=new_text
          self.disk_recent=self.last_ram_element_text
          return self.new_predicted_rating

        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
    
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    

    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])

    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""

    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
5902,2,"              lines = f.readlines()
            if len(lines) == self.disk_max:
              lines.pop(0)
            with open(self.cache_disk, ""w"") as f:
              f.writelines(lines)",TODO,lines,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_recent = """"
        self.disk_recent = """"
        self.new_predicted_rating = """"
        self.last_ram_element_text = """"
        self.last_ram_element_rating = 0
        
    def get(self, new_text):

        if Cache.get_from_ram_cache(self,new_text) is not None:
          self.ram_recent = new_text
          return Cache.get_from_ram_cache(self,new_text)
        
        elif Cache.get_from_disk_cache(self,new_text) is not None:
          self.disk_recent = new_text
          return Cache.get_from_disk_cache(self,new_text)

        else:
          self.new_predicted_rating=Cache.get(self,new_text)

          if len(self.cache_ram) == self.ram_max:
            with open(self.cache_disk, ""r"") as f:
              lines = f.readlines()
            if len(lines) == self.disk_max:
              lines.pop(0)
            with open(self.cache_disk, ""w"") as f:
              f.writelines(lines)

            list_ram_elemnts = list(self.cache_ram.keys())
            self.last_ram_element_text = list_ram_elemnts[0]
            self.last_ram_element_rating = Cache.get_from_ram_cache(self,self.last_ram_element_text)
            Cache.remove_from_ram_cache(self,self.last_ram_element_text)
            Cache.add_to_disk_cache(self,self.last_ram_element_text,self.last_ram_element_rating)

          Cache.add_to_ram_cache(self,new_text,self.new_predicted_rating)
          self.ram_recent=new_text
          self.disk_recent=self.last_ram_element_text
          return self.new_predicted_rating

        raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        self.ram_recent = """"
        self.disk_recent = """"
        self.new_predicted_rating = """"
        self.last_ram_element_text = """"
        self.last_ram_element_rating = 0
        
    def get(self, new_text):

        if Cache.get_from_ram_cache(self,new_text) is not None:
          self.ram_recent = new_text
          return Cache.get_from_ram_cache(self,new_text)
        
        elif Cache.get_from_disk_cache(self,new_text) is not None:
          self.disk_recent = new_text
          return Cache.get_from_disk_cache(self,new_text)

        else:
          self.new_predicted_rating=Cache.get(self,new_text)

          if len(self.cache_ram) == self.ram_max:
            with open(self.cache_disk, ""r"") as f:
              lines = f.readlines()
            if len(lines) == self.disk_max:
              lines.pop(0)
            with open(self.cache_disk, ""w"") as f:
              f.writelines(lines)

            list_ram_elemnts = list(self.cache_ram.keys())
            self.last_ram_element_text = list_ram_elemnts[0]
            self.last_ram_element_rating = Cache.get_from_ram_cache(self,self.last_ram_element_text)
            Cache.remove_from_ram_cache(self,self.last_ram_element_text)
            Cache.add_to_disk_cache(self,self.last_ram_element_text,self.last_ram_element_rating)

          Cache.add_to_ram_cache(self,new_text,self.new_predicted_rating)
          self.ram_recent=new_text
          self.disk_recent=self.last_ram_element_text
          return self.new_predicted_rating

        raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
    
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    

    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])

    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""

    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
7907,1,"        ram_value = self.get_from_ram_cache(new_text)
        if ram_value is not None:
          self.ram_keys.remove(new_text)
          self.ram_keys.append(new_text)
          return ram_value",TODO,ram_value,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_keys = collections.deque()
        self.disk_keys = collections.deque()

    def get(self, new_text):
        # implement the caching algorithm
        ram_value = self.get_from_ram_cache(new_text)
        if ram_value is not None:
          self.ram_keys.remove(new_text)
          self.ram_keys.append(new_text)
          return ram_value
        
        disk_value = self.get_from_disk_cache(new_text)
        if disk_value is not None:
          self.disk_keys.remove(new_text)
          self.disk_keys.append(new_text)
          return disk_value
        
        predicted_rating = predict(new_text)
        if self.ram_size() == self.ram_max:
          if self.disk_size() == self.disk_max:
            least_recent_disk_key = self.disk_keys.popleft()
            self.remove_from_disk_cache(least_recent_disk_key)
          least_recent_ram_key = self.ram_keys.popleft()
          rating = self.get_from_ram_cache(least_recent_ram_key)
          self.remove_from_ram_cache(least_recent_ram_key)
          self.add_to_disk_cache(least_recent_ram_key, rating)
          self.disk_keys.append(least_recent_ram_key)

        self.add_to_ram_cache(new_text, predicted_rating)
        self.ram_keys.append(new_text)
        return predicted_rating
        # raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_keys = collections.deque()
        self.disk_keys = collections.deque()

    def get(self, new_text):
        # implement the caching algorithm
        ram_value = self.get_from_ram_cache(new_text)
        if ram_value is not None:
          self.ram_keys.remove(new_text)
          self.ram_keys.append(new_text)
          return ram_value
        
        disk_value = self.get_from_disk_cache(new_text)
        if disk_value is not None:
          self.disk_keys.remove(new_text)
          self.disk_keys.append(new_text)
          return disk_value
        
        predicted_rating = predict(new_text)
        if self.ram_size() == self.ram_max:
          if self.disk_size() == self.disk_max:
            least_recent_disk_key = self.disk_keys.popleft()
            self.remove_from_disk_cache(least_recent_disk_key)
          least_recent_ram_key = self.ram_keys.popleft()
          rating = self.get_from_ram_cache(least_recent_ram_key)
          self.remove_from_ram_cache(least_recent_ram_key)
          self.add_to_disk_cache(least_recent_ram_key, rating)
          self.disk_keys.append(least_recent_ram_key)

        self.add_to_ram_cache(new_text, predicted_rating)
        self.ram_keys.append(new_text)
        return predicted_rating
        # raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    # print(""predic is correct"")
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    # print(""RAM is correct"")
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
7907,2,"        disk_value = self.get_from_disk_cache(new_text)
        if disk_value is not None:
          self.disk_keys.remove(new_text)
          self.disk_keys.append(new_text)
          return disk_value",TODO,disk_value,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_keys = collections.deque()
        self.disk_keys = collections.deque()

    def get(self, new_text):
        # implement the caching algorithm
        ram_value = self.get_from_ram_cache(new_text)
        if ram_value is not None:
          self.ram_keys.remove(new_text)
          self.ram_keys.append(new_text)
          return ram_value
        
        disk_value = self.get_from_disk_cache(new_text)
        if disk_value is not None:
          self.disk_keys.remove(new_text)
          self.disk_keys.append(new_text)
          return disk_value
        
        predicted_rating = predict(new_text)
        if self.ram_size() == self.ram_max:
          if self.disk_size() == self.disk_max:
            least_recent_disk_key = self.disk_keys.popleft()
            self.remove_from_disk_cache(least_recent_disk_key)
          least_recent_ram_key = self.ram_keys.popleft()
          rating = self.get_from_ram_cache(least_recent_ram_key)
          self.remove_from_ram_cache(least_recent_ram_key)
          self.add_to_disk_cache(least_recent_ram_key, rating)
          self.disk_keys.append(least_recent_ram_key)

        self.add_to_ram_cache(new_text, predicted_rating)
        self.ram_keys.append(new_text)
        return predicted_rating
        # raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_keys = collections.deque()
        self.disk_keys = collections.deque()

    def get(self, new_text):
        # implement the caching algorithm
        ram_value = self.get_from_ram_cache(new_text)
        if ram_value is not None:
          self.ram_keys.remove(new_text)
          self.ram_keys.append(new_text)
          return ram_value
        
        disk_value = self.get_from_disk_cache(new_text)
        if disk_value is not None:
          self.disk_keys.remove(new_text)
          self.disk_keys.append(new_text)
          return disk_value
        
        predicted_rating = predict(new_text)
        if self.ram_size() == self.ram_max:
          if self.disk_size() == self.disk_max:
            least_recent_disk_key = self.disk_keys.popleft()
            self.remove_from_disk_cache(least_recent_disk_key)
          least_recent_ram_key = self.ram_keys.popleft()
          rating = self.get_from_ram_cache(least_recent_ram_key)
          self.remove_from_ram_cache(least_recent_ram_key)
          self.add_to_disk_cache(least_recent_ram_key, rating)
          self.disk_keys.append(least_recent_ram_key)

        self.add_to_ram_cache(new_text, predicted_rating)
        self.ram_keys.append(new_text)
        return predicted_rating
        # raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    # print(""predic is correct"")
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    # print(""RAM is correct"")
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
7907,3,"        predicted_rating = predict(new_text)
        if self.ram_size() == self.ram_max:
          if self.disk_size() == self.disk_max:
            least_recent_disk_key = self.disk_keys.popleft()
            self.remove_from_disk_cache(least_recent_disk_key)
          least_recent_ram_key = self.ram_keys.popleft()
          rating = self.get_from_ram_cache(least_recent_ram_key)
          self.remove_from_ram_cache(least_recent_ram_key)
          self.add_to_disk_cache(least_recent_ram_key, rating)
          self.disk_keys.append(least_recent_ram_key)

        self.add_to_ram_cache(new_text, predicted_rating)
        self.ram_keys.append(new_text)
        return predicted_rating",TODO,predicted_rating,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_keys = collections.deque()
        self.disk_keys = collections.deque()

    def get(self, new_text):
        # implement the caching algorithm
        ram_value = self.get_from_ram_cache(new_text)
        if ram_value is not None:
          self.ram_keys.remove(new_text)
          self.ram_keys.append(new_text)
          return ram_value
        
        disk_value = self.get_from_disk_cache(new_text)
        if disk_value is not None:
          self.disk_keys.remove(new_text)
          self.disk_keys.append(new_text)
          return disk_value
        
        predicted_rating = predict(new_text)
        if self.ram_size() == self.ram_max:
          if self.disk_size() == self.disk_max:
            least_recent_disk_key = self.disk_keys.popleft()
            self.remove_from_disk_cache(least_recent_disk_key)
          least_recent_ram_key = self.ram_keys.popleft()
          rating = self.get_from_ram_cache(least_recent_ram_key)
          self.remove_from_ram_cache(least_recent_ram_key)
          self.add_to_disk_cache(least_recent_ram_key, rating)
          self.disk_keys.append(least_recent_ram_key)

        self.add_to_ram_cache(new_text, predicted_rating)
        self.ram_keys.append(new_text)
        return predicted_rating
        # raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_keys = collections.deque()
        self.disk_keys = collections.deque()

    def get(self, new_text):
        # implement the caching algorithm
        ram_value = self.get_from_ram_cache(new_text)
        if ram_value is not None:
          self.ram_keys.remove(new_text)
          self.ram_keys.append(new_text)
          return ram_value
        
        disk_value = self.get_from_disk_cache(new_text)
        if disk_value is not None:
          self.disk_keys.remove(new_text)
          self.disk_keys.append(new_text)
          return disk_value
        
        predicted_rating = predict(new_text)
        if self.ram_size() == self.ram_max:
          if self.disk_size() == self.disk_max:
            least_recent_disk_key = self.disk_keys.popleft()
            self.remove_from_disk_cache(least_recent_disk_key)
          least_recent_ram_key = self.ram_keys.popleft()
          rating = self.get_from_ram_cache(least_recent_ram_key)
          self.remove_from_ram_cache(least_recent_ram_key)
          self.add_to_disk_cache(least_recent_ram_key, rating)
          self.disk_keys.append(least_recent_ram_key)

        self.add_to_ram_cache(new_text, predicted_rating)
        self.ram_keys.append(new_text)
        return predicted_rating
        # raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    # print(""predic is correct"")
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    # print(""RAM is correct"")
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
7907,4,"          least_recent_ram_key = self.ram_keys.popleft()
          rating = self.get_from_ram_cache(least_recent_ram_key)
          self.remove_from_ram_cache(least_recent_ram_key)
          self.add_to_disk_cache(least_recent_ram_key, rating)
          self.disk_keys.append(least_recent_ram_key)",TODO,least_recent_ram_key,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_keys = collections.deque()
        self.disk_keys = collections.deque()

    def get(self, new_text):
        # implement the caching algorithm
        ram_value = self.get_from_ram_cache(new_text)
        if ram_value is not None:
          self.ram_keys.remove(new_text)
          self.ram_keys.append(new_text)
          return ram_value
        
        disk_value = self.get_from_disk_cache(new_text)
        if disk_value is not None:
          self.disk_keys.remove(new_text)
          self.disk_keys.append(new_text)
          return disk_value
        
        predicted_rating = predict(new_text)
        if self.ram_size() == self.ram_max:
          if self.disk_size() == self.disk_max:
            least_recent_disk_key = self.disk_keys.popleft()
            self.remove_from_disk_cache(least_recent_disk_key)
          least_recent_ram_key = self.ram_keys.popleft()
          rating = self.get_from_ram_cache(least_recent_ram_key)
          self.remove_from_ram_cache(least_recent_ram_key)
          self.add_to_disk_cache(least_recent_ram_key, rating)
          self.disk_keys.append(least_recent_ram_key)

        self.add_to_ram_cache(new_text, predicted_rating)
        self.ram_keys.append(new_text)
        return predicted_rating
        # raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_keys = collections.deque()
        self.disk_keys = collections.deque()

    def get(self, new_text):
        # implement the caching algorithm
        ram_value = self.get_from_ram_cache(new_text)
        if ram_value is not None:
          self.ram_keys.remove(new_text)
          self.ram_keys.append(new_text)
          return ram_value
        
        disk_value = self.get_from_disk_cache(new_text)
        if disk_value is not None:
          self.disk_keys.remove(new_text)
          self.disk_keys.append(new_text)
          return disk_value
        
        predicted_rating = predict(new_text)
        if self.ram_size() == self.ram_max:
          if self.disk_size() == self.disk_max:
            least_recent_disk_key = self.disk_keys.popleft()
            self.remove_from_disk_cache(least_recent_disk_key)
          least_recent_ram_key = self.ram_keys.popleft()
          rating = self.get_from_ram_cache(least_recent_ram_key)
          self.remove_from_ram_cache(least_recent_ram_key)
          self.add_to_disk_cache(least_recent_ram_key, rating)
          self.disk_keys.append(least_recent_ram_key)

        self.add_to_ram_cache(new_text, predicted_rating)
        self.ram_keys.append(new_text)
        return predicted_rating
        # raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    # print(""predic is correct"")
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    # print(""RAM is correct"")
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
7907,5,"          rating = self.get_from_ram_cache(least_recent_ram_key)
          self.remove_from_ram_cache(least_recent_ram_key)
          self.add_to_disk_cache(least_recent_ram_key, rating)",TODO,rating,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_keys = collections.deque()
        self.disk_keys = collections.deque()

    def get(self, new_text):
        # implement the caching algorithm
        ram_value = self.get_from_ram_cache(new_text)
        if ram_value is not None:
          self.ram_keys.remove(new_text)
          self.ram_keys.append(new_text)
          return ram_value
        
        disk_value = self.get_from_disk_cache(new_text)
        if disk_value is not None:
          self.disk_keys.remove(new_text)
          self.disk_keys.append(new_text)
          return disk_value
        
        predicted_rating = predict(new_text)
        if self.ram_size() == self.ram_max:
          if self.disk_size() == self.disk_max:
            least_recent_disk_key = self.disk_keys.popleft()
            self.remove_from_disk_cache(least_recent_disk_key)
          least_recent_ram_key = self.ram_keys.popleft()
          rating = self.get_from_ram_cache(least_recent_ram_key)
          self.remove_from_ram_cache(least_recent_ram_key)
          self.add_to_disk_cache(least_recent_ram_key, rating)
          self.disk_keys.append(least_recent_ram_key)

        self.add_to_ram_cache(new_text, predicted_rating)
        self.ram_keys.append(new_text)
        return predicted_rating
        # raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_keys = collections.deque()
        self.disk_keys = collections.deque()

    def get(self, new_text):
        # implement the caching algorithm
        ram_value = self.get_from_ram_cache(new_text)
        if ram_value is not None:
          self.ram_keys.remove(new_text)
          self.ram_keys.append(new_text)
          return ram_value
        
        disk_value = self.get_from_disk_cache(new_text)
        if disk_value is not None:
          self.disk_keys.remove(new_text)
          self.disk_keys.append(new_text)
          return disk_value
        
        predicted_rating = predict(new_text)
        if self.ram_size() == self.ram_max:
          if self.disk_size() == self.disk_max:
            least_recent_disk_key = self.disk_keys.popleft()
            self.remove_from_disk_cache(least_recent_disk_key)
          least_recent_ram_key = self.ram_keys.popleft()
          rating = self.get_from_ram_cache(least_recent_ram_key)
          self.remove_from_ram_cache(least_recent_ram_key)
          self.add_to_disk_cache(least_recent_ram_key, rating)
          self.disk_keys.append(least_recent_ram_key)

        self.add_to_ram_cache(new_text, predicted_rating)
        self.ram_keys.append(new_text)
        return predicted_rating
        # raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    # print(""predic is correct"")
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    # print(""RAM is correct"")
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
7907,6,"            least_recent_disk_key = self.disk_keys.popleft()
            self.remove_from_disk_cache(least_recent_disk_key)",TODO,least_recent_disk_key,"class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_keys = collections.deque()
        self.disk_keys = collections.deque()

    def get(self, new_text):
        # implement the caching algorithm
        ram_value = self.get_from_ram_cache(new_text)
        if ram_value is not None:
          self.ram_keys.remove(new_text)
          self.ram_keys.append(new_text)
          return ram_value
        
        disk_value = self.get_from_disk_cache(new_text)
        if disk_value is not None:
          self.disk_keys.remove(new_text)
          self.disk_keys.append(new_text)
          return disk_value
        
        predicted_rating = predict(new_text)
        if self.ram_size() == self.ram_max:
          if self.disk_size() == self.disk_max:
            least_recent_disk_key = self.disk_keys.popleft()
            self.remove_from_disk_cache(least_recent_disk_key)
          least_recent_ram_key = self.ram_keys.popleft()
          rating = self.get_from_ram_cache(least_recent_ram_key)
          self.remove_from_ram_cache(least_recent_ram_key)
          self.add_to_disk_cache(least_recent_ram_key, rating)
          self.disk_keys.append(least_recent_ram_key)

        self.add_to_ram_cache(new_text, predicted_rating)
        self.ram_keys.append(new_text)
        return predicted_rating
        # raise NotImplementedError(""Complete this function!"")","class MostRecentCache(Cache):
    def __init__(self, ram_max, disk_max):
        '''
        Args:
            ram_max (Integer) - max number of elements that at any moment of time should be in hot storage
            disk_max (Integer) - max number of elements that at any moment of time should be in cold storage
        '''
        super().__init__()
        self.ram_max = ram_max
        self.disk_max = disk_max
        
        # initialize additional fields here
        self.ram_keys = collections.deque()
        self.disk_keys = collections.deque()

    def get(self, new_text):
        # implement the caching algorithm
        ram_value = self.get_from_ram_cache(new_text)
        if ram_value is not None:
          self.ram_keys.remove(new_text)
          self.ram_keys.append(new_text)
          return ram_value
        
        disk_value = self.get_from_disk_cache(new_text)
        if disk_value is not None:
          self.disk_keys.remove(new_text)
          self.disk_keys.append(new_text)
          return disk_value
        
        predicted_rating = predict(new_text)
        if self.ram_size() == self.ram_max:
          if self.disk_size() == self.disk_max:
            least_recent_disk_key = self.disk_keys.popleft()
            self.remove_from_disk_cache(least_recent_disk_key)
          least_recent_ram_key = self.ram_keys.popleft()
          rating = self.get_from_ram_cache(least_recent_ram_key)
          self.remove_from_ram_cache(least_recent_ram_key)
          self.add_to_disk_cache(least_recent_ram_key, rating)
          self.disk_keys.append(least_recent_ram_key)

        self.add_to_ram_cache(new_text, predicted_rating)
        self.ram_keys.append(new_text)
        return predicted_rating
        # raise NotImplementedError(""Complete this function!"")
def test_most_recent_cache():
    c = MostRecentCache(5, 20)

    for i in range(200):
        c.get('test' + str(i))
        
    assert list(c.cache_ram.keys()) == ['test195', 'test196', 'test197', 'test198', 'test199'], ""RAM cache incorrect!""
    
    keys_disk = set()
    with open(c.cache_disk, ""r"") as f:
        for line in f:
            li = line.replace('\n', '').split(' ')
            keys_disk.add(li[0])
    assert keys_disk == {'04a81612669a6503309a3d0a7eb144d47d2822db',
                         '06587144b7eefa0a96389e4edc25db495a24a2cc',
                         '0e51c498407d2bf5bf19a6be003cc65e194337a8',
                         '17dd06e1d4e670fb47b41a753da6a6dfcd87bf30',
                         '26722c1c23e2ab91a5668f711753be9d8a1f6992',
                         '2a373681c4cd01ca424fcc555dd834e417439ce9',
                         '34d422df9f806e415383af86c4400ef7900c4b8f',
                         '5fe02a47e76b0fd9c9297aa1f550df9465990d8b',
                         '6656f767a33b26804aca1f4095ee04b46f9f83dc',
                         '6fbd48d13c9d5e5e4534aec0aca98fd5efecca61',
                         '899ec74db661e24fe58a4f6f302283acd4935ccf',
                         '8e3356790f84402bda95cca9b46fe4b8d8d01e3f',
                         'a15f59d3eb58f1029ccb0a2646d24a3c8a917c29',
                         'a19c3ea08bd1aede544ae465d3799f46d31dd9b7',
                         'ab0fc2097f253a95cc4d689474eb1d7774fa9bb7',
                         'ad1c952a702edca89390aed15dda415100544632',
                         'c34af7f961e92657a743b7390a76e0107c05abc8',
                         'e8efd85f316e5d8594ec24cac041d406ede83fb6',
                         'f8c8ae4bc1e7e9dd003b189787de87c05f413a53',
                         'fed660342e30a9cacdb314c3e8fde3cdab8dba87'}, ""Disk cache incorrect!""
    
    assert c.get('test') == 10, ""Prediction of new values is incorrect!""
    # print(""predic is correct"")
    assert c.get('test196') == 9, ""Prediction of values from RAM is incorrect!""
    # print(""RAM is correct"")
    assert c.get('test178') == 9, ""Prediction of values from disk is incorrect!""
    print(""All tests passed!"")

test_most_recent_cache()"
6785,1,"    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)",TODO,name,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
6785,2,"    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab",TODO,vocab,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
6785,3,"    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()",TODO,description,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
6785,4,"    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])",TODO,descrpition_words,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
6785,5,"    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list",TODO,spans_list,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
6785,6,"        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end",TODO,node,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
6785,7,"        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)",TODO,processedName,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
6785,8,"            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))",TODO,neighborhood_name,"def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")","def preprocess_name(name):
    """"""Perform some text cleaning on the neighborhood name""""""
    if str(name) == ""nan"":
        return None
    
    # convert to lowercase and remove ""-""
    name = name.lower().replace(""-"", "" "")
    
    # replace occurences of multiple spaces with a single space
    return re.sub(r""\s+"", "" "", name)

def check_valid_name(processed_name):
    """"""Check if a neighborhood name is valid""""""
    return processed_name is not None and len(processed_name) > 5


class TrieNode:
    def __init__(self):
        self.children = {}
        self.end = False

class Trie:
    def __init__(self):
        self.root = TrieNode()

    def add(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        node.end= True

    def find(self, word):
        node = self.root
        for char in word:
            if char not in node.children:
                return False
            node = node.children[char]
        return node.end



def build_neighborhood_vocab(neighborhoods):
    """"""
    Construct a vocabulary of processed and valid neighborhood names
    
    args:
        neighborhoods (List[str]) : a list of raw and unique neighborhood names from the dataset
    
    returns:
        collections : a data structure that stores the neighborhood names
    """"""
    vocab = Trie()
    for neighborhoodName in neighborhoods:
        processedName = preprocess_name(neighborhoodName)
        if check_valid_name(processedName):
            vocab.add(processedName)
    return vocab
    
    
def find_spans(description, neighborhood_vocab):
    """"""
    Identify all spans of neighborhood names in a given string description, based on the given neighborhood vocabulary
    
    args:
        description (str) : a string description of an AirBnB home
        neighborhood_vocab (collections) : the vocabulary of all possible neighborhood names
    """"""
    # preprocess the description text, do not modify this code
    if str(description) == ""nan"":
        return []
    description = description.lower()
    for p in string.punctuation:
        description = description.replace(p, "" "")
    description = re.sub(r""\s+"", "" "", description)
    descrpition_words = description.split()
    spans_list = []
    for i in range(len(descrpition_words)):
        for j in range(i, len(descrpition_words)):
            neighborhood_name = ' '.join(descrpition_words[i:j+1])
            if neighborhood_vocab.find(neighborhood_name):
                spans_list.append((i, j, neighborhood_name))
    return spans_list
    # fill in your implementation hered
    raise NotImplementedError(""Implement this task"")
def test_find_spans():
    neighborhood_vocab = build_neighborhood_vocab(airbnb_data.host_neighbourhood)
    test = [find_spans(x, neighborhood_vocab) for x in airbnb_data.description.head(10)]
    ref = [[(48, 49, 'indische buurt')], [], [(7, 7, 'jordaan')], [], [], [], [], [(18, 18, 'jordaan')],
           [(143, 143, 'grachtengordel'), (149, 149, 'grachtengordel')], [(66, 67, 'park view')]]
    assert all([x1 == x2 for x1, x2 in zip(test,ref)]), ""Check your implementation!""
    print(""All tests passed!"")

test_find_spans()
"
7545,1,"    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val",TODO,ret_val,"def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val","def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val
def test_check_list_ids():
    test_input_ids = [15795, 860, 76820, 54886, 6265, 82386, 37194, 87498, 44131, 60263]
    ref = [False, True, True, False, True, False, False, False, False, False]
    assert check_list_ids(test_input_ids, blacklist, whitelist) == ref
    print(""All tests passed!"")

test_check_list_ids()

# let's also see how long it takes to run this function
random.seed(42)
test_input_ids = [random.randint(0, 100000) for x in range(1000000)]
%timeit check_list_ids(test_input_ids, blacklist, whitelist)"
13367,1,"    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val",TODO,ret_val,"def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val","def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val
def test_check_list_ids():
    test_input_ids = [15795, 860, 76820, 54886, 6265, 82386, 37194, 87498, 44131, 60263]
    ref = [False, True, True, False, True, False, False, False, False, False]
    assert check_list_ids(test_input_ids, blacklist, whitelist) == ref
    print(""All tests passed!"")

test_check_list_ids()

# let's also see how long it takes to run this function
random.seed(42)
test_input_ids = [random.randint(0, 100000) for x in range(1000000)]
%timeit check_list_ids(test_input_ids, blacklist, whitelist)"
10523,1,"    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val",TODO,ret_val,"def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val","def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val
def test_check_list_ids():
    test_input_ids = [15795, 860, 76820, 54886, 6265, 82386, 37194, 87498, 44131, 60263]
    ref = [False, True, True, False, True, False, False, False, False, False]
    assert check_list_ids(test_input_ids, blacklist, whitelist) == ref
    print(""All tests passed!"")

test_check_list_ids()

# let's also see how long it takes to run this function
random.seed(42)
test_input_ids = [random.randint(0, 100000) for x in range(1000000)]
%timeit check_list_ids(test_input_ids, blacklist, whitelist)"
10410,1,"    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val",TODO,ret_val,"def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val","def check_list_ids(input_ids, blacklist, whitelist):
    '''
    Checks whether each ID in an input list of IDs is banned, based on the stored blacklist and whiteliist
    
    args:
        input_ids (List[int]) : the user ID that you need to check
        blacklist (collections[int]) : a data structure storing all of the blacklisted IDs
        whitelist (collections[int]) : a data structure storing all of the whitelisted IDs
    
    returns:
        List[bool] : a list having the same length as input_ids, where the entry at index i
            is True if input_ids[i] is banned, and False otherwise
    '''
    ret_val = []
    for id in input_ids:
      ret_val.append(check_single_id(id, blacklist, whitelist))
    return ret_val
def test_check_list_ids():
    test_input_ids = [15795, 860, 76820, 54886, 6265, 82386, 37194, 87498, 44131, 60263]
    ref = [False, True, True, False, True, False, False, False, False, False]
    assert check_list_ids(test_input_ids, blacklist, whitelist) == ref
    print(""All tests passed!"")

test_check_list_ids()

# let's also see how long it takes to run this function
random.seed(42)
test_input_ids = [random.randint(0, 100000) for x in range(1000000)]
%timeit check_list_ids(test_input_ids, blacklist, whitelist)"
27095,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    def split_country(country):
        regex_exp = r""\s\((\w+|\s)+\)""
        return re.sub(regex_exp, """", country)

    country_data['Country'] = country_data['Country'].apply(split_country)
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]",TODO,countries_to_remove,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    def split_country(country):
        regex_exp = r""\s\((\w+|\s)+\)""
        return re.sub(regex_exp, """", country)

    country_data['Country'] = country_data['Country'].apply(split_country)
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    def split_country(country):
        regex_exp = r""\s\((\w+|\s)+\)""
        return re.sub(regex_exp, """", country)

    country_data['Country'] = country_data['Country'].apply(split_country)
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
27095,2,"    country_data['Country'] = country_data['Country'].apply(split_country)
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data",TODO,country_data,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    def split_country(country):
        regex_exp = r""\s\((\w+|\s)+\)""
        return re.sub(regex_exp, """", country)

    country_data['Country'] = country_data['Country'].apply(split_country)
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    def split_country(country):
        regex_exp = r""\s\((\w+|\s)+\)""
        return re.sub(regex_exp, """", country)

    country_data['Country'] = country_data['Country'].apply(split_country)
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
27095,3,"        regex_exp = r""\s\((\w+|\s)+\)""
        return re.sub(regex_exp, """", country)",TODO,regex_exp,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    def split_country(country):
        regex_exp = r""\s\((\w+|\s)+\)""
        return re.sub(regex_exp, """", country)

    country_data['Country'] = country_data['Country'].apply(split_country)
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    def split_country(country):
        regex_exp = r""\s\((\w+|\s)+\)""
        return re.sub(regex_exp, """", country)

    country_data['Country'] = country_data['Country'].apply(split_country)
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
24360,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country']=country_data['Country'].str.replace(r'\s\(.*\)','')
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]",TODO,countries_to_remove,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country']=country_data['Country'].str.replace(r'\s\(.*\)','')
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country']=country_data['Country'].str.replace(r'\s\(.*\)','')
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
24360,2,"    country_data['Country']=country_data['Country'].str.replace(r'\s\(.*\)','')
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data",TODO,country_data,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country']=country_data['Country'].str.replace(r'\s\(.*\)','')
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country']=country_data['Country'].str.replace(r'\s\(.*\)','')
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
26530,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country'] = [each.split(' (')[0] for each in list(country_data['Country'])]
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]",TODO,countries_to_remove,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country'] = [each.split(' (')[0] for each in list(country_data['Country'])]
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country'] = [each.split(' (')[0] for each in list(country_data['Country'])]
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
26530,2,"    country_data['Country'] = [each.split(' (')[0] for each in list(country_data['Country'])]
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data",TODO,country_data,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country'] = [each.split(' (')[0] for each in list(country_data['Country'])]
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country'] = [each.split(' (')[0] for each in list(country_data['Country'])]
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]
    return country_data
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
25265,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    df = country_data.copy()
    df['Country'] = df['Country'].str.split(' \(').str[0]
    df = df[~df['Country'].isin(countries_to_remove)]",TODO,countries_to_remove,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    df = country_data.copy()
    df['Country'] = df['Country'].str.split(' \(').str[0]
    df = df[~df['Country'].isin(countries_to_remove)]
    return df","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    df = country_data.copy()
    df['Country'] = df['Country'].str.split(' \(').str[0]
    df = df[~df['Country'].isin(countries_to_remove)]
    return df
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
25265,2,"    df = country_data.copy()
    df['Country'] = df['Country'].str.split(' \(').str[0]
    df = df[~df['Country'].isin(countries_to_remove)]
    return df",TODO,df,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    df = country_data.copy()
    df['Country'] = df['Country'].str.split(' \(').str[0]
    df = df[~df['Country'].isin(countries_to_remove)]
    return df","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    df = country_data.copy()
    df['Country'] = df['Country'].str.split(' \(').str[0]
    df = df[~df['Country'].isin(countries_to_remove)]
    return df
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
26755,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data[""Country""] = country_data[""Country""].apply(lambda x: re.sub(regex_exp,"""",x))
    return country_data[~country_data[""Country""].isin(countries_to_remove)]",TODO,countries_to_remove,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data[""Country""] = country_data[""Country""].apply(lambda x: re.sub(regex_exp,"""",x))
    return country_data[~country_data[""Country""].isin(countries_to_remove)]","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data[""Country""] = country_data[""Country""].apply(lambda x: re.sub(regex_exp,"""",x))
    return country_data[~country_data[""Country""].isin(countries_to_remove)]
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579

    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
26755,2,"    regex_exp = r""\s\((\w+|\s)+\)""
    country_data[""Country""] = country_data[""Country""].apply(lambda x: re.sub(regex_exp,"""",x))",TODO,regex_exp,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data[""Country""] = country_data[""Country""].apply(lambda x: re.sub(regex_exp,"""",x))
    return country_data[~country_data[""Country""].isin(countries_to_remove)]","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data[""Country""] = country_data[""Country""].apply(lambda x: re.sub(regex_exp,"""",x))
    return country_data[~country_data[""Country""].isin(countries_to_remove)]
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579

    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
24905,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    #print(len(country_data))
    country_data['Country'] = country_data['Country'].str.replace(""([\w\s]*\w)\s*\([\w\s]*\)"", ""(\1)"")
    df_no_nan = country_data[~country_data['Country'].isin(countries_to_remove)]",TODO,countries_to_remove,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    #print(len(country_data))
    country_data['Country'] = country_data['Country'].str.replace(""([\w\s]*\w)\s*\([\w\s]*\)"", ""(\1)"")
    df_no_nan = country_data[~country_data['Country'].isin(countries_to_remove)]
    #print(len(df_no_nan))
    #display(df_no_nan.head())
    #print(pd.unique(df_no_nan['Country']))
    return df_no_nan
    ","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    #print(len(country_data))
    country_data['Country'] = country_data['Country'].str.replace(""([\w\s]*\w)\s*\([\w\s]*\)"", ""(\1)"")
    df_no_nan = country_data[~country_data['Country'].isin(countries_to_remove)]
    #print(len(df_no_nan))
    #display(df_no_nan.head())
    #print(pd.unique(df_no_nan['Country']))
    return df_no_nan
    
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
24905,2,"    df_no_nan = country_data[~country_data['Country'].isin(countries_to_remove)]
    #print(len(df_no_nan))
    #display(df_no_nan.head())
    #print(pd.unique(df_no_nan['Country']))
    return df_no_nan",TODO,df_no_nan,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    #print(len(country_data))
    country_data['Country'] = country_data['Country'].str.replace(""([\w\s]*\w)\s*\([\w\s]*\)"", ""(\1)"")
    df_no_nan = country_data[~country_data['Country'].isin(countries_to_remove)]
    #print(len(df_no_nan))
    #display(df_no_nan.head())
    #print(pd.unique(df_no_nan['Country']))
    return df_no_nan
    ","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    #print(len(country_data))
    country_data['Country'] = country_data['Country'].str.replace(""([\w\s]*\w)\s*\([\w\s]*\)"", ""(\1)"")
    df_no_nan = country_data[~country_data['Country'].isin(countries_to_remove)]
    #print(len(df_no_nan))
    #display(df_no_nan.head())
    #print(pd.unique(df_no_nan['Country']))
    return df_no_nan
    
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
26810,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
   
    #Before dropping NaN
   # print(country_data.head())
    #country = ' '

    
    
    #Simply country names 
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data['Country'] = country_data['Country'].str.replace(regex_exp, '') 
    
    #Remove countries
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]    ",TODO,countries_to_remove,"#Used text processing primer to help removed country affliation so that one the country name remains
countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
   
    #Before dropping NaN
   # print(country_data.head())
    #country = ' '

    
    
    #Simply country names 
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data['Country'] = country_data['Country'].str.replace(regex_exp, '') 
    
    #Remove countries
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]    
   
       
    #remove countries from list
    #country_data = country_data['Country'].str.replace(regex_exp, """")
    #print(len(country_data)) 
    
   # print(f""Length after dropping rows with NaN: {len(country_data)}"")
    #print(f""Unique countries after dropping rows with NaN: {len(country_data['Country'].unique())}"")
    
    
    return country_data","#Used text processing primer to help removed country affliation so that one the country name remains
countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
   
    #Before dropping NaN
   # print(country_data.head())
    #country = ' '

    
    
    #Simply country names 
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data['Country'] = country_data['Country'].str.replace(regex_exp, '') 
    
    #Remove countries
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]    
   
       
    #remove countries from list
    #country_data = country_data['Country'].str.replace(regex_exp, """")
    #print(len(country_data)) 
    
   # print(f""Length after dropping rows with NaN: {len(country_data)}"")
    #print(f""Unique countries after dropping rows with NaN: {len(country_data['Country'].unique())}"")
    
    
    return country_data
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    #df_country_cleaned = preprocess_countries(df_country)
   # print(len(df_country_cleaned))  # Print the length of the cleaned DataFrame

    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
26810,2,"    regex_exp = r""\s\((\w+|\s)+\)""
    country_data['Country'] = country_data['Country'].str.replace(regex_exp, '') ",TODO,regex_exp,"#Used text processing primer to help removed country affliation so that one the country name remains
countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
   
    #Before dropping NaN
   # print(country_data.head())
    #country = ' '

    
    
    #Simply country names 
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data['Country'] = country_data['Country'].str.replace(regex_exp, '') 
    
    #Remove countries
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]    
   
       
    #remove countries from list
    #country_data = country_data['Country'].str.replace(regex_exp, """")
    #print(len(country_data)) 
    
   # print(f""Length after dropping rows with NaN: {len(country_data)}"")
    #print(f""Unique countries after dropping rows with NaN: {len(country_data['Country'].unique())}"")
    
    
    return country_data","#Used text processing primer to help removed country affliation so that one the country name remains
countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
   
    #Before dropping NaN
   # print(country_data.head())
    #country = ' '

    
    
    #Simply country names 
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data['Country'] = country_data['Country'].str.replace(regex_exp, '') 
    
    #Remove countries
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]    
   
       
    #remove countries from list
    #country_data = country_data['Country'].str.replace(regex_exp, """")
    #print(len(country_data)) 
    
   # print(f""Length after dropping rows with NaN: {len(country_data)}"")
    #print(f""Unique countries after dropping rows with NaN: {len(country_data['Country'].unique())}"")
    
    
    return country_data
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    #df_country_cleaned = preprocess_countries(df_country)
   # print(len(df_country_cleaned))  # Print the length of the cleaned DataFrame

    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
26810,3,"    country_data['Country'] = country_data['Country'].str.replace(regex_exp, '') 
    
    #Remove countries
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]    
   
       
    #remove countries from list
    #country_data = country_data['Country'].str.replace(regex_exp, """")
    #print(len(country_data)) 
    
   # print(f""Length after dropping rows with NaN: {len(country_data)}"")
    #print(f""Unique countries after dropping rows with NaN: {len(country_data['Country'].unique())}"")
    
    
    return country_data",TODO,country_data,"#Used text processing primer to help removed country affliation so that one the country name remains
countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
   
    #Before dropping NaN
   # print(country_data.head())
    #country = ' '

    
    
    #Simply country names 
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data['Country'] = country_data['Country'].str.replace(regex_exp, '') 
    
    #Remove countries
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]    
   
       
    #remove countries from list
    #country_data = country_data['Country'].str.replace(regex_exp, """")
    #print(len(country_data)) 
    
   # print(f""Length after dropping rows with NaN: {len(country_data)}"")
    #print(f""Unique countries after dropping rows with NaN: {len(country_data['Country'].unique())}"")
    
    
    return country_data","#Used text processing primer to help removed country affliation so that one the country name remains
countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.
    
    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess
    
    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
   
    #Before dropping NaN
   # print(country_data.head())
    #country = ' '

    
    
    #Simply country names 
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data['Country'] = country_data['Country'].str.replace(regex_exp, '') 
    
    #Remove countries
    country_data = country_data[~country_data['Country'].isin(countries_to_remove)]    
   
       
    #remove countries from list
    #country_data = country_data['Country'].str.replace(regex_exp, """")
    #print(len(country_data)) 
    
   # print(f""Length after dropping rows with NaN: {len(country_data)}"")
    #print(f""Unique countries after dropping rows with NaN: {len(country_data['Country'].unique())}"")
    
    
    return country_data
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579
    
    #df_country_cleaned = preprocess_countries(df_country)
   # print(len(df_country_cleaned))  # Print the length of the cleaned DataFrame

    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
26780,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country'] = country_data['Country'].apply(lambda s : re.sub(' \(.*\)', '', s))
    # print(country_data.Country.unique())
    for country_to_remove in countries_to_remove:",TODO,countries_to_remove,"import re

countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country'] = country_data['Country'].apply(lambda s : re.sub(' \(.*\)', '', s))
    # print(country_data.Country.unique())
    for country_to_remove in countries_to_remove:
      country_data.drop(country_data.loc[country_data['Country'] == country_to_remove].index, inplace = True)
    # print(country_data.drop(country_data.loc[country_data['Country'] in countries_to_remove]))
    # country_data = country_data[country_data['Country'] not in countries_to_remove]
    return country_data

# df_country_cleaned = preprocess_countries(df_country_copy)
# print(df_country_cleaned)

# s = ""Netherlands (Europe)""
# print(re.sub(' \(.*\)', '', s))","import re

countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    country_data['Country'] = country_data['Country'].apply(lambda s : re.sub(' \(.*\)', '', s))
    # print(country_data.Country.unique())
    for country_to_remove in countries_to_remove:
      country_data.drop(country_data.loc[country_data['Country'] == country_to_remove].index, inplace = True)
    # print(country_data.drop(country_data.loc[country_data['Country'] in countries_to_remove]))
    # country_data = country_data[country_data['Country'] not in countries_to_remove]
    return country_data

# df_country_cleaned = preprocess_countries(df_country_copy)
# print(df_country_cleaned)

# s = ""Netherlands (Europe)""
# print(re.sub(' \(.*\)', '', s))
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579

    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
24440,1,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data['Country'] = country_data['Country'].apply(lambda a: re.sub(regex_exp, """", a))
    return country_data[~country_data['Country'].isin(countries_to_remove)]",TODO,countries_to_remove,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data['Country'] = country_data['Country'].apply(lambda a: re.sub(regex_exp, """", a))
    return country_data[~country_data['Country'].isin(countries_to_remove)]","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data['Country'] = country_data['Country'].apply(lambda a: re.sub(regex_exp, """", a))
    return country_data[~country_data['Country'].isin(countries_to_remove)]
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579

    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
24440,2,"    regex_exp = r""\s\((\w+|\s)+\)""
    country_data['Country'] = country_data['Country'].apply(lambda a: re.sub(regex_exp, """", a))",TODO,regex_exp,"countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data['Country'] = country_data['Country'].apply(lambda a: re.sub(regex_exp, """", a))
    return country_data[~country_data['Country'].isin(countries_to_remove)]","countries_to_remove = ['Denmark', 'Antarctica', 'France', 'Europe',
    'Netherlands','United Kingdom', 'Africa', 'South America']

def preprocess_countries(country_data):
    """"""
    Remove countries without temperature information from the input dataframe
    and simplify the country names that also include continent names.

    kwargs:
        country_data (pd.DataFrame) : the input dataframe to preprocess

    return:
        pd.DataFrame : the preprocessed dataframe
    """"""
    regex_exp = r""\s\((\w+|\s)+\)""
    country_data['Country'] = country_data['Country'].apply(lambda a: re.sub(regex_exp, """", a))
    return country_data[~country_data['Country'].isin(countries_to_remove)]
def test_preprocess_countries():
    df_country_cleaned = preprocess_countries(df_country.copy())
    assert df_country_cleaned.columns.equals(df_country.columns)
    assert df_country_cleaned.dtypes.equals(df_country.dtypes)
    assert len(df_country_cleaned) == 544579

    unique_countries = df_country_cleaned[""Country""].unique()
    assert len(unique_countries) == 230
    assert 'Congo' in unique_countries
    assert 'Denmark' not in unique_countries
    print(""All tests passed!"")

test_preprocess_countries()"
25336,1,"    df_global_filtered = df_global_filtered.reset_index(drop=True)
    fig, ax = plt.subplots(figsize=(10, 6))
    data = (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])
    sns.boxplot(data = data, ax = ax)
    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    plt.savefig('q1_plot_temperature.png', dpi=300, bbox_inches='tight')
    plt.show()

    return (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])",TODO,df_global_filtered,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    df_global_filtered = df_global_filtered.reset_index(drop=True)
    fig, ax = plt.subplots(figsize=(10, 6))
    data = (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])
    sns.boxplot(data = data, ax = ax)
    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    plt.savefig('q1_plot_temperature.png', dpi=300, bbox_inches='tight')
    plt.show()

    return (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    df_global_filtered = df_global_filtered.reset_index(drop=True)
    fig, ax = plt.subplots(figsize=(10, 6))
    data = (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])
    sns.boxplot(data = data, ax = ax)
    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    plt.savefig('q1_plot_temperature.png', dpi=300, bbox_inches='tight')
    plt.show()

    return (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])
# df_global_filtered = drop_missing_values(df_global)
# df_country_filtered = drop_missing_values(preprocess_countries(df_country))

# print(len(df_global_filtered['LandAverageTemperature']))
# print(len(df_country_filtered['AverageTemperature']))
# print(df_global_filtered['LandAverageTemperature'].isnull().sum())
# print(df_global_filtered['LandAverageTemperature'].dtype)
# print(df_country_filtered['AverageTemperature'].dtype)
# df_global_filtered.head()
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
25336,2,"    fig, ax = plt.subplots(figsize=(10, 6))
    data = (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])
    sns.boxplot(data = data, ax = ax)
    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')",TODO,ax,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    df_global_filtered = df_global_filtered.reset_index(drop=True)
    fig, ax = plt.subplots(figsize=(10, 6))
    data = (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])
    sns.boxplot(data = data, ax = ax)
    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    plt.savefig('q1_plot_temperature.png', dpi=300, bbox_inches='tight')
    plt.show()

    return (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    df_global_filtered = df_global_filtered.reset_index(drop=True)
    fig, ax = plt.subplots(figsize=(10, 6))
    data = (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])
    sns.boxplot(data = data, ax = ax)
    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    plt.savefig('q1_plot_temperature.png', dpi=300, bbox_inches='tight')
    plt.show()

    return (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])
# df_global_filtered = drop_missing_values(df_global)
# df_country_filtered = drop_missing_values(preprocess_countries(df_country))

# print(len(df_global_filtered['LandAverageTemperature']))
# print(len(df_country_filtered['AverageTemperature']))
# print(df_global_filtered['LandAverageTemperature'].isnull().sum())
# print(df_global_filtered['LandAverageTemperature'].dtype)
# print(df_country_filtered['AverageTemperature'].dtype)
# df_global_filtered.head()
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
25336,3,"    data = (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])
    sns.boxplot(data = data, ax = ax)",TODO,data,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    df_global_filtered = df_global_filtered.reset_index(drop=True)
    fig, ax = plt.subplots(figsize=(10, 6))
    data = (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])
    sns.boxplot(data = data, ax = ax)
    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    plt.savefig('q1_plot_temperature.png', dpi=300, bbox_inches='tight')
    plt.show()

    return (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    df_global_filtered = df_global_filtered.reset_index(drop=True)
    fig, ax = plt.subplots(figsize=(10, 6))
    data = (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])
    sns.boxplot(data = data, ax = ax)
    ax.set_xticklabels(['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    plt.savefig('q1_plot_temperature.png', dpi=300, bbox_inches='tight')
    plt.show()

    return (df_country_filtered['AverageTemperature'],  df_global_filtered['LandAverageTemperature'])
# df_global_filtered = drop_missing_values(df_global)
# df_country_filtered = drop_missing_values(preprocess_countries(df_country))

# print(len(df_global_filtered['LandAverageTemperature']))
# print(len(df_country_filtered['AverageTemperature']))
# print(df_global_filtered['LandAverageTemperature'].isnull().sum())
# print(df_global_filtered['LandAverageTemperature'].dtype)
# print(df_country_filtered['AverageTemperature'].dtype)
# df_global_filtered.head()
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
25911,1,"    country_average = df_country_filtered.AverageTemperature
    df_country_average = pd.DataFrame({'temp':country_average})
    df_country_average['type'] = 'Average Country Temperature'
    global_average = df_global_filtered.LandAverageTemperature
    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])
    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)
    plot.set_ylabels('Average Temperature (C)', fontsize=10)
    
    return (country_average, global_average)",TODO,country_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered.AverageTemperature
    df_country_average = pd.DataFrame({'temp':country_average})
    df_country_average['type'] = 'Average Country Temperature'
    global_average = df_global_filtered.LandAverageTemperature
    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])
    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)
    plot.set_ylabels('Average Temperature (C)', fontsize=10)
    
    return (country_average, global_average)

# df_country_filtered = drop_missing_values(preprocess_countries(df_country))
# df_global_filtered = drop_missing_values(df_global)
# pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered.AverageTemperature
    df_country_average = pd.DataFrame({'temp':country_average})
    df_country_average['type'] = 'Average Country Temperature'
    global_average = df_global_filtered.LandAverageTemperature
    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])
    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)
    plot.set_ylabels('Average Temperature (C)', fontsize=10)
    
    return (country_average, global_average)

# df_country_filtered = drop_missing_values(preprocess_countries(df_country))
# df_global_filtered = drop_missing_values(df_global)
# pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
25911,2,"    df_country_average = pd.DataFrame({'temp':country_average})
    df_country_average['type'] = 'Average Country Temperature'
    global_average = df_global_filtered.LandAverageTemperature
    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])",TODO,df_country_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered.AverageTemperature
    df_country_average = pd.DataFrame({'temp':country_average})
    df_country_average['type'] = 'Average Country Temperature'
    global_average = df_global_filtered.LandAverageTemperature
    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])
    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)
    plot.set_ylabels('Average Temperature (C)', fontsize=10)
    
    return (country_average, global_average)

# df_country_filtered = drop_missing_values(preprocess_countries(df_country))
# df_global_filtered = drop_missing_values(df_global)
# pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered.AverageTemperature
    df_country_average = pd.DataFrame({'temp':country_average})
    df_country_average['type'] = 'Average Country Temperature'
    global_average = df_global_filtered.LandAverageTemperature
    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])
    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)
    plot.set_ylabels('Average Temperature (C)', fontsize=10)
    
    return (country_average, global_average)

# df_country_filtered = drop_missing_values(preprocess_countries(df_country))
# df_global_filtered = drop_missing_values(df_global)
# pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
25911,3,"    global_average = df_global_filtered.LandAverageTemperature
    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])
    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)
    plot.set_ylabels('Average Temperature (C)', fontsize=10)
    
    return (country_average, global_average)",TODO,global_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered.AverageTemperature
    df_country_average = pd.DataFrame({'temp':country_average})
    df_country_average['type'] = 'Average Country Temperature'
    global_average = df_global_filtered.LandAverageTemperature
    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])
    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)
    plot.set_ylabels('Average Temperature (C)', fontsize=10)
    
    return (country_average, global_average)

# df_country_filtered = drop_missing_values(preprocess_countries(df_country))
# df_global_filtered = drop_missing_values(df_global)
# pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered.AverageTemperature
    df_country_average = pd.DataFrame({'temp':country_average})
    df_country_average['type'] = 'Average Country Temperature'
    global_average = df_global_filtered.LandAverageTemperature
    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])
    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)
    plot.set_ylabels('Average Temperature (C)', fontsize=10)
    
    return (country_average, global_average)

# df_country_filtered = drop_missing_values(preprocess_countries(df_country))
# df_global_filtered = drop_missing_values(df_global)
# pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
25911,4,"    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])",TODO,df_gl_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered.AverageTemperature
    df_country_average = pd.DataFrame({'temp':country_average})
    df_country_average['type'] = 'Average Country Temperature'
    global_average = df_global_filtered.LandAverageTemperature
    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])
    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)
    plot.set_ylabels('Average Temperature (C)', fontsize=10)
    
    return (country_average, global_average)

# df_country_filtered = drop_missing_values(preprocess_countries(df_country))
# df_global_filtered = drop_missing_values(df_global)
# pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered.AverageTemperature
    df_country_average = pd.DataFrame({'temp':country_average})
    df_country_average['type'] = 'Average Country Temperature'
    global_average = df_global_filtered.LandAverageTemperature
    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])
    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)
    plot.set_ylabels('Average Temperature (C)', fontsize=10)
    
    return (country_average, global_average)

# df_country_filtered = drop_missing_values(preprocess_countries(df_country))
# df_global_filtered = drop_missing_values(df_global)
# pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
25911,5,"    df = pd.concat([df_country_average,df_gl_average])
    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)",TODO,df,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered.AverageTemperature
    df_country_average = pd.DataFrame({'temp':country_average})
    df_country_average['type'] = 'Average Country Temperature'
    global_average = df_global_filtered.LandAverageTemperature
    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])
    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)
    plot.set_ylabels('Average Temperature (C)', fontsize=10)
    
    return (country_average, global_average)

# df_country_filtered = drop_missing_values(preprocess_countries(df_country))
# df_global_filtered = drop_missing_values(df_global)
# pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered.AverageTemperature
    df_country_average = pd.DataFrame({'temp':country_average})
    df_country_average['type'] = 'Average Country Temperature'
    global_average = df_global_filtered.LandAverageTemperature
    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])
    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)
    plot.set_ylabels('Average Temperature (C)', fontsize=10)
    
    return (country_average, global_average)

# df_country_filtered = drop_missing_values(preprocess_countries(df_country))
# df_global_filtered = drop_missing_values(df_global)
# pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
25911,6,"    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)
    plot.set_ylabels('Average Temperature (C)', fontsize=10)",TODO,plot,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered.AverageTemperature
    df_country_average = pd.DataFrame({'temp':country_average})
    df_country_average['type'] = 'Average Country Temperature'
    global_average = df_global_filtered.LandAverageTemperature
    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])
    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)
    plot.set_ylabels('Average Temperature (C)', fontsize=10)
    
    return (country_average, global_average)

# df_country_filtered = drop_missing_values(preprocess_countries(df_country))
# df_global_filtered = drop_missing_values(df_global)
# pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered.AverageTemperature
    df_country_average = pd.DataFrame({'temp':country_average})
    df_country_average['type'] = 'Average Country Temperature'
    global_average = df_global_filtered.LandAverageTemperature
    df_gl_average = pd.DataFrame({'temp':global_average})
    df_gl_average['type'] = 'Average Global Temperature'
    df = pd.concat([df_country_average,df_gl_average])
    plot = sns.catplot(x=""type"", y=""temp"", kind=""box"", data=df, height=6, aspect=1.5)
    plot.set_ylabels('Average Temperature (C)', fontsize=10)
    
    return (country_average, global_average)

# df_country_filtered = drop_missing_values(preprocess_countries(df_country))
# df_global_filtered = drop_missing_values(df_global)
# pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
25126,1,"    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']
    
    df = pd.concat([country_average, global_average], axis=1)
    df.columns = ['Average Country Temperature', 'Average Global Temperature']
    df.boxplot()
    plt.ylabel('Average Temperature (C)')
    plt.show()
    
    return (country_average, global_average)",TODO,country_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    
    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']
    
    df = pd.concat([country_average, global_average], axis=1)
    df.columns = ['Average Country Temperature', 'Average Global Temperature']
    df.boxplot()
    plt.ylabel('Average Temperature (C)')
    plt.show()
    
    return (country_average, global_average)","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    
    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']
    
    df = pd.concat([country_average, global_average], axis=1)
    df.columns = ['Average Country Temperature', 'Average Global Temperature']
    df.boxplot()
    plt.ylabel('Average Temperature (C)')
    plt.show()
    
    return (country_average, global_average)
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
25126,2,"    global_average = df_global_filtered['LandAverageTemperature']
    
    df = pd.concat([country_average, global_average], axis=1)
    df.columns = ['Average Country Temperature', 'Average Global Temperature']
    df.boxplot()
    plt.ylabel('Average Temperature (C)')
    plt.show()
    
    return (country_average, global_average)",TODO,global_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    
    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']
    
    df = pd.concat([country_average, global_average], axis=1)
    df.columns = ['Average Country Temperature', 'Average Global Temperature']
    df.boxplot()
    plt.ylabel('Average Temperature (C)')
    plt.show()
    
    return (country_average, global_average)","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    
    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']
    
    df = pd.concat([country_average, global_average], axis=1)
    df.columns = ['Average Country Temperature', 'Average Global Temperature']
    df.boxplot()
    plt.ylabel('Average Temperature (C)')
    plt.show()
    
    return (country_average, global_average)
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
25126,3,"    df = pd.concat([country_average, global_average], axis=1)
    df.columns = ['Average Country Temperature', 'Average Global Temperature']
    df.boxplot()",TODO,df,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    
    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']
    
    df = pd.concat([country_average, global_average], axis=1)
    df.columns = ['Average Country Temperature', 'Average Global Temperature']
    df.boxplot()
    plt.ylabel('Average Temperature (C)')
    plt.show()
    
    return (country_average, global_average)","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    
    country_average = df_country_filtered['AverageTemperature']
    global_average = df_global_filtered['LandAverageTemperature']
    
    df = pd.concat([country_average, global_average], axis=1)
    df.columns = ['Average Country Temperature', 'Average Global Temperature']
    df.boxplot()
    plt.ylabel('Average Temperature (C)')
    plt.show()
    
    return (country_average, global_average)
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
26176,1,"    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]

    fig, ax1 = plt.subplots()
    # ax1.set_xlabel('Average Country Temperature')
    ax1.set_ylabel('Average Temperature(C)')
    ax1.grid(visible = True)

    ax1.boxplot([country_average.to_numpy(), global_average.to_numpy()], labels = ['Average Country Temperature', 'Avg Global Temperature'])



    # ax2 = ax1.twinx()
    # ax2.set_xlabel('Avg Global Temperature')
    # ax2.boxplot(global_average.to_numpy())

    return (country_average, global_average)",TODO,country_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]

    fig, ax1 = plt.subplots()
    # ax1.set_xlabel('Average Country Temperature')
    ax1.set_ylabel('Average Temperature(C)')
    ax1.grid(visible = True)

    ax1.boxplot([country_average.to_numpy(), global_average.to_numpy()], labels = ['Average Country Temperature', 'Avg Global Temperature'])



    # ax2 = ax1.twinx()
    # ax2.set_xlabel('Avg Global Temperature')
    # ax2.boxplot(global_average.to_numpy())

    return (country_average, global_average)","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]

    fig, ax1 = plt.subplots()
    # ax1.set_xlabel('Average Country Temperature')
    ax1.set_ylabel('Average Temperature(C)')
    ax1.grid(visible = True)

    ax1.boxplot([country_average.to_numpy(), global_average.to_numpy()], labels = ['Average Country Temperature', 'Avg Global Temperature'])



    # ax2 = ax1.twinx()
    # ax2.set_xlabel('Avg Global Temperature')
    # ax2.boxplot(global_average.to_numpy())

    return (country_average, global_average)
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
26176,2,"    global_average = df_global_filtered[""LandAverageTemperature""]

    fig, ax1 = plt.subplots()
    # ax1.set_xlabel('Average Country Temperature')
    ax1.set_ylabel('Average Temperature(C)')
    ax1.grid(visible = True)

    ax1.boxplot([country_average.to_numpy(), global_average.to_numpy()], labels = ['Average Country Temperature', 'Avg Global Temperature'])



    # ax2 = ax1.twinx()
    # ax2.set_xlabel('Avg Global Temperature')
    # ax2.boxplot(global_average.to_numpy())

    return (country_average, global_average)",TODO,global_average,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]

    fig, ax1 = plt.subplots()
    # ax1.set_xlabel('Average Country Temperature')
    ax1.set_ylabel('Average Temperature(C)')
    ax1.grid(visible = True)

    ax1.boxplot([country_average.to_numpy(), global_average.to_numpy()], labels = ['Average Country Temperature', 'Avg Global Temperature'])



    # ax2 = ax1.twinx()
    # ax2.set_xlabel('Avg Global Temperature')
    # ax2.boxplot(global_average.to_numpy())

    return (country_average, global_average)","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]

    fig, ax1 = plt.subplots()
    # ax1.set_xlabel('Average Country Temperature')
    ax1.set_ylabel('Average Temperature(C)')
    ax1.grid(visible = True)

    ax1.boxplot([country_average.to_numpy(), global_average.to_numpy()], labels = ['Average Country Temperature', 'Avg Global Temperature'])



    # ax2 = ax1.twinx()
    # ax2.set_xlabel('Avg Global Temperature')
    # ax2.boxplot(global_average.to_numpy())

    return (country_average, global_average)
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
26176,3,"    fig, ax1 = plt.subplots()
    # ax1.set_xlabel('Average Country Temperature')
    ax1.set_ylabel('Average Temperature(C)')
    ax1.grid(visible = True)

    ax1.boxplot([country_average.to_numpy(), global_average.to_numpy()], labels = ['Average Country Temperature', 'Avg Global Temperature'])",TODO,ax1,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]

    fig, ax1 = plt.subplots()
    # ax1.set_xlabel('Average Country Temperature')
    ax1.set_ylabel('Average Temperature(C)')
    ax1.grid(visible = True)

    ax1.boxplot([country_average.to_numpy(), global_average.to_numpy()], labels = ['Average Country Temperature', 'Avg Global Temperature'])



    # ax2 = ax1.twinx()
    # ax2.set_xlabel('Avg Global Temperature')
    # ax2.boxplot(global_average.to_numpy())

    return (country_average, global_average)","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    country_average = df_country_filtered[""AverageTemperature""]
    global_average = df_global_filtered[""LandAverageTemperature""]

    fig, ax1 = plt.subplots()
    # ax1.set_xlabel('Average Country Temperature')
    ax1.set_ylabel('Average Temperature(C)')
    ax1.grid(visible = True)

    ax1.boxplot([country_average.to_numpy(), global_average.to_numpy()], labels = ['Average Country Temperature', 'Avg Global Temperature'])



    # ax2 = ax1.twinx()
    # ax2.set_xlabel('Avg Global Temperature')
    # ax2.boxplot(global_average.to_numpy())

    return (country_average, global_average)
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
14074,1,"    df_new = pd.concat([df_country_filtered[""AverageTemperature""],df_global_filtered[""LandAverageTemperature""]],axis = 1)
    df_new = df_new.rename(columns={'AverageTemperature': 'Average Country Temperature', 'LandAverageTemperature': 'Average Global Temperature'})
    
    sns.set(rc={'figure.figsize':(6,5)})
    sns.set_style(""whitegrid"")
    plot1 = sns.boxplot(x=""variable"", y=""value"", data=pd.melt(df_new),width=0.2)",TODO,df_new,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    df_new = pd.concat([df_country_filtered[""AverageTemperature""],df_global_filtered[""LandAverageTemperature""]],axis = 1)
    df_new = df_new.rename(columns={'AverageTemperature': 'Average Country Temperature', 'LandAverageTemperature': 'Average Global Temperature'})
    
    sns.set(rc={'figure.figsize':(6,5)})
    sns.set_style(""whitegrid"")
    plot1 = sns.boxplot(x=""variable"", y=""value"", data=pd.melt(df_new),width=0.2)
    plot1.set(xlabel ="""", ylabel = ""Average Temperature (C)"")
    
    return(df_country_filtered[""AverageTemperature""],df_global_filtered[""LandAverageTemperature""])","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    df_new = pd.concat([df_country_filtered[""AverageTemperature""],df_global_filtered[""LandAverageTemperature""]],axis = 1)
    df_new = df_new.rename(columns={'AverageTemperature': 'Average Country Temperature', 'LandAverageTemperature': 'Average Global Temperature'})
    
    sns.set(rc={'figure.figsize':(6,5)})
    sns.set_style(""whitegrid"")
    plot1 = sns.boxplot(x=""variable"", y=""value"", data=pd.melt(df_new),width=0.2)
    plot1.set(xlabel ="""", ylabel = ""Average Temperature (C)"")
    
    return(df_country_filtered[""AverageTemperature""],df_global_filtered[""LandAverageTemperature""])
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
14074,2,"    plot1 = sns.boxplot(x=""variable"", y=""value"", data=pd.melt(df_new),width=0.2)
    plot1.set(xlabel ="""", ylabel = ""Average Temperature (C)"")",TODO,plot1,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    df_new = pd.concat([df_country_filtered[""AverageTemperature""],df_global_filtered[""LandAverageTemperature""]],axis = 1)
    df_new = df_new.rename(columns={'AverageTemperature': 'Average Country Temperature', 'LandAverageTemperature': 'Average Global Temperature'})
    
    sns.set(rc={'figure.figsize':(6,5)})
    sns.set_style(""whitegrid"")
    plot1 = sns.boxplot(x=""variable"", y=""value"", data=pd.melt(df_new),width=0.2)
    plot1.set(xlabel ="""", ylabel = ""Average Temperature (C)"")
    
    return(df_country_filtered[""AverageTemperature""],df_global_filtered[""LandAverageTemperature""])","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    df_new = pd.concat([df_country_filtered[""AverageTemperature""],df_global_filtered[""LandAverageTemperature""]],axis = 1)
    df_new = df_new.rename(columns={'AverageTemperature': 'Average Country Temperature', 'LandAverageTemperature': 'Average Global Temperature'})
    
    sns.set(rc={'figure.figsize':(6,5)})
    sns.set_style(""whitegrid"")
    plot1 = sns.boxplot(x=""variable"", y=""value"", data=pd.melt(df_new),width=0.2)
    plot1.set(xlabel ="""", ylabel = ""Average Temperature (C)"")
    
    return(df_country_filtered[""AverageTemperature""],df_global_filtered[""LandAverageTemperature""])
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
26681,1,"    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]
    fig, ax = plt.subplots()
    sns.set_theme(style=""whitegrid"")
    ax.boxplot(data)",TODO,data,"def plot_temperature(df_country_filtered, df_global_filtered):
    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]
    fig, ax = plt.subplots()
    sns.set_theme(style=""whitegrid"")
    ax.boxplot(data)
    ax.set_ylabel('Average Temperature (C)')
    ax.set_xticklabels(['Average Country Temperature','Avg Global Temperature'])
    ax.grid(visible=True)
    plt.savefig('Q2_plot_temperature.png')
    plt.show()
    return (pd.Series(df_country_filtered['AverageTemperature']) , pd.Series(df_global_filtered['LandAverageTemperature']))","def plot_temperature(df_country_filtered, df_global_filtered):
    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]
    fig, ax = plt.subplots()
    sns.set_theme(style=""whitegrid"")
    ax.boxplot(data)
    ax.set_ylabel('Average Temperature (C)')
    ax.set_xticklabels(['Average Country Temperature','Avg Global Temperature'])
    ax.grid(visible=True)
    plt.savefig('Q2_plot_temperature.png')
    plt.show()
    return (pd.Series(df_country_filtered['AverageTemperature']) , pd.Series(df_global_filtered['LandAverageTemperature']))
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
26681,2,"    fig, ax = plt.subplots()
    sns.set_theme(style=""whitegrid"")
    ax.boxplot(data)
    ax.set_ylabel('Average Temperature (C)')
    ax.set_xticklabels(['Average Country Temperature','Avg Global Temperature'])
    ax.grid(visible=True)",TODO,ax,"def plot_temperature(df_country_filtered, df_global_filtered):
    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]
    fig, ax = plt.subplots()
    sns.set_theme(style=""whitegrid"")
    ax.boxplot(data)
    ax.set_ylabel('Average Temperature (C)')
    ax.set_xticklabels(['Average Country Temperature','Avg Global Temperature'])
    ax.grid(visible=True)
    plt.savefig('Q2_plot_temperature.png')
    plt.show()
    return (pd.Series(df_country_filtered['AverageTemperature']) , pd.Series(df_global_filtered['LandAverageTemperature']))","def plot_temperature(df_country_filtered, df_global_filtered):
    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]
    fig, ax = plt.subplots()
    sns.set_theme(style=""whitegrid"")
    ax.boxplot(data)
    ax.set_ylabel('Average Temperature (C)')
    ax.set_xticklabels(['Average Country Temperature','Avg Global Temperature'])
    ax.grid(visible=True)
    plt.savefig('Q2_plot_temperature.png')
    plt.show()
    return (pd.Series(df_country_filtered['AverageTemperature']) , pd.Series(df_global_filtered['LandAverageTemperature']))
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
24796,1,"    data = (df_country_filtered[""AverageTemperature""], df_global_filtered[""LandAverageTemperature""])
    fig, ax = plt.subplots(figsize=(10,7))
    ax.boxplot(data)
    ax.set_xticklabels([""Average Country Temperature"",""Average Global Temperature""])
    ax.grid(axis='y')
    ax.set_ylabel(""Average Temperature (C)"")
    ax.set_title(""Boxplot of Average Country Temperature and Average Global Temperature"")
    return data",TODO,data,"def plot_temperature(df_country_filtered, df_global_filtered):

    data = (df_country_filtered[""AverageTemperature""], df_global_filtered[""LandAverageTemperature""])
    fig, ax = plt.subplots(figsize=(10,7))
    ax.boxplot(data)
    ax.set_xticklabels([""Average Country Temperature"",""Average Global Temperature""])
    ax.grid(axis='y')
    ax.set_ylabel(""Average Temperature (C)"")
    ax.set_title(""Boxplot of Average Country Temperature and Average Global Temperature"")
    return data","def plot_temperature(df_country_filtered, df_global_filtered):

    data = (df_country_filtered[""AverageTemperature""], df_global_filtered[""LandAverageTemperature""])
    fig, ax = plt.subplots(figsize=(10,7))
    ax.boxplot(data)
    ax.set_xticklabels([""Average Country Temperature"",""Average Global Temperature""])
    ax.grid(axis='y')
    ax.set_ylabel(""Average Temperature (C)"")
    ax.set_title(""Boxplot of Average Country Temperature and Average Global Temperature"")
    return data
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
24796,2,"    fig, ax = plt.subplots(figsize=(10,7))
    ax.boxplot(data)
    ax.set_xticklabels([""Average Country Temperature"",""Average Global Temperature""])
    ax.grid(axis='y')
    ax.set_ylabel(""Average Temperature (C)"")
    ax.set_title(""Boxplot of Average Country Temperature and Average Global Temperature"")",TODO,ax,"def plot_temperature(df_country_filtered, df_global_filtered):

    data = (df_country_filtered[""AverageTemperature""], df_global_filtered[""LandAverageTemperature""])
    fig, ax = plt.subplots(figsize=(10,7))
    ax.boxplot(data)
    ax.set_xticklabels([""Average Country Temperature"",""Average Global Temperature""])
    ax.grid(axis='y')
    ax.set_ylabel(""Average Temperature (C)"")
    ax.set_title(""Boxplot of Average Country Temperature and Average Global Temperature"")
    return data","def plot_temperature(df_country_filtered, df_global_filtered):

    data = (df_country_filtered[""AverageTemperature""], df_global_filtered[""LandAverageTemperature""])
    fig, ax = plt.subplots(figsize=(10,7))
    ax.boxplot(data)
    ax.set_xticklabels([""Average Country Temperature"",""Average Global Temperature""])
    ax.grid(axis='y')
    ax.set_ylabel(""Average Temperature (C)"")
    ax.set_title(""Boxplot of Average Country Temperature and Average Global Temperature"")
    return data
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
13719,1,"    fig, ax = plt.subplots(1, 1, figsize=(18,10))
    plt.grid()
    avg_temp, land_avg_temp = df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']
    ax.boxplot([avg_temp, land_avg_temp])
    ax.set_xticks(ticks=[1,2], labels=['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')",TODO,ax,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig, ax = plt.subplots(1, 1, figsize=(18,10))
    plt.grid()
    avg_temp, land_avg_temp = df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']
    ax.boxplot([avg_temp, land_avg_temp])
    ax.set_xticks(ticks=[1,2], labels=['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    
    return avg_temp, land_avg_temp","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig, ax = plt.subplots(1, 1, figsize=(18,10))
    plt.grid()
    avg_temp, land_avg_temp = df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']
    ax.boxplot([avg_temp, land_avg_temp])
    ax.set_xticks(ticks=[1,2], labels=['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    
    return avg_temp, land_avg_temp
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
13719,2,"    avg_temp, land_avg_temp = df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']
    ax.boxplot([avg_temp, land_avg_temp])
    ax.set_xticks(ticks=[1,2], labels=['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    
    return avg_temp, land_avg_temp",TODO,avg_temp,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig, ax = plt.subplots(1, 1, figsize=(18,10))
    plt.grid()
    avg_temp, land_avg_temp = df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']
    ax.boxplot([avg_temp, land_avg_temp])
    ax.set_xticks(ticks=[1,2], labels=['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    
    return avg_temp, land_avg_temp","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig, ax = plt.subplots(1, 1, figsize=(18,10))
    plt.grid()
    avg_temp, land_avg_temp = df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']
    ax.boxplot([avg_temp, land_avg_temp])
    ax.set_xticks(ticks=[1,2], labels=['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    
    return avg_temp, land_avg_temp
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
13719,3,"    avg_temp, land_avg_temp = df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']
    ax.boxplot([avg_temp, land_avg_temp])
    ax.set_xticks(ticks=[1,2], labels=['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    
    return avg_temp, land_avg_temp",TODO,land_avg_temp,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig, ax = plt.subplots(1, 1, figsize=(18,10))
    plt.grid()
    avg_temp, land_avg_temp = df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']
    ax.boxplot([avg_temp, land_avg_temp])
    ax.set_xticks(ticks=[1,2], labels=['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    
    return avg_temp, land_avg_temp","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.
    
    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed
    
    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig, ax = plt.subplots(1, 1, figsize=(18,10))
    plt.grid()
    avg_temp, land_avg_temp = df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']
    ax.boxplot([avg_temp, land_avg_temp])
    ax.set_xticks(ticks=[1,2], labels=['Average Country Temperature', 'Average Global Temperature'])
    ax.set_ylabel('Average Temperature (C)')
    
    return avg_temp, land_avg_temp
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
26491,1,"    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]

    labels = ['Average Country Temperature', 'Average Global Temperature']

    plt.boxplot(data, labels=labels)",TODO,data,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig = plt.figure(figsize =(10, 7))

    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]

    labels = ['Average Country Temperature', 'Average Global Temperature']

    plt.boxplot(data, labels=labels)
    plt.ylabel('Average Temperature (C)')
    plt.grid(axis='y')
    plt.show()

    return (df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature'])","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig = plt.figure(figsize =(10, 7))

    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]

    labels = ['Average Country Temperature', 'Average Global Temperature']

    plt.boxplot(data, labels=labels)
    plt.ylabel('Average Temperature (C)')
    plt.grid(axis='y')
    plt.show()

    return (df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature'])
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
26491,2,"    labels = ['Average Country Temperature', 'Average Global Temperature']

    plt.boxplot(data, labels=labels)",TODO,labels,"def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig = plt.figure(figsize =(10, 7))

    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]

    labels = ['Average Country Temperature', 'Average Global Temperature']

    plt.boxplot(data, labels=labels)
    plt.ylabel('Average Temperature (C)')
    plt.grid(axis='y')
    plt.show()

    return (df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature'])","def plot_temperature(df_country_filtered, df_global_filtered):
    """"""
    Generate the box plots of the average temperature values in the country and global temperature dataset.

    args:
        df_country_filtered (pd.DataFrame) : the dataframe of temperature by country with country names
            filtered and empty rows removed
        df_global_filtered (pd.DataFrame) : the dataframe of global temperature with empty rows removed

    returns:
        Tuple(country_average, global_average)
            country_average (pd.Series) : all the data points in the AverageTemperature column (first group)
            global_average (pd.Series) : all the data points in the LandAverageTemperature column (second group)
    """"""
    fig = plt.figure(figsize =(10, 7))

    data = [df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature']]

    labels = ['Average Country Temperature', 'Average Global Temperature']

    plt.boxplot(data, labels=labels)
    plt.ylabel('Average Temperature (C)')
    plt.grid(axis='y')
    plt.show()

    return (df_country_filtered['AverageTemperature'], df_global_filtered['LandAverageTemperature'])
def test_plot_temperature():
    df_country_filtered = drop_missing_values(preprocess_countries(df_country))
    df_global_filtered = drop_missing_values(df_global)
    pw_country, pw_global = plot_temperature(df_country_filtered, df_global_filtered)
    assert pw_country.max() == 38.842
    assert pw_global.max()  == 15.482
    print(""All test cases passed!"")

test_plot_temperature()"
25757,1,"dfc = get_preprocessed_data(global_data = False)
dfc.head()",TODO,dfc,"dfc = get_preprocessed_data(global_data = False)
dfc.head()","dfc = get_preprocessed_data(global_data = False)
dfc.head()
def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.
    
    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries
    
    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    avg_temp = df_country.groupby('Country')['AverageTemperature'].mean()
    avg_temp = avg_temp.sort_values(ascending=False)

    # print(avg_temp.head())

    return avg_temp
    pass

def test_avg_temp_by_country():
    dfc = get_preprocessed_data(global_data = False)
    average_temps = avg_temp_by_country(dfc)
    assert average_temps.name == ""AverageTemperature""
    assert list(average_temps.index[:5]) == [""Djibouti"", ""Mali"", ""Burkina Faso"", ""Senegal"", ""Aruba""]
    assert list(average_temps.index[-5:]) == ['Iceland', 'South Georgia And The South Sandwich Isla', 'Norway',
       'Svalbard And Jan Mayen', 'Greenland']
    assert average_temps.dtype == np.float64
    
    assert len(average_temps) == 230
    assert abs(average_temps[""Albania""] - 12.61) < 0.01
    assert abs(average_temps[""China""] - 6.62) < 0.01
    print(""All tests passed!"")

test_avg_temp_by_country()"
24592,1,"dfc = get_preprocessed_data(global_data = False)
display(dfc.head(5))
# dfc[""month""] = dfc[""dt""].apply(lambda x: str(x)[5:7])
dfc = dfc.drop(columns=[""dt"", ""AverageTemperatureUncertainty""])
dfc.groupby(""Country"").agg(""mean"").sort_values(by=""AverageTemperature"", ascending=False)",TODO,dfc,"dfc = get_preprocessed_data(global_data = False)
display(dfc.head(5))
# dfc[""month""] = dfc[""dt""].apply(lambda x: str(x)[5:7])
dfc = dfc.drop(columns=[""dt"", ""AverageTemperatureUncertainty""])
dfc.groupby(""Country"").agg(""mean"").sort_values(by=""AverageTemperature"", ascending=False)","dfc = get_preprocessed_data(global_data = False)
display(dfc.head(5))
# dfc[""month""] = dfc[""dt""].apply(lambda x: str(x)[5:7])
dfc = dfc.drop(columns=[""dt"", ""AverageTemperatureUncertainty""])
dfc.groupby(""Country"").agg(""mean"").sort_values(by=""AverageTemperature"", ascending=False)
def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.
    
    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries
    
    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    
    d1 = df_country.copy()
    d1 = d1.drop(columns=[""dt"", ""AverageTemperatureUncertainty""])
    
    final = d1.groupby(""Country"").agg(""mean"").sort_values(by=""AverageTemperature"", ascending=False)
    # print(final)
    
    return final.squeeze('columns')
    
    pass
def test_avg_temp_by_country():
    dfc = get_preprocessed_data(global_data = False)
    average_temps = avg_temp_by_country(dfc)
    assert average_temps.name == ""AverageTemperature""
    assert list(average_temps.index[:5]) == [""Djibouti"", ""Mali"", ""Burkina Faso"", ""Senegal"", ""Aruba""]
    assert list(average_temps.index[-5:]) == ['Iceland', 'South Georgia And The South Sandwich Isla', 'Norway',
       'Svalbard And Jan Mayen', 'Greenland']
    assert average_temps.dtype == np.float64
    
    assert len(average_temps) == 230
    assert abs(average_temps[""Albania""] - 12.61) < 0.01
    assert abs(average_temps[""China""] - 6.62) < 0.01
    print(""All tests passed!"")

test_avg_temp_by_country()"
27147,1,"    df_country = df_country.groupby(""Country"").agg(AverageTemperature=(""AverageTemperature"",""mean""))
    df_country.sort_values(by=""AverageTemperature"",axis=0,inplace=True,ascending=False)

    return df_country.squeeze()",TODO,df_country,"def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.
    
    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries
    
    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    df_country = df_country.groupby(""Country"").agg(AverageTemperature=(""AverageTemperature"",""mean""))
    df_country.sort_values(by=""AverageTemperature"",axis=0,inplace=True,ascending=False)

    return df_country.squeeze()
    pass","def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.
    
    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries
    
    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    df_country = df_country.groupby(""Country"").agg(AverageTemperature=(""AverageTemperature"",""mean""))
    df_country.sort_values(by=""AverageTemperature"",axis=0,inplace=True,ascending=False)

    return df_country.squeeze()
    pass
def test_avg_temp_by_country():
    dfc = get_preprocessed_data(global_data = False)
    average_temps = avg_temp_by_country(dfc)
    assert average_temps.name == ""AverageTemperature""
    assert list(average_temps.index[:5]) == [""Djibouti"", ""Mali"", ""Burkina Faso"", ""Senegal"", ""Aruba""]
    assert list(average_temps.index[-5:]) == ['Iceland', 'South Georgia And The South Sandwich Isla', 'Norway',
       'Svalbard And Jan Mayen', 'Greenland']
    assert average_temps.dtype == np.float64
    
    assert len(average_temps) == 230
    assert abs(average_temps[""Albania""] - 12.61) < 0.01
    assert abs(average_temps[""China""] - 6.62) < 0.01
    print(""All tests passed!"")

test_avg_temp_by_country()"
13930,1,"    avg_temp = df_country.groupby('Country')['AverageTemperature'].mean().sort_values(ascending=False)
    return avg_temp",TODO,avg_temp,"def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.
    
    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries
    
    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    avg_temp = df_country.groupby('Country')['AverageTemperature'].mean().sort_values(ascending=False)
    return avg_temp
    #pass","def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.
    
    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries
    
    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    avg_temp = df_country.groupby('Country')['AverageTemperature'].mean().sort_values(ascending=False)
    return avg_temp
    #pass
def test_avg_temp_by_country():
    dfc = get_preprocessed_data(global_data = False)
    average_temps = avg_temp_by_country(dfc)
    assert average_temps.name == ""AverageTemperature""
    assert list(average_temps.index[:5]) == [""Djibouti"", ""Mali"", ""Burkina Faso"", ""Senegal"", ""Aruba""]
    assert list(average_temps.index[-5:]) == ['Iceland', 'South Georgia And The South Sandwich Isla', 'Norway',
       'Svalbard And Jan Mayen', 'Greenland']
    assert average_temps.dtype == np.float64
    
    assert len(average_temps) == 230
    assert abs(average_temps[""Albania""] - 12.61) < 0.01
    assert abs(average_temps[""China""] - 6.62) < 0.01
    print(""All tests passed!"")

test_avg_temp_by_country()"
26182,1,"    res = df_country.groupby('Country').agg({'AverageTemperature': 'mean'})
    res = res.squeeze()
    res = res.sort_values(ascending=False)
    return res",TODO,res,"def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.
    
    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries
    
    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    res = df_country.groupby('Country').agg({'AverageTemperature': 'mean'})
    res = res.squeeze()
    res = res.sort_values(ascending=False)
    return res","def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.
    
    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries
    
    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    res = df_country.groupby('Country').agg({'AverageTemperature': 'mean'})
    res = res.squeeze()
    res = res.sort_values(ascending=False)
    return res
def test_avg_temp_by_country():
    dfc = get_preprocessed_data(global_data = False)
    average_temps = avg_temp_by_country(dfc)
    assert average_temps.name == ""AverageTemperature""
    assert list(average_temps.index[:5]) == [""Djibouti"", ""Mali"", ""Burkina Faso"", ""Senegal"", ""Aruba""]
    assert list(average_temps.index[-5:]) == ['Iceland', 'South Georgia And The South Sandwich Isla', 'Norway',
       'Svalbard And Jan Mayen', 'Greenland']
    assert average_temps.dtype == np.float64
    
    assert len(average_temps) == 230
    assert abs(average_temps[""Albania""] - 12.61) < 0.01
    assert abs(average_temps[""China""] - 6.62) < 0.01
    print(""All tests passed!"")

test_avg_temp_by_country()"
24417,1,"    df = df_country.groupby('Country').aggregate('mean')['AverageTemperature'].reset_index().\
                                        sort_values(['AverageTemperature'], ascending=False).\
                                        set_index('Country')
    return df['AverageTemperature']",TODO,df,"def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.
    
    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries
    
    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    df = df_country.groupby('Country').aggregate('mean')['AverageTemperature'].reset_index().\
                                        sort_values(['AverageTemperature'], ascending=False).\
                                        set_index('Country')
    return df['AverageTemperature']","def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.
    
    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries
    
    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    df = df_country.groupby('Country').aggregate('mean')['AverageTemperature'].reset_index().\
                                        sort_values(['AverageTemperature'], ascending=False).\
                                        set_index('Country')
    return df['AverageTemperature']
def test_avg_temp_by_country():
    dfc = get_preprocessed_data(global_data = False)
    average_temps = avg_temp_by_country(dfc)
    assert average_temps.name == ""AverageTemperature""
    assert list(average_temps.index[:5]) == [""Djibouti"", ""Mali"", ""Burkina Faso"", ""Senegal"", ""Aruba""]
    assert list(average_temps.index[-5:]) == ['Iceland', 'South Georgia And The South Sandwich Isla', 'Norway',
       'Svalbard And Jan Mayen', 'Greenland']
    assert average_temps.dtype == np.float64
    
    assert len(average_temps) == 230
    assert abs(average_temps[""Albania""] - 12.61) < 0.01
    assert abs(average_temps[""China""] - 6.62) < 0.01
    print(""All tests passed!"")

test_avg_temp_by_country()"
25387,1,"    grouped = df_country.groupby(by='Country').mean()
    return grouped['AverageTemperature'].sort_values(ascending=False)",TODO,grouped,"def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.
    
    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries
    
    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    grouped = df_country.groupby(by='Country').mean()
    return grouped['AverageTemperature'].sort_values(ascending=False)","def avg_temp_by_country(df_country):
    """"""
    Compute the average temperature in each country and order the countries by average temperatures from high to low.
    
    args:
        df_country (pd.DataFrame) : preprocessed data frame of temperatures across countries
    
    return:
        pd.Series : a descending series of average temperature values, indexed by country
    """"""
    grouped = df_country.groupby(by='Country').mean()
    return grouped['AverageTemperature'].sort_values(ascending=False)
def test_avg_temp_by_country():
    dfc = get_preprocessed_data(global_data = False)
    average_temps = avg_temp_by_country(dfc)
    assert average_temps.name == ""AverageTemperature""
    assert list(average_temps.index[:5]) == [""Djibouti"", ""Mali"", ""Burkina Faso"", ""Senegal"", ""Aruba""]
    assert list(average_temps.index[-5:]) == ['Iceland', 'South Georgia And The South Sandwich Isla', 'Norway',
       'Svalbard And Jan Mayen', 'Greenland']
    assert average_temps.dtype == np.float64
    
    assert len(average_temps) == 230
    assert abs(average_temps[""Albania""] - 12.61) < 0.01
    assert abs(average_temps[""China""] - 6.62) < 0.01
    print(""All tests passed!"")

test_avg_temp_by_country()"
25628,1,"    year_columns = [i for i in df.columns if re.match(r""Y\d{4}"", i)]
    df.drop_duplicates(inplace=True)
    df[year_columns] = df[year_columns].where(df[year_columns] >= 0, 0)
    df = df.where(~df.isna(), 0)
    df = df.astype({i:np.int64 for i in year_columns})",TODO,year_columns,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    year_columns = [i for i in df.columns if re.match(r""Y\d{4}"", i)]
    df.drop_duplicates(inplace=True)
    df[year_columns] = df[year_columns].where(df[year_columns] >= 0, 0)
    df = df.where(~df.isna(), 0)
    df = df.astype({i:np.int64 for i in year_columns})
    return df    ","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    year_columns = [i for i in df.columns if re.match(r""Y\d{4}"", i)]
    df.drop_duplicates(inplace=True)
    df[year_columns] = df[year_columns].where(df[year_columns] >= 0, 0)
    df = df.where(~df.isna(), 0)
    df = df.astype({i:np.int64 for i in year_columns})
    return df    
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
25628,2,"    year_columns = [i for i in df.columns if re.match(r""Y\d{4}"", i)]
    df.drop_duplicates(inplace=True)
    df[year_columns] = df[year_columns].where(df[year_columns] >= 0, 0)
    df = df.where(~df.isna(), 0)
    df = df.astype({i:np.int64 for i in year_columns})
    return df",TODO,df,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    year_columns = [i for i in df.columns if re.match(r""Y\d{4}"", i)]
    df.drop_duplicates(inplace=True)
    df[year_columns] = df[year_columns].where(df[year_columns] >= 0, 0)
    df = df.where(~df.isna(), 0)
    df = df.astype({i:np.int64 for i in year_columns})
    return df    ","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    year_columns = [i for i in df.columns if re.match(r""Y\d{4}"", i)]
    df.drop_duplicates(inplace=True)
    df[year_columns] = df[year_columns].where(df[year_columns] >= 0, 0)
    df = df.where(~df.isna(), 0)
    df = df.astype({i:np.int64 for i in year_columns})
    return df    
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
24993,1,"    columns = [col for col in df.columns if col.startswith('Y')]
    df[columns] = (df[columns].applymap(lambda x: 0 if x<0 else x)).astype(np.int64)",TODO,columns,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df.drop_duplicates(keep='first', inplace=True)
    df.fillna(0, inplace=True)
    columns = [col for col in df.columns if col.startswith('Y')]
    df[columns] = (df[columns].applymap(lambda x: 0 if x<0 else x)).astype(np.int64)
    return df

    pass","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df.drop_duplicates(keep='first', inplace=True)
    df.fillna(0, inplace=True)
    columns = [col for col in df.columns if col.startswith('Y')]
    df[columns] = (df[columns].applymap(lambda x: 0 if x<0 else x)).astype(np.int64)
    return df

    pass
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
26898,1,"    years = [col for col in df.columns if 'Y' in col]

    df.fillna(0,inplace=True)
    df[years][df[years]<0] = 0

    df[years] = df[years]* (df[years]>0)
    df[years] = df[years].astype('int64')",TODO,years,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""

    df.drop_duplicates(inplace=True)

    years = [col for col in df.columns if 'Y' in col]

    df.fillna(0,inplace=True)
    df[years][df[years]<0] = 0

    df[years] = df[years]* (df[years]>0)
    df[years] = df[years].astype('int64')
    return df
    ","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""

    df.drop_duplicates(inplace=True)

    years = [col for col in df.columns if 'Y' in col]

    df.fillna(0,inplace=True)
    df[years][df[years]<0] = 0

    df[years] = df[years]* (df[years]>0)
    df[years] = df[years].astype('int64')
    return df
    
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
24708,1,"    df = df.drop_duplicates()
    yearColumns = []
    for c in df.columns:
      if len(c) == 5 and c[0] == 'Y' and c[1:].isdigit():
        yearColumns.append(c)
    df[yearColumns] = df[yearColumns].fillna(0)
    df[yearColumns] = df[yearColumns].astype(int)
    for c in yearColumns:
      df[c][df[c] < 0] = 0
    return df",TODO,df,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates()
    yearColumns = []
    for c in df.columns:
      if len(c) == 5 and c[0] == 'Y' and c[1:].isdigit():
        yearColumns.append(c)
    df[yearColumns] = df[yearColumns].fillna(0)
    df[yearColumns] = df[yearColumns].astype(int)
    for c in yearColumns:
      df[c][df[c] < 0] = 0
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates()
    yearColumns = []
    for c in df.columns:
      if len(c) == 5 and c[0] == 'Y' and c[1:].isdigit():
        yearColumns.append(c)
    df[yearColumns] = df[yearColumns].fillna(0)
    df[yearColumns] = df[yearColumns].astype(int)
    for c in yearColumns:
      df[c][df[c] < 0] = 0
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)

    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")

test_get_cleaned_food_data()"
24708,2,"    yearColumns = []
    for c in df.columns:
      if len(c) == 5 and c[0] == 'Y' and c[1:].isdigit():
        yearColumns.append(c)
    df[yearColumns] = df[yearColumns].fillna(0)
    df[yearColumns] = df[yearColumns].astype(int)
    for c in yearColumns:",TODO,yearColumns,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates()
    yearColumns = []
    for c in df.columns:
      if len(c) == 5 and c[0] == 'Y' and c[1:].isdigit():
        yearColumns.append(c)
    df[yearColumns] = df[yearColumns].fillna(0)
    df[yearColumns] = df[yearColumns].astype(int)
    for c in yearColumns:
      df[c][df[c] < 0] = 0
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates()
    yearColumns = []
    for c in df.columns:
      if len(c) == 5 and c[0] == 'Y' and c[1:].isdigit():
        yearColumns.append(c)
    df[yearColumns] = df[yearColumns].fillna(0)
    df[yearColumns] = df[yearColumns].astype(int)
    for c in yearColumns:
      df[c][df[c] < 0] = 0
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)

    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")

test_get_cleaned_food_data()"
27093,1,"    result = df.drop_duplicates()
    years = [year for year in result.columns if year.startswith(""Y"")]
    for year in years:
        result[year] = result[year].apply(lambda x: max(x, 0))
        result[year] = result[year].apply(lambda x: 0 if np.isnan(x) else x)
        result[year] = result[year].astype(np.int64)
    return result",TODO,result,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    result = df.drop_duplicates()
    years = [year for year in result.columns if year.startswith(""Y"")]
    for year in years:
        result[year] = result[year].apply(lambda x: max(x, 0))
        result[year] = result[year].apply(lambda x: 0 if np.isnan(x) else x)
        result[year] = result[year].astype(np.int64)
    return result","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    result = df.drop_duplicates()
    years = [year for year in result.columns if year.startswith(""Y"")]
    for year in years:
        result[year] = result[year].apply(lambda x: max(x, 0))
        result[year] = result[year].apply(lambda x: 0 if np.isnan(x) else x)
        result[year] = result[year].astype(np.int64)
    return result
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)

    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")

test_get_cleaned_food_data()"
27093,2,"    years = [year for year in result.columns if year.startswith(""Y"")]
    for year in years:",TODO,years,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    result = df.drop_duplicates()
    years = [year for year in result.columns if year.startswith(""Y"")]
    for year in years:
        result[year] = result[year].apply(lambda x: max(x, 0))
        result[year] = result[year].apply(lambda x: 0 if np.isnan(x) else x)
        result[year] = result[year].astype(np.int64)
    return result","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.

    kwargs:
        df (pd.DataFrame) : the food production dataframe.

    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    result = df.drop_duplicates()
    years = [year for year in result.columns if year.startswith(""Y"")]
    for year in years:
        result[year] = result[year].apply(lambda x: max(x, 0))
        result[year] = result[year].apply(lambda x: 0 if np.isnan(x) else x)
        result[year] = result[year].astype(np.int64)
    return result
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)

    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")

test_get_cleaned_food_data()"
14121,1,"    year_cols = [i for i in df.columns if re.search(r'^(y)([0-9][0-9][0-9][0-9])$', str.lower(i)) != None]
    for i in year_cols:",TODO,year_cols,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    year_cols = [i for i in df.columns if re.search(r'^(y)([0-9][0-9][0-9][0-9])$', str.lower(i)) != None]
    for i in year_cols:
        df.loc[df[i] < 0, i] = 0
        df[i] = df[i].fillna(0)
        df[i] = df[i].astype('int64')
    return df.drop_duplicates()","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    year_cols = [i for i in df.columns if re.search(r'^(y)([0-9][0-9][0-9][0-9])$', str.lower(i)) != None]
    for i in year_cols:
        df.loc[df[i] < 0, i] = 0
        df[i] = df[i].fillna(0)
        df[i] = df[i].astype('int64')
    return df.drop_duplicates()
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
13696,1,"    year_columns = [y for y in df.columns if y.startswith('Y')]
    df[year_columns] = np.where(df[year_columns] < 0, 0, df[year_columns])
    df = df.fillna(0)
    df[year_columns] = df[year_columns].astype(int)",TODO,year_columns,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df.drop_duplicates(inplace=True)
    year_columns = [y for y in df.columns if y.startswith('Y')]
    df[year_columns] = np.where(df[year_columns] < 0, 0, df[year_columns])
    df = df.fillna(0)
    df[year_columns] = df[year_columns].astype(int)
    
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df.drop_duplicates(inplace=True)
    year_columns = [y for y in df.columns if y.startswith('Y')]
    df[year_columns] = np.where(df[year_columns] < 0, 0, df[year_columns])
    df = df.fillna(0)
    df[year_columns] = df[year_columns].astype(int)
    
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
13696,2,"    df.drop_duplicates(inplace=True)
    year_columns = [y for y in df.columns if y.startswith('Y')]
    df[year_columns] = np.where(df[year_columns] < 0, 0, df[year_columns])
    df = df.fillna(0)
    df[year_columns] = df[year_columns].astype(int)
    
    return df",TODO,df,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df.drop_duplicates(inplace=True)
    year_columns = [y for y in df.columns if y.startswith('Y')]
    df[year_columns] = np.where(df[year_columns] < 0, 0, df[year_columns])
    df = df.fillna(0)
    df[year_columns] = df[year_columns].astype(int)
    
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df.drop_duplicates(inplace=True)
    year_columns = [y for y in df.columns if y.startswith('Y')]
    df[year_columns] = np.where(df[year_columns] < 0, 0, df[year_columns])
    df = df.fillna(0)
    df[year_columns] = df[year_columns].astype(int)
    
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
26313,1,"    df = df.copy()
    
    # remove duplicates
    df = df.drop_duplicates()
    # replace all negative values in year cols with 0
    years = ['Y' + str(i) for i in range(1961, 2014)] # last year is 2013
    df[years] = df[years].clip(lower = 0)
    # fill in missing vals with 0
    df = df.fillna(0)
    # convert all year cols to int64
    df[years] = df[years].astype(np.int64)
    return df",TODO,df,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.copy()
    
    # remove duplicates
    df = df.drop_duplicates()
    # replace all negative values in year cols with 0
    years = ['Y' + str(i) for i in range(1961, 2014)] # last year is 2013
    df[years] = df[years].clip(lower = 0)
    # fill in missing vals with 0
    df = df.fillna(0)
    # convert all year cols to int64
    df[years] = df[years].astype(np.int64)
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.copy()
    
    # remove duplicates
    df = df.drop_duplicates()
    # replace all negative values in year cols with 0
    years = ['Y' + str(i) for i in range(1961, 2014)] # last year is 2013
    df[years] = df[years].clip(lower = 0)
    # fill in missing vals with 0
    df = df.fillna(0)
    # convert all year cols to int64
    df[years] = df[years].astype(np.int64)
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
26313,2,"    years = ['Y' + str(i) for i in range(1961, 2014)] # last year is 2013
    df[years] = df[years].clip(lower = 0)
    # fill in missing vals with 0
    df = df.fillna(0)
    # convert all year cols to int64
    df[years] = df[years].astype(np.int64)",TODO,years,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.copy()
    
    # remove duplicates
    df = df.drop_duplicates()
    # replace all negative values in year cols with 0
    years = ['Y' + str(i) for i in range(1961, 2014)] # last year is 2013
    df[years] = df[years].clip(lower = 0)
    # fill in missing vals with 0
    df = df.fillna(0)
    # convert all year cols to int64
    df[years] = df[years].astype(np.int64)
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.copy()
    
    # remove duplicates
    df = df.drop_duplicates()
    # replace all negative values in year cols with 0
    years = ['Y' + str(i) for i in range(1961, 2014)] # last year is 2013
    df[years] = df[years].clip(lower = 0)
    # fill in missing vals with 0
    df = df.fillna(0)
    # convert all year cols to int64
    df[years] = df[years].astype(np.int64)
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
25683,1,"    df = df.drop_duplicates()
    year_subset = df.columns.difference([""Area Code"", ""Area Abbreviation"", ""Area Code"", ""Area"", ""Item Code"",\
                                        ""Item"",	""Element Code"", ""Element"", ""Unit"", ""latitude"", ""longitude""])
    for year in year_subset:
        df[year] = np.where(df[year] < 0, 0, df[year])
    df.fillna(0, inplace=True)
    for year in year_subset:
        df[year] = df[year].astype(np.int64)
    return df",TODO,df,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates()
    year_subset = df.columns.difference([""Area Code"", ""Area Abbreviation"", ""Area Code"", ""Area"", ""Item Code"",\
                                        ""Item"",	""Element Code"", ""Element"", ""Unit"", ""latitude"", ""longitude""])
    for year in year_subset:
        df[year] = np.where(df[year] < 0, 0, df[year])
    df.fillna(0, inplace=True)
    for year in year_subset:
        df[year] = df[year].astype(np.int64)
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates()
    year_subset = df.columns.difference([""Area Code"", ""Area Abbreviation"", ""Area Code"", ""Area"", ""Item Code"",\
                                        ""Item"",	""Element Code"", ""Element"", ""Unit"", ""latitude"", ""longitude""])
    for year in year_subset:
        df[year] = np.where(df[year] < 0, 0, df[year])
    df.fillna(0, inplace=True)
    for year in year_subset:
        df[year] = df[year].astype(np.int64)
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
25683,2,"    year_subset = df.columns.difference([""Area Code"", ""Area Abbreviation"", ""Area Code"", ""Area"", ""Item Code"",\
                                        ""Item"",	""Element Code"", ""Element"", ""Unit"", ""latitude"", ""longitude""])
    for year in year_subset:
        df[year] = np.where(df[year] < 0, 0, df[year])
    df.fillna(0, inplace=True)
    for year in year_subset:",TODO,year_subset,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates()
    year_subset = df.columns.difference([""Area Code"", ""Area Abbreviation"", ""Area Code"", ""Area"", ""Item Code"",\
                                        ""Item"",	""Element Code"", ""Element"", ""Unit"", ""latitude"", ""longitude""])
    for year in year_subset:
        df[year] = np.where(df[year] < 0, 0, df[year])
    df.fillna(0, inplace=True)
    for year in year_subset:
        df[year] = df[year].astype(np.int64)
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    df = df.drop_duplicates()
    year_subset = df.columns.difference([""Area Code"", ""Area Abbreviation"", ""Area Code"", ""Area"", ""Item Code"",\
                                        ""Item"",	""Element Code"", ""Element"", ""Unit"", ""latitude"", ""longitude""])
    for year in year_subset:
        df[year] = np.where(df[year] < 0, 0, df[year])
    df.fillna(0, inplace=True)
    for year in year_subset:
        df[year] = df[year].astype(np.int64)
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
13951,1,"    year_cols = [col for col in df.columns if 'Y' in col]
    #for each year column
    for col in year_cols:
        #set negative values to 0
        df[col] = df[col].apply(lambda x : 0 if x < 0 else x)
    #fill missing values with 0
    df.fillna(0, inplace = True)
    #set data type to int64
    df[year_cols] = df[year_cols].astype(np.int64)",TODO,year_cols,"def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    #drop duplicates
    df.drop_duplicates(keep = 'first', inplace = True, ignore_index = True)
    #extract the year columns
    year_cols = [col for col in df.columns if 'Y' in col]
    #for each year column
    for col in year_cols:
        #set negative values to 0
        df[col] = df[col].apply(lambda x : 0 if x < 0 else x)
    #fill missing values with 0
    df.fillna(0, inplace = True)
    #set data type to int64
    df[year_cols] = df[year_cols].astype(np.int64)
    return df","def get_cleaned_food_data(df = df_food_production):
    """"""
    Clean the food production dataset by removing duplicates and replacing outlier / missing values with 0.
    
    kwargs:
        df (pd.DataFrame) : the food production dataframe.
    
    return:
        pd.DataFrame : the cleaned dataframe
    """"""
    #drop duplicates
    df.drop_duplicates(keep = 'first', inplace = True, ignore_index = True)
    #extract the year columns
    year_cols = [col for col in df.columns if 'Y' in col]
    #for each year column
    for col in year_cols:
        #set negative values to 0
        df[col] = df[col].apply(lambda x : 0 if x < 0 else x)
    #fill missing values with 0
    df.fillna(0, inplace = True)
    #set data type to int64
    df[year_cols] = df[year_cols].astype(np.int64)
    return df
def test_get_cleaned_food_data():
    df_food_cleaned = get_cleaned_food_data()
    assert list(df_food_cleaned.index) == list(range(21477))
    assert df_food_cleaned.columns.equals(df_food_production.columns)
    
    for i in range(1961, 2014):
        assert df_food_cleaned[f""Y{i}""].dtype == np.int64

    assert df_food_cleaned.isnull().sum().sum() == 0
    assert df_food_cleaned[""Y2012""].min() == df_food_cleaned[""Y2013""].min() == 0
    assert df_food_cleaned.loc[21475, ""Y2011""] == 0
    assert df_food_cleaned.loc[0, ""Y2004""] == 3249
    assert df_food_cleaned.loc[3, ""Y2013""] == 89
    print(""All tests passed!"")
    
test_get_cleaned_food_data()"
26034,1,"    df_food_cleaned = df_food_cleaned.groupby('Area')[df_food_cleaned.columns.tolist()[10:]].sum().reset_index().rename_axis('Year')
    #display(df_food_cleaned)
    for x in df_food_cleaned.columns.tolist()[1:]:
      df_food_cleaned = df_food_cleaned.rename(columns = {x: np.dtype('int64').type(x[1:])})

    #display(df_food_cleaned)
    lists = df_food_cleaned.columns.tolist()
    df_food_cleaned.rename(columns ={'Area': 'Country'}, inplace = True)
    df_food_cleaned.set_index('Country', inplace = True)
    #df_food_cleaned.set_index('Year').rename_axis('Year')
    df_transpose = df_food_cleaned.T",TODO,df_food_cleaned,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    #display(df_food_cleaned.head())
    df_food_cleaned = df_food_cleaned.groupby('Area')[df_food_cleaned.columns.tolist()[10:]].sum().reset_index().rename_axis('Year')
    #display(df_food_cleaned)
    for x in df_food_cleaned.columns.tolist()[1:]:
      df_food_cleaned = df_food_cleaned.rename(columns = {x: np.dtype('int64').type(x[1:])})

    #display(df_food_cleaned)
    lists = df_food_cleaned.columns.tolist()
    df_food_cleaned.rename(columns ={'Area': 'Country'}, inplace = True)
    df_food_cleaned.set_index('Country', inplace = True)
    #df_food_cleaned.set_index('Year').rename_axis('Year')
    df_transpose = df_food_cleaned.T
    df_transpose.index = pd.to_numeric(df_transpose.index, errors='coerce', downcast='integer')
    df_transpose.index.name='Year'
    return df_transpose","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    #display(df_food_cleaned.head())
    df_food_cleaned = df_food_cleaned.groupby('Area')[df_food_cleaned.columns.tolist()[10:]].sum().reset_index().rename_axis('Year')
    #display(df_food_cleaned)
    for x in df_food_cleaned.columns.tolist()[1:]:
      df_food_cleaned = df_food_cleaned.rename(columns = {x: np.dtype('int64').type(x[1:])})

    #display(df_food_cleaned)
    lists = df_food_cleaned.columns.tolist()
    df_food_cleaned.rename(columns ={'Area': 'Country'}, inplace = True)
    df_food_cleaned.set_index('Country', inplace = True)
    #df_food_cleaned.set_index('Year').rename_axis('Year')
    df_transpose = df_food_cleaned.T
    df_transpose.index = pd.to_numeric(df_transpose.index, errors='coerce', downcast='integer')
    df_transpose.index.name='Year'
    return df_transpose
def test_aggregate_transpose_farming_data():
    get_cleaned_food_data()
    df_food_cleaned = get_cleaned_food_data()
    aggregate_transpose_farming_data(df_food_cleaned)
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()

    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64

    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64

    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
26034,2,"    df_transpose = df_food_cleaned.T
    df_transpose.index = pd.to_numeric(df_transpose.index, errors='coerce', downcast='integer')
    df_transpose.index.name='Year'
    return df_transpose",TODO,df_transpose,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    #display(df_food_cleaned.head())
    df_food_cleaned = df_food_cleaned.groupby('Area')[df_food_cleaned.columns.tolist()[10:]].sum().reset_index().rename_axis('Year')
    #display(df_food_cleaned)
    for x in df_food_cleaned.columns.tolist()[1:]:
      df_food_cleaned = df_food_cleaned.rename(columns = {x: np.dtype('int64').type(x[1:])})

    #display(df_food_cleaned)
    lists = df_food_cleaned.columns.tolist()
    df_food_cleaned.rename(columns ={'Area': 'Country'}, inplace = True)
    df_food_cleaned.set_index('Country', inplace = True)
    #df_food_cleaned.set_index('Year').rename_axis('Year')
    df_transpose = df_food_cleaned.T
    df_transpose.index = pd.to_numeric(df_transpose.index, errors='coerce', downcast='integer')
    df_transpose.index.name='Year'
    return df_transpose","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    #display(df_food_cleaned.head())
    df_food_cleaned = df_food_cleaned.groupby('Area')[df_food_cleaned.columns.tolist()[10:]].sum().reset_index().rename_axis('Year')
    #display(df_food_cleaned)
    for x in df_food_cleaned.columns.tolist()[1:]:
      df_food_cleaned = df_food_cleaned.rename(columns = {x: np.dtype('int64').type(x[1:])})

    #display(df_food_cleaned)
    lists = df_food_cleaned.columns.tolist()
    df_food_cleaned.rename(columns ={'Area': 'Country'}, inplace = True)
    df_food_cleaned.set_index('Country', inplace = True)
    #df_food_cleaned.set_index('Year').rename_axis('Year')
    df_transpose = df_food_cleaned.T
    df_transpose.index = pd.to_numeric(df_transpose.index, errors='coerce', downcast='integer')
    df_transpose.index.name='Year'
    return df_transpose
def test_aggregate_transpose_farming_data():
    get_cleaned_food_data()
    df_food_cleaned = get_cleaned_food_data()
    aggregate_transpose_farming_data(df_food_cleaned)
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()

    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64

    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64

    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
26889,1,"    df_food_cleaned = df_food_cleaned.rename(columns={'Area': 'Country'})

    group_df =  df_food_cleaned.groupby('Country').sum()",TODO,df_food_cleaned,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    df_food_cleaned = df_food_cleaned.rename(columns={'Area': 'Country'})

    group_df =  df_food_cleaned.groupby('Country').sum()
    year_columns = [col for col in group_df.columns if col.startswith('Y')]
    df = group_df[year_columns]
    df = df.rename(columns=lambda x: int(x[1:]))
    df_swapped = df.transpose()
    
    df_swapped.rename_axis('Year', inplace=True)
    return(df_swapped)","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    df_food_cleaned = df_food_cleaned.rename(columns={'Area': 'Country'})

    group_df =  df_food_cleaned.groupby('Country').sum()
    year_columns = [col for col in group_df.columns if col.startswith('Y')]
    df = group_df[year_columns]
    df = df.rename(columns=lambda x: int(x[1:]))
    df_swapped = df.transpose()
    
    df_swapped.rename_axis('Year', inplace=True)
    return(df_swapped)
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
26889,2,"    group_df =  df_food_cleaned.groupby('Country').sum()
    year_columns = [col for col in group_df.columns if col.startswith('Y')]
    df = group_df[year_columns]",TODO,group_df,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    df_food_cleaned = df_food_cleaned.rename(columns={'Area': 'Country'})

    group_df =  df_food_cleaned.groupby('Country').sum()
    year_columns = [col for col in group_df.columns if col.startswith('Y')]
    df = group_df[year_columns]
    df = df.rename(columns=lambda x: int(x[1:]))
    df_swapped = df.transpose()
    
    df_swapped.rename_axis('Year', inplace=True)
    return(df_swapped)","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    df_food_cleaned = df_food_cleaned.rename(columns={'Area': 'Country'})

    group_df =  df_food_cleaned.groupby('Country').sum()
    year_columns = [col for col in group_df.columns if col.startswith('Y')]
    df = group_df[year_columns]
    df = df.rename(columns=lambda x: int(x[1:]))
    df_swapped = df.transpose()
    
    df_swapped.rename_axis('Year', inplace=True)
    return(df_swapped)
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
26889,3,"    year_columns = [col for col in group_df.columns if col.startswith('Y')]
    df = group_df[year_columns]",TODO,year_columns,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    df_food_cleaned = df_food_cleaned.rename(columns={'Area': 'Country'})

    group_df =  df_food_cleaned.groupby('Country').sum()
    year_columns = [col for col in group_df.columns if col.startswith('Y')]
    df = group_df[year_columns]
    df = df.rename(columns=lambda x: int(x[1:]))
    df_swapped = df.transpose()
    
    df_swapped.rename_axis('Year', inplace=True)
    return(df_swapped)","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    df_food_cleaned = df_food_cleaned.rename(columns={'Area': 'Country'})

    group_df =  df_food_cleaned.groupby('Country').sum()
    year_columns = [col for col in group_df.columns if col.startswith('Y')]
    df = group_df[year_columns]
    df = df.rename(columns=lambda x: int(x[1:]))
    df_swapped = df.transpose()
    
    df_swapped.rename_axis('Year', inplace=True)
    return(df_swapped)
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
26889,4,"    df = group_df[year_columns]
    df = df.rename(columns=lambda x: int(x[1:]))
    df_swapped = df.transpose()",TODO,df,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    df_food_cleaned = df_food_cleaned.rename(columns={'Area': 'Country'})

    group_df =  df_food_cleaned.groupby('Country').sum()
    year_columns = [col for col in group_df.columns if col.startswith('Y')]
    df = group_df[year_columns]
    df = df.rename(columns=lambda x: int(x[1:]))
    df_swapped = df.transpose()
    
    df_swapped.rename_axis('Year', inplace=True)
    return(df_swapped)","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    df_food_cleaned = df_food_cleaned.rename(columns={'Area': 'Country'})

    group_df =  df_food_cleaned.groupby('Country').sum()
    year_columns = [col for col in group_df.columns if col.startswith('Y')]
    df = group_df[year_columns]
    df = df.rename(columns=lambda x: int(x[1:]))
    df_swapped = df.transpose()
    
    df_swapped.rename_axis('Year', inplace=True)
    return(df_swapped)
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
26889,5,"    df_swapped = df.transpose()
    
    df_swapped.rename_axis('Year', inplace=True)
    return(df_swapped)",TODO,df_swapped,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    df_food_cleaned = df_food_cleaned.rename(columns={'Area': 'Country'})

    group_df =  df_food_cleaned.groupby('Country').sum()
    year_columns = [col for col in group_df.columns if col.startswith('Y')]
    df = group_df[year_columns]
    df = df.rename(columns=lambda x: int(x[1:]))
    df_swapped = df.transpose()
    
    df_swapped.rename_axis('Year', inplace=True)
    return(df_swapped)","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    df_food_cleaned = df_food_cleaned.rename(columns={'Area': 'Country'})

    group_df =  df_food_cleaned.groupby('Country').sum()
    year_columns = [col for col in group_df.columns if col.startswith('Y')]
    df = group_df[year_columns]
    df = df.rename(columns=lambda x: int(x[1:]))
    df_swapped = df.transpose()
    
    df_swapped.rename_axis('Year', inplace=True)
    return(df_swapped)
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
24639,1,"    year_columns = [col for col in df_food_cleaned.columns if re.match(r""Y\d{4}"", col)]
    df = df_food_cleaned[[""Area""]+year_columns].groupby(""Area"").sum().T",TODO,year_columns,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_columns = [col for col in df_food_cleaned.columns if re.match(r""Y\d{4}"", col)]
    df = df_food_cleaned[[""Area""]+year_columns].groupby(""Area"").sum().T
    df.index = df.index.str.extract(r""Y(\d{4})"")[0].astype(int)
    df.index.name = ""Year""
    df.columns.name = ""Country""
    
    return df","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_columns = [col for col in df_food_cleaned.columns if re.match(r""Y\d{4}"", col)]
    df = df_food_cleaned[[""Area""]+year_columns].groupby(""Area"").sum().T
    df.index = df.index.str.extract(r""Y(\d{4})"")[0].astype(int)
    df.index.name = ""Year""
    df.columns.name = ""Country""
    
    return df
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
24639,2,"    df = df_food_cleaned[[""Area""]+year_columns].groupby(""Area"").sum().T
    df.index = df.index.str.extract(r""Y(\d{4})"")[0].astype(int)
    df.index.name = ""Year""
    df.columns.name = ""Country""
    
    return df",TODO,df,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_columns = [col for col in df_food_cleaned.columns if re.match(r""Y\d{4}"", col)]
    df = df_food_cleaned[[""Area""]+year_columns].groupby(""Area"").sum().T
    df.index = df.index.str.extract(r""Y(\d{4})"")[0].astype(int)
    df.index.name = ""Year""
    df.columns.name = ""Country""
    
    return df","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
    year_columns = [col for col in df_food_cleaned.columns if re.match(r""Y\d{4}"", col)]
    df = df_food_cleaned[[""Area""]+year_columns].groupby(""Area"").sum().T
    df.index = df.index.str.extract(r""Y(\d{4})"")[0].astype(int)
    df.index.name = ""Year""
    df.columns.name = ""Country""
    
    return df
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
13732,1,"    df1 = df_food_cleaned.groupby('Area').sum()
    df = df1.T",TODO,df1,"def aggregate_transpose_farming_data(df_food_cleaned):
   
    df1 = df_food_cleaned.groupby('Area').sum()
    df = df1.T
    ndf = df[5:]
    ndf.index = ndf.index.str[1:].astype(int)
    ndf.index.name = ""Year""
    ndf.columns.name = 'Country'
    for col in ndf.columns:
        ndf[col]=ndf[col].astype(np.int64)
    return ndf
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""","def aggregate_transpose_farming_data(df_food_cleaned):
   
    df1 = df_food_cleaned.groupby('Area').sum()
    df = df1.T
    ndf = df[5:]
    ndf.index = ndf.index.str[1:].astype(int)
    ndf.index.name = ""Year""
    ndf.columns.name = 'Country'
    for col in ndf.columns:
        ndf[col]=ndf[col].astype(np.int64)
    return ndf
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
13732,2,"    df = df1.T
    ndf = df[5:]",TODO,df,"def aggregate_transpose_farming_data(df_food_cleaned):
   
    df1 = df_food_cleaned.groupby('Area').sum()
    df = df1.T
    ndf = df[5:]
    ndf.index = ndf.index.str[1:].astype(int)
    ndf.index.name = ""Year""
    ndf.columns.name = 'Country'
    for col in ndf.columns:
        ndf[col]=ndf[col].astype(np.int64)
    return ndf
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""","def aggregate_transpose_farming_data(df_food_cleaned):
   
    df1 = df_food_cleaned.groupby('Area').sum()
    df = df1.T
    ndf = df[5:]
    ndf.index = ndf.index.str[1:].astype(int)
    ndf.index.name = ""Year""
    ndf.columns.name = 'Country'
    for col in ndf.columns:
        ndf[col]=ndf[col].astype(np.int64)
    return ndf
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
13732,3,"    ndf = df[5:]
    ndf.index = ndf.index.str[1:].astype(int)
    ndf.index.name = ""Year""
    ndf.columns.name = 'Country'
    for col in ndf.columns:
        ndf[col]=ndf[col].astype(np.int64)
    return ndf",TODO,ndf,"def aggregate_transpose_farming_data(df_food_cleaned):
   
    df1 = df_food_cleaned.groupby('Area').sum()
    df = df1.T
    ndf = df[5:]
    ndf.index = ndf.index.str[1:].astype(int)
    ndf.index.name = ""Year""
    ndf.columns.name = 'Country'
    for col in ndf.columns:
        ndf[col]=ndf[col].astype(np.int64)
    return ndf
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""","def aggregate_transpose_farming_data(df_food_cleaned):
   
    df1 = df_food_cleaned.groupby('Area').sum()
    df = df1.T
    ndf = df[5:]
    ndf.index = ndf.index.str[1:].astype(int)
    ndf.index.name = ""Year""
    ndf.columns.name = 'Country'
    for col in ndf.columns:
        ndf[col]=ndf[col].astype(np.int64)
    return ndf
    """"""
    Compute the total Food and Feed production per country per year
    
    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data
    
    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index). 
    """"""
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()
    
    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64
    
    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64
    
    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
26924,1,"    df_melted = df_food_cleaned.melt(id_vars=['Area', 'Element'],
                                      value_vars=[col for col in df_food_cleaned if col.startswith('Y')],
                                      var_name='Year', value_name='Total')

    df_melted['Year'] = df_melted['Year'].str[1:].astype(int)

    df_pivot = df_melted.pivot_table(index='Year', columns='Area', values='Total', aggfunc='sum')",TODO,df_melted,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    df_melted = df_food_cleaned.melt(id_vars=['Area', 'Element'],
                                      value_vars=[col for col in df_food_cleaned if col.startswith('Y')],
                                      var_name='Year', value_name='Total')

    df_melted['Year'] = df_melted['Year'].str[1:].astype(int)

    df_pivot = df_melted.pivot_table(index='Year', columns='Area', values='Total', aggfunc='sum')
    df_pivot.columns.name = ""Country""

    return df_pivot","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    df_melted = df_food_cleaned.melt(id_vars=['Area', 'Element'],
                                      value_vars=[col for col in df_food_cleaned if col.startswith('Y')],
                                      var_name='Year', value_name='Total')

    df_melted['Year'] = df_melted['Year'].str[1:].astype(int)

    df_pivot = df_melted.pivot_table(index='Year', columns='Area', values='Total', aggfunc='sum')
    df_pivot.columns.name = ""Country""

    return df_pivot
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()

    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64

    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64

    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
26924,2,"    df_pivot = df_melted.pivot_table(index='Year', columns='Area', values='Total', aggfunc='sum')
    df_pivot.columns.name = ""Country""

    return df_pivot",TODO,df_pivot,"def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    df_melted = df_food_cleaned.melt(id_vars=['Area', 'Element'],
                                      value_vars=[col for col in df_food_cleaned if col.startswith('Y')],
                                      var_name='Year', value_name='Total')

    df_melted['Year'] = df_melted['Year'].str[1:].astype(int)

    df_pivot = df_melted.pivot_table(index='Year', columns='Area', values='Total', aggfunc='sum')
    df_pivot.columns.name = ""Country""

    return df_pivot","def aggregate_transpose_farming_data(df_food_cleaned):
    """"""
    Compute the total Food and Feed production per country per year

    args:
        df_food_cleaned (pd.DataFrame) : the cleaned food production data

    return:
        pd.DataFrame : a dataframe where each cell denotes the total food production
        of a country (column) in a year (row index).
    """"""
    df_melted = df_food_cleaned.melt(id_vars=['Area', 'Element'],
                                      value_vars=[col for col in df_food_cleaned if col.startswith('Y')],
                                      var_name='Year', value_name='Total')

    df_melted['Year'] = df_melted['Year'].str[1:].astype(int)

    df_pivot = df_melted.pivot_table(index='Year', columns='Area', values='Total', aggfunc='sum')
    df_pivot.columns.name = ""Country""

    return df_pivot
def test_aggregate_transpose_farming_data():
    df_food_cleaned = get_cleaned_food_data()
    df_food_agg = aggregate_transpose_farming_data(df_food_cleaned)
    countries = df_food_cleaned[""Area""].unique()

    # the year indexes are from 1961 to 2013
    assert list(df_food_agg.index) == list(range(1961, 2014))
    assert df_food_agg.index.name == ""Year""
    assert df_food_agg.index.dtype == np.int64

    # the column names are all the countries
    assert sorted(df_food_agg.columns) == sorted(countries)
    assert df_food_agg.columns.name == ""Country""
    for col in df_food_agg.columns:
        assert df_food_agg[col].dtype == np.int64

    # check the aggregation
    assert df_food_agg.loc[1965, ""Australia""] == 28961
    assert df_food_agg.loc[1971, ""United States Of America""] == 678905
    print(""All tests passed!"")

test_aggregate_transpose_farming_data()"
23363,1,"    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps",TODO,indicator,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
23363,2,"    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenmu,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
23363,3,"    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenzi,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
14790,1,"    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps",TODO,indicator,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
14790,2,"    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenmu,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
14790,3,"    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenzi,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
16754,1,"    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps",TODO,indicator,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
16754,2,"    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenmu,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
16754,3,"    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenzi,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
15781,1,"    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps",TODO,indicator,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
15781,2,"    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenmu,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
15781,3,"    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenzi,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
21435,1,"    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps",TODO,indicator,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
21435,2,"    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenmu,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
21435,3,"    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenzi,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
23245,1,"    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps",TODO,indicator,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
23245,2,"    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenmu,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
23245,3,"    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]",TODO,fenzi,"def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass","def predict_item_item(X, W, item_means, eps=1e-12):
    """"""
    Using the item-item similarity matrix, return the predicted ratings matrix.
    
    args: 
        X (np.array[num_users, num_movies]) : the actual ratings matrix
        W (np.array[num_movies, num_movies]) : movie-movie similarity weight matrix
        item_means (np.array[num_movies, ]) : mean-movie-rating array
        eps (float) : smoothing constant to avoid division by zero

    return:
        np.array[num_users, num_movies] : the predicted ratings matrix
    """"""
    indicator = (X != 0)

    fenmu = ((X - item_means[None,:])*indicator ).dot(W)
    fenzi  = indicator.dot(np.abs(W)) + eps
    
    return fenmu/fenzi + item_means[None,:]
    
    
    pass
# X = np.array([[0, 3], [0, 1], [2, 3]])
# item_means = np.array([2, 7/3])
# W_item_cosine = cosine_similarity(X.T)
# X_predicted = predict_item_item(X, W_item_cosine, item_means)
# X_predicted
# local test
def test_predict_item_item():
    X = np.array([[0, 3], [0, 1], [2, 3]])
    item_means = np.array([2, 7/3])
    W_item_cosine = cosine_similarity(X.T)
    X_predicted = predict_item_item(X, W_item_cosine, item_means)
    assert X_predicted.shape == X.shape
    assert np.allclose(X_predicted, [[2.66666667, 3], [0.66666667, 1], [2.27177979, 2.72822021]])
    print(""All tests passed!"")

test_predict_item_item()"
