{
    "PyTorch Recipes": {
        "Loading data in PyTorch": [
            [
                "PyTorch features extensive neural network building blocks with a simple,\nintuitive, and stable API. PyTorch includes packages to prepare and load\ncommon datasets for your model.",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "At the heart of PyTorch data loading utility is the\n\nclass. It represents a Python iterable over a dataset. Libraries in\nPyTorch offer built-in high-quality datasets for you to use in\n.\nThese datasets are currently available in:",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "with more to come.\nUsing the Yesno dataset from torchaudio.datasets.YESNO, we will\ndemonstrate how to effectively and efficiently load data from a PyTorch\nDataset into a PyTorch DataLoader.",
                        "markdown"
                    ]
                ]
            },
            {
                "Setup": [
                    [
                        "Before we begin, we need to install torchaudio to have access to the\ndataset.",
                        "markdown"
                    ],
                    [
                        "# pip install torchaudio",
                        "code"
                    ],
                    [
                        "To run in Google Colab, uncomment the following line:",
                        "markdown"
                    ],
                    [
                        "# !pip install torchaudio",
                        "code"
                    ]
                ]
            },
            {
                "Steps": [
                    [
                        "Import all necessary libraries for loading our data",
                        "markdown"
                    ],
                    [
                        "Access the data in the dataset",
                        "markdown"
                    ],
                    [
                        "Loading the data",
                        "markdown"
                    ],
                    [
                        "Iterate over the data",
                        "markdown"
                    ],
                    [
                        "[Optional] Visualize the data",
                        "markdown"
                    ]
                ]
            },
            {
                "1. Import necessary libraries for loading our data": [
                    [
                        "For this recipe, we will use torch and torchaudio. Depending on\nwhat built-in datasets you use, you can also install and import\ntorchvision or torchtext.",
                        "markdown"
                    ],
                    [
                        "import torch\nimport torchaudio",
                        "code"
                    ]
                ]
            },
            {
                "2. Access the data in the dataset": [
                    [
                        "The Yesno dataset in torchaudio features sixty recordings of one\nindividual saying yes or no in Hebrew; with each recording being eight\nwords long ().",
                        "markdown"
                    ],
                    [
                        "torchaudio.datasets.YESNO creates a dataset for YesNo.",
                        "markdown"
                    ],
                    [
                        "(\n     root='./',\n     url='http://www.openslr.org/resources/1/waves_yesno.tar.gz',\n     folder_in_archive='waves_yesno',\n     download=True)",
                        "code"
                    ],
                    [
                        "Each item in the dataset is a tuple of the form: (waveform, sample_rate,\nlabels).",
                        "markdown"
                    ],
                    [
                        "You must set a root for the Yesno dataset, which is where the\ntraining and testing dataset will exist. The other parameters are\noptional, with their default values shown. Here is some additional\nuseful info on the other parameters:",
                        "markdown"
                    ],
                    [
                        "# * ``download``: If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.\n#\n# Let\u2019s access our Yesno data:\n#\n\n# A data point in Yesno is a tuple (waveform, sample_rate, labels) where labels\n# is a list of integers with 1 for yes and 0 for no.\nyesno_data = ('./', download=True)\n\n# Pick data point number 3 to see an example of the the yesno_data:\nn = 3\nwaveform, sample_rate, labels = yesno_data[n]\nprint(\"Waveform: {}\\nSample rate: {}\\nLabels: {}\".format(waveform, sample_rate, labels))",
                        "code"
                    ],
                    [
                        "When using this data in practice, it is best practice to provision the\ndata into a \u201ctraining\u201d dataset and a \u201ctesting\u201d dataset. This ensures\nthat you have out-of-sample data to test the performance of your model.",
                        "markdown"
                    ]
                ]
            },
            {
                "3. Loading the data": [
                    [
                        "Now that we have access to the dataset, we must pass it through\ntorch.utils.data.DataLoader. The DataLoader combines the dataset\nand a sampler, returning an iterable over the dataset.",
                        "markdown"
                    ],
                    [
                        "data_loader = (yesno_data,\n                                          batch_size=1,\n                                          shuffle=True)",
                        "code"
                    ]
                ]
            },
            {
                "4. Iterate over the data": [
                    [
                        "Our data is now iterable using the data_loader. This will be\nnecessary when we begin training our model! You will notice that now\neach data entry in the data_loader object is converted to a tensor\ncontaining tensors representing our waveform, sample rate, and labels.",
                        "markdown"
                    ],
                    [
                        "for data in data_loader:\n  print(\"Data: \", data)\n  print(\"Waveform: {}\\nSample rate: {}\\nLabels: {}\".format(data[0], data[1], data[2]))\n  break",
                        "code"
                    ]
                ]
            },
            {
                "5. [Optional] Visualize the data": [
                    [
                        "You can optionally visualize your data to further understand the output\nfrom your DataLoader.",
                        "markdown"
                    ],
                    [
                        "import matplotlib.pyplot as plt\n\nprint(data[0][0].numpy())\n\nplt.figure()\nplt.plot(waveform.t().numpy())",
                        "code"
                    ],
                    [
                        "Congratulations! You have successfully loaded data in PyTorch.",
                        "markdown"
                    ]
                ]
            },
            {
                "Learn More": [
                    [
                        "Take a look at these other recipes to continue your learning:",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Defining a Neural Network in PyTorch": [
            [
                "Deep learning uses artificial neural networks (models), which are\ncomputing systems that are composed of many layers of interconnected\nunits. By passing data through these interconnected units, a neural\nnetwork is able to learn how to approximate the computations required to\ntransform inputs into outputs. In PyTorch, neural networks can be\nconstructed using the torch.nn package.",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "PyTorch provides the elegantly designed modules and classes, including\ntorch.nn, to help you create and train neural networks. An\nnn.Module contains layers, and a method forward(input) that\nreturns the output.",
                        "markdown"
                    ],
                    [
                        "In this recipe, we will use torch.nn to define a neural network\nintended for the .",
                        "markdown"
                    ]
                ]
            },
            {
                "Setup": [
                    [
                        "Before we begin, we need to install torch if it isn\u2019t already\navailable.",
                        "markdown"
                    ],
                    [
                        "pip install torch",
                        "code"
                    ]
                ]
            },
            {
                "Steps": [
                    [
                        "Import all necessary libraries for loading our data",
                        "markdown"
                    ],
                    [
                        "Define and initialize the neural network",
                        "markdown"
                    ],
                    [
                        "Specify how data will pass through your model",
                        "markdown"
                    ],
                    [
                        "[Optional] Pass data through your model to test",
                        "markdown"
                    ],
                    {
                        "1. Import necessary libraries for loading our data": [
                            [
                                "For this recipe, we will use torch and its subsidiaries torch.nn\nand torch.nn.functional.",
                                "markdown"
                            ],
                            [
                                "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2. Define and intialize the neural network": [
                            [
                                "Our network will recognize images. We will use a process built into\nPyTorch called convolution. Convolution adds each element of an image to\nits local neighbors, weighted by a kernel, or a small matrix, that\nhelps us extract certain features (like edge detection, sharpness,\nblurriness, etc.) from the input image.",
                                "markdown"
                            ],
                            [
                                "There are two requirements for defining the Net class of your model.\nThe first is writing an __init__ function that references\nnn.Module. This function is where you define the fully connected\nlayers in your neural network.",
                                "markdown"
                            ],
                            [
                                "Using convolution, we will define our model to take 1 input image\nchannel, and output match our target of 10 labels representing numbers 0\nthrough 9. This algorithm is yours to create, we will follow a standard\nMNIST algorithm.",
                                "markdown"
                            ],
                            [
                                "class Net():\n    def __init__(self):\n      super(Net, self).__init__()\n\n      # First 2D convolutional layer, taking in 1 input channel (image),\n      # outputting 32 convolutional features, with a square kernel size of 3\n      self.conv1 = (1, 32, 3, 1)\n      # Second 2D convolutional layer, taking in the 32 input layers,\n      # outputting 64 convolutional features, with a square kernel size of 3\n      self.conv2 = (32, 64, 3, 1)\n\n      # Designed to ensure that adjacent pixels are either all 0s or all active\n      # with an input probability\n      self.dropout1 = (0.25)\n      self.dropout2 = (0.5)\n\n      # First fully connected layer\n      self.fc1 = (9216, 128)\n      # Second fully connected layer that outputs our 10 labels\n      self.fc2 = (128, 10)\n\nmy_nn = Net()\nprint(my_nn)",
                                "code"
                            ],
                            [
                                "We have finished defining our neural network, now we have to define how\nour data will pass through it.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "3. Specify how data will pass through your model": [
                            [
                                "When you use PyTorch to build a model, you just have to define the\nforward function, that will pass the data into the computation graph\n(i.e. our neural network). This will represent our feed-forward\nalgorithm.",
                                "markdown"
                            ],
                            [
                                "You can use any of the Tensor operations in the forward function.",
                                "markdown"
                            ],
                            [
                                "class Net():\n    def __init__(self):\n      super(Net, self).__init__()\n      self.conv1 = (1, 32, 3, 1)\n      self.conv2 = (32, 64, 3, 1)\n      self.dropout1 = (0.25)\n      self.dropout2 = (0.5)\n      self.fc1 = (9216, 128)\n      self.fc2 = (128, 10)\n\n    # x represents our data\n    def forward(self, x):\n      # Pass data through conv1\n      x = self.conv1(x)\n      # Use the rectified-linear activation function over x\n      x = (x)\n\n      x = self.conv2(x)\n      x = (x)\n\n      # Run max pooling over x\n      x = (x, 2)\n      # Pass data through dropout1\n      x = self.dropout1(x)\n      # Flatten x with start_dim=1\n      x = (x, 1)\n      # Pass data through fc1\n      x = self.fc1(x)\n      x = (x)\n      x = self.dropout2(x)\n      x = self.fc2(x)\n\n      # Apply softmax to x\n      output = (x, dim=1)\n      return output",
                                "code"
                            ]
                        ]
                    },
                    {
                        "4. [Optional] Pass data through your model to test": [
                            [
                                "To ensure we receive our desired output, let\u2019s test our model by passing\nsome random data through it.",
                                "markdown"
                            ],
                            [
                                "# Equates to one random 28x28 image\nrandom_data = ((1, 1, 28, 28))\n\nmy_nn = Net()\nresult = my_nn(random_data)\nprint (result)",
                                "code"
                            ],
                            [
                                "Each number in this resulting tensor equates to the prediction of the\nlabel the random tensor is associated to.",
                                "markdown"
                            ],
                            [
                                "Congratulations! You have successfully defined a neural network in\nPyTorch.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Learn More": [
                    [
                        "Take a look at these other recipes to continue your learning:",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "What is a state_dict in PyTorch": [
            [
                "In PyTorch, the learnable parameters (i.e. weights and biases) of a\ntorch.nn.Module model are contained in the model\u2019s parameters\n(accessed with model.parameters()). A state_dict is simply a\nPython dictionary object that maps each layer to its parameter tensor.",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "A state_dict is an integral entity if you are interested in saving\nor loading models from PyTorch.\nBecause state_dict objects are Python dictionaries, they can be\neasily saved, updated, altered, and restored, adding a great deal of\nmodularity to PyTorch models and optimizers.\nNote that only layers with learnable parameters (convolutional layers,\nlinear layers, etc.) and registered buffers (batchnorm\u2019s running_mean)\nhave entries in the model\u2019s state_dict. Optimizer objects\n(torch.optim) also have a state_dict, which contains information\nabout the optimizer\u2019s state, as well as the hyperparameters used.\nIn this recipe, we will see how state_dict is used with a simple\nmodel.",
                        "markdown"
                    ]
                ]
            },
            {
                "Setup": [
                    [
                        "Before we begin, we need to install torch if it isn\u2019t already\navailable.",
                        "markdown"
                    ],
                    [
                        "pip install torch",
                        "code"
                    ]
                ]
            },
            {
                "Steps": [
                    [
                        "Import all necessary libraries for loading our data",
                        "markdown"
                    ],
                    [
                        "Define and intialize the neural network",
                        "markdown"
                    ],
                    [
                        "Initialize the optimizer",
                        "markdown"
                    ],
                    [
                        "Access the model and optimizer state_dict",
                        "markdown"
                    ],
                    {
                        "1. Import necessary libraries for loading our data": [
                            [
                                "For this recipe, we will use torch and its subsidiaries torch.nn\nand torch.optim.",
                                "markdown"
                            ],
                            [
                                "import torch\nimport torch.nn as nn\nimport torch.optim as optim",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2. Define and intialize the neural network": [
                            [
                                "For sake of example, we will create a neural network for training\nimages. To learn more see the Defining a Neural Network recipe.",
                                "markdown"
                            ],
                            [
                                "class Net():\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\nprint(net)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "3. Initialize the optimizer": [
                            [
                                "We will use SGD with momentum.",
                                "markdown"
                            ],
                            [
                                "optimizer = (net.parameters(), lr=0.001, momentum=0.9)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "4. Access the model and optimizer state_dict": [
                            [
                                "Now that we have constructed our model and optimizer, we can understand\nwhat is preserved in their respective state_dict properties.",
                                "markdown"
                            ],
                            [
                                "# Print model's state_dict\nprint(\"Model's state_dict:\")\nfor param_tensor in net.state_dict():\n    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n\nprint()\n\n# Print optimizer's state_dict\nprint(\"Optimizer's state_dict:\")\nfor var_name in optimizer.state_dict():\n    print(var_name, \"\\t\", optimizer.state_dict()[var_name])",
                                "code"
                            ],
                            [
                                "This information is relevant for saving and loading the model and\noptimizers for future use.",
                                "markdown"
                            ],
                            [
                                "Congratulations! You have successfully used state_dict in PyTorch.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Learn More": [
                    [
                        "Take a look at these other recipes to continue your learning:",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Developing Custom PyTorch Dataloaders": [
            [
                "A significant amount of the effort applied to developing machine\nlearning algorithms is related to data preparation. PyTorch provides\nmany tools to make data loading easy and hopefully, makes your code more\nreadable. In this recipe, you will learn how to:\n<blockquote>\n\nCreate a custom dataset leveraging the PyTorch dataset APIs;\nCreate callable custom transforms that can be composable; and\nPut these components together to create a custom dataloader.\n\n</blockquote>",
                "markdown"
            ],
            [
                "Please note, to run this tutorial, ensure the following packages are\ninstalled:\n<blockquote>\n\nscikit-image: For image io and transforms\npandas: For easier csv parsing\n\n</blockquote>",
                "markdown"
            ],
            [
                "As a point of attribution, this recipe is based on the original tutorial\nfrom  and was later\nedited by .",
                "markdown"
            ],
            {
                "Setup": [
                    [
                        "First let\u2019s import all of the needed libraries for this recipe.",
                        "markdown"
                    ],
                    [
                        "from __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nplt.ion()   # interactive mode",
                        "code"
                    ]
                ]
            },
            {
                "Part 1: The Dataset": [
                    [
                        "The dataset we are going to deal with is that of facial pose. Overall,\n68 different landmark points are annotated for each face.",
                        "markdown"
                    ],
                    [
                        "As a next step, please download the dataset from\n so that the\nimages are in a directory named \u2018data/faces/\u2019.",
                        "markdown"
                    ],
                    [
                        "<strong>Note:</strong> This dataset was actually generated by applying\n\non images from the imagenet dataset containing the \u2018face\u2019 tag.",
                        "markdown"
                    ],
                    [
                        "!wget https://download.pytorch.org/tutorial/faces.zip\n!mkdir data/faces/\nimport zipfile\nwith zipfile.ZipFile(\"faces.zip\",\"r\") as zip_ref:\nzip_ref.extractall(\"/data/faces/\")\n%cd /data/faces/",
                        "code"
                    ],
                    [
                        "The dataset comes with a csv file with annotations which looks like\nthis:",
                        "markdown"
                    ],
                    [
                        "image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y\n0805personali01.jpg,27,83,27,98, ... 84,134\n1084239450_e76e00b7e7.jpg,70,236,71,257, ... ,128,312",
                        "code"
                    ],
                    [
                        "Let\u2019s quickly read the CSV and get the annotations in an (N, 2) array\nwhere N is the number of landmarks.",
                        "markdown"
                    ],
                    [
                        "landmarks_frame = pd.read_csv('faces/face_landmarks.csv')\n\nn = 65\nimg_name = landmarks_frame.iloc[n, 0]\nlandmarks = landmarks_frame.iloc[n, 1:]\nlandmarks = np.asarray(landmarks)\nlandmarks = landmarks.astype('float').reshape(-1, 2)\n\nprint('Image name: {}'.format(img_name))\nprint('Landmarks shape: {}'.format(landmarks.shape))\nprint('First 4 Landmarks: {}'.format(landmarks[:4]))",
                        "code"
                    ],
                    {
                        "1.1 Write a simple helper function to show an image": [
                            [
                                "Next let\u2019s write a simple helper function to show an image, its landmarks and use it to show a sample.",
                                "markdown"
                            ],
                            [
                                "def show_landmarks(image, landmarks):\n    \"\"\"Show image with landmarks\"\"\"\n    plt.imshow(image)\n    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\nplt.figure()\nshow_landmarks(io.imread(os.path.join('faces/', img_name)),\n               landmarks)\nplt.show()",
                                "code"
                            ]
                        ]
                    },
                    {
                        "1.2 Create a dataset class": [
                            [
                                "Now lets talk about the PyTorch dataset class",
                                "markdown"
                            ],
                            [
                                "torch.utils.data.Dataset is an abstract class representing a\ndataset. Your custom dataset should inherit Dataset and override the\nfollowing methods:\n\n__len__ so that len(dataset) returns the size of the dataset.\n__getitem__ to support indexing such that dataset[i] can be\nused to get :math:i\u00a0th sample",
                                "markdown"
                            ],
                            [
                                "Let\u2019s create a dataset class for our face landmarks dataset. We will\nread the csv in __init__ but leave the reading of images to\n__getitem__. This is memory efficient because all the images are not\nstored in the memory at once but read as required.",
                                "markdown"
                            ],
                            [
                                "Here we show a sample of our dataset in the forma of a dict\n{'image': image, 'landmarks': landmarks}. Our dataset will take an\noptional argument transform so that any required processing can be\napplied on the sample. We will see the usefulness of transform in\nanother recipe.",
                                "markdown"
                            ],
                            [
                                "class FaceLandmarksDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.landmarks_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.root_dir,\n                                self.landmarks_frame.iloc[idx, 0])\n        image = io.imread(img_name)\n        landmarks = self.landmarks_frame.iloc[idx, 1:]\n        landmarks = np.array([landmarks])\n        landmarks = landmarks.astype('float').reshape(-1, 2)\n        sample = {'image': image, 'landmarks': landmarks}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample",
                                "code"
                            ]
                        ]
                    },
                    {
                        "1.3 Iterate through data samples": [
                            [
                                "Next let\u2019s instantiate this class and iterate through the data samples.\nWe will print the sizes of first 4 samples and show their landmarks.",
                                "markdown"
                            ],
                            [
                                "face_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                    root_dir='faces/')\n\nfig = plt.figure()\n\nfor i in range(len(face_dataset)):\n    sample = face_dataset[i]\n\n    print(i, sample['image'].shape, sample['landmarks'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{}'.format(i))\n    ax.axis('off')\n    show_landmarks(**sample)\n\n    if i == 3:\n        plt.show()\n        break",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Part 2: Data Tranformations": [
                    [
                        "Now that we have a dataset to work with and have done some level of\ncustomization, we can move to creating custom transformations. In\ncomputer vision, these come in handy to help generalize algorithms and\nimprove accuracy. A suite of transformations used at training time is\ntypically referred to as data augmentation and is a common practice for\nmodern model development.",
                        "markdown"
                    ],
                    [
                        "One issue common in handling datasets is that the samples may not all be\nthe same size. Most neural networks expect the images of a fixed size.\nTherefore, we will need to write some prepocessing code. Let\u2019s create\nthree transforms:\n\nRescale: to scale the image\nRandomCrop: to crop from image randomly. This is data\naugmentation.\nToTensor: to convert the numpy images to torch images (we need to\nswap axes).",
                        "markdown"
                    ],
                    [
                        "We will write them as callable classes instead of simple functions so\nthat parameters of the transform need not be passed everytime it\u2019s\ncalled. For this, we just need to implement __call__ method and if\nrequired, __init__ method. We can then use a transform like this:",
                        "markdown"
                    ],
                    [
                        "tsfm = Transform(params)\ntransformed_sample = tsfm(sample)",
                        "code"
                    ],
                    [
                        "Observe below how these transforms had to be applied both on the image\nand landmarks.",
                        "markdown"
                    ],
                    {
                        "2.1 Create callable classes": [
                            [
                                "Let\u2019s start with creating callable classes for each transform",
                                "markdown"
                            ],
                            [
                                "class Rescale(object):\n    \"\"\"Rescale the image in a sample to a given size.\n\n    Args:\n        output_size (tuple or int): Desired output size. If tuple, output is\n            matched to output_size. If int, smaller of image edges is matched\n            to output_size keeping aspect ratio the same.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h &gt; w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n\n        img = transform.resize(image, (new_h, new_w))\n\n        # h and w are swapped for landmarks because for images,\n        # x and y axes are axis 1 and 0 respectively\n        landmarks = landmarks * [new_w / w, new_h / h]\n\n        return {'image': img, 'landmarks': landmarks}\n\n\nclass RandomCrop(object):\n    \"\"\"Crop randomly the image in a sample.\n\n    Args:\n        output_size (tuple or int): Desired output size. If int, square crop\n            is made.\n    \"\"\"\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        if isinstance(output_size, int):\n            self.output_size = (output_size, output_size)\n        else:\n            assert len(output_size) == 2\n            self.output_size = output_size\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n\n        h, w = image.shape[:2]\n        new_h, new_w = self.output_size\n\n        top = np.random.randint(0, h - new_h)\n        left = np.random.randint(0, w - new_w)\n\n        image = image[top: top + new_h,\n                      left: left + new_w]\n\n        landmarks = landmarks - [left, top]\n\n        return {'image': image, 'landmarks': landmarks}\n\n\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        image, landmarks = sample['image'], sample['landmarks']\n\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((2, 0, 1))\n        return {'image': torch.from_numpy(image),\n                'landmarks': torch.from_numpy(landmarks)}",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2.2 Compose transforms and apply to a sample": [
                            [
                                "Next let\u2019s compose these transforms and apply to a sample",
                                "markdown"
                            ],
                            [
                                "Let\u2019s say we want to rescale the shorter side of the image to 256 and\nthen randomly crop a square of size 224 from it. i.e, we want to compose\nRescale and RandomCrop transforms.\ntorchvision.transforms.Compose is a simple callable class which\nallows us to do this.",
                                "markdown"
                            ],
                            [
                                "scale = Rescale(256)\ncrop = RandomCrop(128)\ncomposed = transforms.Compose([Rescale(256),\n                               RandomCrop(224)])\n\n# Apply each of the above transforms on sample.\nfig = plt.figure()\nsample = face_dataset[65]\nfor i, tsfrm in enumerate([scale, crop, composed]):\n    transformed_sample = tsfrm(sample)\n\n    ax = plt.subplot(1, 3, i + 1)\n    plt.tight_layout()\n    ax.set_title(type(tsfrm).__name__)\n    show_landmarks(**transformed_sample)\n\nplt.show()",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2.3 Iterate through the dataset": [
                            [
                                "Next we will iterate through the dataset",
                                "markdown"
                            ],
                            [
                                "Let\u2019s put this all together to create a dataset with composed\ntransforms. To summarize, every time this dataset is sampled:\n\nAn image is read from the file on the fly\nTransforms are applied on the read image\nSince one of the transforms is random, data is augmentated on\nsampling",
                                "markdown"
                            ],
                            [
                                "We can iterate over the created dataset with a for i in range loop\nas before.",
                                "markdown"
                            ],
                            [
                                "transformed_dataset = FaceLandmarksDataset(csv_file='faces/face_landmarks.csv',\n                                           root_dir='faces/',\n                                           transform=transforms.Compose([\n                                               Rescale(256),\n                                               RandomCrop(224),\n                                               ToTensor()\n                                           ]))\n\nfor i in range(len(transformed_dataset)):\n    sample = transformed_dataset[i]\n\n    print(i, sample['image'].size(), sample['landmarks'].size())\n\n    if i == 3:\n        break",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Part 3: The Dataloader": [
                    [
                        "By operating on the dataset directly, we are losing out on a lot of\nfeatures by using a simple for loop to iterate over the data. In\nparticular, we are missing out on:\n\nBatching the data\nShuffling the data\nLoad the data in parallel using multiprocessing workers.",
                        "markdown"
                    ],
                    [
                        "torch.utils.data.DataLoader is an iterator which provides all these\nfeatures. Parameters used below should be clear. One parameter of\ninterest is collate_fn. You can specify how exactly the samples need\nto be batched using collate_fn. However, default collate should work\nfine for most use cases.",
                        "markdown"
                    ],
                    [
                        "dataloader = DataLoader(transformed_dataset, batch_size=4,\n                        shuffle=True, num_workers=4)\n\n\n# Helper function to show a batch\ndef show_landmarks_batch(sample_batched):\n    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n    images_batch, landmarks_batch = \\\n            sample_batched['image'], sample_batched['landmarks']\n    batch_size = len(images_batch)\n    im_size = images_batch.size(2)\n\n    grid = utils.make_grid(images_batch)\n    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n\n    for i in range(batch_size):\n        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size,\n                    landmarks_batch[i, :, 1].numpy(),\n                    s=10, marker='.', c='r')\n\n        plt.title('Batch from dataloader')\n\nfor i_batch, sample_batched in enumerate(dataloader):\n    print(i_batch, sample_batched['image'].size(),\n          sample_batched['landmarks'].size())\n\n    # observe 4th batch and stop.\n    if i_batch == 3:\n        plt.figure()\n        show_landmarks_batch(sample_batched)\n        plt.axis('off')\n        plt.ioff()\n        plt.show()\n        break",
                        "code"
                    ],
                    [
                        "Now that you\u2019ve learned how to create a custom dataloader with PyTorch,\nwe recommend diving deeper into the docs and customizing your workflow\neven further. You can learn more in the torch.utils.data docs\n.",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Model Interpretability using Captum": [
            [
                "Captum helps you understand how the data features impact your model\npredictions or neuron activations, shedding light on how your model\noperates.",
                "markdown"
            ],
            [
                "Using Captum, you can apply a wide range of state-of-the-art feature\nattribution algorithms such as Guided GradCam and\nIntegrated Gradients in a unified way.",
                "markdown"
            ],
            [
                "In this recipe you will learn how to use Captum to:",
                "markdown"
            ],
            [
                "Attribute the predictions of an image classifier to their corresponding image features.",
                "markdown"
            ],
            [
                "Visualize the attribution results.",
                "markdown"
            ],
            {
                "Before you begin": [
                    [
                        "Make sure Captum is installed in your active Python environment. Captum\nis available both on GitHub, as a pip package, or as a conda\npackage. For detailed instructions, consult the installation guide at",
                        "markdown"
                    ],
                    [
                        "For a model, we use a built-in image classifier in PyTorch. Captum can\nreveal which parts of a sample image support certain predictions made by\nthe model.",
                        "markdown"
                    ],
                    [
                        "import torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nmodel = (pretrained=True).eval()\n\nresponse = requests.get(\"https://image.freepik.com/free-photo/two-beautiful-puppies-cat-dog_58409-6024.jpg\")\nimg = Image.open(BytesIO(response.content))\n\ncenter_crop = ([\n (256),\n (224),\n])\n\nnormalize = ([\n    (),               # converts the image to a tensor with values between 0 and 1\n    (                # normalize to follow 0-centered imagenet pixel rgb distribution\n     mean=[0.485, 0.456, 0.406],\n     std=[0.229, 0.224, 0.225]\n    )\n])\ninput_img = normalize(center_crop(img)).unsqueeze(0)",
                        "code"
                    ]
                ]
            },
            {
                "Computing Attribution": [
                    [
                        "Among the top-3 predictions of the models are classes 208 and 283 which\ncorrespond to dog and cat.",
                        "markdown"
                    ],
                    [
                        "Let us attribute each of these predictions to the corresponding part of\nthe input, using Captum\u2019s Occlusion algorithm.",
                        "markdown"
                    ],
                    [
                        "from captum.attr import Occlusion\n\nocclusion = Occlusion(model)\n\nstrides = (3, 9, 9)               # smaller = more fine-grained attribution but slower\ntarget=208,                       # Labrador index in ImageNet\nsliding_window_shapes=(3,45, 45)  # choose size enough to change object appearance\nbaselines = 0                     # values to occlude the image with. 0 corresponds to gray\n\nattribution_dog = occlusion.attribute(input_img,\n                                       strides = strides,\n                                       target=target,\n                                       sliding_window_shapes=sliding_window_shapes,\n                                       baselines=baselines)\n\n\ntarget=283,                       # Persian cat index in ImageNet\nattribution_cat = occlusion.attribute(input_img,\n                                       strides = strides,\n                                       target=target,\n                                       sliding_window_shapes=sliding_window_shapes,\n                                       baselines=0)",
                        "code"
                    ],
                    [
                        "Besides Occlusion, Captum features many algorithms such as\nIntegrated Gradients, Deconvolution,\nGuidedBackprop, Guided GradCam, DeepLift, and\nGradientShap. All of these algorithms are subclasses of\nAttribution which expects your model as a callable forward_func\nupon initialization and has an attribute(...) method which returns\nthe attribution result in a unified format.",
                        "markdown"
                    ],
                    [
                        "Let us visualize the computed attribution results in case of images.",
                        "markdown"
                    ]
                ]
            },
            {
                "Visualizing the Results": [
                    [
                        "Captum\u2019s visualization utility provides out-of-the-box methods\nto visualize attribution results both for pictorial and for textual\ninputs.",
                        "markdown"
                    ],
                    [
                        "import numpy as np\nfrom captum.attr import visualization as viz\n\n# Convert the compute attribution tensor into an image-like numpy array\nattribution_dog = np.transpose(attribution_dog.squeeze().cpu().detach().numpy(), (1,2,0))\n\nvis_types = [\"heat_map\", \"original_image\"]\nvis_signs = [\"all\", \"all\"] # \"positive\", \"negative\", or \"all\" to show both\n# positive attribution indicates that the presence of the area increases the prediction score\n# negative attribution indicates distractor areas whose absence increases the score\n\n_ = viz.visualize_image_attr_multiple(attribution_dog,\n                                      np.array(center_crop(img)),\n                                      vis_types,\n                                      vis_signs,\n                                      [\"attribution for dog\", \"image\"],\n                                      show_colorbar = True\n                                     )\n\n\nattribution_cat = np.transpose(attribution_cat.squeeze().cpu().detach().numpy(), (1,2,0))\n\n_ = viz.visualize_image_attr_multiple(attribution_cat,\n                                      np.array(center_crop(img)),\n                                      [\"heat_map\", \"original_image\"],\n                                      [\"all\", \"all\"], # positive/negative attribution or all\n                                      [\"attribution for cat\", \"image\"],\n                                      show_colorbar = True\n                                     )",
                        "code"
                    ],
                    [
                        "If your data is textual, visualization.visualize_text() offers a\ndedicated view to explore attribution on top of the input text. Find out\nmore at ",
                        "markdown"
                    ]
                ]
            },
            {
                "Final Notes": [
                    [
                        "Captum can handle most model types in PyTorch across modalities\nincluding vision, text, and more. With Captum you can: * Attribute a\nspecific output to the model input as illustrated above. * Attribute a\nspecific output to a hidden-layer neuron (see Captum API reference). *\nAttribute a hidden-layer neuron response to the model input (see Captum\nAPI reference).",
                        "markdown"
                    ],
                    [
                        "For complete API of the supported methods and a list of tutorials,\nconsult our website ",
                        "markdown"
                    ],
                    [
                        "Another useful post by Gilbert Tanner:",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Dynamic Quantization": [
            [
                "In this recipe you will see how to take advantage of Dynamic\nQuantization to accelerate inference on an LSTM-style recurrent neural\nnetwork. This reduces the size of the model weights and speeds up model\nexecution.",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "There are a number of trade-offs that can be made when designing neural\nnetworks. During model developmenet and training you can alter the\nnumber of layers and number of parameters in a recurrent neural network\nand trade-off accuracy against model size and/or model latency or\nthroughput. Such changes can take lot of time and compute resources\nbecause you are iterating over the model training. Quantization gives\nyou a way to make a similar trade off between performance and model\naccuracy with a known model after training is completed.",
                        "markdown"
                    ],
                    [
                        "You can give it a try in a single session and you will certainly reduce\nyour model size significantly and may get a significant latency\nreduction without losing a lot of accuracy.",
                        "markdown"
                    ]
                ]
            },
            {
                "What is dynamic quantization?": [
                    [
                        "Quantizing a network means converting it to use a reduced precision\ninteger representation for the weights and/or activations. This saves on\nmodel size and allows the use of higher throughput math operations on\nyour CPU or GPU.",
                        "markdown"
                    ],
                    [
                        "When converting from floating point to integer values you are\nessentially multiplying the floating point value by some scale factor\nand rounding the result to a whole number. The various quantization\napproaches differ in the way they approach determining that scale\nfactor.",
                        "markdown"
                    ],
                    [
                        "The key idea with dynamic quantization as described here is that we are\ngoing to determine the scale factor for activations dynamically based on\nthe data range observed at runtime. This ensures that the scale factor\nis \u201ctuned\u201d so that as much signal as possible about each observed\ndataset is preserved.",
                        "markdown"
                    ],
                    [
                        "The model parameters on the other hand are known during model conversion\nand they are converted ahead of time and stored in INT8 form.",
                        "markdown"
                    ],
                    [
                        "Arithmetic in the quantized model is done using vectorized INT8\ninstructions. Accumulation is typically done with INT16 or INT32 to\navoid overflow. This higher precision value is scaled back to INT8 if\nthe next layer is quantized or converted to FP32 for output.",
                        "markdown"
                    ],
                    [
                        "Dynamic quantization is relatively free of tuning parameters which makes\nit well suited to be added into production pipelines as a standard part\nof converting LSTM models to deployment.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "Limitations on the approach taken here",
                        "markdown"
                    ],
                    [
                        "This recipe provides a quick introduction to the dynamic quantization\nfeatures in PyTorch and the workflow for using it. Our focus is on\nexplaining the specific functions used to convert the model. We will\nmake a number of significant simplifications in the interest of brevity\nand clarity",
                        "markdown"
                    ],
                    [
                        "You will start with a minimal LSTM network",
                        "markdown"
                    ],
                    [
                        "You are simply going to initialize the network with a random hidden\nstate",
                        "markdown"
                    ],
                    [
                        "You are going to test the network with random inputs",
                        "markdown"
                    ],
                    [
                        "You are not going to train the network in this tutorial",
                        "markdown"
                    ],
                    [
                        "You will see that the quantized form of this network is smaller and\nruns faster than the floating point network we started with",
                        "markdown"
                    ],
                    [
                        "You will see that the output values are generally in the same\nballpark as the output of the FP32 network, but we are not\ndemonstrating here the expected accuracy loss on a real trained\nnetwork",
                        "markdown"
                    ],
                    [
                        "You will see how dynamic quantization is done and be able to see\nsuggestive reductions in memory use and latency times. Providing a\ndemonstration that the technique can preserve high levels of model\naccuracy on a trained LSTM is left to a more advanced tutorial. If you\nwant to move right away to that more rigorous treatment please proceed\nto the .",
                        "markdown"
                    ]
                ]
            },
            {
                "Steps": [
                    [
                        "This recipe has 5 steps.",
                        "markdown"
                    ],
                    [
                        "Set Up - Here you define a very simple LSTM, import modules, and establish\nsome random input tensors.",
                        "markdown"
                    ],
                    [
                        "Do the Quantization - Here you instantiate a floating point model and then create quantized\nversion of it.",
                        "markdown"
                    ],
                    [
                        "Look at Model Size - Here you show that the model size gets smaller.",
                        "markdown"
                    ],
                    [
                        "Look at Latency - Here you run the two models and compare model runtime (latency).",
                        "markdown"
                    ],
                    [
                        "Look at Accuracy - Here you run the two models and compare outputs.",
                        "markdown"
                    ],
                    {
                        "1: Set Up": [
                            [
                                "This is a straightfoward bit of code to set up for the rest of the\nrecipe.",
                                "markdown"
                            ],
                            [
                                "The unique module we are importing here is torch.quantization which\nincludes PyTorch\u2019s quantized operators and conversion functions. We also\ndefine a very simple LSTM model and set up some inputs.",
                                "markdown"
                            ],
                            [
                                "# import the modules used here in this recipe\nimport torch\nimport torch.quantization\nimport torch.nn as nn\nimport copy\nimport os\nimport time\n\n# define a very, very simple LSTM for demonstration purposes\n# in this case, we are wrapping nn.LSTM, one layer, no pre or post processing\n# inspired by\n# https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html, by Robert Guthrie\n# and https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html\nclass lstm_for_demonstration():\n  \"\"\"Elementary Long Short Term Memory style model which simply wraps nn.LSTM\n     Not to be used for anything other than demonstration.\n  \"\"\"\n  def __init__(self,in_dim,out_dim,depth):\n     super(lstm_for_demonstration,self).__init__()\n     self.lstm = (in_dim,out_dim,depth)\n\n  def forward(self,inputs,hidden):\n     out,hidden = self.lstm(inputs,hidden)\n     return out, hidden\n\n\n(29592)  # set the seed for reproducibility\n\n#shape parameters\nmodel_dimension=8\nsequence_length=20\nbatch_size=1\nlstm_depth=1\n\n# random data for input\ninputs = (sequence_length,batch_size,model_dimension)\n# hidden is actually is a tuple of the initial hidden state and the initial cell state\nhidden = ((lstm_depth,batch_size,model_dimension), (lstm_depth,batch_size,model_dimension))",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2: Do the Quantization": [
                            [
                                "Now we get to the fun part. First we create an instance of the model\ncalled float_lstm then we are going to quantize it. We\u2019re going to use\nthe",
                                "markdown"
                            ],
                            [
                                "torch.quantization.quantize_dynamic()",
                                "code"
                            ],
                            [
                                "function here ()\nwhich takes the model, then a list of the submodules which we want to\nhave quantized if they appear, then the datatype we are targeting. This\nfunction returns a quantized version of the original model as a new\nmodule.",
                                "markdown"
                            ],
                            [
                                "That\u2019s all it takes.",
                                "markdown"
                            ],
                            [
                                " # here is our floating point instance\nfloat_lstm = lstm_for_demonstration(model_dimension, model_dimension,lstm_depth)\n\n# this is the call that does the work\nquantized_lstm = torch.quantization.quantize_dynamic(\n    float_lstm, {, }, dtype=torch.qint8\n)\n\n# show the changes that were made\nprint('Here is the floating point version of this module:')\nprint(float_lstm)\nprint('')\nprint('and now the quantized version:')\nprint(quantized_lstm)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "3. Look at Model Size": [
                            [
                                "Ok, so we\u2019ve quantized the model. What does that get us? Well the first\nbenefit is that we\u2019ve replaced the FP32 model parameters with INT8\nvalues (and some recorded scale factors). This means about 75% less data\nto store and move around. With the default values the reduction shown\nbelow will be less than 75% but if you increase the model size above\n(for example you can set model dimension to something like 80) this will\nconverge towards 4x smaller as the stored model size dominated more and\nmore by the parameter values.",
                                "markdown"
                            ],
                            [
                                "def print_size_of_model(model, label=\"\"):\n    (model.state_dict(), \"temp.p\")\n    size=os.path.getsize(\"temp.p\")\n    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n    os.remove('temp.p')\n    return size\n\n# compare the sizes\nf=print_size_of_model(float_lstm,\"fp32\")\nq=print_size_of_model(quantized_lstm,\"int8\")\nprint(\"{0:.2f} times smaller\".format(f/q))",
                                "code"
                            ]
                        ]
                    },
                    {
                        "4. Look at Latency": [
                            [
                                "The second benefit is that the quantized model will typically run\nfaster. This is due to a combinations of effects including at least:",
                                "markdown"
                            ],
                            [
                                "Less time spent moving parameter data in",
                                "markdown"
                            ],
                            [
                                "Faster INT8 operations",
                                "markdown"
                            ],
                            [
                                "As you will see the quantized version of this super-simple network runs\nfaster. This will generally be true of more complex networks but as they\nsay \u201cyour milage may vary\u201d depending on a number of factors including\nthe structure of the model and the hardware you are running on.",
                                "markdown"
                            ],
                            [
                                "# compare the performance\nprint(\"Floating point FP32\")\n# %timeit float_lstm.forward(inputs, hidden)\n\nprint(\"Quantized INT8\")\n# %timeit quantized_lstm.forward(inputs,hidden)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "5: Look at Accuracy": [
                            [
                                "We are not going to do a careful look at accuracy here because we are\nworking with a randomly initialized network rather than a properly\ntrained one. However, I think it is worth quickly showing that the\nquantized network does produce output tensors that are \u201cin the same\nballpark\u201d as the original one.",
                                "markdown"
                            ],
                            [
                                "For a more detailed analysis please see the more advanced tutorials\nreferenced at the end of this recipe.",
                                "markdown"
                            ],
                            [
                                "# run the float model\nout1, hidden1 = float_lstm(inputs, hidden)\nmag1 = (abs(out1)).item()\nprint('mean absolute value of output tensor values in the FP32 model is {0:.5f} '.format(mag1))\n\n# run the quantized model\nout2, hidden2 = quantized_lstm(inputs, hidden)\nmag2 = (abs(out2)).item()\nprint('mean absolute value of output tensor values in the INT8 model is {0:.5f}'.format(mag2))\n\n# compare them\nmag3 = (abs(out1-out2)).item()\nprint('mean absolute value of the difference between the output tensors is {0:.5f} or {1:.2f} percent'.format(mag3,mag3/mag1*100))",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Learn More": [
                    [
                        "We\u2019ve explained what dynamic quantization is, what benefits it brings,\nand you have used the torch.quantization.quantize_dynamic() function\nto quickly quantize a simple LSTM model.",
                        "markdown"
                    ],
                    [
                        "This was a fast and high level treatment of this material; for more\ndetail please continue learning with .",
                        "markdown"
                    ]
                ]
            }
        ],
        "Saving and loading models across devices in PyTorch": [
            [
                "There may be instances where you want to save and load your neural\nnetworks across different devices.",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "Saving and loading models across devices is relatively straightforward\nusing PyTorch. In this recipe, we will experiment with saving and\nloading models across CPUs and GPUs.",
                        "markdown"
                    ]
                ]
            },
            {
                "Setup": [
                    [
                        "In order for every code block to run properly in this recipe, you must\nfirst change the runtime to \u201cGPU\u201d or higher. Once you do, we need to\ninstall torch if it isn\u2019t already available.",
                        "markdown"
                    ],
                    [
                        "pip install torch",
                        "code"
                    ]
                ]
            },
            {
                "Steps": [
                    [
                        "Import all necessary libraries for loading our data",
                        "markdown"
                    ],
                    [
                        "Define and intialize the neural network",
                        "markdown"
                    ],
                    [
                        "Save on a GPU, load on a CPU",
                        "markdown"
                    ],
                    [
                        "Save on a GPU, load on a GPU",
                        "markdown"
                    ],
                    [
                        "Save on a CPU, load on a GPU",
                        "markdown"
                    ],
                    [
                        "Saving and loading DataParallel models",
                        "markdown"
                    ],
                    {
                        "1. Import necessary libraries for loading our data": [
                            [
                                "For this recipe, we will use torch and its subsidiaries torch.nn\nand torch.optim.",
                                "markdown"
                            ],
                            [
                                "import torch\nimport torch.nn as nn\nimport torch.optim as optim",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2. Define and intialize the neural network": [
                            [
                                "For sake of example, we will create a neural network for training\nimages. To learn more see the Defining a Neural Network recipe.",
                                "markdown"
                            ],
                            [
                                "class Net():\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\nprint(net)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "3. Save on GPU, Load on CPU": [
                            [
                                "When loading a model on a CPU that was trained with a GPU, pass\ntorch.device('cpu') to the map_location argument in the\ntorch.load() function.",
                                "markdown"
                            ],
                            [
                                "# Specify a path to save to\nPATH = \"model.pt\"\n\n# Save\n(net.state_dict(), PATH)\n\n# Load\ndevice = ('cpu')\nmodel = Net()\nmodel.load_state_dict((PATH, map_location=device))",
                                "code"
                            ],
                            [
                                "In this case, the storages underlying the tensors are dynamically\nremapped to the CPU device using the map_location argument.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "4. Save on GPU, Load on GPU": [
                            [
                                "When loading a model on a GPU that was trained and saved on GPU, simply\nconvert the initialized model to a CUDA optimized model using\nmodel.to(torch.device('cuda')).",
                                "markdown"
                            ],
                            [
                                "Be sure to use the .to(torch.device('cuda')) function on all model\ninputs to prepare the data for the model.",
                                "markdown"
                            ],
                            [
                                "# Save\n(net.state_dict(), PATH)\n\n# Load\ndevice = (\"cuda\")\nmodel = Net()\nmodel.load_state_dict((PATH))\nmodel.to(device)",
                                "code"
                            ],
                            [
                                "Note that calling my_tensor.to(device) returns a new copy of\nmy_tensor on GPU. It does NOT overwrite my_tensor. Therefore,\nremember to manually overwrite tensors:\nmy_tensor = my_tensor.to(torch.device('cuda')).",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "5. Save on CPU, Load on GPU": [
                            [
                                "When loading a model on a GPU that was trained and saved on CPU, set the\nmap_location argument in the torch.load() function to\ncuda:device_id. This loads the model to a given GPU device.",
                                "markdown"
                            ],
                            [
                                "Be sure to call model.to(torch.device('cuda')) to convert the\nmodel\u2019s parameter tensors to CUDA tensors.",
                                "markdown"
                            ],
                            [
                                "Finally, also be sure to use the .to(torch.device('cuda')) function\non all model inputs to prepare the data for the CUDA optimized model.",
                                "markdown"
                            ],
                            [
                                "# Save\n(net.state_dict(), PATH)\n\n# Load\ndevice = (\"cuda\")\nmodel = Net()\n# Choose whatever GPU device number you want\nmodel.load_state_dict((PATH, map_location=\"cuda:0\"))\n# Make sure to call input = input.to(device) on any input tensors that you feed to the model\nmodel.to(device)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "6. Saving torch.nn.DataParallel Models": [
                            [
                                "torch.nn.DataParallel is a model wrapper that enables parallel GPU\nutilization.",
                                "markdown"
                            ],
                            [
                                "To save a DataParallel model generically, save the\nmodel.module.state_dict(). This way, you have the flexibility to\nload the model any way you want to any device you want.",
                                "markdown"
                            ],
                            [
                                "# Save\n(net.module.state_dict(), PATH)\n\n# Load to whatever device you want",
                                "code"
                            ],
                            [
                                "Congratulations! You have successfully saved and loaded models across\ndevices in PyTorch.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Learn More": [
                    [
                        "Take a look at these other recipes to continue your learning:",
                        "markdown"
                    ],
                    [
                        "TBD",
                        "markdown"
                    ],
                    [
                        "TBD",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Saving and loading a general checkpoint in PyTorch": [
            [
                "Saving and loading a general checkpoint model for inference or\nresuming training can be helpful for picking up where you last left off.\nWhen saving a general checkpoint, you must save more than just the\nmodel\u2019s state_dict. It is important to also save the optimizer\u2019s\nstate_dict, as this contains buffers and parameters that are updated as\nthe model trains. Other items that you may want to save are the epoch\nyou left off on, the latest recorded training loss, external\ntorch.nn.Embedding layers, and more, based on your own algorithm.",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "To save multiple checkpoints, you must organize them in a dictionary and\nuse torch.save() to serialize the dictionary. A common PyTorch\nconvention is to save these checkpoints using the .tar file\nextension. To load the items, first initialize the model and optimizer,\nthen load the dictionary locally using torch.load(). From here, you can\neasily access the saved items by simply querying the dictionary as you\nwould expect.",
                        "markdown"
                    ],
                    [
                        "In this recipe, we will explore how to save and load multiple\ncheckpoints.",
                        "markdown"
                    ]
                ]
            },
            {
                "Setup": [
                    [
                        "Before we begin, we need to install torch if it isn\u2019t already\navailable.",
                        "markdown"
                    ],
                    [
                        "pip install torch",
                        "code"
                    ]
                ]
            },
            {
                "Steps": [
                    [
                        "Import all necessary libraries for loading our data",
                        "markdown"
                    ],
                    [
                        "Define and initialize the neural network",
                        "markdown"
                    ],
                    [
                        "Initialize the optimizer",
                        "markdown"
                    ],
                    [
                        "Save the general checkpoint",
                        "markdown"
                    ],
                    [
                        "Load the general checkpoint",
                        "markdown"
                    ],
                    {
                        "1. Import necessary libraries for loading our data": [
                            [
                                "For this recipe, we will use torch and its subsidiaries torch.nn\nand torch.optim.",
                                "markdown"
                            ],
                            [
                                "import torch\nimport torch.nn as nn\nimport torch.optim as optim",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2. Define and initialize the neural network": [
                            [
                                "For sake of example, we will create a neural network for training\nimages. To learn more see the Defining a Neural Network recipe.",
                                "markdown"
                            ],
                            [
                                "class Net():\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\nprint(net)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "3. Initialize the optimizer": [
                            [
                                "We will use SGD with momentum.",
                                "markdown"
                            ],
                            [
                                "optimizer = (net.parameters(), lr=0.001, momentum=0.9)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "4. Save the general checkpoint": [
                            [
                                "Collect all relevant information and build your dictionary.",
                                "markdown"
                            ],
                            [
                                "# Additional information\nEPOCH = 5\nPATH = \"model.pt\"\nLOSS = 0.4\n\n({\n            'epoch': EPOCH,\n            'model_state_dict': net.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': LOSS,\n            }, PATH)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "5. Load the general checkpoint": [
                            [
                                "Remember to first initialize the model and optimizer, then load the\ndictionary locally.",
                                "markdown"
                            ],
                            [
                                "model = Net()\noptimizer = (net.parameters(), lr=0.001, momentum=0.9)\n\ncheckpoint = (PATH)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nepoch = checkpoint['epoch']\nloss = checkpoint['loss']\n\nmodel.eval()\n# - or -\nmodel.train()",
                                "code"
                            ],
                            [
                                "You must call model.eval() to set dropout and batch normalization\nlayers to evaluation mode before running inference. Failing to do this\nwill yield inconsistent inference results.",
                                "markdown"
                            ],
                            [
                                "If you wish to resuming training, call model.train() to ensure these\nlayers are in training mode.",
                                "markdown"
                            ],
                            [
                                "Congratulations! You have successfully saved and loaded a general\ncheckpoint for inference and/or resuming training in PyTorch.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Learn More": [
                    [
                        "Take a look at these other recipes to continue your learning:",
                        "markdown"
                    ],
                    [
                        "TBD",
                        "markdown"
                    ],
                    [
                        "TBD",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Saving and loading models for inference in PyTorch": [
            [
                "There are two approaches for saving and loading models for inference in\nPyTorch. The first is saving and loading the state_dict, and the\nsecond is saving and loading the entire model.",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "Saving the model\u2019s state_dict with the torch.save() function\nwill give you the most flexibility for restoring the model later. This\nis the recommended method for saving models, because it is only really\nnecessary to save the trained model\u2019s learned parameters.\nWhen saving and loading an entire model, you save the entire module\nusing Python\u2019s\n module. Using\nthis approach yields the most intuitive syntax and involves the least\namount of code. The disadvantage of this approach is that the serialized\ndata is bound to the specific classes and the exact directory structure\nused when the model is saved. The reason for this is because pickle does\nnot save the model class itself. Rather, it saves a path to the file\ncontaining the class, which is used during load time. Because of this,\nyour code can break in various ways when used in other projects or after\nrefactors.\nIn this recipe, we will explore both ways on how to save and load models\nfor inference.",
                        "markdown"
                    ]
                ]
            },
            {
                "Setup": [
                    [
                        "Before we begin, we need to install torch if it isn\u2019t already\navailable.",
                        "markdown"
                    ],
                    [
                        "pip install torch",
                        "code"
                    ]
                ]
            },
            {
                "Steps": [
                    [
                        "Import all necessary libraries for loading our data",
                        "markdown"
                    ],
                    [
                        "Define and intialize the neural network",
                        "markdown"
                    ],
                    [
                        "Initialize the optimizer",
                        "markdown"
                    ],
                    [
                        "Save and load the model via state_dict",
                        "markdown"
                    ],
                    [
                        "Save and load the entire model",
                        "markdown"
                    ],
                    {
                        "1. Import necessary libraries for loading our data": [
                            [
                                "For this recipe, we will use torch and its subsidiaries torch.nn\nand torch.optim.",
                                "markdown"
                            ],
                            [
                                "import torch\nimport torch.nn as nn\nimport torch.optim as optim",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2. Define and intialize the neural network": [
                            [
                                "For sake of example, we will create a neural network for training\nimages. To learn more see the Defining a Neural Network recipe.",
                                "markdown"
                            ],
                            [
                                "class Net():\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\nprint(net)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "3. Initialize the optimizer": [
                            [
                                "We will use SGD with momentum.",
                                "markdown"
                            ],
                            [
                                "optimizer = (net.parameters(), lr=0.001, momentum=0.9)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "4. Save and load the model via state_dict": [
                            [
                                "Let\u2019s save and load our model using just state_dict.",
                                "markdown"
                            ],
                            [
                                "# Specify a path\nPATH = \"state_dict_model.pt\"\n\n# Save\n(net.state_dict(), PATH)\n\n# Load\nmodel = Net()\nmodel.load_state_dict((PATH))\nmodel.eval()",
                                "code"
                            ],
                            [
                                "A common PyTorch convention is to save models using either a .pt or\n.pth file extension.",
                                "markdown"
                            ],
                            [
                                "Notice that the load_state_dict() function takes a dictionary\nobject, NOT a path to a saved object. This means that you must\ndeserialize the saved state_dict before you pass it to the\nload_state_dict() function. For example, you CANNOT load using\nmodel.load_state_dict(PATH).",
                                "markdown"
                            ],
                            [
                                "Remember too, that you must call model.eval() to set dropout and\nbatch normalization layers to evaluation mode before running inference.\nFailing to do this will yield inconsistent inference results.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "5. Save and load entire model": [
                            [
                                "Now let\u2019s try the same thing with the entire model.",
                                "markdown"
                            ],
                            [
                                "# Specify a path\nPATH = \"entire_model.pt\"\n\n# Save\n(net, PATH)\n\n# Load\nmodel = (PATH)\nmodel.eval()",
                                "code"
                            ],
                            [
                                "Again here, remember that you must call model.eval() to set dropout and\nbatch normalization layers to evaluation mode before running inference.",
                                "markdown"
                            ],
                            [
                                "Congratulations! You have successfully saved and load models for\ninference in PyTorch.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Learn More": [
                    [
                        "Take a look at these other recipes to continue your learning:",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Saving and loading multiple models in one file using PyTorch": [
            [
                "Saving and loading multiple models can be helpful for reusing models\nthat you have previously trained.",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "When saving a model comprised of multiple torch.nn.Modules, such as\na GAN, a sequence-to-sequence model, or an ensemble of models, you must\nsave a dictionary of each model\u2019s state_dict and corresponding\noptimizer. You can also save any other items that may aid you in\nresuming training by simply appending them to the dictionary.\nTo load the models, first initialize the models and optimizers, then\nload the dictionary locally using torch.load(). From here, you can\neasily access the saved items by simply querying the dictionary as you\nwould expect.\nIn this recipe, we will demonstrate how to save multiple models to one\nfile using PyTorch.",
                        "markdown"
                    ]
                ]
            },
            {
                "Setup": [
                    [
                        "Before we begin, we need to install torch if it isn\u2019t already\navailable.",
                        "markdown"
                    ],
                    [
                        "pip install torch",
                        "code"
                    ]
                ]
            },
            {
                "Steps": [
                    [
                        "Import all necessary libraries for loading our data",
                        "markdown"
                    ],
                    [
                        "Define and intialize the neural network",
                        "markdown"
                    ],
                    [
                        "Initialize the optimizer",
                        "markdown"
                    ],
                    [
                        "Save multiple models",
                        "markdown"
                    ],
                    [
                        "Load multiple models",
                        "markdown"
                    ],
                    {
                        "1. Import necessary libraries for loading our data": [
                            [
                                "For this recipe, we will use torch and its subsidiaries torch.nn\nand torch.optim.",
                                "markdown"
                            ],
                            [
                                "import torch\nimport torch.nn as nn\nimport torch.optim as optim",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2. Define and initialize the neural network": [
                            [
                                "For sake of example, we will create a neural network for training\nimages. To learn more see the Defining a Neural Network recipe. Build\ntwo variables for the models to eventually save.",
                                "markdown"
                            ],
                            [
                                "class Net():\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnetA = Net()\nnetB = Net()",
                                "code"
                            ]
                        ]
                    },
                    {
                        "3. Initialize the optimizer": [
                            [
                                "We will use SGD with momentum to build an optimizer for each model we\ncreated.",
                                "markdown"
                            ],
                            [
                                "optimizerA = (netA.parameters(), lr=0.001, momentum=0.9)\noptimizerB = (netB.parameters(), lr=0.001, momentum=0.9)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "4. Save multiple models": [
                            [
                                "Collect all relevant information and build your dictionary.",
                                "markdown"
                            ],
                            [
                                "# Specify a path to save to\nPATH = \"model.pt\"\n\n({\n            'modelA_state_dict': netA.state_dict(),\n            'modelB_state_dict': netB.state_dict(),\n            'optimizerA_state_dict': optimizerA.state_dict(),\n            'optimizerB_state_dict': optimizerB.state_dict(),\n            }, PATH)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "4. Load multiple models": [
                            [
                                "Remember to first initialize the models and optimizers, then load the\ndictionary locally.",
                                "markdown"
                            ],
                            [
                                "modelA = Net()\nmodelB = Net()\noptimModelA = (modelA.parameters(), lr=0.001, momentum=0.9)\noptimModelB = (modelB.parameters(), lr=0.001, momentum=0.9)\n\ncheckpoint = (PATH)\nmodelA.load_state_dict(checkpoint['modelA_state_dict'])\nmodelB.load_state_dict(checkpoint['modelB_state_dict'])\noptimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])\noptimizerB.load_state_dict(checkpoint['optimizerB_state_dict'])\n\nmodelA.eval()\nmodelB.eval()\n# - or -\nmodelA.train()\nmodelB.train()",
                                "code"
                            ],
                            [
                                "You must call model.eval() to set dropout and batch normalization\nlayers to evaluation mode before running inference. Failing to do this\nwill yield inconsistent inference results.",
                                "markdown"
                            ],
                            [
                                "If you wish to resuming training, call model.train() to ensure these\nlayers are in training mode.",
                                "markdown"
                            ],
                            [
                                "Congratulations! You have successfully saved and loaded multiple models\nin PyTorch.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Learn More": [
                    [
                        "Take a look at these other recipes to continue your learning:",
                        "markdown"
                    ],
                    [
                        "TBD",
                        "markdown"
                    ],
                    [
                        "TBD",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Warmstarting model using parameters from a different model in PyTorch": [
            [
                "Partially loading a model or loading a partial model are common\nscenarios when transfer learning or training a new complex model.\nLeveraging trained parameters, even if only a few are usable, will help\nto warmstart the training process and hopefully help your model converge\nmuch faster than training from scratch.",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "Whether you are loading from a partial state_dict, which is missing\nsome keys, or loading a state_dict with more keys than the model\nthat you are loading into, you can set the strict argument to False\nin the load_state_dict() function to ignore non-matching keys.\nIn this recipe, we will experiment with warmstarting a model using\nparameters of a different model.",
                        "markdown"
                    ]
                ]
            },
            {
                "Setup": [
                    [
                        "Before we begin, we need to install torch if it isn\u2019t already\navailable.",
                        "markdown"
                    ],
                    [
                        "pip install torch",
                        "code"
                    ]
                ]
            },
            {
                "Steps": [
                    [
                        "Import all necessary libraries for loading our data",
                        "markdown"
                    ],
                    [
                        "Define and intialize the neural network A and B",
                        "markdown"
                    ],
                    [
                        "Save model A",
                        "markdown"
                    ],
                    [
                        "Load into model B",
                        "markdown"
                    ],
                    {
                        "1. Import necessary libraries for loading our data": [
                            [
                                "For this recipe, we will use torch and its subsidiaries torch.nn\nand torch.optim.",
                                "markdown"
                            ],
                            [
                                "import torch\nimport torch.nn as nn\nimport torch.optim as optim",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2. Define and intialize the neural network A and B": [
                            [
                                "For sake of example, we will create a neural network for training\nimages. To learn more see the Defining a Neural Network recipe. We will\ncreate two neural networks for sake of loading one parameter of type A\ninto type B.",
                                "markdown"
                            ],
                            [
                                "class NetA():\n    def __init__(self):\n        super(NetA, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnetA = NetA()\n\nclass NetB():\n    def __init__(self):\n        super(NetB, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnetB = NetB()",
                                "code"
                            ]
                        ]
                    },
                    {
                        "3. Save model A": [
                            [
                                "# Specify a path to save to\nPATH = \"model.pt\"\n\n(netA.state_dict(), PATH)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "4. Load into model B": [
                            [
                                "If you want to load parameters from one layer to another, but some keys\ndo not match, simply change the name of the parameter keys in the\nstate_dict that you are loading to match the keys in the model that you\nare loading into.",
                                "markdown"
                            ],
                            [
                                "netB.load_state_dict((PATH), strict=False)",
                                "code"
                            ],
                            [
                                "You can see that all keys matched successfully!",
                                "markdown"
                            ],
                            [
                                "Congratulations! You have successfully warmstarted a model using\nparameters from a different model in PyTorch.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Learn More": [
                    [
                        "Take a look at these other recipes to continue your learning:",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Zeroing out gradients in PyTorch": [
            [
                "It is beneficial to zero out gradients when building a neural network.\nThis is because by default, gradients are accumulated in buffers (i.e,\nnot overwritten) whenever .backward() is called.",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "When training your neural network, models are able to increase their\naccuracy through gradient descent. In short, gradient descent is the\nprocess of minimizing our loss (or error) by tweaking the weights and\nbiases in our model.",
                        "markdown"
                    ],
                    [
                        "torch.Tensor is the central class of PyTorch. When you create a\ntensor, if you set its attribute .requires_grad as True, the\npackage tracks all operations on it. This happens on subsequent backward\npasses. The gradient for this tensor will be accumulated into .grad\nattribute. The accumulation (or sum) of all the gradients is calculated\nwhen .backward() is called on the loss tensor.",
                        "markdown"
                    ],
                    [
                        "There are cases where it may be necessary to zero-out the gradients of a\ntensor. For example: when you start your training loop, you should zero\nout the gradients so that you can perform this tracking correctly.\nIn this recipe, we will learn how to zero out gradients using the\nPyTorch library. We will demonstrate how to do this by training a neural\nnetwork on the CIFAR10 dataset built into PyTorch.",
                        "markdown"
                    ]
                ]
            },
            {
                "Setup": [
                    [
                        "Since we will be training data in this recipe, if you are in a runable\nnotebook, it is best to switch the runtime to GPU or TPU.\nBefore we begin, we need to install torch and torchvision if\nthey aren\u2019t already available.",
                        "markdown"
                    ],
                    [
                        "pip install torchvision",
                        "code"
                    ]
                ]
            },
            {
                "Steps": [
                    [
                        "Steps 1 through 4 set up our data and neural network for training. The\nprocess of zeroing out the gradients happens in step 5. If you already\nhave your data and neural network built, skip to 5.",
                        "markdown"
                    ],
                    [
                        "Import all necessary libraries for loading our data",
                        "markdown"
                    ],
                    [
                        "Load and normalize the dataset",
                        "markdown"
                    ],
                    [
                        "Build the neural network",
                        "markdown"
                    ],
                    [
                        "Define the loss function",
                        "markdown"
                    ],
                    [
                        "Zero the gradients while training the network",
                        "markdown"
                    ],
                    {
                        "1. Import necessary libraries for loading our data": [
                            [
                                "For this recipe, we will just be using torch and torchvision to\naccess the dataset.",
                                "markdown"
                            ],
                            [
                                "import torch\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch.optim as optim\n\nimport torchvision\nimport torchvision.transforms as transforms",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2. Load and normalize the dataset": [
                            [
                                "PyTorch features various built-in datasets (see the Loading Data recipe\nfor more information).",
                                "markdown"
                            ],
                            [
                                "transform = (\n    [(),\n     ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = (root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = (trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n\ntestset = (root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = (testset, batch_size=4,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')",
                                "code"
                            ]
                        ]
                    },
                    {
                        "3. Build the neural network": [
                            [
                                "We will use a convolutional neural network. To learn more see the\nDefining a Neural Network recipe.",
                                "markdown"
                            ],
                            [
                                "class Net():\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, 120)\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = self.pool((self.conv1(x)))\n        x = self.pool((self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = (self.fc1(x))\n        x = (self.fc2(x))\n        x = self.fc3(x)\n        return x",
                                "code"
                            ]
                        ]
                    },
                    {
                        "4. Define a Loss function and optimizer": [
                            [
                                "Let\u2019s use a Classification Cross-Entropy loss and SGD with momentum.",
                                "markdown"
                            ],
                            [
                                "net = Net()\ncriterion = ()\noptimizer = (net.parameters(), lr=0.001, momentum=0.9)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "5. Zero the gradients while training the network": [
                            [
                                "This is when things start to get interesting. We simply have to loop\nover our data iterator, and feed the inputs to the network and optimize.",
                                "markdown"
                            ],
                            [
                                "Notice that for each entity of data, we zero out the gradients. This is\nto ensure that we aren\u2019t tracking any unnecessary information when we\ntrain our neural network.",
                                "markdown"
                            ],
                            [
                                "for epoch in range(2):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 2000 == 1999:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 2000))\n            running_loss = 0.0\n\nprint('Finished Training')",
                                "code"
                            ],
                            [
                                "You can also use model.zero_grad(). This is the same as using\noptimizer.zero_grad() as long as all your model parameters are in\nthat optimizer. Use your best judgement to decide which one to use.",
                                "markdown"
                            ],
                            [
                                "Congratulations! You have successfully zeroed out gradients PyTorch.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Learn More": [
                    [
                        "Take a look at these other recipes to continue your learning:",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Pytorch Mobile Performance Recipes": [
            {
                "Introduction": [
                    [
                        "Performance (aka latency) is crucial to most, if not all,\napplications and use-cases of ML model inference on mobile devices.",
                        "markdown"
                    ],
                    [
                        "Today, PyTorch executes the models on the CPU backend pending availability\nof other hardware backends such as GPU, DSP, and NPU.",
                        "markdown"
                    ],
                    [
                        "In this recipe, you will learn:",
                        "markdown"
                    ],
                    [
                        "How to optimize your model to help decrease execution time (higher performance, lower latency) on the mobile device.",
                        "markdown"
                    ],
                    [
                        "How to benchmark (to check if optimizations helped your use case).",
                        "markdown"
                    ]
                ]
            },
            {
                "Model preparation": [
                    [
                        "We will start with preparing to optimize your model to help decrease execution time\n(higher performance, lower latency) on the mobile device.",
                        "markdown"
                    ],
                    {
                        "Setup": [
                            [
                                "First we need to installed pytorch using conda or pip with version at least 1.5.0.",
                                "markdown"
                            ],
                            [
                                "conda install pytorch torchvision -c pytorch",
                                "code"
                            ],
                            [
                                "or",
                                "markdown"
                            ],
                            [
                                "pip install torch torchvision",
                                "code"
                            ],
                            [
                                "Code your model:",
                                "markdown"
                            ],
                            [
                                "import torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nclass AnnotatedConvBnReLUModel(torch.nn.Module):\n    def __init__(self):\n        super(AnnotatedConvBnReLUModel, self).__init__()\n        self.conv = torch.nn.Conv2d(3, 5, 3, bias=False).to(dtype=torch.float)\n        self.bn = torch.nn.BatchNorm2d(5).to(dtype=torch.float)\n        self.relu = torch.nn.ReLU(inplace=True)\n        self.quant = torch.quantization.QuantStub()\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        x = x.contiguous(memory_format=torch.channels_last)\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dequant(x)\n        return x\n\nmodel = AnnotatedConvBnReLUModel()",
                                "code"
                            ],
                            [
                                "torch.quantization.QuantStub and torch.quantization.DeQuantStub() are no-op stubs, which will be used for quantization step.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "1. Fuse operators using torch.quantization.fuse_modules": [
                            [
                                "Do not be confused that fuse_modules is in the quantization package.\nIt works for all torch.nn.Module.",
                                "markdown"
                            ],
                            [
                                "torch.quantization.fuse_modules fuses a list of modules into a single module.\nIt fuses only the following sequence of modules:",
                                "markdown"
                            ],
                            [
                                "Convolution, Batch normalization",
                                "markdown"
                            ],
                            [
                                "Convolution, Batch normalization, Relu",
                                "markdown"
                            ],
                            [
                                "Convolution, Relu",
                                "markdown"
                            ],
                            [
                                "Linear, Relu",
                                "markdown"
                            ],
                            [
                                "This script will fuse Convolution, Batch Normalization and Relu in previously declared model.",
                                "markdown"
                            ],
                            [
                                "torch.quantization.fuse_modules(model, [['conv', 'bn', 'relu']], inplace=True)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2. Quantize your model": [
                            [
                                "You can find more about PyTorch quantization in\n.",
                                "markdown"
                            ],
                            [
                                "Quantization of the model not only moves computation to int8,\nbut also reduces the size of your model on a disk.\nThat size reduction helps to reduce disk read operations during the first load of the model and decreases the amount of RAM.\nBoth of those resources can be crucial for the performance of mobile applications.\nThis code does quantization, using stub for model calibration function, you can find more about it .",
                                "markdown"
                            ],
                            [
                                "model.qconfig = torch.quantization.get_default_qconfig('qnnpack')\ntorch.quantization.prepare(model, inplace=True)\n# Calibrate your model\ndef calibrate(model, calibration_data):\n    # Your calibration code here\n    return\ncalibrate(model, [])\ntorch.quantization.convert(model, inplace=True)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "3. Use torch.utils.mobile_optimizer": [
                            [
                                "Torch mobile_optimizer package does several optimizations with the scripted model,\nwhich will help to conv2d and linear operations.\nIt pre-packs model weights in an optimized format and fuses ops above with relu\nif it is the next operation.",
                                "markdown"
                            ],
                            [
                                "First we script the result model from previous step:",
                                "markdown"
                            ],
                            [
                                "torchscript_model = torch.jit.script(model)",
                                "code"
                            ],
                            [
                                "Next we call optimize_for_mobile and save model on the disk.",
                                "markdown"
                            ],
                            [
                                "torchscript_model_optimized = optimize_for_mobile(torchscript_model)\ntorch.jit.save(torchscript_model_optimized, \"model.pt\")",
                                "code"
                            ]
                        ]
                    },
                    {
                        "4. Prefer Using Channels Last Tensor memory format": [
                            [
                                "Channels Last(NHWC) memory format was introduced in PyTorch 1.4.0. It is supported only for four-dimensional tensors. This memory format gives a better memory locality for most operators, especially convolution. Our measurements showed a 3x speedup of MobileNetV2 model compared with the default Channels First(NCHW) format.",
                                "markdown"
                            ],
                            [
                                "At the moment of writing this recipe, PyTorch Android java API does not support using inputs in Channels Last memory format. But it can be used on the TorchScript model level, by adding the conversion to it for model inputs.",
                                "markdown"
                            ],
                            [
                                "def forward(self, x):\n    x = x.contiguous(memory_format=torch.channels_last)\n    ...",
                                "code"
                            ],
                            [
                                "This conversion is zero cost if your input is already in Channels Last memory format. After it, all operators will work preserving ChannelsLast memory format.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "5. Android - Reusing tensors for forward": [
                            [
                                "This part of the recipe is Android only.",
                                "markdown"
                            ],
                            [
                                "Memory is a critical resource for android performance, especially on old devices.\nTensors can need a significant amount of memory.\nFor example, standard computer vision tensor contains 1*3*224*224 elements,\nassuming that data type is float and will need 588Kb of memory.",
                                "markdown"
                            ],
                            [
                                "FloatBuffer buffer = Tensor.allocateFloatBuffer(1*3*224*224);\nTensor tensor = Tensor.fromBlob(buffer, new long[]{1, 3, 224, 224});",
                                "code"
                            ],
                            [
                                "Here we allocate native memory as java.nio.FloatBuffer and creating org.pytorch.Tensor which storage will be pointing to the memory of the allocated buffer.",
                                "markdown"
                            ],
                            [
                                "For most of the use cases, we do not do model forward only once, repeating it with some frequency or as fast as possible.",
                                "markdown"
                            ],
                            [
                                "If we are doing new memory allocation for every module forward - that will be suboptimal.\nInstead of this, we can reuse the same memory that we allocated on the previous step, fill it with new data, and run module forward again on the same tensor object.",
                                "markdown"
                            ],
                            [
                                "You can check how it looks in code in .",
                                "markdown"
                            ],
                            [
                                "protected AnalysisResult analyzeImage(ImageProxy image, int rotationDegrees) {\n  if (mModule == null) {\n    mModule = Module.load(moduleFileAbsoluteFilePath);\n    mInputTensorBuffer =\n    Tensor.allocateFloatBuffer(3 * 224 * 224);\n    mInputTensor = Tensor.fromBlob(mInputTensorBuffer, new long[]{1, 3, 224, 224});\n  }\n\n  TensorImageUtils.imageYUV420CenterCropToFloatBuffer(\n      image.getImage(), rotationDegrees,\n      224, 224,\n      TensorImageUtils.TORCHVISION_NORM_MEAN_RGB,\n      TensorImageUtils.TORCHVISION_NORM_STD_RGB,\n      mInputTensorBuffer, 0);\n\n  Tensor outputTensor = mModule.forward(IValue.from(mInputTensor)).toTensor();\n}",
                                "code"
                            ],
                            [
                                "Member fields mModule, mInputTensorBuffer and mInputTensor are initialized only once\nand buffer is refilled using org.pytorch.torchvision.TensorImageUtils.imageYUV420CenterCropToFloatBuffer.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Benchmarking": [
                    [
                        "The best way to benchmark (to check if optimizations helped your use case) - is to measure your particular use case that you want to optimize, as performance behavior can vary in different environments.",
                        "markdown"
                    ],
                    [
                        "PyTorch distribution provides a way to benchmark naked binary that runs the model forward,\nthis approach can give more stable measurements rather than testing inside the application.",
                        "markdown"
                    ],
                    {
                        "Android - Benchmarking Setup": [
                            [
                                "This part of the recipe is Android only.",
                                "markdown"
                            ],
                            [
                                "For this you first need to build benchmark binary:",
                                "markdown"
                            ],
                            [
                                "&lt;from-your-root-pytorch-dir&gt;\nrm -rf build_android\nBUILD_PYTORCH_MOBILE=1 ANDROID_ABI=arm64-v8a ./scripts/build_android.sh -DBUILD_BINARY=ON",
                                "code"
                            ],
                            [
                                "You should have arm64 binary at: build_android/bin/speed_benchmark_torch.\nThis binary takes --model=&lt;path-to-model&gt;, --input_dim=\"1,3,224,224\" as dimension information for the input and --input_type=\"float\" as the type of the input as arguments.",
                                "markdown"
                            ],
                            [
                                "Once you have your android device connected,\npush speedbenchark_torch binary and your model to the phone:",
                                "markdown"
                            ],
                            [
                                "adb push &lt;speedbenchmark-torch&gt; /data/local/tmp\nadb push &lt;path-to-scripted-model&gt; /data/local/tmp",
                                "code"
                            ],
                            [
                                "Now we are ready to benchmark your model:",
                                "markdown"
                            ],
                            [
                                "adb shell \"/data/local/tmp/speed_benchmark_torch --model=/data/local/tmp/model.pt\" --input_dims=\"1,3,224,224\" --input_type=\"float\"\n----- output -----\nStarting benchmark.\nRunning warmup runs.\nMain runs.\nMain run finished. Microseconds per iter: 121318. Iters per second: 8.24281",
                                "code"
                            ]
                        ]
                    },
                    {
                        "iOS - Benchmarking Setup": [
                            [
                                "For iOS, we\u2019ll be using our  as the benchmarking tool.",
                                "markdown"
                            ],
                            [
                                "To begin with, let\u2019s apply the optimize_for_mobile method to our python script located at . Simply modify the code as below.",
                                "markdown"
                            ],
                            [
                                "import torch\nimport torchvision\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nmodel = torchvision.models.mobilenet_v2(pretrained=True)\nmodel.eval()\nexample = torch.rand(1, 3, 224, 224)\ntraced_script_module = torch.jit.trace(model, example)\ntorchscript_model_optimized = optimize_for_mobile(traced_script_module)\ntorch.jit.save(torchscript_model_optimized, \"model.pt\")",
                                "code"
                            ],
                            [
                                "Now let\u2019s run python trace_model.py. If everything works well, we should be able to generate our optimized model in the benchmark directory.",
                                "markdown"
                            ],
                            [
                                "Next, we\u2019re going to build the PyTorch libraries from source.",
                                "markdown"
                            ],
                            [
                                "BUILD_PYTORCH_MOBILE=1 IOS_ARCH=arm64 ./scripts/build_ios.sh",
                                "code"
                            ],
                            [
                                "Now that we have the optimized model and PyTorch ready, it\u2019s time to generate our XCode project and do benchmarking. To do that, we\u2019ll be using a ruby script - <cite>setup.rb</cite> which does the heavy lifting jobs of setting up the XCode project.",
                                "markdown"
                            ],
                            [
                                "ruby setup.rb",
                                "code"
                            ],
                            [
                                "Now open the <cite>TestApp.xcodeproj</cite> and plug in your iPhone, you\u2019re ready to go. Below is an example result from iPhoneX",
                                "markdown"
                            ],
                            [
                                "TestApp[2121:722447] Main runs\nTestApp[2121:722447] Main run finished. Milliseconds per iter: 28.767\nTestApp[2121:722447] Iters per second: : 34.762\nTestApp[2121:722447] Done.",
                                "code"
                            ]
                        ]
                    }
                ]
            }
        ],
        "Automatic Mixed Precision": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                " provides convenience methods for mixed precision,\nwhere some operations use the torch.float32 (float) datatype and other operations\nuse torch.float16 (half). Some ops, like linear layers and convolutions,\nare much faster in float16 or bfloat16. Other ops, like reductions, often require the dynamic\nrange of float32.  Mixed precision tries to match each op to its appropriate datatype,\nwhich can reduce your network\u2019s runtime and memory footprint.",
                "markdown"
            ],
            [
                "Ordinarily, \u201cautomatic mixed precision training\u201d uses  and\n together.",
                "markdown"
            ],
            [
                "This recipe measures the performance of a simple network in default precision,\nthen walks through adding autocast and GradScaler to run the same network in\nmixed precision with improved performance.",
                "markdown"
            ],
            [
                "You may download and run this recipe as a standalone Python script.\nThe only requirements are PyTorch 1.6 or later and a CUDA-capable GPU.",
                "markdown"
            ],
            [
                "Mixed precision primarily benefits Tensor Core-enabled architectures (Volta, Turing, Ampere).\nThis recipe should show significant (2-3X) speedup on those architectures.\nOn earlier architectures (Kepler, Maxwell, Pascal), you may observe a modest speedup.\nRun nvidia-smi to display your GPU\u2019s architecture.",
                "markdown"
            ],
            [
                "import torch, time, gc\n\n# Timing utilities\nstart_time = None\n\ndef start_timer():\n    global start_time\n    gc.collect()\n    ()\n    ()\n    ()\n    start_time = time.time()\n\ndef end_timer_and_print(local_msg):\n    ()\n    end_time = time.time()\n    print(\"\\n\" + local_msg)\n    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n    print(\"Max memory used by tensors = {} bytes\".format(()))",
                "code"
            ],
            {
                "A simple network": [
                    [
                        "The following sequence of linear layers and ReLUs should show a speedup with mixed precision.",
                        "markdown"
                    ],
                    [
                        "def make_model(in_size, out_size, num_layers):\n    layers = []\n    for _ in range(num_layers - 1):\n        layers.append((in_size, in_size))\n        layers.append(())\n    layers.append((in_size, out_size))\n    return (*tuple(layers)).cuda()",
                        "code"
                    ],
                    [
                        "batch_size, in_size, out_size, and num_layers are chosen to be large enough to saturate the GPU with work.\nTypically, mixed precision provides the greatest speedup when the GPU is saturated.\nSmall networks may be CPU bound, in which case mixed precision won\u2019t improve performance.\nSizes are also chosen such that linear layers\u2019 participating dimensions are multiples of 8,\nto permit Tensor Core usage on Tensor Core-capable GPUs (see  below).",
                        "markdown"
                    ],
                    [
                        "Exercise: Vary participating sizes and see how the mixed precision speedup changes.",
                        "markdown"
                    ],
                    [
                        "batch_size = 512 # Try, for example, 128, 256, 513.\nin_size = 4096\nout_size = 4096\nnum_layers = 3\nnum_batches = 50\nepochs = 3\n\n# Creates data in default precision.\n# The same data is used for both default and mixed precision trials below.\n# You don't need to manually change inputs' dtype when enabling mixed precision.\ndata = [(batch_size, in_size, device=\"cuda\") for _ in range(num_batches)]\ntargets = [(batch_size, out_size, device=\"cuda\") for _ in range(num_batches)]\n\nloss_fn = ().cuda()",
                        "code"
                    ]
                ]
            },
            {
                "Default Precision": [
                    [
                        "Without torch.cuda.amp, the following simple network executes all ops in default precision (torch.float32):",
                        "markdown"
                    ],
                    [
                        "net = make_model(in_size, out_size, num_layers)\nopt = (net.parameters(), lr=0.001)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        output = net(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\nend_timer_and_print(\"Default precision:\")",
                        "code"
                    ]
                ]
            },
            {
                "Adding autocast": [
                    [
                        "Instances of \nserve as context managers that allow regions of your script to run in mixed precision.",
                        "markdown"
                    ],
                    [
                        "In these regions, CUDA ops run in a dtype chosen by autocast\nto improve performance while maintaining accuracy.\nSee the \nfor details on what precision autocast chooses for each op, and under what circumstances.",
                        "markdown"
                    ],
                    [
                        "for epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        # Runs the forward pass under autocast.\n        with (device_type='cuda', dtype=torch.float16):\n            output = net(input)\n            # output is float16 because linear layers autocast to float16.\n            assert output.dtype is torch.float16\n\n            loss = loss_fn(output, target)\n            # loss is float32 because mse_loss layers autocast to float32.\n            assert loss.dtype is torch.float32\n\n        # Exits autocast before backward().\n        # Backward passes under autocast are not recommended.\n        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n        loss.backward()\n        opt.step()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance",
                        "code"
                    ]
                ]
            },
            {
                "Adding GradScaler": [
                    [
                        "helps prevent gradients with small magnitudes from flushing to zero\n(\u201cunderflowing\u201d) when training with mixed precision.",
                        "markdown"
                    ],
                    [
                        "performs the steps of gradient scaling conveniently.",
                        "markdown"
                    ],
                    [
                        "# Constructs scaler once, at the beginning of the convergence run, using default args.\n# If your network fails to converge with default GradScaler args, please file an issue.\n# The same GradScaler instance should be used for the entire convergence run.\n# If you perform multiple convergence runs in the same script, each run should use\n# a dedicated fresh GradScaler instance.  GradScaler instances are lightweight.\nscaler = ()\n\nfor epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        with (device_type='cuda', dtype=torch.float16):\n            output = net(input)\n            loss = loss_fn(output, target)\n\n        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n        scaler.scale(loss).backward()\n\n        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n        # otherwise, optimizer.step() is skipped.\n        scaler.step(opt)\n\n        # Updates the scale for next iteration.\n        scaler.update()\n\n        opt.zero_grad() # set_to_none=True here can modestly improve performance",
                        "code"
                    ]
                ]
            },
            {
                "All together: \u201cAutomatic Mixed Precision\u201d": [
                    [
                        "(The following also demonstrates enabled, an optional convenience argument to autocast and GradScaler.\nIf False, autocast and GradScaler\u2018s calls become no-ops.\nThis allows switching between default precision and mixed precision without if/else statements.)",
                        "markdown"
                    ],
                    [
                        "use_amp = True\n\nnet = make_model(in_size, out_size, num_layers)\nopt = (net.parameters(), lr=0.001)\nscaler = (enabled=use_amp)\n\nstart_timer()\nfor epoch in range(epochs):\n    for input, target in zip(data, targets):\n        with (device_type='cuda', dtype=torch.float16, enabled=use_amp):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance\nend_timer_and_print(\"Mixed precision:\")",
                        "code"
                    ]
                ]
            },
            {
                "Inspecting/modifying gradients (e.g., clipping)": [
                    [
                        "All gradients produced by scaler.scale(loss).backward() are scaled.  If you wish to modify or inspect\nthe parameters\u2019 .grad attributes between backward() and scaler.step(optimizer), you should\nunscale them first using .",
                        "markdown"
                    ],
                    [
                        "for epoch in range(0): # 0 epochs, this section is for illustration only\n    for input, target in zip(data, targets):\n        with (device_type='cuda', dtype=torch.float16):\n            output = net(input)\n            loss = loss_fn(output, target)\n        scaler.scale(loss).backward()\n\n        # Unscales the gradients of optimizer's assigned params in-place\n        scaler.unscale_(opt)\n\n        # Since the gradients of optimizer's assigned params are now unscaled, clips as usual.\n        # You may use the same value for max_norm here as you would without gradient scaling.\n        (net.parameters(), max_norm=0.1)\n\n        scaler.step(opt)\n        scaler.update()\n        opt.zero_grad() # set_to_none=True here can modestly improve performance",
                        "code"
                    ]
                ]
            },
            {
                "Saving/Resuming": [
                    [
                        "To save/resume Amp-enabled runs with bitwise accuracy, use\n and\n.",
                        "markdown"
                    ],
                    [
                        "When saving, save the scaler state dict alongside the usual model and optimizer state dicts.\nDo this either at the beginning of an iteration before any forward passes, or at the end of\nan iteration after scaler.update().",
                        "markdown"
                    ],
                    [
                        "checkpoint = {\"model\": net.state_dict(),\n              \"optimizer\": opt.state_dict(),\n              \"scaler\": scaler.state_dict()}\n# Write checkpoint as desired, e.g.,\n# torch.save(checkpoint, \"filename\")",
                        "code"
                    ],
                    [
                        "When resuming, load the scaler state dict alongside the model and optimizer state dicts.",
                        "markdown"
                    ],
                    [
                        "# Read checkpoint as desired, e.g.,\n# dev = torch.cuda.current_device()\n# checkpoint = torch.load(\"filename\",\n#                         map_location = lambda storage, loc: storage.cuda(dev))\nnet.load_state_dict(checkpoint[\"model\"])\nopt.load_state_dict(checkpoint[\"optimizer\"])\nscaler.load_state_dict(checkpoint[\"scaler\"])",
                        "code"
                    ],
                    [
                        "If a checkpoint was created from a run <em>without</em> Amp, and you want to resume training <em>with</em> Amp,\nload model and optimizer states from the checkpoint as usual.  The checkpoint won\u2019t contain a saved scaler state, so\nuse a fresh instance of GradScaler.",
                        "markdown"
                    ],
                    [
                        "If a checkpoint was created from a run <em>with</em> Amp and you want to resume training <em>without</em> Amp,\nload model and optimizer states from the checkpoint as usual, and ignore the saved scaler state.",
                        "markdown"
                    ]
                ]
            },
            {
                "Inference/Evaluation": [
                    [
                        "autocast may be used by itself to wrap inference or evaluation forward passes. GradScaler is not necessary.",
                        "markdown"
                    ]
                ]
            },
            {
                "Advanced topics": [
                    [
                        "See the  for advanced use cases including:",
                        "markdown"
                    ],
                    [
                        "Gradient accumulation",
                        "markdown"
                    ],
                    [
                        "Gradient penalty/double backward",
                        "markdown"
                    ],
                    [
                        "Networks with multiple models, optimizers, or losses",
                        "markdown"
                    ],
                    [
                        "Multiple GPUs (torch.nn.DataParallel or torch.nn.parallel.DistributedDataParallel)",
                        "markdown"
                    ],
                    [
                        "Custom autograd functions (subclasses of torch.autograd.Function)",
                        "markdown"
                    ],
                    [
                        "If you perform multiple convergence runs in the same script, each run should use\na dedicated fresh GradScaler instance.  GradScaler instances are lightweight.",
                        "markdown"
                    ],
                    [
                        "If you\u2019re registering a custom C++ op with the dispatcher, see the\n\nof the dispatcher tutorial.",
                        "markdown"
                    ]
                ]
            },
            {
                "Troubleshooting": [
                    {
                        "Speedup with Amp is minor": [
                            [
                                "Your network may fail to saturate the GPU(s) with work, and is therefore CPU bound. Amp\u2019s effect on GPU performance\nwon\u2019t matter.",
                                "markdown"
                            ],
                            [
                                "A rough rule of thumb to saturate the GPU is to increase batch and/or network size(s)\nas much as you can without running OOM.",
                                "markdown"
                            ],
                            [
                                "Try to avoid excessive CPU-GPU synchronization (.item() calls, or printing values from CUDA tensors).",
                                "markdown"
                            ],
                            [
                                "Try to avoid sequences of many small CUDA ops (coalesce these into a few large CUDA ops if you can).",
                                "markdown"
                            ],
                            [
                                "Your network may be GPU compute bound (lots of matmuls/convolutions) but your GPU does not have Tensor Cores.\nIn this case a reduced speedup is expected.",
                                "markdown"
                            ],
                            [
                                "Matmul dimensions are not Tensor Core-friendly.  Make sure matmuls\u2019 participating sizes are multiples of 8.\n(For NLP models with encoders/decoders, this can be subtle.  Also, convolutions used to have similar size constraints\nfor Tensor Core use, but for CuDNN versions 7.3 and later, no such constraints exist.  See\n for guidance.)",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Loss is inf/NaN": [
                            [
                                "First, check if your network fits an .\nSee also .",
                                "markdown"
                            ],
                            [
                                "If you\u2019re confident your Amp usage is correct, you may need to file an issue, but before doing so, it\u2019s helpful to gather the following information:",
                                "markdown"
                            ],
                            [
                                "Disable autocast or GradScaler individually (by passing enabled=False to their constructor) and see if infs/NaNs persist.",
                                "markdown"
                            ],
                            [
                                "If you suspect part of your network (e.g., a complicated loss function) overflows , run that forward region in float32\nand see if infs/NaNs persist.\n\u2019s last code snippet\nshows forcing a subregion to run in float32 (by locally disabling autocast and casting the subregion\u2019s inputs).",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Type mismatch error (may manifest as CUDNN_STATUS_BAD_PARAM)": [
                            [
                                "Autocast tries to cover all ops that benefit from or require casting.\n\nare chosen based on numerical properties, but also on experience.\nIf you see a type mismatch error in an autocast-enabled forward region or a backward pass following that region,\nit\u2019s possible autocast missed an op.",
                                "markdown"
                            ],
                            [
                                "Please file an issue with the error backtrace.  export TORCH_SHOW_CPP_STACKTRACES=1 before running your script to provide\nfine-grained information on which backend op is failing.",
                                "markdown"
                            ],
                            [
                                "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ]
                        ]
                    }
                ]
            }
        ],
        "Changing default device": [
            [
                "It is common practice to write PyTorch code in a device-agnostic way,\nand then switch between CPU and CUDA depending on what hardware is available.\nTypically, to do this you might have used if-statements and cuda() calls\nto do this:",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                "This recipe requires PyTorch 2.0.0 or later.",
                "markdown"
            ],
            [
                "import torch\n\nUSE_CUDA = False\n\n = (20, 30)\nif USE_CUDA:\n    ()\n\ndevice = 'cpu'\nif USE_CUDA:\n    device = 'cuda'\n = (128, 20, device=device)\nprint(().device)",
                "code"
            ],
            [
                "cpu",
                "code"
            ],
            [
                "PyTorch now also has a context manager which can take care of the\ndevice transfer automatically. Here is an example:",
                "markdown"
            ],
            [
                "with ('cuda'):\n     = (20, 30)\n    print()\n    print(((128, 20)).device)",
                "code"
            ],
            [
                "cuda:0\ncuda:0",
                "code"
            ],
            [
                "You can also set it globally like this:",
                "markdown"
            ],
            [
                "('cuda')\n\n = (20, 30)\nprint()\nprint(((128, 20)).device)",
                "code"
            ],
            [
                "cuda:0\ncuda:0",
                "code"
            ],
            [
                "This function imposes a slight performance cost on every Python\ncall to the torch API (not just factory functions). If this\nis causing problems for you, please comment on",
                "markdown"
            ],
            [
                "<strong>Total running time of the script:</strong> ( 0 minutes  1.718 seconds)",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ]
        ],
        "How to use TensorBoard with PyTorch": [
            [
                "TensorBoard is a visualization toolkit for machine learning experimentation.\nTensorBoard allows tracking and visualizing metrics such as loss and accuracy,\nvisualizing the model graph, viewing histograms, displaying images and much more.\nIn this tutorial we are going to cover TensorBoard installation,\nbasic usage with PyTorch, and how to visualize data you logged in TensorBoard UI.",
                "markdown"
            ],
            {
                "Installation": [
                    [
                        "PyTorch should be installed to log models and metrics into TensorBoard log\ndirectory. The following command will install PyTorch 1.4+ via\nAnaconda (recommended):",
                        "markdown"
                    ],
                    [
                        "$ conda install pytorch torchvision -c pytorch",
                        "code"
                    ],
                    [
                        "or pip",
                        "markdown"
                    ],
                    [
                        "$ pip install torch torchvision",
                        "code"
                    ]
                ]
            },
            {
                "Using TensorBoard in PyTorch": [
                    [
                        "Let\u2019s now try using TensorBoard with PyTorch! Before logging anything,\nwe need to create a SummaryWriter instance.",
                        "markdown"
                    ],
                    [
                        "import torch\nfrom torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()",
                        "code"
                    ],
                    [
                        "Writer will output to ./runs/ directory by default.",
                        "markdown"
                    ]
                ]
            },
            {
                "Log scalars": [
                    [
                        "In machine learning, it\u2019s important to understand key metrics such as\nloss and how they change during training. Scalar helps to save\nthe loss value of each training step, or the accuracy after each epoch.",
                        "markdown"
                    ],
                    [
                        "To log a scalar value, use\nadd_scalar(tag, scalar_value, global_step=None, walltime=None).\nFor example, lets create a simple linear regression training, and\nlog loss value using add_scalar",
                        "markdown"
                    ],
                    [
                        "x = (-5, 5, 0.1).view(-1, 1)\ny = -5 * x + 0.1 * (x.size())\n\nmodel = (1, 1)\ncriterion = ()\noptimizer = (model.parameters(), lr = 0.1)\n\ndef train_model(iter):\n    for epoch in range(iter):\n        y1 = model(x)\n        loss = criterion(y1, y)\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ntrain_model(10)\nwriter.flush()",
                        "code"
                    ],
                    [
                        "Call flush() method to make sure that all pending events\nhave been written to disk.",
                        "markdown"
                    ],
                    [
                        "See \nto find more TensorBoard visualization types you can log.",
                        "markdown"
                    ],
                    [
                        "If you do not need the summary writer anymore, call close() method.",
                        "markdown"
                    ],
                    [
                        "writer.close()",
                        "code"
                    ]
                ]
            },
            {
                "Run TensorBoard": [
                    [
                        "Install TensorBoard through the command line to visualize data you logged",
                        "markdown"
                    ],
                    [
                        "$ pip install tensorboard",
                        "code"
                    ],
                    [
                        "Now, start TensorBoard, specifying the root log directory you used above.\nArgument logdir points to directory where TensorBoard will look to find\nevent files that it can display. TensorBoard will recursively walk\nthe directory structure rooted at logdir, looking for .*tfevents.* files.",
                        "markdown"
                    ],
                    [
                        "$ tensorboard --logdir=runs",
                        "code"
                    ],
                    [
                        "Go to the URL it provides OR to ",
                        "markdown"
                    ],
                    [
                        "This dashboard shows how the loss and accuracy change with every epoch.\nYou can use it to also track training speed, learning rate, and other\nscalar values. It\u2019s helpful to compare these metrics across different\ntraining runs to improve your model.",
                        "markdown"
                    ]
                ]
            },
            {
                "Share TensorBoard dashboards": [
                    [
                        " lets you upload and share\nyour ML experiment results with anyone. Use TensorBoard.dev to host,\ntrack, and share your TensorBoard dashboards.",
                        "markdown"
                    ],
                    [
                        "Install the latest version of TensorBoard to use the uploader.",
                        "markdown"
                    ],
                    [
                        "$ pip install tensorboard --upgrade",
                        "code"
                    ],
                    [
                        "Use a simple command to upload and share your TensorBoard.",
                        "markdown"
                    ],
                    [
                        "$ tensorboard dev upload --logdir runs \\\n--name \"My latest experiment\" \\ # optional\n--description \"Simple comparison of several hyperparameters\" # optional",
                        "code"
                    ],
                    [
                        "For help, run $ tensorboard dev --help.",
                        "markdown"
                    ],
                    [
                        "<strong>Note:</strong> Uploaded TensorBoards are public and visible to everyone.\nDo not upload sensitive data.",
                        "markdown"
                    ],
                    [
                        "View your TensorBoard live at URL provided in your terminal.\nE.g. ",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "TensorBoard.dev currently supports scalars, graphs, histograms, distributions, hparams, and text dashboards.",
                        "markdown"
                    ]
                ]
            },
            {
                "Learn More": [
                    [
                        " docs",
                        "markdown"
                    ],
                    [
                        " tutorial",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Performance Tuning Guide": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "Performance Tuning Guide is a set of optimizations and best practices which can\naccelerate training and inference of deep learning models in PyTorch. Presented\ntechniques often can be implemented by changing only a few lines of code and can\nbe applied to a wide range of deep learning models across all domains.",
                "markdown"
            ],
            {
                "General optimizations": [
                    {
                        "Enable async data loading and augmentation": [
                            [
                                "supports asynchronous data loading and data augmentation in separate worker\nsubprocesses. The default setting for DataLoader is num_workers=0,\nwhich means that the data loading is synchronous and done in the main process.\nAs a result the main training process has to wait for the data to be available\nto continue the execution.",
                                "markdown"
                            ],
                            [
                                "Setting num_workers &gt; 0 enables asynchronous data loading and overlap\nbetween the training and data loading. num_workers should be tuned\ndepending on the workload, CPU, GPU, and location of training data.",
                                "markdown"
                            ],
                            [
                                "DataLoader accepts pin_memory argument, which defaults to False.\nWhen using a GPU it\u2019s better to set pin_memory=True, this instructs\nDataLoader to use pinned memory and enables faster and asynchronous memory\ncopy from the host to the GPU.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Disable gradient calculation for validation or inference": [
                            [
                                "PyTorch saves intermediate buffers from all operations which involve tensors\nthat require gradients. Typically gradients aren\u2019t needed for validation or\ninference.\n\ncontext manager can be applied to disable gradient calculation within a\nspecified block of code, this accelerates execution and reduces the amount of\nrequired memory.\n\ncan also be used as a function decorator.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Disable bias for convolutions directly followed by a batch norm": [
                            [
                                "has bias parameter which defaults to True (the same is true for\n\nand\n\n).",
                                "markdown"
                            ],
                            [
                                "If a nn.Conv2d layer is directly followed by a nn.BatchNorm2d layer,\nthen the bias in the convolution is not needed, instead use\nnn.Conv2d(..., bias=False, ....). Bias is not needed because in the first\nstep BatchNorm subtracts the mean, which effectively cancels out the\neffect of bias.",
                                "markdown"
                            ],
                            [
                                "This is also applicable to 1d and 3d convolutions as long as BatchNorm (or\nother normalization layer) normalizes on the same dimension as convolution\u2019s\nbias.",
                                "markdown"
                            ],
                            [
                                "Models available from \nalready implement this optimization.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Use parameter.grad = None instead of model.zero_grad() or optimizer.zero_grad()": [
                            [
                                "Instead of calling:",
                                "markdown"
                            ],
                            [
                                "model.zero_grad()\n# or\noptimizer.zero_grad()",
                                "code"
                            ],
                            [
                                "to zero out gradients, use the following method instead:",
                                "markdown"
                            ],
                            [
                                "for param in model.parameters():\n    param.grad = None",
                                "code"
                            ],
                            [
                                "The second code snippet does not zero the memory of each individual parameter,\nalso the subsequent backward pass uses assignment instead of addition to store\ngradients, this reduces the number of memory operations.",
                                "markdown"
                            ],
                            [
                                "Setting gradient to None has a slightly different numerical behavior than\nsetting it to zero, for more details refer to the\n.",
                                "markdown"
                            ],
                            [
                                "Alternatively, starting from PyTorch 1.7, call model or\noptimizer.zero_grad(set_to_none=True).",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Fuse pointwise operations": [
                            [
                                "Pointwise operations (elementwise addition, multiplication, math functions -\nsin(), cos(), sigmoid() etc.) can be fused into a single kernel\nto amortize memory access time and kernel launch time.",
                                "markdown"
                            ],
                            [
                                " can fuse kernels\nautomatically, although there could be additional fusion opportunities not yet\nimplemented in the compiler, and not all device types are supported equally.",
                                "markdown"
                            ],
                            [
                                "Pointwise operations are memory-bound, for each operation PyTorch launches a\nseparate kernel. Each kernel loads data from the memory, performs computation\n(this step is usually inexpensive) and stores results back into the memory.",
                                "markdown"
                            ],
                            [
                                "Fused operator launches only one kernel for multiple fused pointwise ops and\nloads/stores data only once to the memory. This makes JIT very useful for\nactivation functions, optimizers, custom RNN cells etc.",
                                "markdown"
                            ],
                            [
                                "In the simplest case fusion can be enabled by applying\n\ndecorator to the function definition, for example:",
                                "markdown"
                            ],
                            [
                                "@torch.jit.script\ndef fused_gelu(x):\n    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))",
                                "code"
                            ],
                            [
                                "Refer to\n\nfor more advanced use cases.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Enable channels_last memory format for computer vision models": [
                            [
                                "PyTorch 1.5 introduced support for channels_last memory format for\nconvolutional networks. This format is meant to be used in conjunction with\n to further accelerate\nconvolutional neural networks with\n.",
                                "markdown"
                            ],
                            [
                                "Support for channels_last is experimental, but it\u2019s expected to work for\nstandard computer vision models (e.g. ResNet-50, SSD). To convert models to\nchannels_last format follow\n.\nThe tutorial includes a section on\n.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Checkpoint intermediate buffers": [
                            [
                                "Buffer checkpointing is a technique to mitigate the memory capacity burden of\nmodel training. Instead of storing inputs of all layers to compute upstream\ngradients in backward propagation, it stores the inputs of a few layers and\nthe others are recomputed during backward pass. The reduced memory\nrequirements enables increasing the batch size that can improve utilization.",
                                "markdown"
                            ],
                            [
                                "Checkpointing targets should be selected carefully. The best is not to store\nlarge layer outputs that have small re-computation cost. The example target\nlayers are activation functions (e.g. ReLU, Sigmoid, Tanh),\nup/down sampling and matrix-vector operations with small accumulation depth.",
                                "markdown"
                            ],
                            [
                                "PyTorch supports a native\n\nAPI to automatically perform checkpointing and recomputation.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Disable debugging APIs": [
                            [
                                "Many PyTorch APIs are intended for debugging and should be disabled for\nregular training runs:",
                                "markdown"
                            ],
                            [
                                "anomaly detection:\n\nor",
                                "markdown"
                            ],
                            [
                                "profiler related:\n,",
                                "markdown"
                            ],
                            [
                                "autograd gradcheck:\n\nor",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "CPU specific optimizations": [
                    {
                        "Utilize Non-Uniform Memory Access (NUMA) Controls": [
                            [
                                "NUMA or non-uniform memory access is a memory layout design used in data center machines meant to take advantage of locality of memory in multi-socket machines with multiple memory controllers and blocks. Generally speaking, all deep learning workloads, training or inference, get better performance without accessing hardware resources across NUMA nodes. Thus, inference can be run with multiple instances, each instance runs on one socket, to raise throughput. For training tasks on single node, distributed training is recommended to make each training process run on one socket.",
                                "markdown"
                            ],
                            [
                                "In general cases the following command executes a PyTorch script on cores on the Nth node only, and avoids cross-socket memory access to reduce memory access overhead.",
                                "markdown"
                            ],
                            [
                                "# numactl --cpunodebind=N --membind=N python &lt;pytorch_script&gt;",
                                "code"
                            ],
                            [
                                "More detailed descriptions can be found .",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Utilize OpenMP": [
                            [
                                "OpenMP is utilized to bring better performance for parallel computation tasks.\nOMP_NUM_THREADS is the easiest switch that can be used to accelerate computations. It determines number of threads used for OpenMP computations.\nCPU affinity setting controls how workloads are distributed over multiple cores. It affects communication overhead, cache line invalidation overhead, or page thrashing, thus proper setting of CPU affinity brings performance benefits. GOMP_CPU_AFFINITY or KMP_AFFINITY determines how to bind OpenMP* threads to physical processing units. Detailed information can be found .",
                                "markdown"
                            ],
                            [
                                "With the following command, PyTorch run the task on N OpenMP threads.",
                                "markdown"
                            ],
                            [
                                "# export OMP_NUM_THREADS=N",
                                "code"
                            ],
                            [
                                "Typically, the following environment variables are used to set for CPU affinity with GNU OpenMP implementation. OMP_PROC_BIND specifies whether threads may be moved between processors. Setting it to CLOSE keeps OpenMP threads close to the primary thread in contiguous place partitions. OMP_SCHEDULE determines how OpenMP threads are scheduled. GOMP_CPU_AFFINITY binds threads to specific CPUs.",
                                "markdown"
                            ],
                            [
                                "# export OMP_SCHEDULE=STATIC\n# export OMP_PROC_BIND=CLOSE\n# export GOMP_CPU_AFFINITY=\"N-M\"",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Intel OpenMP Runtime Library (libiomp)": [
                            [
                                "By default, PyTorch uses GNU OpenMP (GNU libgomp) for parallel computation. On Intel platforms, Intel OpenMP Runtime Library (libiomp) provides OpenMP API specification support. It sometimes brings more performance benefits compared to libgomp. Utilizing environment variable LD_PRELOAD can switch OpenMP library to libiomp:",
                                "markdown"
                            ],
                            [
                                "# export LD_PRELOAD=&lt;path&gt;/libiomp5.so:$LD_PRELOAD",
                                "code"
                            ],
                            [
                                "Similar to CPU affinity settings in GNU OpenMP, environment variables are provided in libiomp to control CPU affinity settings.\nKMP_AFFINITY binds OpenMP threads to physical processing units. KMP_BLOCKTIME sets the time, in milliseconds, that a thread should wait, after completing the execution of a parallel region, before sleeping. In most cases, setting KMP_BLOCKTIME to 1 or 0 yields good performances.\nThe following commands show a common settings with Intel OpenMP Runtime Library.",
                                "markdown"
                            ],
                            [
                                "# export KMP_AFFINITY=granularity=fine,compact,1,0\n# export KMP_BLOCKTIME=1",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Switch Memory allocator": [
                            [
                                "For deep learning workloads, Jemalloc or TCMalloc can get better performance by reusing memory as much as possible than default malloc funtion.  is a general purpose malloc implementation that emphasizes fragmentation avoidance and scalable concurrency support.  also features a couple of optimizations to speed up program executions. One of them is holding memory in caches to speed up access of commonly-used objects. Holding such caches even after deallocation also helps avoid costly system calls if such memory is later re-allocated.\nUse environment variable LD_PRELOAD to take advantage of one of them.",
                                "markdown"
                            ],
                            [
                                "# export LD_PRELOAD=&lt;jemalloc.so/tcmalloc.so&gt;:$LD_PRELOAD",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Use oneDNN Graph with TorchScript for inference": [
                            [
                                "oneDNN Graph can significantly boost inference performance. It fuses some compute-intensive operations such as convolution, matmul with their neighbor operations.\nIn PyTorch 2.0, it is supported as a beta feature for Float32 &amp; BFloat16 data-types.\noneDNN Graph receives the model\u2019s graph and identifies candidates for operator-fusion with respect to the shape of the example input.\nA model should be JIT-traced using an example input.\nSpeed-up would then be observed after a couple of warm-up iterations for inputs with the same shape as the example input.\nThe example code-snippets below are for resnet50, but they can very well be extended to use oneDNN Graph with custom models as well.",
                                "markdown"
                            ],
                            [
                                "# Only this extra line of code is required to use oneDNN Graph\ntorch.jit.enable_onednn_fusion(True)",
                                "code"
                            ],
                            [
                                "Using the oneDNN Graph API requires just one extra line of code for inference with Float32.\nIf you are using oneDNN Graph, please avoid calling torch.jit.optimize_for_inference.",
                                "markdown"
                            ],
                            [
                                "# sample input should be of the same shape as expected inputs\nsample_input = [torch.rand(32, 3, 224, 224)]\n# Using resnet50 from TorchVision in this example for illustrative purposes,\n# but the line below can indeed be modified to use custom models as well.\nmodel = getattr(torchvision.models, \"resnet50\")().eval()\n# Tracing the model with example input\ntraced_model = torch.jit.trace(model, sample_input)\n# Invoking torch.jit.freeze\ntraced_model = torch.jit.freeze(traced_model)",
                                "code"
                            ],
                            [
                                "Once a model is JIT-traced with a sample input, it can then be used for inference after a couple of warm-up runs.",
                                "markdown"
                            ],
                            [
                                "with torch.no_grad():\n    # a couple of warmup runs\n    traced_model(*sample_input)\n    traced_model(*sample_input)\n    # speedup would be observed after warmup runs\n    traced_model(*sample_input)",
                                "code"
                            ],
                            [
                                "While the JIT fuser for oneDNN Graph also supports inference with BFloat16 datatype,\nperformance benefit with oneDNN Graph is only exhibited by machines with AVX512_BF16 ISA.\nThe following code snippets serves as an example of using BFloat16 datatype for inference with oneDNN Graph:",
                                "markdown"
                            ],
                            [
                                "# AMP for JIT mode is enabled by default, and is divergent with its eager mode counterpart\ntorch._C._jit_set_autocast_mode(False)\n\nwith torch.no_grad(), torch.cpu.amp.autocast(cache_enabled=False, dtype=torch.bfloat16):\n    model = torch.jit.trace(model, (example_input))\n    model = torch.jit.freeze(model)\n    # a couple of warmup runs\n    model(example_input)\n    model(example_input)\n    # speedup would be observed in subsequent runs.\n    model(example_input)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Train a model on CPU with PyTorch DistributedDataParallel(DDP) functionality": [
                            [
                                "For small scale models or memory-bound models, such as DLRM, training on CPU is also a good choice. On a machine with multiple sockets, distributed training brings a high-efficient hardware resource usage to accelerate the training process. , optimized with Intel(R) oneCCL (collective commnications library) for efficient distributed deep learning training implementing such collectives like allreduce, allgather, alltoall, implements PyTorch C10D ProcessGroup API and can be dynamically loaded as external ProcessGroup. Upon optimizations implemented in PyTorch DDP moduel, torhc-ccl accelerates communication operations. Beside the optimizations made to communication kernels, torch-ccl also features simultaneous computation-communication functionality.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "GPU specific optimizations": [
                    {
                        "Enable cuDNN auto-tuner": [
                            [
                                " supports many algorithms\nto compute a convolution. Autotuner runs a short benchmark and selects the\nkernel with the best performance on a given hardware for a given input size.",
                                "markdown"
                            ],
                            [
                                "For convolutional networks (other types currently not supported), enable cuDNN\nautotuner before launching the training loop by setting:",
                                "markdown"
                            ],
                            [
                                "torch.backends.cudnn.benchmark = True",
                                "code"
                            ],
                            [
                                "the auto-tuner decisions may be non-deterministic; different algorithm may\nbe selected for different runs.  For more details see",
                                "markdown"
                            ],
                            [
                                "in some rare cases, such as with highly variable input sizes,  it\u2019s better\nto run convolutional networks with autotuner disabled to avoid the overhead\nassociated with algorithm selection for each input size.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Avoid unnecessary CPU-GPU synchronization": [
                            [
                                "Avoid unnecessary synchronizations, to let the CPU run ahead of the\naccelerator as much as possible to make sure that the accelerator work queue\ncontains many operations.",
                                "markdown"
                            ],
                            [
                                "When possible, avoid operations which require synchronizations, for example:",
                                "markdown"
                            ],
                            [
                                "print(cuda_tensor)",
                                "markdown"
                            ],
                            [
                                "cuda_tensor.item()",
                                "markdown"
                            ],
                            [
                                "memory copies: tensor.cuda(),  cuda_tensor.cpu() and equivalent\ntensor.to(device) calls",
                                "markdown"
                            ],
                            [
                                "cuda_tensor.nonzero()",
                                "markdown"
                            ],
                            [
                                "python control flow which depends on results of operations performed on cuda\ntensors e.g. if (cuda_tensor != 0).all()",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Create tensors directly on the target device": [
                            [
                                "Instead of calling torch.rand(size).cuda() to generate a random tensor,\nproduce the output directly on the target device:\ntorch.rand(size, device=torch.device('cuda')).",
                                "markdown"
                            ],
                            [
                                "This is applicable to all functions which create new tensors and accept\ndevice argument:\n,\n,\n\nand similar.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Use mixed precision and AMP": [
                            [
                                "Mixed precision leverages\n\nand offers up to 3x overall speedup on Volta and newer GPU architectures. To\nuse Tensor Cores AMP should be enabled and matrix/tensor dimensions should\nsatisfy requirements for calling kernels that use Tensor Cores.",
                                "markdown"
                            ],
                            [
                                "To use Tensor Cores:",
                                "markdown"
                            ],
                            [
                                "set sizes to multiples of 8 (to map onto dimensions of Tensor Cores)",
                                "markdown"
                            ],
                            [
                                "see\n\nfor more details and guidelines specific to layer type",
                                "markdown"
                            ],
                            [
                                "if layer size is derived from other parameters rather than fixed, it can\nstill be explicitly padded e.g. vocabulary size in NLP models",
                                "markdown"
                            ],
                            [
                                "enable AMP",
                                "markdown"
                            ],
                            [
                                "Introduction to Mixed Precision Training and AMP:\n,",
                                "markdown"
                            ],
                            [
                                "native PyTorch AMP is available starting from PyTorch 1.6:\n,\n,",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Pre-allocate memory in case of variable input length": [
                            [
                                "Models for speech recognition or for NLP are often trained on input tensors\nwith variable sequence length. Variable length can be problematic for PyTorch\ncaching allocator and can lead to reduced performance or to unexpected\nout-of-memory errors. If a batch with a short sequence length is followed by\nan another batch with longer sequence length, then PyTorch is forced to\nrelease intermediate buffers from previous iteration and to re-allocate new\nbuffers. This process is time consuming and causes fragmentation in the\ncaching allocator which may result in out-of-memory errors.",
                                "markdown"
                            ],
                            [
                                "A typical solution is to implement pre-allocation. It consists of the\nfollowing steps:",
                                "markdown"
                            ],
                            [
                                "generate a (usually random) batch of inputs with maximum sequence length\n(either corresponding to max length in the training dataset or to some\npredefined threshold)",
                                "markdown"
                            ],
                            [
                                "execute a forward and a backward pass with the generated batch, do not\nexecute an optimizer or a learning rate scheduler, this step pre-allocates\nbuffers of maximum size, which can be reused in subsequent\ntraining iterations",
                                "markdown"
                            ],
                            [
                                "zero out gradients",
                                "markdown"
                            ],
                            [
                                "proceed to regular training",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Distributed optimizations": [
                    {
                        "Use efficient data-parallel backend": [
                            [
                                "PyTorch has two ways to implement data-parallel training:",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "DistributedDataParallel offers much better performance and scaling to\nmultiple-GPUs. For more information refer to the\n\nfrom PyTorch documentation.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Skip unnecessary all-reduce if training with DistributedDataParallel and gradient accumulation": [
                            [
                                "By default\n\nexecutes gradient all-reduce after every backward pass to compute the average\ngradient over all workers participating in the training. If training uses\ngradient accumulation over N steps, then all-reduce is not necessary after\nevery training step, it\u2019s only required to perform all-reduce after the last\ncall to backward, just before the execution of the optimizer.",
                                "markdown"
                            ],
                            [
                                "DistributedDataParallel provides\n\ncontext manager which disables gradient all-reduce for particular iteration.\nno_sync() should be applied to first N-1 iterations of gradient\naccumulation, the last iteration should follow the default execution and\nperform the required gradient all-reduce.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Match the order of layers in constructors and during the execution if using DistributedDataParallel(find_unused_parameters=True)": [
                            [
                                "with find_unused_parameters=True uses the order of layers and parameters\nfrom model constructors to build buckets for DistributedDataParallel\ngradient all-reduce. DistributedDataParallel overlaps all-reduce with the\nbackward pass. All-reduce for a particular bucket is asynchronously triggered\nonly when all gradients for parameters in a given bucket are available.",
                                "markdown"
                            ],
                            [
                                "To maximize the amount of overlap, the order in model constructors should\nroughly match the order during the execution. If the order doesn\u2019t match, then\nall-reduce for the entire bucket waits for the gradient which is the last to\narrive, this may reduce the overlap between backward pass and all-reduce,\nall-reduce may end up being exposed, which slows down the training.",
                                "markdown"
                            ],
                            [
                                "DistributedDataParallel with find_unused_parameters=False (which is\nthe default setting) relies on automatic bucket formation based on order of\noperations encountered during the backward pass. With\nfind_unused_parameters=False it\u2019s not necessary to reorder layers or\nparameters to achieve optimal performance.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Load-balance workload in a distributed setting": [
                            [
                                "Load imbalance typically may happen for models processing sequential data\n(speech recognition, translation, language models etc.). If one device\nreceives a batch of data with sequence length longer than sequence lengths for\nthe remaining devices, then all devices wait for the worker which finishes\nlast. Backward pass functions as an implicit synchronization point in a\ndistributed setting with\n\nbackend.",
                                "markdown"
                            ],
                            [
                                "There are multiple ways to solve the load balancing problem. The core idea is\nto distribute workload over all workers as uniformly as possible within each\nglobal batch. For example Transformer solves imbalance by forming batches with\napproximately constant number of tokens (and variable number of sequences in a\nbatch), other models solve imbalance by bucketing samples with similar\nsequence length or even by sorting dataset by sequence length.",
                                "markdown"
                            ],
                            [
                                "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ]
                        ]
                    }
                ]
            }
        ],
        "Timer quick start": [
            [
                "In this tutorial, we\u2019re going to cover the primary APIs of\n<cite>torch.utils.benchmark.Timer</cite>. The PyTorch Timer is based on the\n\nAPI, with several PyTorch specific modifications. Familiarity with the\nbuiltin <cite>Timer</cite> class is not required for this tutorial, however we assume\nthat the reader is familiar with the fundamentals of performance work.",
                "markdown"
            ],
            [
                "A more comprehensive performace tuning tutorial is available at:\n<blockquote>",
                "markdown"
            ],
            [
                "</blockquote>\n<dl class=\"simple\">\n<dt><strong>Contents:</strong></dt><dd>",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "</dd>\n</dl>",
                "markdown"
            ],
            {
                "1. Defining a Timer": [
                    [
                        "A <cite>Timer</cite> serves as a task definition.",
                        "markdown"
                    ],
                    [
                        "from torch.utils.benchmark import \n\ntimer = (\n    # The computation which will be run in a loop and timed.\n    stmt=\"x * y\",\n\n    # `setup` will be run before calling the measurement loop, and is used to\n    # populate any state which is needed by `stmt`\n    setup=\"\"\"\n        x = torch.ones((128,))\n        y = torch.ones((128,))\n    \"\"\",\n\n    # Alternately, `globals` can be used to pass variables from the outer scope.\n    # -------------------------------------------------------------------------\n    # globals={\n    #     \"x\": torch.ones((128,)),\n    #     \"y\": torch.ones((128,)),\n    # },\n\n    # Control the number of threads that PyTorch uses. (Default: 1)\n    num_threads=1,\n)",
                        "code"
                    ]
                ]
            },
            {
                "2. Wall time: <cite>Timer.blocked_autorange(\u2026)</cite>": [
                    [
                        "This method will handle details such as picking a suitable number if repeats,\nfixing the number of threads, and providing a convenient representation of\nthe results.",
                        "markdown"
                    ],
                    [
                        "# Measurement objects store the results of multiple repeats, and provide\n# various utility features.\nfrom torch.utils.benchmark import \n\nm:  = timer.blocked_autorange(min_run_time=1)\nprint(m)\n\n\n\n<strong>Snippet wall time.</strong>",
                        "code"
                    ],
                    [
                        "     &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f1929a38ed0&gt;\n     x * y\n     setup:\n       x = torch.ones((128,))\n       y = torch.ones((128,))\n\n       Median: 2.34 us\n       IQR:    0.07 us (2.31 to 2.38)\n       424 measurements, 1000 runs per measurement, 1 thread",
                        "code"
                    ]
                ]
            },
            {
                "3. C++ snippets": [
                    [
                        "from torch.utils.benchmark import Language\n\ncpp_timer = (\n    \"x * y;\",\n    \"\"\"\n        auto x = torch::ones({128});\n        auto y = torch::ones({128});\n    \"\"\",\n    language=Language.CPP,\n)\n\nprint(cpp_timer.blocked_autorange(min_run_time=1))\n\n\n\n<strong>C++ snippet wall time.</strong>",
                        "code"
                    ],
                    [
                        "     &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f192b019ed0&gt;\n     x * y;\n     setup:\n       auto x = torch::ones({128});\n       auto y = torch::ones({128});\n\n       Median: 1.21 us\n       IQR:    0.03 us (1.20 to 1.23)\n       83 measurements, 10000 runs per measurement, 1 thread",
                        "code"
                    ],
                    [
                        "Unsurprisingly, the C++ snippet is both faster and has lower variation.",
                        "markdown"
                    ]
                ]
            },
            {
                "4. Instruction counts: <cite>Timer.collect_callgrind(\u2026)</cite>": [
                    [
                        "For deep dive investigations, <cite>Timer.collect_callgrind</cite> wraps\n<cite>Callgrind &lt;https://valgrind.org/docs/manual/cl-manual.html&gt;</cite> in order to\ncollect instruction counts. These are useful as they offer fine grained and\ndeterministic (or very low noise in the case of Python) insights into how a\nsnippet is run.",
                        "markdown"
                    ],
                    [
                        "from torch.utils.benchmark import , FunctionCounts\n\nstats:  = cpp_timer.collect_callgrind()\nprint(stats)\n\n\n\n<strong>C++ Callgrind stats (summary)</strong>",
                        "code"
                    ],
                    [
                        "     &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7f1929a35850&gt;\n     x * y;\n     setup:\n       auto x = torch::ones({128});\n       auto y = torch::ones({128});\n\n                             All          Noisy symbols removed\n         Instructions:       563600                     563600\n         Baseline:                0                          0\n     100 runs per measurement, 1 thread",
                        "code"
                    ]
                ]
            },
            {
                "5. Instruction counts: Delving deeper": [
                    [
                        "The string representation of CallgrindStats is similar to that of\nMeasurement. <cite>Noisy symbols</cite> are a Python concept (removing calls in the\nCPython interpreter which are known to be noisy).",
                        "markdown"
                    ],
                    [
                        "For more detailed analysis, however, we will want to look at specific calls.\n<cite>CallgrindStats.stats()</cite> returns a FunctionCounts object to make this easier.\nConceptually, FunctionCounts can be thought of as a tuple of pairs with some\nutility methods, where each pair is <cite>(number of instructions, file path and\nfunction name)</cite>.\n<dl>\n<dt>A note on paths:</dt><dd>",
                        "markdown"
                    ],
                    [
                        "One generally doesn\u2019t care about absolute path. For instance, the full path\nand function name for a multiply call is something like:\n<blockquote>",
                        "markdown"
                    ],
                    [
                        "/the/prefix/to/your/pytorch/install/dir/pytorch/build/aten/src/ATen/core/TensorMethods.cpp:at::Tensor::mul(at::Tensor const&amp;) const [/the/path/to/your/conda/install/miniconda3/envs/ab_ref/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so]\n</blockquote>",
                        "markdown"
                    ],
                    [
                        "when in reality, all of the information that we\u2019re interested in can be\nrepresented in:\n<blockquote>",
                        "markdown"
                    ],
                    [
                        "build/aten/src/ATen/core/TensorMethods.cpp:at::Tensor::mul(at::Tensor const&amp;) const\n</blockquote>",
                        "markdown"
                    ],
                    [
                        "CallgrindStats.as_standardized() makes a best effort to strip low signal\nportions of the file path, as well as the shared object and is generally\nrecommended.\n</dd>\n</dl>",
                        "markdown"
                    ],
                    [
                        "inclusive_stats = stats.as_standardized().stats(inclusive=False)\nprint(inclusive_stats[:10])\n\n\n\n<strong>C++ Callgrind stats (detailed)</strong>",
                        "code"
                    ],
                    [
                        "     torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7f192a6dfd90&gt;\n       47264  ???:_int_free\n       25963  ???:_int_malloc\n       19900  build/../aten/src/ATen/TensorIter ... (at::TensorIteratorConfig const&amp;)\n       18000  ???:__tls_get_addr\n       13500  ???:malloc\n       11300  build/../c10/util/SmallVector.h:a ... (at::TensorIteratorConfig const&amp;)\n       10345  ???:_int_memalign\n       10000  build/../aten/src/ATen/TensorIter ... (at::TensorIteratorConfig const&amp;)\n        9200  ???:free\n        8000  build/../c10/util/SmallVector.h:a ... IteratorBase::get_strides() const\n\n     Total: 173472",
                        "code"
                    ],
                    [
                        "That\u2019s still quite a lot to digest. Let\u2019s use the <cite>FunctionCounts.transform</cite>\nmethod to trim some of the function path, and discard the function called.\nWhen we do, the counts of any collisions (e.g. <cite>foo.h:a()</cite> and <cite>foo.h:b()</cite>\nwill both map to <cite>foo.h</cite>) will be added together.",
                        "markdown"
                    ],
                    [
                        "import os\nimport re\n\ndef group_by_file(fn_name: str):\n    if fn_name.startswith(\"???\"):\n        fn_dir, fn_file = fn_name.split(\":\")[:2]\n    else:\n        fn_dir, fn_file = os.path.split(fn_name.split(\":\")[0])\n        fn_dir = re.sub(\"^.*build/../\", \"\", fn_dir)\n        fn_dir = re.sub(\"^.*torch/\", \"torch/\", fn_dir)\n\n    return f\"{fn_dir:&lt;15} {fn_file}\"\n\nprint(inclusive_stats.transform(group_by_file)[:10])\n\n\n\n<strong>Callgrind stats (condensed)</strong>",
                        "code"
                    ],
                    [
                        "     &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7f192995d750&gt;\n       118200  aten/src/ATen   TensorIterator.cpp\n        65000  c10/util        SmallVector.h\n        47264  ???             _int_free\n        25963  ???             _int_malloc\n        20900  c10/util        intrusive_ptr.h\n        18000  ???             __tls_get_addr\n        15900  c10/core        TensorImpl.h\n        15100  c10/core        CPUAllocator.cpp\n        13500  ???             malloc\n        12500  c10/core        TensorImpl.cpp\n\n     Total: 352327",
                        "code"
                    ]
                ]
            },
            {
                "6. A/B testing with Callgrind": [
                    [
                        "One of the most useful features of instruction counts is they allow fine\ngrained comparison of computation, which is critical when analyzing\nperformance.",
                        "markdown"
                    ],
                    [
                        "To see this in action, lets compare our multiplication of two size 128\nTensors with a {128} x {1} multiplication, which will broadcast the second\nTensor:\n<blockquote>",
                        "markdown"
                    ],
                    [
                        "result = {a0 * b0, a1 * b0, \u2026, a127 * b0}\n</blockquote>",
                        "markdown"
                    ],
                    [
                        "broadcasting_stats = (\n    \"x * y;\",\n    \"\"\"\n        auto x = torch::ones({128});\n        auto y = torch::ones({1});\n    \"\"\",\n    language=Language.CPP,\n).collect_callgrind().as_standardized().stats(inclusive=False)",
                        "code"
                    ],
                    [
                        "Often we want to A/B test two different environments. (e.g. testing a PR, or\nexperimenting with compile flags.) This is quite simple, as CallgrindStats,\nFunctionCounts, and Measurement are all pickleable. Simply save measurements\nfrom each environment, and load them in a single process for analysis.",
                        "markdown"
                    ],
                    [
                        "import pickle\n\n# Let's round trip `broadcasting_stats` just to show that we can.\nbroadcasting_stats = pickle.loads(pickle.dumps(broadcasting_stats))\n\n\n# And now to diff the two tasks:\ndelta = broadcasting_stats - inclusive_stats\n\ndef extract_fn_name(fn: str):\n    \"\"\"Trim everything except the function name.\"\"\"\n    fn = \":\".join(fn.split(\":\")[1:])\n    return re.sub(r\"\\(.+\\)\", \"(...)\", fn)\n\n# We use `.transform` to make the diff readable:\nprint(delta.transform(extract_fn_name))\n\n\n\n<strong>Instruction count delta</strong>",
                        "code"
                    ],
                    [
                        "     &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7f192995d750&gt;\n         17600  at::TensorIteratorBase::compute_strides(...)\n         12700  at::TensorIteratorBase::allocate_or_resize_outputs()\n         10200  c10::SmallVectorImpl&lt;long&gt;::operator=(...)\n          7400  at::infer_size(...)\n          6200  at::TensorIteratorBase::invert_perm(...) const\n          6064  _int_free\n          5100  at::TensorIteratorBase::reorder_dimensions()\n          4300  malloc\n          4300  at::TensorIteratorBase::compatible_stride(...) const\n           ...\n           -28  _int_memalign\n          -100  c10::impl::check_tensor_options_and_extract_memory_format(...)\n          -300  __memcmp_avx2_movbe\n          -400  at::detail::empty_cpu(...)\n         -1100  at::TensorIteratorBase::numel() const\n         -1300  void at::native::(...)\n         -2400  c10::TensorImpl::is_contiguous(...) const\n         -6100  at::TensorIteratorBase::compute_fast_setup_type(...)\n        -22600  at::TensorIteratorBase::fast_set_up(...)\n\n     Total: 58091",
                        "code"
                    ],
                    [
                        "So the broadcasting version takes an extra 580 instructions per call (recall\nthat we\u2019re collecting 100 runs per sample), or about 10%. There are quite a\nfew TensorIterator calls, so lets drill down to those. FunctionCounts.filter\nmakes this easy.",
                        "markdown"
                    ],
                    [
                        "print(delta.transform(extract_fn_name).filter(lambda fn: \"TensorIterator\" in fn))\n\n\n\n<strong>Instruction count delta (filter)</strong>",
                        "code"
                    ],
                    [
                        "     &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7f19299544d0&gt;\n         17600  at::TensorIteratorBase::compute_strides(...)\n         12700  at::TensorIteratorBase::allocate_or_resize_outputs()\n          6200  at::TensorIteratorBase::invert_perm(...) const\n          5100  at::TensorIteratorBase::reorder_dimensions()\n          4300  at::TensorIteratorBase::compatible_stride(...) const\n          4000  at::TensorIteratorBase::compute_shape(...)\n          2300  at::TensorIteratorBase::coalesce_dimensions()\n          1600  at::TensorIteratorBase::build(...)\n         -1100  at::TensorIteratorBase::numel() const\n         -6100  at::TensorIteratorBase::compute_fast_setup_type(...)\n        -22600  at::TensorIteratorBase::fast_set_up(...)\n\n     Total: 24000",
                        "code"
                    ],
                    [
                        "This makes plain what is going on: there is a fast path in TensorIterator\nsetup, but in the {128} x {1} case we miss it and have to do a more general\nanalysis which is more expensive. The most prominent call omitted by the\nfilter is <cite>c10::SmallVectorImpl&lt;long&gt;::operator=(\u2026)</cite>, which is also part\nof the more general setup.",
                        "markdown"
                    ]
                ]
            },
            {
                "7. Wrapping up": [
                    [
                        "In summary, use <cite>Timer.blocked_autorange</cite> to collect wall times. If timing\nvariation is too high, increase <cite>min_run_time</cite>, or move to C++ snippets if\nconvenient.",
                        "markdown"
                    ],
                    [
                        "For fine grained analysis, use <cite>Timer.collect_callgrind</cite> to measure\ninstruction counts and <cite>FunctionCounts.(__add__ / __sub__ / transform / filter)</cite>\nto slice-and-dice them.",
                        "markdown"
                    ]
                ]
            },
            {
                "8. Footnotes\n<blockquote>\n\n<dl class=\"simple\">\n<dt>Implied <cite>import torch</cite></dt><dd>": [
                    [
                        "If <cite>globals</cite> does not contain \u201ctorch\u201d, Timer will automatically\npopulate it. This means that <cite>Timer(\u201ctorch.empty(())\u201d)</cite> will work.\n(Though other imports should be placed in <cite>setup</cite>,\ne.g. <cite>Timer(\u201cnp.zeros(())\u201d, \u201cimport numpy as np\u201d)</cite>)\n</dd>\n</dl>\n\n<dl class=\"simple\">\n<dt>REL_WITH_DEB_INFO</dt><dd>",
                        "markdown"
                    ],
                    [
                        "In order to provide full information about the PyTorch internals which\nare executed, Callgrind needs access to C++ debug symbols. This is\naccomplished by setting REL_WITH_DEB_INFO=1 when building PyTorch.\nOtherwise function calls will be opaque. (The resultant CallgrindStats\nwill warn if debug symbols are missing.)\n</dd>\n</dl>\n\n\n</blockquote>",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "PyTorch Profiler": [
            [
                "This recipe explains how to use PyTorch profiler and measure the time and\nmemory consumption of the model\u2019s operators.",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "PyTorch includes a simple profiler API that is useful when user needs\nto determine the most expensive operators in the model.",
                        "markdown"
                    ],
                    [
                        "In this recipe, we will use a simple Resnet model to demonstrate how to\nuse profiler to analyze model performance.",
                        "markdown"
                    ]
                ]
            },
            {
                "Setup": [
                    [
                        "To install torch and torchvision use the following command:",
                        "markdown"
                    ],
                    [
                        "pip install torch torchvision",
                        "code"
                    ]
                ]
            },
            {
                "Steps": [
                    [
                        "Import all necessary libraries",
                        "markdown"
                    ],
                    [
                        "Instantiate a simple Resnet model",
                        "markdown"
                    ],
                    [
                        "Using profiler to analyze execution time",
                        "markdown"
                    ],
                    [
                        "Using profiler to analyze memory consumption",
                        "markdown"
                    ],
                    [
                        "Using tracing functionality",
                        "markdown"
                    ],
                    [
                        "Examining stack traces",
                        "markdown"
                    ],
                    [
                        "Visualizing data as a flamegraph",
                        "markdown"
                    ],
                    [
                        "Using profiler to analyze long-running jobs",
                        "markdown"
                    ],
                    {
                        "1. Import all necessary libraries": [
                            [
                                "In this recipe we will use torch, torchvision.models\nand profiler modules:",
                                "markdown"
                            ],
                            [
                                "import torch\nimport torchvision.models as models\nfrom torch.profiler import , record_function, ProfilerActivity",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2. Instantiate a simple Resnet model": [
                            [
                                "Let\u2019s create an instance of a Resnet model and prepare an input\nfor it:",
                                "markdown"
                            ],
                            [
                                "model = ()\ninputs = (5, 3, 224, 224)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "3. Using profiler to analyze execution time": [
                            [
                                "PyTorch profiler is enabled through the context manager and accepts\na number of parameters, some of the most useful are:\n\n<dl class=\"simple\">\n<dt>activities - a list of activities to profile:</dt><dd>",
                                "markdown"
                            ],
                            [
                                "ProfilerActivity.CPU - PyTorch operators, TorchScript functions and\nuser-defined code labels (see record_function below);",
                                "markdown"
                            ],
                            [
                                "ProfilerActivity.CUDA - on-device CUDA kernels;\n\n</dd>\n</dl>",
                                "markdown"
                            ],
                            [
                                "record_shapes - whether to record shapes of the operator inputs;",
                                "markdown"
                            ],
                            [
                                "profile_memory - whether to report amount of memory consumed by\nmodel\u2019s Tensors;",
                                "markdown"
                            ],
                            [
                                "use_cuda - whether to measure execution time of CUDA kernels.",
                                "markdown"
                            ],
                            [
                                "Note: when using CUDA, profiler also shows the runtime CUDA events\noccuring on the host.",
                                "markdown"
                            ],
                            [
                                "Let\u2019s see how we can use profiler to analyze the execution time:",
                                "markdown"
                            ],
                            [
                                "with (activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n    with record_function(\"model_inference\"):\n        model(inputs)",
                                "code"
                            ],
                            [
                                "Note that we can use record_function context manager to label\narbitrary code ranges with user provided names\n(model_inference is used as a label in the example above).",
                                "markdown"
                            ],
                            [
                                "Profiler allows one to check which operators were called during the\nexecution of a code range wrapped with a profiler context manager.\nIf multiple profiler ranges are active at the same time (e.g. in\nparallel PyTorch threads), each profiling context manager tracks only\nthe operators of its corresponding range.\nProfiler also automatically profiles the async tasks launched\nwith torch.jit._fork and (in case of a backward pass)\nthe backward pass operators launched with backward() call.",
                                "markdown"
                            ],
                            [
                                "Let\u2019s print out the stats for the execution above:",
                                "markdown"
                            ],
                            [
                                "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))",
                                "code"
                            ],
                            [
                                "The output will look like (omitting some columns):",
                                "markdown"
                            ],
                            [
                                "# ---------------------------------  ------------  ------------  ------------  ------------\n#                              Name      Self CPU     CPU total  CPU time avg    # of Calls\n# ---------------------------------  ------------  ------------  ------------  ------------\n#                   model_inference       5.509ms      57.503ms      57.503ms             1\n#                      aten::conv2d     231.000us      31.931ms       1.597ms            20\n#                 aten::convolution     250.000us      31.700ms       1.585ms            20\n#                aten::_convolution     336.000us      31.450ms       1.573ms            20\n#          aten::mkldnn_convolution      30.838ms      31.114ms       1.556ms            20\n#                  aten::batch_norm     211.000us      14.693ms     734.650us            20\n#      aten::_batch_norm_impl_index     319.000us      14.482ms     724.100us            20\n#           aten::native_batch_norm       9.229ms      14.109ms     705.450us            20\n#                        aten::mean     332.000us       2.631ms     125.286us            21\n#                      aten::select       1.668ms       2.292ms       8.988us           255\n# ---------------------------------  ------------  ------------  ------------  ------------\n# Self CPU time total: 57.549ms",
                                "code"
                            ],
                            [
                                "Here we see that, as expected, most of the time is spent in convolution (and specifically in mkldnn_convolution\nfor PyTorch compiled with MKL-DNN support).\nNote the difference between self cpu time and cpu time - operators can call other operators, self cpu time excludes time\nspent in children operator calls, while total cpu time includes it. You can choose to sort by the self cpu time by passing\nsort_by=\"self_cpu_time_total\" into the table call.",
                                "markdown"
                            ],
                            [
                                "To get a finer granularity of results and include operator input shapes, pass group_by_input_shape=True\n(note: this requires running the profiler with record_shapes=True):",
                                "markdown"
                            ],
                            [
                                "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------------  ------------  -------------------------------------------\n#                              Name     CPU total                                 Input Shapes\n# ---------------------------------  ------------  -------------------------------------------\n#                   model_inference      57.503ms                                           []\n#                      aten::conv2d       8.008ms      [5,64,56,56], [64,64,3,3], [], ..., []]\n#                 aten::convolution       7.956ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n#                aten::_convolution       7.909ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n#          aten::mkldnn_convolution       7.834ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n#                      aten::conv2d       6.332ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#                 aten::convolution       6.303ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#                aten::_convolution       6.273ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#          aten::mkldnn_convolution       6.233ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#                      aten::conv2d       4.751ms  [[5,256,14,14], [256,256,3,3], [], ..., []]\n# ---------------------------------  ------------  -------------------------------------------\n# Self CPU time total: 57.549ms",
                                "code"
                            ],
                            [
                                "Note the occurence of aten::convolution twice with different input shapes.",
                                "markdown"
                            ],
                            [
                                "Profiler can also be used to analyze performance of models executed on GPUs:",
                                "markdown"
                            ],
                            [
                                "model = ().cuda()\ninputs = (5, 3, 224, 224).cuda()\n\nwith (activities=[\n        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n    with record_function(\"model_inference\"):\n        model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))",
                                "code"
                            ],
                            [
                                "(Note: the first use of CUDA profiling may bring an extra overhead.)",
                                "markdown"
                            ],
                            [
                                "The resulting table output:",
                                "markdown"
                            ],
                            [
                                "# (omitting some columns)\n# -------------------------------------------------------  ------------  ------------\n#                                                    Name     Self CUDA    CUDA total\n# -------------------------------------------------------  ------------  ------------\n#                                         model_inference       0.000us      11.666ms\n#                                            aten::conv2d       0.000us      10.484ms\n#                                       aten::convolution       0.000us      10.484ms\n#                                      aten::_convolution       0.000us      10.484ms\n#                              aten::_convolution_nogroup       0.000us      10.484ms\n#                                       aten::thnn_conv2d       0.000us      10.484ms\n#                               aten::thnn_conv2d_forward      10.484ms      10.484ms\n# void at::native::im2col_kernel&lt;float&gt;(long, float co...       3.844ms       3.844ms\n#                                       sgemm_32x32x32_NN       3.206ms       3.206ms\n#                                   sgemm_32x32x32_NN_vec       3.093ms       3.093ms\n# -------------------------------------------------------  ------------  ------------\n# Self CPU time total: 23.015ms\n# Self CUDA time total: 11.666ms",
                                "code"
                            ],
                            [
                                "Note the occurence of on-device kernels in the output (e.g. sgemm_32x32x32_NN).",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "4. Using profiler to analyze memory consumption": [
                            [
                                "PyTorch profiler can also show the amount of memory (used by the model\u2019s tensors)\nthat was allocated (or released) during the execution of the model\u2019s operators.\nIn the output below, \u2018self\u2019 memory corresponds to the memory allocated (released)\nby the operator, excluding the children calls to the other operators.\nTo enable memory profiling functionality pass profile_memory=True.",
                                "markdown"
                            ],
                            [
                                "model = ()\ninputs = (5, 3, 224, 224)\n\nwith (activities=[ProfilerActivity.CPU],\n        profile_memory=True, record_shapes=True) as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------------  ------------  ------------  ------------\n#                              Name       CPU Mem  Self CPU Mem    # of Calls\n# ---------------------------------  ------------  ------------  ------------\n#                       aten::empty      94.79 Mb      94.79 Mb           121\n#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n#                       aten::addmm      19.53 Kb      19.53 Kb             1\n#               aten::empty_strided         572 b         572 b            25\n#                     aten::resize_         240 b         240 b             6\n#                         aten::abs         480 b         240 b             4\n#                         aten::add         160 b         160 b            20\n#               aten::masked_select         120 b         112 b             1\n#                          aten::ne         122 b          53 b             6\n#                          aten::eq          60 b          30 b             2\n# ---------------------------------  ------------  ------------  ------------\n# Self CPU time total: 53.064ms\n\nprint(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n\n# (omitting some columns)\n# ---------------------------------  ------------  ------------  ------------\n#                              Name       CPU Mem  Self CPU Mem    # of Calls\n# ---------------------------------  ------------  ------------  ------------\n#                       aten::empty      94.79 Mb      94.79 Mb           121\n#                  aten::batch_norm      47.41 Mb           0 b            20\n#      aten::_batch_norm_impl_index      47.41 Mb           0 b            20\n#           aten::native_batch_norm      47.41 Mb           0 b            20\n#                      aten::conv2d      47.37 Mb           0 b            20\n#                 aten::convolution      47.37 Mb           0 b            20\n#                aten::_convolution      47.37 Mb           0 b            20\n#          aten::mkldnn_convolution      47.37 Mb           0 b            20\n#                  aten::max_pool2d      11.48 Mb           0 b             1\n#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n# ---------------------------------  ------------  ------------  ------------\n# Self CPU time total: 53.064ms",
                                "code"
                            ]
                        ]
                    },
                    {
                        "5. Using tracing functionality": [
                            [
                                "Profiling results can be outputted as a .json trace file:",
                                "markdown"
                            ],
                            [
                                "model = ().cuda()\ninputs = (5, 3, 224, 224).cuda()\n\nwith (activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n    model(inputs)\n\nprof.export_chrome_trace(\"trace.json\")",
                                "code"
                            ],
                            [
                                "You can examine the sequence of profiled operators and CUDA kernels\nin Chrome trace viewer (chrome://tracing):",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "6. Examining stack traces": [
                            [
                                "Profiler can be used to analyze Python and TorchScript stack traces:",
                                "markdown"
                            ],
                            [
                                "with (\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    with_stack=True,\n) as prof:\n    model(inputs)\n\n# Print aggregated stats\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=2))\n\n# (omitting some columns)\n# -------------------------  -----------------------------------------------------------\n#                      Name  Source Location\n# -------------------------  -----------------------------------------------------------\n# aten::thnn_conv2d_forward  .../torch/nn/modules/conv.py(439): _conv_forward\n#                            .../torch/nn/modules/conv.py(443): forward\n#                            .../torch/nn/modules/module.py(1051): _call_impl\n#                            .../site-packages/torchvision/models/resnet.py(63): forward\n#                            .../torch/nn/modules/module.py(1051): _call_impl\n#\n# aten::thnn_conv2d_forward  .../torch/nn/modules/conv.py(439): _conv_forward\n#                            .../torch/nn/modules/conv.py(443): forward\n#                            .../torch/nn/modules/module.py(1051): _call_impl\n#                            .../site-packages/torchvision/models/resnet.py(59): forward\n#                            .../torch/nn/modules/module.py(1051): _call_impl\n#\n# -------------------------  -----------------------------------------------------------\n# Self CPU time total: 34.016ms\n# Self CUDA time total: 11.659ms",
                                "code"
                            ],
                            [
                                "Note the two convolutions and the two callsites in torchvision/models/resnet.py script.",
                                "markdown"
                            ],
                            [
                                "(Warning: stack tracing adds an extra profiling overhead.)",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "7. Visualizing data as a flamegraph": [
                            [
                                "Execution time (self_cpu_time_total and self_cuda_time_total metrics) and stack traces\ncan also be visualized as a flame graph. To do this, first export the raw data using export_stacks (requires with_stack=True):",
                                "markdown"
                            ],
                            [
                                "prof.export_stacks(\"/tmp/profiler_stacks.txt\", \"self_cuda_time_total\")",
                                "code"
                            ],
                            [
                                "We recommend using e.g.  to generate an\ninteractive SVG:",
                                "markdown"
                            ],
                            [
                                "# git clone https://github.com/brendangregg/FlameGraph\n# cd FlameGraph\n# ./flamegraph.pl --title \"CUDA time\" --countname \"us.\" /tmp/profiler_stacks.txt &gt; perf_viz.svg",
                                "code"
                            ]
                        ]
                    },
                    {
                        "8. Using profiler to analyze long-running jobs": [
                            [
                                "PyTorch profiler offers an additional API to handle long-running jobs\n(such as training loops). Tracing all of the execution can be\nslow and result in very large trace files. To avoid this, use optional\narguments:",
                                "markdown"
                            ],
                            [
                                "schedule - specifies a function that takes an integer argument (step number)\nas an input and returns an action for the profiler, the best way to use this parameter\nis to use torch.profiler.schedule helper function that can generate a schedule for you;",
                                "markdown"
                            ],
                            [
                                "on_trace_ready - specifies a function that takes a reference to the profiler as\nan input and is called by the profiler each time the new trace is ready.",
                                "markdown"
                            ],
                            [
                                "To illustrate how the API works, let\u2019s first consider the following example with\ntorch.profiler.schedule helper function:",
                                "markdown"
                            ],
                            [
                                "from torch.profiler import \n\nmy_schedule = (\n    skip_first=10,\n    wait=5,\n    warmup=1,\n    active=3,\n    repeat=2)",
                                "code"
                            ],
                            [
                                "Profiler assumes that the long-running job is composed of steps, numbered\nstarting from zero. The example above defines the following sequence of actions\nfor the profiler:",
                                "markdown"
                            ],
                            [
                                "Parameter skip_first tells profiler that it should ignore the first 10 steps\n(default value of skip_first is zero);",
                                "markdown"
                            ],
                            [
                                "After the first skip_first steps, profiler starts executing profiler cycles;",
                                "markdown"
                            ],
                            [
                                "Each cycle consists of three phases:",
                                "markdown"
                            ],
                            [
                                "idling (wait=5 steps), during this phase profiler is not active;",
                                "markdown"
                            ],
                            [
                                "warming up (warmup=1 steps), during this phase profiler starts tracing, but\nthe results are discarded; this phase is used to discard the samples obtained by\nthe profiler at the beginning of the trace since they are usually skewed by an extra\noverhead;",
                                "markdown"
                            ],
                            [
                                "active tracing (active=3 steps), during this phase profiler traces and records data;",
                                "markdown"
                            ],
                            [
                                "An optional repeat parameter specifies an upper bound on the number of cycles.\nBy default (zero value), profiler will execute cycles as long as the job runs.",
                                "markdown"
                            ],
                            [
                                "Thus, in the example above, profiler will skip the first 15 steps, spend the next step on the warm up,\nactively record the next 3 steps, skip another 5 steps, spend the next step on the warm up, actively\nrecord another 3 steps. Since the repeat=2 parameter value is specified, the profiler will stop\nthe recording after the first two cycles.",
                                "markdown"
                            ],
                            [
                                "At the end of each cycle profiler calls the specified on_trace_ready function and passes itself as\nan argument. This function is used to process the new trace - either by obtaining the table output or\nby saving the output on disk as a trace file.",
                                "markdown"
                            ],
                            [
                                "To send the signal to the profiler that the next step has started, call prof.step() function.\nThe current profiler step is stored in prof.step_num.",
                                "markdown"
                            ],
                            [
                                "The following example shows how to use all of the concepts above:",
                                "markdown"
                            ],
                            [
                                "def trace_handler(p):\n    output = p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10)\n    print(output)\n    p.export_chrome_trace(\"/tmp/trace_\" + str(p.step_num) + \".json\")\n\nwith (\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    =(\n        wait=1,\n        warmup=1,\n        active=2),\n    on_trace_ready=trace_handler\n) as p:\n    for idx in range(8):\n        model(inputs)\n        p.step()",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Learn More": [
                    [
                        "Take a look at the following recipes/tutorials to continue your learning:",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        " tutorial",
                        "markdown"
                    ],
                    [
                        " tutorial",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "PyTorch Benchmark": [
            [
                "This recipe provides a quick-start guide to using PyTorch\nbenchmark module to measure and compare code performance.",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "Benchmarking is an important step in writing code. It helps\nus validate that our code meets performance expectations,\ncompare different approaches to solving the same problem and\nprevent performance regressions.",
                        "markdown"
                    ],
                    [
                        "There are many options when it comes to benchmarking PyTorch code\nincluding the Python builtin timeit module. However, benchmarking\nPyTorch code has many caveats that can be easily overlooked such as\nmanaging the number of threads and synchronizing CUDA devices. Moreover,\ngenerating Tensor inputs for benchmarking can be quite tedious.",
                        "markdown"
                    ],
                    [
                        "This recipe demonstrates how to use PyTorch benchmark module to avoid\ncommon mistakes while making it easier to compare performance of\ndifferent code, generate input for benchmarking and more.",
                        "markdown"
                    ]
                ]
            },
            {
                "Setup": [
                    [
                        "Before we begin, install torch if it isn\u2019t already available.",
                        "markdown"
                    ],
                    [
                        "pip install torch",
                        "code"
                    ]
                ]
            },
            {
                "Steps": [
                    [
                        "Defining functions to benchmark",
                        "markdown"
                    ],
                    [
                        "Benchmarking with timeit.Timer",
                        "markdown"
                    ],
                    [
                        "Benchmarking with torch.utils.benchmark.Timer",
                        "markdown"
                    ],
                    [
                        "Benchmarking with <cite>Blocked Autorange</cite>",
                        "markdown"
                    ],
                    [
                        "Comparing benchmark results",
                        "markdown"
                    ],
                    [
                        "Saving/Loading benchmark results",
                        "markdown"
                    ],
                    [
                        "Generating inputs with <cite>Fuzzed Parameters</cite>",
                        "markdown"
                    ],
                    [
                        "Collecting instruction counts with <cite>Callgrind</cite>",
                        "markdown"
                    ],
                    {
                        "1. Defining functions to benchmark": [
                            [
                                "As of the time of this writing, \ndoes not support batched mode, so we will compare two approaches to\nimplementing it using existing torch operators: one approach uses a\ncombination of mul and sum while the other reduces the problem to bmm.",
                                "markdown"
                            ],
                            [
                                "import torch\n\n\ndef batched_dot_mul_sum(a, b):\n    '''Computes batched dot by multiplying and summing'''\n    return a.mul(b).sum(-1)\n\n\ndef batched_dot_bmm(a, b):\n    '''Computes batched dot by reducing to bmm'''\n    a = a.reshape(-1, 1, a.shape[-1])\n    b = b.reshape(-1, b.shape[-1], 1)\n    return (a, b).flatten(-3)\n\n\n# Input for benchmarking\nx = (10000, 64)\n\n# Ensure that both functions compute the same output\nassert batched_dot_mul_sum(x, x).allclose(batched_dot_bmm(x, x))",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2. Benchmarking with timeit.Timer": [
                            [
                                "First, let\u2019s benchmark the code using Python\u2019s builtin timeit module.\nWe keep the benchmark code simple here so we can compare the defaults\nof timeit and torch.utils.benchmark.",
                                "markdown"
                            ],
                            [
                                "import timeit\n\nt0 = timeit.Timer(\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x})\n\nt1 = timeit.Timer(\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x})\n\nprint(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:&gt;5.1f} us')\nprint(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:&gt;5.1f} us')\n\n\n\nOutput",
                                "code"
                            ],
                            [
                                " mul_sum(x, x):  111.6 us\n bmm(x, x):       70.0 us",
                                "code"
                            ]
                        ]
                    },
                    {
                        "3. Benchmarking with torch.utils.benchmark.Timer": [
                            [
                                "PyTorch benchmark module was designed to be familiar to those who\nhave used the timeit module before. However, its defaults make it\neasier and safer to use for benchmarking PyTorch code. Let\u2019s first\ncompare the same basic API as above.",
                                "markdown"
                            ],
                            [
                                "import torch.utils.benchmark as benchmark\n\nt0 = (\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x})\n\nt1 = (\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x})\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\n\n\nOutput",
                                "code"
                            ],
                            [
                                " &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0&gt;\n batched_dot_mul_sum(x, x)\n setup: from __main__ import batched_dot_mul_sum\n   379.29 us\n   1 measurement, 100 runs , 1 thread\n &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb103d67048&gt;\n batched_dot_bmm(x, x)\n setup: from __main__ import batched_dot_bmm\n   716.42 us\n   1 measurement, 100 runs , 1 thread",
                                "code"
                            ],
                            [
                                "Even though the APIs are the same for the basic functionality, there\nare some important differences. benchmark.Timer.timeit() returns the\ntime per run as opposed to the total runtime like timeit.Timer.timeit()\ndoes. PyTorch benchmark module also provides formatted string\nrepresentations for printing the results.",
                                "markdown"
                            ],
                            [
                                "Another important difference, and the reason why the results diverge\nis that PyTorch benchmark module runs in a single thread by default.\nWe can change the number of threads with the num_threads arg.",
                                "markdown"
                            ],
                            [
                                "torch.utils.benchmark.Timer takes several additional arguments\nincluding: <cite>label</cite>, <cite>sub_label</cite>, <cite>description</cite> and <cite>env</cite> which change\nthe __repr__ of the measurement object returned and are used for\ngrouping the results (more on this later).",
                                "markdown"
                            ],
                            [
                                "num_threads = ()\nprint(f'Benchmarking on {num_threads} threads')\n\nt0 = (\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using mul and sum')\n\nt1 = (\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x},\n    num_threads=num_threads,\n    label='Multithreaded batch dot',\n    sub_label='Implemented using bmm')\n\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\n\n\nOutput",
                                "code"
                            ],
                            [
                                " Benchmarking on 40 threads\n &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb103d54080&gt;\n Multithreaded batch dot: Implemented using mul and sum\n setup: from __main__ import batched_dot_mul_sum\n   118.47 us\n   1 measurement, 100 runs , 40 threads\n &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;\n Multithreaded batch dot: Implemented using bmm\n setup: from __main__ import batched_dot_bmm\n   68.21 us\n   1 measurement, 100 runs , 40 threads",
                                "code"
                            ],
                            [
                                "Running benchmark with all threads available gives similar results\nas the timeit module. More importantly, which version is faster\ndepends on how many threads we run the code with. This is why it\u2019s\nimportant to benchmark the code with thread settings that are\nrepresentative of real use cases. Another important thing to remember\nis to synchronize CPU and CUDA when benchmarking on the GPU. Let\u2019s run\nthe above benchmarks again on a CUDA tensor and see what happens.",
                                "markdown"
                            ],
                            [
                                "x = (10000, 1024, device='cuda')\n\nt0 = timeit.Timer(\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x})\n\nt1 = timeit.Timer(\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x})\n\n# Ran each twice to show difference before/after warmup\nprint(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:&gt;5.1f} us')\nprint(f'mul_sum(x, x):  {t0.timeit(100) / 100 * 1e6:&gt;5.1f} us')\nprint(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:&gt;5.1f} us')\nprint(f'bmm(x, x):      {t1.timeit(100) / 100 * 1e6:&gt;5.1f} us')\n\n\n\nOutput",
                                "code"
                            ],
                            [
                                " mul_sum(x, x):   27.6 us\n mul_sum(x, x):   25.3 us\n bmm(x, x):      2775.5 us\n bmm(x, x):       22.4 us",
                                "code"
                            ],
                            [
                                "t0 = (\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='from __main__ import batched_dot_mul_sum',\n    globals={'x': x})\n\nt1 = (\n    stmt='batched_dot_bmm(x, x)',\n    setup='from __main__ import batched_dot_bmm',\n    globals={'x': x})\n\n# Run only once since benchmark module does warmup for us\nprint(t0.timeit(100))\nprint(t1.timeit(100))\n\n\n\nOutput",
                                "code"
                            ],
                            [
                                " &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d080&gt;\n batched_dot_mul_sum(x, x)\n setup: from __main__ import batched_dot_mul_sum\n   232.93 us\n   1 measurement, 100 runs , 1 thread\n &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0&gt;\n batched_dot_bmm(x, x)\n setup: from __main__ import batched_dot_bmm\n   181.04 us\n   1 measurement, 100 runs , 1 thread",
                                "code"
                            ],
                            [
                                "The results reveal something interesting. The first run of the bmm\nversion using the timeit module takes much longer than the second\nrun. This is because bmm calls into <cite>cuBLAS</cite> which needs to be\nloaded the first time it\u2019s called which takes some time. This is why\nit\u2019s important to do a warmup run before benchmarking, luckily for\nus, PyTorch\u2019s benchmark module takes care of that.",
                                "markdown"
                            ],
                            [
                                "The difference in the results between timeit and benchmark modules\nis because the <cite>timeit</cite> module is not synchronizing CUDA and is thus only\ntiming the time to launch the kernel. PyTorch\u2019s benchmark module does\nthe synchronization for us.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "4. Benchmarking with <cite>Blocked Autorange</cite>": [
                            [
                                "While timeit.Timer.autorange takes a single continuous measurement\nof at least 0.2 seconds, <cite>torch.utils.benchmark.blocked_autorange</cite>\ntakes many measurements whose times total at least 0.2 seconds (which\ncan be changed by the <cite>min_run_time</cite> parameter) subject to the constraint\nthat timing overhead is a small fraction of the overall measurement.\nThis is accomplished by first running with an increasing number of runs\nper loop until the runtime is much larger than measurement overhead\n(which also serves as a warm up), and then taking measurements until\nthe target time is reached. This has the useful properties that it wastes\nless data and allows us to compute statistics to estimate the reliability\nof the measurements.",
                                "markdown"
                            ],
                            [
                                "m0 = t0.blocked_autorange()\nm1 = t1.blocked_autorange()\n\nprint(m0)\nprint(m1)\n\n\n\nOutput",
                                "code"
                            ],
                            [
                                " &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d0f0&gt;\n batched_dot_mul_sum(x, x)\n setup: from __main__ import batched_dot_mul_sum\n   231.79 us\n   1 measurement, 1000 runs , 1 thread\n &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb10400d080&gt;\n batched_dot_bmm(x, x)\n setup: from __main__ import batched_dot_bmm\n   Median: 162.08 us\n   2 measurements, 1000 runs per measurement, 1 thread",
                                "code"
                            ],
                            [
                                "We can also inspect the individual statistics from the returned\nmeasurements object.",
                                "markdown"
                            ],
                            [
                                "print(f\"Mean:   {m0.mean * 1e6:6.2f} us\")\nprint(f\"Median: {m0.median * 1e6:6.2f} us\")\n\n\n\nOutput",
                                "code"
                            ],
                            [
                                " Mean:   231.79 us\n Median: 231.79 us",
                                "code"
                            ]
                        ]
                    },
                    {
                        "5. Comparing benchmark results": [
                            [
                                "So far we\u2019ve been comparing our two versions of batched dot against a\nsingle input. In practice, we want to try a combination of inputs as\nwell as different number of threads. The Compare class helps display\nthe results of many measurements in a formatted table. It uses the\nannotations described above (<cite>label</cite>, <cite>sub_label</cite>, <cite>num_threads</cite>, etc.) as\nwell as <cite>description</cite> to group and organize the table. Let\u2019s use\nCompare to see how our functions perform for different input sizes\nand number of threads.",
                                "markdown"
                            ],
                            [
                                "from itertools import product\n\n# Compare takes a list of measurements which we'll save in results.\nresults = []\n\nsizes = [1, 64, 1024, 10000]\nfor b, n in product(sizes, sizes):\n    # label and sub_label are the rows\n    # description is the column\n    label = 'Batched dot'\n    sub_label = f'[{b}, {n}]'\n    x = ((b, n))\n    for num_threads in [1, 4, 16, 32]:\n        results.append((\n            stmt='batched_dot_mul_sum(x, x)',\n            setup='from __main__ import batched_dot_mul_sum',\n            globals={'x': x},\n            num_threads=num_threads,\n            label=label,\n            sub_label=sub_label,\n            description='mul/sum',\n        ).blocked_autorange(min_run_time=1))\n        results.append((\n            stmt='batched_dot_bmm(x, x)',\n            setup='from __main__ import batched_dot_bmm',\n            globals={'x': x},\n            num_threads=num_threads,\n            label=label,\n            sub_label=sub_label,\n            description='bmm',\n        ).blocked_autorange(min_run_time=1))\n\ncompare = benchmark.Compare(results)\ncompare.print()\n\n\n\nOutput",
                                "code"
                            ],
                            [
                                " [--------------- Batched dot ----------------]\n                       |  mul/sum   |    bmm\n 1 threads: -----------------------------------\n       [1, 1]          |       5.9  |      11.2\n       [1, 64]         |       6.4  |      11.4\n       [1, 1024]       |       6.7  |      14.2\n       [1, 10000]      |      10.2  |      23.7\n       [64, 1]         |       6.3  |      11.5\n       [64, 64]        |       8.6  |      15.4\n       [64, 1024]      |      39.4  |     204.4\n       [64, 10000]     |     274.9  |     748.5\n       [1024, 1]       |       7.7  |      17.8\n       [1024, 64]      |      40.3  |      76.4\n       [1024, 1024]    |     432.4  |    2795.9\n       [1024, 10000]   |   22657.3  |   11899.5\n       [10000, 1]      |      16.9  |      74.8\n       [10000, 64]     |     300.3  |     609.4\n       [10000, 1024]   |   23098.6  |   27246.1\n       [10000, 10000]  |  267073.7  |  118823.7\n 4 threads: -----------------------------------\n       [1, 1]          |       6.0  |      11.5\n       [1, 64]         |       6.2  |      11.2\n       [1, 1024]       |       6.8  |      14.3\n       [1, 10000]      |      10.2  |      23.7\n       [64, 1]         |       6.3  |      16.2\n       [64, 64]        |       8.8  |      18.2\n       [64, 1024]      |      41.5  |     189.1\n       [64, 10000]     |      91.7  |     849.1\n       [1024, 1]       |       7.6  |      17.4\n       [1024, 64]      |      43.5  |      33.5\n       [1024, 1024]    |     135.4  |    2782.3\n       [1024, 10000]   |    7471.1  |   11874.0\n       [10000, 1]      |      16.8  |      33.9\n       [10000, 64]     |     118.7  |     173.2\n       [10000, 1024]   |    7264.6  |   27824.7\n       [10000, 10000]  |  100060.9  |  121499.0\n 16 threads: ----------------------------------\n       [1, 1]          |       6.0  |      11.3\n       [1, 64]         |       6.2  |      11.2\n       [1, 1024]       |       6.9  |      14.2\n       [1, 10000]      |      10.3  |      23.8\n       [64, 1]         |       6.4  |      24.1\n       [64, 64]        |       9.0  |      23.8\n       [64, 1024]      |      54.1  |     188.5\n       [64, 10000]     |      49.9  |     748.0\n       [1024, 1]       |       7.6  |      23.4\n       [1024, 64]      |      55.5  |      28.2\n       [1024, 1024]    |      66.9  |    2773.9\n       [1024, 10000]   |    6111.5  |   12833.7\n       [10000, 1]      |      16.9  |      27.5\n       [10000, 64]     |      59.5  |      73.7\n       [10000, 1024]   |    6295.9  |   27062.0\n       [10000, 10000]  |   71804.5  |  120365.8\n 32 threads: ----------------------------------\n       [1, 1]          |       5.9  |      11.3\n       [1, 64]         |       6.2  |      11.3\n       [1, 1024]       |       6.7  |      14.2\n       [1, 10000]      |      10.5  |      23.8\n       [64, 1]         |       6.3  |      31.7\n       [64, 64]        |       9.1  |      30.4\n       [64, 1024]      |      72.0  |     190.4\n       [64, 10000]     |     103.1  |     746.9\n       [1024, 1]       |       7.6  |      28.4\n       [1024, 64]      |      70.5  |      31.9\n       [1024, 1024]    |      65.6  |    2804.6\n       [1024, 10000]   |    6764.0  |   11871.4\n       [10000, 1]      |      17.8  |      31.8\n       [10000, 64]     |     110.3  |      56.0\n       [10000, 1024]   |    6640.2  |   27592.2\n       [10000, 10000]  |   73003.4  |  120083.2\n\n Times are in microseconds (us).",
                                "code"
                            ],
                            [
                                "The results above indicate that the version which reduces to bmm\nis better for larger tensors running on multiple threads, while for\nsmaller and/or single thread code, the other version is better.",
                                "markdown"
                            ],
                            [
                                "Compare also provides functions for changing the table format",
                                "markdown"
                            ],
                            [
                                "compare.trim_significant_figures()\ncompare.colorize()\ncompare.print()",
                                "code"
                            ]
                        ]
                    },
                    {
                        "6. Saving/Loading benchmark results": [
                            [
                                "<cite>Measurements</cite> (and <cite>CallgrindStats</cite> which are described in section 8)\nare pickleable. This makes A/B testing easy, as you can collect\nmeasurements from two separate environments, pickle them, and then\nload both in a single environment. Timer even takes an <cite>env</cite>\nconstructor argument so that such A/B testing works seamlessly.",
                                "markdown"
                            ],
                            [
                                "Let\u2019s imagine that rather than two Python functions, the add/sum\nand bmm approaches were in two different builds of PyTorch.\nThe example below demonstrates how one might A/B test them. For\nsimplicity, we only use a subset of shapes, and simply round trip\nresults through pickle rather than actually using multiple environments\nand writing results to disk.",
                                "markdown"
                            ],
                            [
                                "import pickle\n\nab_test_results = []\nfor env in ('environment A: mul/sum', 'environment B: bmm'):\n    for b, n in ((1, 1), (1024, 10000), (10000, 1)):\n        x = ((b, n))\n        dot_fn = (batched_dot_mul_sum if env == 'environment A: mul/sum' else batched_dot_bmm)\n        m = (\n            stmt='batched_dot(x, x)',\n            globals={'x': x, 'batched_dot': dot_fn},\n            num_threads=1,\n            label='Batched dot',\n            description=f'[{b}, {n}]',\n            env=env,\n        ).blocked_autorange(min_run_time=1)\n        ab_test_results.append(pickle.dumps(m))\n\nab_results = [pickle.loads(i) for i in ab_test_results]\ncompare = benchmark.Compare(ab_results)\ncompare.trim_significant_figures()\ncompare.colorize()\ncompare.print()\n\n\n\nOutput",
                                "code"
                            ],
                            [
                                " [------------------------------------- Batched dot -------------------------------------]\n                                                |  [1, 1]  |  [1024, 10000]  |  [10000, 1]\n 1 threads: ------------------------------------------------------------------------------\n   (environment A: mul/sum)  batched_dot(x, x)  |     7    |      36000      |      21\n   (environment B: bmm)      batched_dot(x, x)  |    14    |      40000      |      85\n\n Times are in microseconds (us).",
                                "code"
                            ],
                            [
                                "# And just to show that we can round trip all of the results from earlier:\nround_tripped_results = pickle.loads(pickle.dumps(results))\nassert(str(benchmark.Compare(results)) == str(benchmark.Compare(round_tripped_results)))",
                                "code"
                            ]
                        ]
                    },
                    {
                        "7. Generating inputs with <cite>Fuzzed Parameters</cite>": [
                            [
                                "As we\u2019ve seen in the previous section, there can be some stark\nperformance differences depending on the input tensors. Hence, it\nis a good idea to run benchmarks on a number of different inputs.\nHowever, creating all these input tensors can be tedious which is\nwhere torch.utils.benchmark.Fuzzer and related classes come in.\nLet\u2019s take a look at how we can use the Fuzzer to create some test\ncases for the benchmark.",
                                "markdown"
                            ],
                            [
                                "from torch.utils.benchmark import Fuzzer, FuzzedParameter, FuzzedTensor, ParameterAlias\n\n# Generates random tensors with 128 to 10000000 elements and sizes k0 and k1 chosen from a\n# loguniform distribution in [1, 10000], 40% of which will be discontiguous on average.\nexample_fuzzer = Fuzzer(\n    parameters = [\n        FuzzedParameter('k0', minval=1, maxval=10000, distribution='loguniform'),\n        FuzzedParameter('k1', minval=1, maxval=10000, distribution='loguniform'),\n    ],\n    tensors = [\n        FuzzedTensor('x', size=('k0', 'k1'), min_elements=128, max_elements=10000000, probability_contiguous=0.6)\n    ],\n    seed=0,\n)\n\nresults = []\nfor tensors, tensor_params, params in example_fuzzer.take(10):\n    # description is the column label\n    sub_label=f\"{params['k0']:&lt;6} x {params['k1']:&lt;4} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n    results.append((\n        stmt='batched_dot_mul_sum(x, x)',\n        setup='from __main__ import batched_dot_mul_sum',\n        globals=tensors,\n        label='Batched dot',\n        sub_label=sub_label,\n        description='mul/sum',\n    ).blocked_autorange(min_run_time=1))\n    results.append((\n        stmt='batched_dot_bmm(x, x)',\n        setup='from __main__ import batched_dot_bmm',\n        globals=tensors,\n        label='Batched dot',\n        sub_label=sub_label,\n        description='bmm',\n    ).blocked_autorange(min_run_time=1))\n\ncompare = benchmark.Compare(results)\ncompare.trim_significant_figures()\ncompare.print()\n\n\n\nOutput",
                                "code"
                            ],
                            [
                                " [--------------------- Batched dot ---------------------]\n                                      |  mul/sum  |   bmm\n 1 threads: ----------------------------------------------\n       725    x 257                   |      87   |    180\n       49     x 383                   |      15   |     30\n       34     x 1468                  |      30   |    118\n       187    x 5039                  |     400   |   1200\n       2140   x 1296 (discontiguous)  |    2000   |  41000\n       78     x 1598                  |      74   |    310\n       519    x 763                   |     190   |   1500\n       141    x 1082                  |      87   |    500\n       78     x 5    (discontiguous)  |       9   |     20\n       187    x 1                     |      12   |     10\n\n Times are in microseconds (us).",
                                "code"
                            ],
                            [
                                "There is a lot of flexibility for defining your own Fuzzers which\nis great for creating a powerful set of inputs to benchmark. But to\nmake things even simpler, PyTorch benchmark module comes with some\nbuitin Fuzzers for common benchmarking needs. Let\u2019s take a look at\nhow we can use one of these builtin fuzzers.",
                                "markdown"
                            ],
                            [
                                "from torch.utils.benchmark.op_fuzzers import binary\n\nresults = []\nfor tensors, tensor_params, params in binary.BinaryOpFuzzer(seed=0).take(10):\n    sub_label=f\"{params['k0']:&lt;6} x {params['k1']:&lt;4} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n    results.append((\n        stmt='batched_dot_mul_sum(x, x)',\n        setup='from __main__ import batched_dot_mul_sum',\n        globals=tensors,\n        label='Batched dot',\n        sub_label=sub_label,\n        description='mul/sum',\n    ).blocked_autorange(min_run_time=1))\n    results.append((\n        stmt='batched_dot_bmm(x, x)',\n        setup='from __main__ import batched_dot_bmm',\n        globals=tensors,\n        label='Batched dot',\n        sub_label=sub_label,\n        description='bmm',\n    ).blocked_autorange(min_run_time=1))\n\ncompare = benchmark.Compare(results)\ncompare.trim_significant_figures()\ncompare.colorize(rowwise=True)\ncompare.print()\n\n\n\nOutput",
                                "code"
                            ],
                            [
                                " [----------------------- Batched dot ------------------------]\n                                          |  mul/sum  |   bmm\n 1 threads: ---------------------------------------------------\n       64     x 473  (discontiguous)      |    10000  |   40000\n       16384  x 12642115 (discontiguous)  |       31  |      78\n       8192   x 892                       |     4800  |   20400\n       512    x 64   (discontiguous)      |   110000  |  400000\n       493    x 27   (discontiguous)      |     1100  |    2440\n       118    x 32   (discontiguous)      |      870  |    2030\n       16     x 495  (discontiguous)      |    23600  |   24000\n       488    x 62374                     |    90000  |  100000\n       240372 x 69                        |    40000  |   16000\n       40156  x 32   (discontiguous)      |     2670  |    5000\n\n Times are in microseconds (us).",
                                "code"
                            ]
                        ]
                    },
                    {
                        "8. Collecting instruction counts with <cite>Callgrind</cite>": [
                            [
                                "One of the challenges of optimizing code is the variation and opacity of\nwall time. There are many sources of non-determinism, from adaptive clock\nspeeds to resource contention with other processes. Furthermore, end-to-end\ntime gives no insight into where time is being spent, which is really what\nwe\u2019re interested in when optimizing code.",
                                "markdown"
                            ],
                            [
                                "A complementary approach is to also collect instruction counts. These counts\nare a proxy metric and do not capture all aspects of performance\n(e.g. memory or I/O bound tasks), however they do have several useful\nproperties. Instruction counts are reproducible, insensitive to environmental\nvariation, and offer fine grained insight into where a program is spending\ncycles.",
                                "markdown"
                            ],
                            [
                                "To see the utility of instruction counts, let us look at how we might\nreduce the overhead of <cite>batched_dot_mul_sum</cite>. The obvious solution is to\nmove it to C++, so we avoid going between Python and C++ multiple times.",
                                "markdown"
                            ],
                            [
                                "Fortunately, the source is nearly identical. One question that we have to ask\nin C++ is whether we should take arguments by value or reference.",
                                "markdown"
                            ],
                            [
                                "batched_dot_src = \"\"\"\\\n/* ---- Python ---- */\n// def batched_dot_mul_sum(a, b):\n//     return a.mul(b).sum(-1)\n\ntorch::Tensor batched_dot_mul_sum_v0(\n    const torch::Tensor a,\n    const torch::Tensor b) {\n  return a.mul(b).sum(-1);\n}\n\ntorch::Tensor batched_dot_mul_sum_v1(\n    const torch::Tensor&amp; a,\n    const torch::Tensor&amp; b) {\n  return a.mul(b).sum(-1);\n}\n\"\"\"\n\n\n# PyTorch makes it easy to test our C++ implementations by providing a utility\n# to JIT compile C++ source into Python extensions:\nimport os\nfrom torch.utils import cpp_extension\ncpp_lib = (\n    name='cpp_lib',\n    cpp_sources=batched_dot_src,\n    extra_cflags=['-O3'],\n    extra_include_paths=[\n        # `load_inline` needs to know where to find Pybind11 headers.\n        os.path.join(os.getenv('CONDA_PREFIX'), 'include')\n    ],\n    functions=['batched_dot_mul_sum_v0', 'batched_dot_mul_sum_v1']\n)\n\n# `load_inline` will create a shared object that is loaded into Python. When we collect\n# instruction counts Timer will create a subprocess, so we need to re-import it. The\n# import process is slightly more complicated for C extensions, but that's all we're\n# doing here.\nmodule_import_str = f\"\"\"\\\n# https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path\nimport importlib.util\nspec = importlib.util.spec_from_file_location(\"cpp_lib\", {repr(cpp_lib.__file__)})\ncpp_lib = importlib.util.module_from_spec(spec)\nspec.loader.exec_module(cpp_lib)\"\"\"\n\nimport textwrap\ndef pretty_print(result):\n    \"\"\"Import machinery for cpp_lib.so can get repetitive to look at.\"\"\"\n    print(repr(result).replace(textwrap.indent(module_import_str, \"  \"), \"  import cpp_lib\"))\n\n\nt_baseline = (\n    stmt='batched_dot_mul_sum(x, x)',\n    setup='''\\\nfrom __main__ import batched_dot_mul_sum\nx = torch.randn(2, 2)''')\n\nt0 = (\n    stmt='cpp_lib.batched_dot_mul_sum_v0(x, x)',\n    setup=f'''\\\n{module_import_str}\nx = torch.randn(2, 2)''')\n\nt1 = (\n    stmt='cpp_lib.batched_dot_mul_sum_v1(x, x)',\n    setup=f'''\\\n{module_import_str}\nx = torch.randn(2, 2)''')\n\n# Moving to C++ did indeed reduce overhead, but it's hard to tell which\n# calling convention is more efficient. v1 (call with references) seems to\n# be a bit faster, but it's within measurement error.\npretty_print(t_baseline.blocked_autorange())\npretty_print(t0.blocked_autorange())\npretty_print(t1.blocked_autorange())\n\n\n\nOutput",
                                "code"
                            ],
                            [
                                " &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;\n batched_dot_mul_sum(x, x)\n setup:\n   from __main__ import batched_dot_mul_sum\n   x = torch.randn(2, 2)\n\n   6.92 us\n   1 measurement, 100000 runs , 1 thread\n &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;\n cpp_lib.batched_dot_mul_sum_v0(x, x)\n setup:\n   import cpp_lib\n   x = torch.randn(2, 2)\n\n   5.29 us\n   1 measurement, 100000 runs , 1 thread\n &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7fb16935d2e8&gt;\n cpp_lib.batched_dot_mul_sum_v1(x, x)\n setup:\n   import cpp_lib\n   x = torch.randn(2, 2)\n\n   5.22 us\n   1 measurement, 100000 runs , 1 thread",
                                "code"
                            ],
                            [
                                "# Let's use Callgrind to determine which is better.\nstats_v0 = t0.collect_callgrind()\nstats_v1 = t1.collect_callgrind()\n\npretty_print(stats_v0)\npretty_print(stats_v1)\n\n# `.as_standardized` removes file names and some path prefixes, and makes\n# it easier to read the function symbols.\nstats_v0 = stats_v0.as_standardized()\nstats_v1 = stats_v1.as_standardized()\n\n# `.delta` diffs the instruction counts, and `.denoise` removes several\n# functions in the Python interpreter that are known to have significant\n# jitter.\ndelta = stats_v1.delta(stats_v0).denoise()\n\n# `.transform` is a convenience API for transforming function names. It is\n# useful for increasing cancelation when diff-ing instructions, as well as\n# just generally improving readability.\nreplacements = (\n    (\"???:void pybind11\", \"pybind11\"),\n    (\"batched_dot_mul_sum_v0\", \"batched_dot_mul_sum_v1\"),\n    (\"at::Tensor, at::Tensor\", \"...\"),\n    (\"at::Tensor const&amp;, at::Tensor const&amp;\", \"...\"),\n    (\"auto torch::detail::wrap_pybind_function_impl_\", \"wrap_pybind_function_impl_\"),\n)\nfor before, after in replacements:\n    delta = delta.transform(lambda l: l.replace(before, after))\n\n# We can use print options to control how much of the function to display.\n(linewidth=160)\n\n# Once parsed, the instruction counts make clear that passing `a` and `b`\n# by reference is more efficient as it skips some c10::TensorImpl bookkeeping\n# for the intermediate Tensors, and is also works better with PyBind11. This\n# is consistent with our noisy wall time observations.\nprint(delta)\n\n\n\nOutput",
                                "code"
                            ],
                            [
                                " &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fb0f06e7630&gt;\n cpp_lib.batched_dot_mul_sum_v0(x, x)\n setup:\n   import cpp_lib\n   x = torch.randn(2, 2)\n\n                            All          Noisy symbols removed\n     Instructions:      2392671                    2392671\n     Baseline:             4367                       4367\n 100 runs per measurement, 1 thread\n Warning: PyTorch was not built with debug symbols.\n          Source information may be limited. Rebuild with\n          REL_WITH_DEB_INFO=1 for more detailed results.\n &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.CallgrindStats object at 0x7fb10400d208&gt;\n cpp_lib.batched_dot_mul_sum_v1(x, x)\n setup:\n   import cpp_lib\n   x = torch.randn(2, 2)\n\n                            All          Noisy symbols removed\n     Instructions:      2378978                    2378978\n     Baseline:             4367                       4367\n     100 runs per measurement, 1 thread\n     Warning: PyTorch was not built with debug symbols.\n              Source information may be limited. Rebuild with\n              REL_WITH_DEB_INFO=1 for more detailed results.\n     &lt;torch.utils.benchmark.utils.valgrind_wrapper.timer_interface.FunctionCounts object at 0x7fb1000ab358&gt;\n           86  ???:0x000000000020d9e0\n       56  ???:0x000000000020db10\n    -1100  pybind11::cpp_function::initialize&lt;wrap_pybind_function_impl_&lt;at::Tensor ... r (&amp;)(...), std::integer_sequence&lt;unsigned long, 0ul, 1ul&gt;)::{lambda(...)\n    -1600  ???:wrap_pybind_function_impl_&lt;at::Tensor (&amp;)(...), 0ul, 1ul&gt;(at::Tensor (&amp;)(...), std::integer_sequence&lt;unsigned long, 0ul, 1ul&gt;)::{lambda(...)\n    -5200  ???:c10::intrusive_ptr&lt;c10::TensorImpl, c10::UndefinedTensorImpl&gt;::reset_()\n    -5935  ???:0x000000000022c0e0\n\n Total: -13693",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Learn More": [
                    [
                        "Take a look at these other recipes to continue your learning:",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ]
    },
    "Introduction to PyTorch": {
        "Learn the Basics": [
            [
                "Authors:\n,\n,\n,\n,",
                "markdown"
            ],
            [
                "Most machine learning workflows involve working with data, creating models, optimizing model\nparameters, and saving the trained models. This tutorial introduces you to a complete ML workflow\nimplemented in PyTorch, with links to learn more about each of these concepts.",
                "markdown"
            ],
            [
                "We\u2019ll use the FashionMNIST dataset to train a neural network that predicts if an input image belongs\nto one of the following classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker,\nBag, or Ankle boot.",
                "markdown"
            ],
            [
                "<cite>This tutorial assumes a basic familiarity with Python and Deep Learning concepts.</cite>",
                "markdown"
            ],
            {
                "Running the Tutorial Code": [
                    [
                        "You can run this tutorial in a couple of ways:",
                        "markdown"
                    ],
                    [
                        "<strong>In the cloud</strong>: This is the easiest way to get started! Each section has a \u201cRun in Microsoft Learn\u201d and \u201cRun in Google Colab\u201d link at the top, which opens an integrated notebook in Microsoft Learn or Google Colab, respectively, with the code in a fully-hosted environment.",
                        "markdown"
                    ],
                    [
                        "<strong>Locally</strong>: This option requires you to setup PyTorch and TorchVision first on your local machine (). Download the notebook or copy the code into your favorite IDE.",
                        "markdown"
                    ]
                ]
            },
            {
                "How to Use this Guide": [
                    [
                        "If you\u2019re familiar with other deep learning frameworks, check out the  first\nto quickly familiarize yourself with PyTorch\u2019s API.",
                        "markdown"
                    ],
                    [
                        "If you\u2019re new to deep learning frameworks, head right into the first section of our step-by-step guide: .\n\n0. \n1. \n2. \n3. \n4. \n5. \n6. \n7. ",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Quickstart": [
            [
                "This section runs through the API for common tasks in machine learning. Refer to the links in each section to dive deeper.",
                "markdown"
            ],
            {
                "Working with data": [
                    [
                        "PyTorch has two :\ntorch.utils.data.DataLoader and torch.utils.data.Dataset.\nDataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around\nthe Dataset.",
                        "markdown"
                    ],
                    [
                        "import torch\nfrom torch import nn\nfrom torch.utils.data import \nfrom torchvision import datasets\nfrom torchvision.transforms import ",
                        "code"
                    ],
                    [
                        "PyTorch offers domain-specific libraries such as ,\n, and ,\nall of which include datasets. For this tutorial, we  will be using a TorchVision dataset.",
                        "markdown"
                    ],
                    [
                        "The torchvision.datasets module contains Dataset objects for many real-world vision data like\nCIFAR, COCO (). In this tutorial, we\nuse the FashionMNIST dataset. Every TorchVision Dataset includes two arguments: transform and\ntarget_transform to modify the samples and labels respectively.",
                        "markdown"
                    ],
                    [
                        "# Download training data from open datasets.\n = (\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=(),\n)\n\n# Download test data from open datasets.\n = (\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=(),\n)",
                        "code"
                    ],
                    [
                        "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/26421880 [00:00&lt;?, ?it/s]\n  0%|          | 32768/26421880 [00:00&lt;01:28, 299753.88it/s]\n  0%|          | 65536/26421880 [00:00&lt;01:29, 295294.13it/s]\n  0%|          | 131072/26421880 [00:00&lt;01:01, 425672.21it/s]\n  1%|          | 229376/26421880 [00:00&lt;00:43, 602749.34it/s]\n  2%|1         | 425984/26421880 [00:00&lt;00:25, 1020255.52it/s]\n  2%|2         | 589824/26421880 [00:00&lt;00:22, 1169971.20it/s]\n  4%|3         | 983040/26421880 [00:00&lt;00:13, 1930260.95it/s]\n  6%|6         | 1703936/26421880 [00:00&lt;00:07, 3363722.84it/s]\n 12%|#1        | 3145728/26421880 [00:01&lt;00:03, 6338540.79it/s]\n 23%|##2       | 6062080/26421880 [00:01&lt;00:01, 12413292.37it/s]\n 35%|###4      | 9207808/26421880 [00:01&lt;00:01, 17148848.13it/s]\n 46%|####5     | 12025856/26421880 [00:01&lt;00:00, 19444941.06it/s]\n 57%|#####6    | 14974976/26421880 [00:01&lt;00:00, 21433382.45it/s]\n 67%|######6   | 17629184/26421880 [00:01&lt;00:00, 22108810.91it/s]\n 79%|#######8  | 20742144/26421880 [00:01&lt;00:00, 23708973.23it/s]\n 90%|######### | 23855104/26421880 [00:01&lt;00:00, 24886110.73it/s]\n100%|##########| 26421880/26421880 [00:01&lt;00:00, 14551885.44it/s]\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/29515 [00:00&lt;?, ?it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 273566.55it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 271935.02it/s]\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/4422102 [00:00&lt;?, ?it/s]\n  1%|          | 32768/4422102 [00:00&lt;00:14, 299418.88it/s]\n  1%|1         | 65536/4422102 [00:00&lt;00:14, 298442.09it/s]\n  3%|2         | 131072/4422102 [00:00&lt;00:09, 433828.47it/s]\n  5%|5         | 229376/4422102 [00:00&lt;00:06, 615340.99it/s]\n 11%|#1        | 491520/4422102 [00:00&lt;00:03, 1251385.57it/s]\n 21%|##1       | 950272/4422102 [00:00&lt;00:01, 2243412.69it/s]\n 44%|####3     | 1933312/4422102 [00:00&lt;00:00, 4423447.14it/s]\n 87%|########6 | 3833856/4422102 [00:00&lt;00:00, 8515899.82it/s]\n100%|##########| 4422102/4422102 [00:00&lt;00:00, 5009664.10it/s]\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/5148 [00:00&lt;?, ?it/s]\n100%|##########| 5148/5148 [00:00&lt;00:00, 23780040.74it/s]\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw",
                        "code"
                    ],
                    [
                        "We pass the Dataset as an argument to DataLoader. This wraps an iterable over our dataset, and supports\nautomatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element\nin the dataloader iterable will return a batch of 64 features and labels.",
                        "markdown"
                    ],
                    [
                        "batch_size = 64\n\n# Create data loaders.\n = (, batch_size=batch_size)\n = (, batch_size=batch_size)\n\nfor , y in :\n    print(f\"Shape of X [N, C, H, W]: {.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break",
                        "code"
                    ],
                    [
                        "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\nShape of y: torch.Size([64]) torch.int64",
                        "code"
                    ],
                    [
                        "Read more about .",
                        "markdown"
                    ]
                ]
            },
            {
                "Creating Models": [
                    [
                        "To define a neural network in PyTorch, we create a class that inherits\nfrom . We define the layers of the network\nin the __init__ function and specify how data will pass through the network in the forward function. To accelerate\noperations in the neural network, we move it to the GPU if available.",
                        "markdown"
                    ],
                    [
                        "# Get cpu or gpu device for training.\ndevice = \"cuda\" if () else \"mps\" if () else \"cpu\"\nprint(f\"Using {device} device\")\n\n# Define model\nclass NeuralNetwork():\n    def __init__(self):\n        super().__init__()\n        self.flatten = ()\n        self.linear_relu_stack = (\n            (28*28, 512),\n            (),\n            (512, 512),\n            (),\n            (512, 10)\n        )\n\n    def forward(self, ):\n         = self.flatten()\n        logits = self.linear_relu_stack()\n        return logits\n\nmodel = ().to(device)\nprint(model)",
                        "code"
                    ],
                    [
                        "Using cuda device\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)",
                        "code"
                    ],
                    [
                        "Read more about .",
                        "markdown"
                    ]
                ]
            },
            {
                "Optimizing the Model Parameters": [
                    [
                        "To train a model, we need a \nand an .",
                        "markdown"
                    ],
                    [
                        " = ()\n = ((), lr=1e-3)",
                        "code"
                    ],
                    [
                        "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and\nbackpropagates the prediction error to adjust the model\u2019s parameters.",
                        "markdown"
                    ],
                    [
                        "def train(dataloader, model, , ):\n    size = len(dataloader.dataset)\n    ()\n    for batch, (, y) in enumerate(dataloader):\n        , y = .to(device), y.to(device)\n\n        # Compute prediction error\n         = model()\n        loss = (, y)\n\n        # Backpropagation\n        ()\n        loss.backward()\n        ()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len()\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")",
                        "code"
                    ],
                    [
                        "We also check the model\u2019s performance against the test dataset to ensure it is learning.",
                        "markdown"
                    ],
                    [
                        "def test(dataloader, model, ):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    ()\n    test_loss, correct = 0, 0\n    with ():\n        for , y in dataloader:\n            , y = .to(device), y.to(device)\n             = model()\n            test_loss += (, y).item()\n            correct += (.argmax(1) == y).type().sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")",
                        "code"
                    ],
                    [
                        "The training process is conducted over several iterations (<em>epochs</em>). During each epoch, the model learns\nparameters to make better predictions. We print the model\u2019s accuracy and loss at each epoch; we\u2019d like to see the\naccuracy increase and the loss decrease with every epoch.",
                        "markdown"
                    ],
                    [
                        "epochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(, model, , )\n    test(, model, )\nprint(\"Done!\")",
                        "code"
                    ],
                    [
                        "Epoch 1\n-------------------------------\nloss: 2.296901  [   64/60000]\nloss: 2.276506  [ 6464/60000]\nloss: 2.256961  [12864/60000]\nloss: 2.258251  [19264/60000]\nloss: 2.231319  [25664/60000]\nloss: 2.210411  [32064/60000]\nloss: 2.218347  [38464/60000]\nloss: 2.173729  [44864/60000]\nloss: 2.179739  [51264/60000]\nloss: 2.154059  [57664/60000]\nTest Error:\n Accuracy: 46.7%, Avg loss: 2.135564\n\nEpoch 2\n-------------------------------\nloss: 2.144533  [   64/60000]\nloss: 2.123762  [ 6464/60000]\nloss: 2.059470  [12864/60000]\nloss: 2.091630  [19264/60000]\nloss: 2.025001  [25664/60000]\nloss: 1.967701  [32064/60000]\nloss: 2.001757  [38464/60000]\nloss: 1.900702  [44864/60000]\nloss: 1.923718  [51264/60000]\nloss: 1.860082  [57664/60000]\nTest Error:\n Accuracy: 54.1%, Avg loss: 1.845273\n\nEpoch 3\n-------------------------------\nloss: 1.876047  [   64/60000]\nloss: 1.834949  [ 6464/60000]\nloss: 1.711895  [12864/60000]\nloss: 1.777222  [19264/60000]\nloss: 1.656412  [25664/60000]\nloss: 1.615331  [32064/60000]\nloss: 1.642911  [38464/60000]\nloss: 1.530612  [44864/60000]\nloss: 1.575070  [51264/60000]\nloss: 1.480866  [57664/60000]\nTest Error:\n Accuracy: 60.6%, Avg loss: 1.491688\n\nEpoch 4\n-------------------------------\nloss: 1.552643  [   64/60000]\nloss: 1.514459  [ 6464/60000]\nloss: 1.366315  [12864/60000]\nloss: 1.456150  [19264/60000]\nloss: 1.334569  [25664/60000]\nloss: 1.333665  [32064/60000]\nloss: 1.346708  [38464/60000]\nloss: 1.263595  [44864/60000]\nloss: 1.315365  [51264/60000]\nloss: 1.223128  [57664/60000]\nTest Error:\n Accuracy: 62.4%, Avg loss: 1.244198\n\nEpoch 5\n-------------------------------\nloss: 1.311815  [   64/60000]\nloss: 1.292572  [ 6464/60000]\nloss: 1.132950  [12864/60000]\nloss: 1.249755  [19264/60000]\nloss: 1.121590  [25664/60000]\nloss: 1.146547  [32064/60000]\nloss: 1.162641  [38464/60000]\nloss: 1.094516  [44864/60000]\nloss: 1.152009  [51264/60000]\nloss: 1.070775  [57664/60000]\nTest Error:\n Accuracy: 64.2%, Avg loss: 1.087599\n\nDone!",
                        "code"
                    ],
                    [
                        "Read more about .",
                        "markdown"
                    ]
                ]
            },
            {
                "Saving Models": [
                    [
                        "A common way to save a model is to serialize the internal state dictionary (containing the model parameters).",
                        "markdown"
                    ],
                    [
                        "((), \"model.pth\")\nprint(\"Saved PyTorch Model State to model.pth\")",
                        "code"
                    ],
                    [
                        "Saved PyTorch Model State to model.pth",
                        "code"
                    ]
                ]
            },
            {
                "Loading Models": [
                    [
                        "The process for loading a model includes re-creating the model structure and loading\nthe state dictionary into it.",
                        "markdown"
                    ],
                    [
                        "model = ()\n((\"model.pth\"))",
                        "code"
                    ],
                    [
                        "&lt;All keys matched successfully&gt;",
                        "code"
                    ],
                    [
                        "This model can now be used to make predictions.",
                        "markdown"
                    ],
                    [
                        "classes = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\n()\n, y = [0][0], [0][1]\nwith ():\n     = model()\n    predicted, actual = classes[[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')",
                        "code"
                    ],
                    [
                        "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"",
                        "code"
                    ],
                    [
                        "Read more about .",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  51.560 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Tensors": [
            [
                "Tensors are a specialized data structure that are very similar to arrays and matrices.\nIn PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model\u2019s parameters.",
                "markdown"
            ],
            [
                "Tensors are similar to  ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and\nNumPy arrays can often share the same underlying memory, eliminating the need to copy data (see ). Tensors\nare also optimized for automatic differentiation (we\u2019ll see more about that later in the \nsection). If you\u2019re familiar with ndarrays, you\u2019ll be right at home with the Tensor API. If not, follow along!",
                "markdown"
            ],
            [
                "import torch\nimport numpy as np",
                "code"
            ],
            {
                "Initializing a Tensor": [
                    [
                        "Tensors can be initialized in various ways. Take a look at the following examples:",
                        "markdown"
                    ],
                    [
                        "<strong>Directly from data</strong>",
                        "markdown"
                    ],
                    [
                        "Tensors can be created directly from data. The data type is automatically inferred.",
                        "markdown"
                    ],
                    [
                        "data = [[1, 2],[3, 4]]\n = (data)",
                        "code"
                    ],
                    [
                        "<strong>From a NumPy array</strong>",
                        "markdown"
                    ],
                    [
                        "Tensors can be created from NumPy arrays (and vice versa - see ).",
                        "markdown"
                    ],
                    [
                        "np_array = np.array(data)\n = (np_array)",
                        "code"
                    ],
                    [
                        "<strong>From another tensor:</strong>",
                        "markdown"
                    ],
                    [
                        "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden.",
                        "markdown"
                    ],
                    [
                        " = () # retains the properties of x_data\nprint(f\"Ones Tensor: \\n {} \\n\")\n\n = (, dtype=) # overrides the datatype of x_data\nprint(f\"Random Tensor: \\n {} \\n\")",
                        "code"
                    ],
                    [
                        "Ones Tensor:\n tensor([[1, 1],\n        [1, 1]])\n\nRandom Tensor:\n tensor([[0.1539, 0.6085],\n        [0.9970, 0.2234]])",
                        "code"
                    ],
                    [
                        "<strong>With random or constant values:</strong>",
                        "markdown"
                    ],
                    [
                        "shape is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor.",
                        "markdown"
                    ],
                    [
                        "shape = (2,3,)\n = (shape)\n = (shape)\n = (shape)\n\nprint(f\"Random Tensor: \\n {} \\n\")\nprint(f\"Ones Tensor: \\n {} \\n\")\nprint(f\"Zeros Tensor: \\n {}\")",
                        "code"
                    ],
                    [
                        "Random Tensor:\n tensor([[0.0614, 0.3925, 0.6718],\n        [0.0782, 0.7854, 0.7167]])\n\nOnes Tensor:\n tensor([[1., 1., 1.],\n        [1., 1., 1.]])\n\nZeros Tensor:\n tensor([[0., 0., 0.],\n        [0., 0., 0.]])",
                        "code"
                    ]
                ]
            },
            {
                "Attributes of a Tensor": [
                    [
                        "Tensor attributes describe their shape, datatype, and the device on which they are stored.",
                        "markdown"
                    ],
                    [
                        " = (3,4)\n\nprint(f\"Shape of tensor: {.shape}\")\nprint(f\"Datatype of tensor: {}\")\nprint(f\"Device tensor is stored on: {}\")",
                        "code"
                    ],
                    [
                        "Shape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu",
                        "code"
                    ]
                ]
            },
            {
                "Operations on Tensors": [
                    [
                        "Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing,\nindexing, slicing), sampling and more are\ncomprehensively described .",
                        "markdown"
                    ],
                    [
                        "Each of these operations can be run on the GPU (at typically higher speeds than on a\nCPU). If you\u2019re using Colab, allocate a GPU by going to Runtime &gt; Change runtime type &gt; GPU.",
                        "markdown"
                    ],
                    [
                        "By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using\n.to method (after checking for GPU availability). Keep in mind that copying large tensors\nacross devices can be expensive in terms of time and memory!",
                        "markdown"
                    ],
                    [
                        "# We move our tensor to the GPU if available\nif ():\n     = .to(\"cuda\")",
                        "code"
                    ],
                    [
                        "Try out some of the operations from the list.\nIf you\u2019re familiar with the NumPy API, you\u2019ll find the Tensor API a breeze to use.",
                        "markdown"
                    ],
                    [
                        "<strong>Standard numpy-like indexing and slicing:</strong>",
                        "markdown"
                    ],
                    [
                        " = (4, 4)\nprint(f\"First row: {[0]}\")\nprint(f\"First column: {[:, 0]}\")\nprint(f\"Last column: {[..., -1]}\")\n[:,1] = 0\nprint()",
                        "code"
                    ],
                    [
                        "First row: tensor([1., 1., 1., 1.])\nFirst column: tensor([1., 1., 1., 1.])\nLast column: tensor([1., 1., 1., 1.])\ntensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])",
                        "code"
                    ],
                    [
                        "<strong>Joining tensors</strong> You can use torch.cat to concatenate a sequence of tensors along a given dimension.\nSee also ,\nanother tensor joining option that is subtly different from torch.cat.",
                        "markdown"
                    ],
                    [
                        " = ([, , ], dim=1)\nprint()",
                        "code"
                    ],
                    [
                        "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])",
                        "code"
                    ],
                    [
                        "<strong>Arithmetic operations</strong>",
                        "markdown"
                    ],
                    [
                        "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n# ``tensor.T`` returns the transpose of a tensor\n =  @ \n = .matmul()\n\n = ()\n(, , out=)\n\n\n# This computes the element-wise product. z1, z2, z3 will have the same value\n =  * \n = .mul()\n\n = ()\n(, , out=)",
                        "code"
                    ],
                    [
                        "tensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])",
                        "code"
                    ],
                    [
                        "<strong>Single-element tensors</strong> If you have a one-element tensor, for example by aggregating all\nvalues of a tensor into one value, you can convert it to a Python\nnumerical value using item():",
                        "markdown"
                    ],
                    [
                        " = .sum()\nagg_item = .item()\nprint(agg_item, type(agg_item))",
                        "code"
                    ],
                    [
                        "12.0 &lt;class 'float'&gt;",
                        "code"
                    ],
                    [
                        "<strong>In-place operations</strong>\nOperations that store the result into the operand are called in-place. They are denoted by a _ suffix.\nFor example: x.copy_(y), x.t_(), will change x.",
                        "markdown"
                    ],
                    [
                        "print(f\"{} \\n\")\n.add_(5)\nprint()",
                        "code"
                    ],
                    [
                        "tensor([[1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.],\n        [1., 0., 1., 1.]])\n\ntensor([[6., 5., 6., 6.],\n        [6., 5., 6., 6.],\n        [6., 5., 6., 6.],\n        [6., 5., 6., 6.]])",
                        "code"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss\nof history. Hence, their use is discouraged.",
                        "markdown"
                    ]
                ]
            },
            {
                "Bridge with NumPy": [
                    [
                        "Tensors on the CPU and NumPy arrays can share their underlying memory\nlocations, and changing one will change the other.",
                        "markdown"
                    ],
                    {
                        "Tensor to NumPy array": [
                            [
                                " = (5)\nprint(f\"t: {}\")\nn = .numpy()\nprint(f\"n: {n}\")",
                                "code"
                            ],
                            [
                                "t: tensor([1., 1., 1., 1., 1.])\nn: [1. 1. 1. 1. 1.]",
                                "code"
                            ],
                            [
                                "A change in the tensor reflects in the NumPy array.",
                                "markdown"
                            ],
                            [
                                ".add_(1)\nprint(f\"t: {}\")\nprint(f\"n: {n}\")",
                                "code"
                            ],
                            [
                                "t: tensor([2., 2., 2., 2., 2.])\nn: [2. 2. 2. 2. 2.]",
                                "code"
                            ]
                        ]
                    },
                    {
                        "NumPy array to Tensor": [
                            [
                                "n = np.ones(5)\n = (n)",
                                "code"
                            ],
                            [
                                "Changes in the NumPy array reflects in the tensor.",
                                "markdown"
                            ],
                            [
                                "np.add(n, 1, out=n)\nprint(f\"t: {}\")\nprint(f\"n: {n}\")",
                                "code"
                            ],
                            [
                                "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\nn: [2. 2. 2. 2. 2.]",
                                "code"
                            ],
                            [
                                "<strong>Total running time of the script:</strong> ( 0 minutes  0.052 seconds)",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ]
                        ]
                    }
                ]
            }
        ],
        "Datasets & DataLoaders": [
            [
                "Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code\nto be decoupled from our model training code for better readability and modularity.\nPyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset\nthat allow you to use pre-loaded datasets as well as your own data.\nDataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around\nthe Dataset to enable easy access to the samples.",
                "markdown"
            ],
            [
                "PyTorch domain libraries provide a number of pre-loaded datasets (such as FashionMNIST) that\nsubclass torch.utils.data.Dataset and implement functions specific to the particular data.\nThey can be used to prototype and benchmark your model. You can find them\nhere: ,\n, and",
                "markdown"
            ],
            {
                "Loading a Dataset": [
                    [
                        "Here is an example of how to load the  dataset from TorchVision.\nFashion-MNIST is a dataset of Zalando\u2019s article images consisting of 60,000 training examples and 10,000 test examples.\nEach example comprises a 28\u00d728 grayscale image and an associated label from one of 10 classes.\n<dl class=\"simple\">\n<dt>We load the  with the following parameters:</dt><dd>",
                        "markdown"
                    ],
                    [
                        "root is the path where the train/test data is stored,",
                        "markdown"
                    ],
                    [
                        "train specifies training or test dataset,",
                        "markdown"
                    ],
                    [
                        "download=True downloads the data from the internet if it\u2019s not available at root.",
                        "markdown"
                    ],
                    [
                        "transform and target_transform specify the feature and label transformations\n\n</dd>\n</dl>",
                        "markdown"
                    ],
                    [
                        "import torch\nfrom torch.utils.data import \nfrom torchvision import datasets\nfrom torchvision.transforms import \nimport matplotlib.pyplot as plt\n\n\n = (\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=()\n)\n\n = (\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=()\n)",
                        "code"
                    ],
                    [
                        "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/26421880 [00:00&lt;?, ?it/s]\n  0%|          | 32768/26421880 [00:00&lt;01:24, 310618.97it/s]\n  0%|          | 65536/26421880 [00:00&lt;01:26, 305195.85it/s]\n  0%|          | 131072/26421880 [00:00&lt;00:59, 442431.56it/s]\n  1%|          | 229376/26421880 [00:00&lt;00:41, 625119.29it/s]\n  2%|1         | 491520/26421880 [00:00&lt;00:20, 1272491.28it/s]\n  4%|3         | 950272/26421880 [00:00&lt;00:11, 2282203.01it/s]\n  7%|7         | 1933312/26421880 [00:00&lt;00:05, 4499420.27it/s]\n 15%|#4        | 3833856/26421880 [00:00&lt;00:02, 8653918.88it/s]\n 26%|##5       | 6782976/26421880 [00:00&lt;00:01, 14441084.20it/s]\n 34%|###4      | 9109504/26421880 [00:01&lt;00:01, 16595929.65it/s]\n 46%|####6     | 12222464/26421880 [00:01&lt;00:00, 20260355.92it/s]\n 58%|#####7    | 15302656/26421880 [00:01&lt;00:00, 22762341.82it/s]\n 70%|######9   | 18415616/26421880 [00:01&lt;00:00, 24544593.90it/s]\n 81%|########1 | 21528576/26421880 [00:01&lt;00:00, 25781543.68it/s]\n 93%|#########3| 24674304/26421880 [00:01&lt;00:00, 26665755.25it/s]\n100%|##########| 26421880/26421880 [00:01&lt;00:00, 16091903.18it/s]\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/29515 [00:00&lt;?, ?it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 272663.94it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 271309.25it/s]\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/4422102 [00:00&lt;?, ?it/s]\n  1%|          | 32768/4422102 [00:00&lt;00:14, 306014.93it/s]\n  1%|1         | 65536/4422102 [00:00&lt;00:14, 304736.93it/s]\n  3%|2         | 131072/4422102 [00:00&lt;00:09, 442373.51it/s]\n  5%|5         | 229376/4422102 [00:00&lt;00:06, 628505.08it/s]\n 11%|#1        | 491520/4422102 [00:00&lt;00:03, 1277413.09it/s]\n 21%|##1       | 950272/4422102 [00:00&lt;00:01, 2289352.69it/s]\n 44%|####3     | 1933312/4422102 [00:00&lt;00:00, 4490802.21it/s]\n 87%|########6 | 3833856/4422102 [00:00&lt;00:00, 8707933.22it/s]\n100%|##########| 4422102/4422102 [00:00&lt;00:00, 5107828.86it/s]\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/5148 [00:00&lt;?, ?it/s]\n100%|##########| 5148/5148 [00:00&lt;00:00, 21965693.79it/s]\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw",
                        "code"
                    ]
                ]
            },
            {
                "Iterating and Visualizing the Dataset": [
                    [
                        "We can index Datasets manually like a list: training_data[index].\nWe use matplotlib to visualize some samples in our training data.",
                        "markdown"
                    ],
                    [
                        "labels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = (len(), size=(1,)).item()\n    ,  = [sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[])\n    plt.axis(\"off\")\n    plt.imshow(.squeeze(), cmap=\"gray\")\nplt.show()\n\n\n<img alt=\"Pullover, Sandal, Bag, Bag, Sneaker, Bag, Pullover, Sandal, Sandal\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_data_tutorial_001.png\" srcset=\"../../_images/sphx_glr_data_tutorial_001.png\"/>",
                        "code"
                    ]
                ]
            },
            {
                "Creating a Custom Dataset for your files": [
                    [
                        "A custom Dataset class must implement three functions: <cite>__init__</cite>, <cite>__len__</cite>, and <cite>__getitem__</cite>.\nTake a look at this implementation; the FashionMNIST images are stored\nin a directory img_dir, and their labels are stored separately in a CSV file annotations_file.",
                        "markdown"
                    ],
                    [
                        "In the next sections, we\u2019ll break down what\u2019s happening in each of these functions.",
                        "markdown"
                    ],
                    [
                        "import os\nimport pandas as pd\nfrom torchvision.io import \n\nclass CustomImageDataset():\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = (img_path)\n         = self.img_labels.iloc[idx, 1]\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n             = self.target_transform()\n        return image, ",
                        "code"
                    ],
                    {
                        "__init__": [
                            [
                                "The __init__ function is run once when instantiating the Dataset object. We initialize\nthe directory containing the images, the annotations file, and both transforms (covered\nin more detail in the next section).",
                                "markdown"
                            ],
                            [
                                "The labels.csv file looks like:",
                                "markdown"
                            ],
                            [
                                "tshirt1.jpg, 0\ntshirt2.jpg, 0\n......\nankleboot999.jpg, 9",
                                "code"
                            ],
                            [
                                "def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n    self.img_labels = pd.read_csv(annotations_file)\n    self.img_dir = img_dir\n    self.transform = transform\n    self.target_transform = target_transform",
                                "code"
                            ]
                        ]
                    },
                    {
                        "__len__": [
                            [
                                "The __len__ function returns the number of samples in our dataset.",
                                "markdown"
                            ],
                            [
                                "Example:",
                                "markdown"
                            ],
                            [
                                "def __len__(self):\n    return len(self.img_labels)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "__getitem__": [
                            [
                                "The __getitem__ function loads and returns a sample from the dataset at the given index idx.\nBased on the index, it identifies the image\u2019s location on disk, converts that to a tensor using read_image, retrieves the\ncorresponding label from the csv data in self.img_labels, calls the transform functions on them (if applicable), and returns the\ntensor image and corresponding label in a tuple.",
                                "markdown"
                            ],
                            [
                                "def __getitem__(self, idx):\n    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n    image = (img_path)\n     = self.img_labels.iloc[idx, 1]\n    if self.transform:\n        image = self.transform(image)\n    if self.target_transform:\n         = self.target_transform()\n    return image, ",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Preparing your data for training with DataLoaders": [
                    [
                        "The Dataset retrieves our dataset\u2019s features and labels one sample at a time. While training a model, we typically want to\npass samples in \u201cminibatches\u201d, reshuffle the data at every epoch to reduce model overfitting, and use Python\u2019s multiprocessing to\nspeed up data retrieval.",
                        "markdown"
                    ],
                    [
                        "DataLoader is an iterable that abstracts this complexity for us in an easy API.",
                        "markdown"
                    ],
                    [
                        "from torch.utils.data import \n\n = (, batch_size=64, shuffle=True)\n = (, batch_size=64, shuffle=True)",
                        "code"
                    ]
                ]
            },
            {
                "Iterate through the DataLoader": [
                    [
                        "We have loaded that dataset into the DataLoader and can iterate through the dataset as needed.\nEach iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively).\nBecause we specified shuffle=True, after we iterate over all batches the data is shuffled (for finer-grained control over\nthe data loading order, take a look at ).",
                        "markdown"
                    ],
                    [
                        "# Display image and label.\n,  = next(iter())\nprint(f\"Feature batch shape: {.size()}\")\nprint(f\"Labels batch shape: {.size()}\")\n = [0].squeeze()\n = [0]\nplt.imshow(, cmap=\"gray\")\nplt.show()\nprint(f\"Label: {}\")\n\n\n<img alt=\"data tutorial\" class=\"sphx-glr-single-img\" src=\"../../_images/sphx_glr_data_tutorial_002.png\" srcset=\"../../_images/sphx_glr_data_tutorial_002.png\"/>",
                        "code"
                    ],
                    [
                        "Feature batch shape: torch.Size([64, 1, 28, 28])\nLabels batch shape: torch.Size([64])\nLabel: 5",
                        "code"
                    ]
                ]
            },
            {
                "Further Reading": [
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  6.518 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Transforms": [
            [
                "Data does not always come in its final processed form that is required for\ntraining machine learning algorithms. We use <strong>transforms</strong> to perform some\nmanipulation of the data and make it suitable for training.",
                "markdown"
            ],
            [
                "All TorchVision datasets have two parameters -transform to modify the features and\ntarget_transform to modify the labels - that accept callables containing the transformation logic.\nThe  module offers\nseveral commonly-used transforms out of the box.",
                "markdown"
            ],
            [
                "The FashionMNIST features are in PIL Image format, and the labels are integers.\nFor training, we need the features as normalized tensors, and the labels as one-hot encoded tensors.\nTo make these transformations, we use ToTensor and Lambda.",
                "markdown"
            ],
            [
                "import torch\nfrom torchvision import datasets\nfrom torchvision.transforms import , \n\n = (\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=(),\n    =(lambda y: (10, dtype=).scatter_(0, (y), value=1))\n)",
                "code"
            ],
            [
                "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/26421880 [00:00&lt;?, ?it/s]\n  0%|          | 32768/26421880 [00:00&lt;01:28, 296688.04it/s]\n  0%|          | 65536/26421880 [00:00&lt;01:29, 295179.74it/s]\n  0%|          | 131072/26421880 [00:00&lt;01:01, 429092.71it/s]\n  1%|          | 229376/26421880 [00:00&lt;00:42, 609327.21it/s]\n  2%|1         | 491520/26421880 [00:00&lt;00:20, 1235121.14it/s]\n  4%|3         | 950272/26421880 [00:00&lt;00:11, 2220353.87it/s]\n  7%|7         | 1933312/26421880 [00:00&lt;00:05, 4376227.17it/s]\n 15%|#4        | 3833856/26421880 [00:00&lt;00:02, 8429427.68it/s]\n 26%|##5       | 6848512/26421880 [00:01&lt;00:01, 14260366.18it/s]\n 37%|###6      | 9732096/26421880 [00:01&lt;00:00, 17771879.75it/s]\n 49%|####8     | 12845056/26421880 [00:01&lt;00:00, 20850503.68it/s]\n 59%|#####9    | 15630336/26421880 [00:01&lt;00:00, 22063757.95it/s]\n 71%|#######   | 18743296/26421880 [00:01&lt;00:00, 23831016.47it/s]\n 83%|########2 | 21823488/26421880 [00:01&lt;00:00, 24969364.88it/s]\n 94%|#########4| 24936448/26421880 [00:01&lt;00:00, 25802545.97it/s]\n100%|##########| 26421880/26421880 [00:01&lt;00:00, 15634638.40it/s]\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/29515 [00:00&lt;?, ?it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 271609.85it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 270390.73it/s]\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/4422102 [00:00&lt;?, ?it/s]\n  1%|          | 32768/4422102 [00:00&lt;00:14, 295442.48it/s]\n  1%|1         | 65536/4422102 [00:00&lt;00:14, 294316.69it/s]\n  3%|2         | 131072/4422102 [00:00&lt;00:10, 428063.45it/s]\n  4%|4         | 196608/4422102 [00:00&lt;00:08, 490174.10it/s]\n 10%|9         | 425984/4422102 [00:00&lt;00:03, 1054556.38it/s]\n 19%|#8        | 819200/4422102 [00:00&lt;00:01, 1895885.94it/s]\n 37%|###7      | 1638400/4422102 [00:00&lt;00:00, 3676846.97it/s]\n 75%|#######4  | 3309568/4422102 [00:00&lt;00:00, 7256005.57it/s]\n100%|##########| 4422102/4422102 [00:00&lt;00:00, 4929402.27it/s]\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/5148 [00:00&lt;?, ?it/s]\n100%|##########| 5148/5148 [00:00&lt;00:00, 21004160.50it/s]\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw",
                "code"
            ],
            {
                "ToTensor()": [
                    [
                        "converts a PIL image or NumPy ndarray into a FloatTensor. and scales\nthe image\u2019s pixel intensity values in the range [0., 1.]",
                        "markdown"
                    ]
                ]
            },
            {
                "Lambda Transforms": [
                    [
                        "Lambda transforms apply any user-defined lambda function. Here, we define a function\nto turn the integer into a one-hot encoded tensor.\nIt first creates a zero tensor of size 10 (the number of labels in our dataset) and calls\n which assigns a\nvalue=1 on the index as given by the label y.",
                        "markdown"
                    ],
                    [
                        " = (lambda y: (\n    10, dtype=).scatter_(dim=0, index=(y), value=1))",
                        "code"
                    ],
                    {
                        "Further Reading": [
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "<strong>Total running time of the script:</strong> ( 0 minutes  5.203 seconds)",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ]
                        ]
                    }
                ]
            }
        ],
        "Build the Neural Network": [
            [
                "Neural networks comprise of layers/modules that perform operations on data.\nThe  namespace provides all the building blocks you need to\nbuild your own neural network. Every module in PyTorch subclasses the .\nA neural network is a module itself that consists of other modules (layers). This nested structure allows for\nbuilding and managing complex architectures easily.",
                "markdown"
            ],
            [
                "In the following sections, we\u2019ll build a neural network to classify images in the FashionMNIST dataset.",
                "markdown"
            ],
            [
                "import os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms",
                "code"
            ],
            {
                "Get Device for Training": [
                    [
                        "We want to be able to train our model on a hardware accelerator like the GPU,\nif it is available. Let\u2019s check to see if\n is available, else we\ncontinue to use the CPU.",
                        "markdown"
                    ],
                    [
                        "device = \"cuda\" if () else \"cpu\"\nprint(f\"Using {device} device\")",
                        "code"
                    ],
                    [
                        "Using cuda device",
                        "code"
                    ]
                ]
            },
            {
                "Define the Class": [
                    [
                        "We define our neural network by subclassing nn.Module, and\ninitialize the neural network layers in __init__. Every nn.Module subclass implements\nthe operations on input data in the forward method.",
                        "markdown"
                    ],
                    [
                        "class NeuralNetwork():\n    def __init__(self):\n        super().__init__()\n        self. = ()\n        self.linear_relu_stack = (\n            (28*28, 512),\n            (),\n            (512, 512),\n            (),\n            (512, 10),\n        )\n\n    def forward(self, x):\n        x = self.(x)\n         = self.linear_relu_stack(x)\n        return ",
                        "code"
                    ],
                    [
                        "We create an instance of NeuralNetwork, and move it to the device, and print\nits structure.",
                        "markdown"
                    ],
                    [
                        "model = ().to(device)\nprint(model)",
                        "code"
                    ],
                    [
                        "NeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)",
                        "code"
                    ],
                    [
                        "To use the model, we pass it the input data. This executes the model\u2019s forward,\nalong with some .\nDo not call model.forward() directly!",
                        "markdown"
                    ],
                    [
                        "Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output.\nWe get the prediction probabilities by passing it through an instance of the nn.Softmax module.",
                        "markdown"
                    ],
                    [
                        " = (1, 28, 28, device=device)\n = model()\n = (dim=1)()\n = .argmax(1)\nprint(f\"Predicted class: {}\")",
                        "code"
                    ],
                    [
                        "Predicted class: tensor([1], device='cuda:0')",
                        "code"
                    ]
                ]
            },
            {
                "Model Layers": [
                    [
                        "Let\u2019s break down the layers in the FashionMNIST model. To illustrate it, we\nwill take a sample minibatch of 3 images of size 28x28 and see what happens to it as\nwe pass it through the network.",
                        "markdown"
                    ],
                    [
                        " = (3,28,28)\nprint(.size())",
                        "code"
                    ],
                    [
                        "torch.Size([3, 28, 28])",
                        "code"
                    ],
                    {
                        "nn.Flatten": [
                            [
                                "We initialize the \nlayer to convert each 2D 28x28 image into a contiguous array of 784 pixel values (\nthe minibatch dimension (at dim=0) is maintained).",
                                "markdown"
                            ],
                            [
                                " = ()\n = ()\nprint(.size())",
                                "code"
                            ],
                            [
                                "torch.Size([3, 784])",
                                "code"
                            ]
                        ]
                    },
                    {
                        "nn.Linear": [
                            [
                                "The \nis a module that applies a linear transformation on the input using its stored weights and biases.",
                                "markdown"
                            ],
                            [
                                " = (in_features=28*28, out_features=20)\n = ()\nprint(.size())",
                                "code"
                            ],
                            [
                                "torch.Size([3, 20])",
                                "code"
                            ]
                        ]
                    },
                    {
                        "nn.ReLU": [
                            [
                                "Non-linear activations are what create the complex mappings between the model\u2019s inputs and outputs.\nThey are applied after linear transformations to introduce <em>nonlinearity</em>, helping neural networks\nlearn a wide variety of phenomena.",
                                "markdown"
                            ],
                            [
                                "In this model, we use  between our\nlinear layers, but there\u2019s other activations to introduce non-linearity in your model.",
                                "markdown"
                            ],
                            [
                                "print(f\"Before ReLU: {}\\n\\n\")\n = ()()\nprint(f\"After ReLU: {}\")",
                                "code"
                            ],
                            [
                                "Before ReLU: tensor([[ 0.1789,  0.1301, -0.1497,  0.5988,  0.0768, -0.0540, -0.2174, -0.3324,\n         -0.5413, -0.1044,  0.5615, -0.2167,  0.0709, -0.0665, -0.1761,  0.5780,\n         -0.2716, -0.2387, -0.1427,  0.2420],\n        [-0.3099, -0.2667,  0.0975,  0.3754,  0.2276, -0.6589, -0.4287, -0.0892,\n         -0.6801, -0.3452,  0.2156, -0.1987,  0.0013,  0.3602, -0.2906,  0.5432,\n         -0.2488, -0.1862, -0.0737,  0.0740],\n        [ 0.1613, -0.0959, -0.1895,  0.4352,  0.3404, -0.2839, -0.4150, -0.4820,\n         -0.4538, -0.3342,  0.5201, -0.3329, -0.1264,  0.1956, -0.2441,  0.2632,\n         -0.1860, -0.2550, -0.1155,  0.2239]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nAfter ReLU: tensor([[0.1789, 0.1301, 0.0000, 0.5988, 0.0768, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.5615, 0.0000, 0.0709, 0.0000, 0.0000, 0.5780, 0.0000, 0.0000,\n         0.0000, 0.2420],\n        [0.0000, 0.0000, 0.0975, 0.3754, 0.2276, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.2156, 0.0000, 0.0013, 0.3602, 0.0000, 0.5432, 0.0000, 0.0000,\n         0.0000, 0.0740],\n        [0.1613, 0.0000, 0.0000, 0.4352, 0.3404, 0.0000, 0.0000, 0.0000, 0.0000,\n         0.0000, 0.5201, 0.0000, 0.0000, 0.1956, 0.0000, 0.2632, 0.0000, 0.0000,\n         0.0000, 0.2239]], grad_fn=&lt;ReluBackward0&gt;)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "nn.Sequential": [
                            [
                                " is an ordered\ncontainer of modules. The data is passed through all the modules in the same order as defined. You can use\nsequential containers to put together a quick network like seq_modules.",
                                "markdown"
                            ],
                            [
                                " = (\n    ,\n    ,\n    (),\n    (20, 10)\n)\n = (3,28,28)\n = ()",
                                "code"
                            ]
                        ]
                    },
                    {
                        "nn.Softmax": [
                            [
                                "The last linear layer of the neural network returns <cite>logits</cite> - raw values in [-infty, infty] - which are passed to the\n module. The logits are scaled to values\n[0, 1] representing the model\u2019s predicted probabilities for each class. dim parameter indicates the dimension along\nwhich the values must sum to 1.",
                                "markdown"
                            ],
                            [
                                " = (dim=1)\n = ()",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Model Parameters": [
                    [
                        "Many layers inside a neural network are <em>parameterized</em>, i.e. have associated weights\nand biases that are optimized during training. Subclassing nn.Module automatically\ntracks all fields defined inside your model object, and makes all parameters\naccessible using your model\u2019s parameters() or named_parameters() methods.",
                        "markdown"
                    ],
                    [
                        "In this example, we iterate over each parameter, and print its size and a preview of its values.",
                        "markdown"
                    ],
                    [
                        "print(f\"Model structure: {model}\\n\\n\")\n\nfor name,  in ():\n    print(f\"Layer: {name} | Size: {.size()} | Values : {[:2]} \\n\")",
                        "code"
                    ],
                    [
                        "Model structure: NeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n\n\nLayer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0110,  0.0225,  0.0328,  ..., -0.0281,  0.0314,  0.0240],\n        [ 0.0006, -0.0191, -0.0094,  ..., -0.0357,  0.0324, -0.0114]],\n       device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nLayer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([ 0.0270, -0.0135], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nLayer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0410,  0.0228, -0.0157,  ...,  0.0431,  0.0202, -0.0306],\n        [ 0.0050,  0.0058, -0.0246,  ...,  0.0314,  0.0082, -0.0298]],\n       device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nLayer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([0.0380, 0.0157], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nLayer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0139,  0.0366, -0.0057,  ..., -0.0234,  0.0136,  0.0403],\n        [-0.0083,  0.0247, -0.0343,  ...,  0.0433,  0.0206,  0.0151]],\n       device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nLayer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0422, 0.0322], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)",
                        "code"
                    ]
                ]
            },
            {
                "Further Reading": [
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.053 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Automatic Differentiation with torch.autograd": [
            [
                "When training neural networks, the most frequently used algorithm is\n<strong>back propagation</strong>. In this algorithm, parameters (model weights) are\nadjusted according to the <strong>gradient</strong> of the loss function with respect\nto the given parameter.",
                "markdown"
            ],
            [
                "To compute those gradients, PyTorch has a built-in differentiation engine\ncalled torch.autograd. It supports automatic computation of gradient for any\ncomputational graph.",
                "markdown"
            ],
            [
                "Consider the simplest one-layer neural network, with input x,\nparameters w and b, and some loss function. It can be defined in\nPyTorch in the following manner:",
                "markdown"
            ],
            [
                "import torch\n\n = (5)  # input tensor\n = (3)  # expected output\n = (5, 3, requires_grad=True)\n = (3, requires_grad=True)\n = (, )+\n = (, )",
                "code"
            ],
            {
                "Tensors, Functions and Computational graph": [
                    [
                        "This code defines the following <strong>computational graph</strong>:\n\n<img alt=\"\" src=\"../../_images/comp-graph.png\"/>",
                        "markdown"
                    ],
                    [
                        "In this network, w and b are <strong>parameters</strong>, which we need to\noptimize. Thus, we need to be able to compute the gradients of loss\nfunction with respect to those variables. In order to do that, we set\nthe requires_grad property of those tensors.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "You can set the value of requires_grad when creating a\ntensor, or later by using x.requires_grad_(True) method.",
                        "markdown"
                    ],
                    [
                        "A function that we apply to tensors to construct computational graph is\nin fact an object of class Function. This object knows how to\ncompute the function in the <em>forward</em> direction, and also how to compute\nits derivative during the <em>backward propagation</em> step. A reference to\nthe backward propagation function is stored in grad_fn property of a\ntensor. You can find more information of Function .",
                        "markdown"
                    ],
                    [
                        "print(f\"Gradient function for z = {.grad_fn}\")\nprint(f\"Gradient function for loss = {.grad_fn}\")",
                        "code"
                    ],
                    [
                        "Gradient function for z = &lt;AddBackward0 object at 0x7f64949ec940&gt;\nGradient function for loss = &lt;BinaryCrossEntropyWithLogitsBackward0 object at 0x7f64949efa90&gt;",
                        "code"
                    ]
                ]
            },
            {
                "Computing Gradients": [
                    [
                        "To optimize weights of parameters in the neural network, we need to\ncompute the derivatives of our loss function with respect to parameters,\nnamely, we need \\(\\frac{\\partial loss}{\\partial w}\\) and\n\\(\\frac{\\partial loss}{\\partial b}\\) under some fixed values of\nx and y. To compute those derivatives, we call\nloss.backward(), and then retrieve the values from w.grad and\nb.grad:",
                        "markdown"
                    ],
                    [
                        "()\nprint()\nprint()",
                        "code"
                    ],
                    [
                        "tensor([[0.0795, 0.1649, 0.2479],\n        [0.0795, 0.1649, 0.2479],\n        [0.0795, 0.1649, 0.2479],\n        [0.0795, 0.1649, 0.2479],\n        [0.0795, 0.1649, 0.2479]])\ntensor([0.0795, 0.1649, 0.2479])",
                        "code"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "We can only obtain the grad properties for the leaf\nnodes of the computational graph, which have requires_grad property\nset to True. For all other nodes in our graph, gradients will not be\navailable.",
                        "markdown"
                    ],
                    [
                        "We can only perform gradient calculations using\nbackward once on a given graph, for performance reasons. If we need\nto do several backward calls on the same graph, we need to pass\nretain_graph=True to the backward call.",
                        "markdown"
                    ]
                ]
            },
            {
                "Disabling Gradient Tracking": [
                    [
                        "By default, all tensors with requires_grad=True are tracking their\ncomputational history and support gradient computation. However, there\nare some cases when we do not need to do that, for example, when we have\ntrained the model and just want to apply it to some input data, i.e. we\nonly want to do <em>forward</em> computations through the network. We can stop\ntracking computations by surrounding our computation code with\ntorch.no_grad() block:",
                        "markdown"
                    ],
                    [
                        " = (, )+\nprint(.requires_grad)\n\nwith ():\n     = (, )+\nprint(.requires_grad)",
                        "code"
                    ],
                    [
                        "True\nFalse",
                        "code"
                    ],
                    [
                        "Another way to achieve the same result is to use the detach() method\non the tensor:",
                        "markdown"
                    ],
                    [
                        " = (, )+\n = .detach()\nprint(.requires_grad)",
                        "code"
                    ],
                    [
                        "False\n\n\n<dl class=\"simple\">\n<dt>There are reasons you might want to disable gradient tracking:</dt><dd>",
                        "code"
                    ],
                    [
                        "To mark some parameters in your neural network as <strong>frozen parameters</strong>.",
                        "markdown"
                    ],
                    [
                        "To <strong>speed up computations</strong> when you are only doing forward pass, because computations on tensors that do\nnot track gradients would be more efficient.\n\n</dd>\n</dl>",
                        "markdown"
                    ]
                ]
            },
            {
                "More on Computational Graphs": [
                    [
                        "Conceptually, autograd keeps a record of data (tensors) and all executed\noperations (along with the resulting new tensors) in a directed acyclic\ngraph (DAG) consisting of\n\nobjects. In this DAG, leaves are the input tensors, roots are the output\ntensors. By tracing this graph from roots to leaves, you can\nautomatically compute the gradients using the chain rule.",
                        "markdown"
                    ],
                    [
                        "In a forward pass, autograd does two things simultaneously:",
                        "markdown"
                    ],
                    [
                        "run the requested operation to compute a resulting tensor",
                        "markdown"
                    ],
                    [
                        "maintain the operation\u2019s <em>gradient function</em> in the DAG.",
                        "markdown"
                    ],
                    [
                        "The backward pass kicks off when .backward() is called on the DAG\nroot. autograd then:",
                        "markdown"
                    ],
                    [
                        "computes the gradients from each .grad_fn,",
                        "markdown"
                    ],
                    [
                        "accumulates them in the respective tensor\u2019s .grad attribute",
                        "markdown"
                    ],
                    [
                        "using the chain rule, propagates all the way to the leaf tensors.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "<strong>DAGs are dynamic in PyTorch</strong>\nAn important thing to note is that the graph is recreated from scratch; after each\n.backward() call, autograd starts populating a new graph. This is\nexactly what allows you to use control flow statements in your model;\nyou can change the shape, size and operations at every iteration if\nneeded.",
                        "markdown"
                    ]
                ]
            },
            {
                "Optional Reading: Tensor Gradients and Jacobian Products": [
                    [
                        "In many cases, we have a scalar loss function, and we need to compute\nthe gradient with respect to some parameters. However, there are cases\nwhen the output function is an arbitrary tensor. In this case, PyTorch\nallows you to compute so-called <strong>Jacobian product</strong>, and not the actual\ngradient.",
                        "markdown"
                    ],
                    [
                        "For a vector function \\(\\vec{y}=f(\\vec{x})\\), where\n\\(\\vec{x}=\\langle x_1,\\dots,x_n\\rangle\\) and\n\\(\\vec{y}=\\langle y_1,\\dots,y_m\\rangle\\), a gradient of\n\\(\\vec{y}\\) with respect to \\(\\vec{x}\\) is given by <strong>Jacobian\nmatrix</strong>:\n\n\\[J=\\left(\\begin{array}{ccc}\n   \\frac{\\partial y_{1}}{\\partial x_{1}} &amp; \\cdots &amp; \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n   \\vdots &amp; \\ddots &amp; \\vdots\\\\\n   \\frac{\\partial y_{m}}{\\partial x_{1}} &amp; \\cdots &amp; \\frac{\\partial y_{m}}{\\partial x_{n}}\n   \\end{array}\\right)\\]",
                        "markdown"
                    ],
                    [
                        "Instead of computing the Jacobian matrix itself, PyTorch allows you to\ncompute <strong>Jacobian Product</strong> \\(v^T\\cdot J\\) for a given input vector\n\\(v=(v_1 \\dots v_m)\\). This is achieved by calling backward with\n\\(v\\) as an argument. The size of \\(v\\) should be the same as\nthe size of the original tensor, with respect to which we want to\ncompute the product:",
                        "markdown"
                    ],
                    [
                        " = (4, 5, requires_grad=True)\n = (+1).pow(2).t()\n((), retain_graph=True)\nprint(f\"First call\\n{}\")\n((), retain_graph=True)\nprint(f\"\\nSecond call\\n{}\")\n.zero_()\n((), retain_graph=True)\nprint(f\"\\nCall after zeroing gradients\\n{}\")",
                        "code"
                    ],
                    [
                        "First call\ntensor([[4., 2., 2., 2., 2.],\n        [2., 4., 2., 2., 2.],\n        [2., 2., 4., 2., 2.],\n        [2., 2., 2., 4., 2.]])\n\nSecond call\ntensor([[8., 4., 4., 4., 4.],\n        [4., 8., 4., 4., 4.],\n        [4., 4., 8., 4., 4.],\n        [4., 4., 4., 8., 4.]])\n\nCall after zeroing gradients\ntensor([[4., 2., 2., 2., 2.],\n        [2., 4., 2., 2., 2.],\n        [2., 2., 4., 2., 2.],\n        [2., 2., 2., 4., 2.]])",
                        "code"
                    ],
                    [
                        "Notice that when we call backward for the second time with the same\nargument, the value of the gradient is different. This happens because\nwhen doing backward propagation, PyTorch <strong>accumulates the\ngradients</strong>, i.e. the value of computed gradients is added to the\ngrad property of all leaf nodes of computational graph. If you want\nto compute the proper gradients, you need to zero out the grad\nproperty before. In real-life training an <em>optimizer</em> helps us to do\nthis.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "Previously we were calling backward() function without\nparameters. This is essentially equivalent to calling\nbackward(torch.tensor(1.0)), which is a useful way to compute the\ngradients in case of a scalar-valued function, such as loss during\nneural network training.",
                        "markdown"
                    ],
                    {
                        "Further Reading": [
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "<strong>Total running time of the script:</strong> ( 0 minutes  0.054 seconds)",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ]
                        ]
                    }
                ]
            }
        ],
        "Optimizing Model Parameters": [
            [
                "Now that we have a model and data it\u2019s time to train, validate and test our model by optimizing its parameters on\nour data. Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates\nthe error in its guess (<em>loss</em>), collects the derivatives of the error with respect to its parameters (as we saw in\nthe ), and <strong>optimizes</strong> these parameters using gradient descent. For a more\ndetailed walkthrough of this process, check out this video on .",
                "markdown"
            ],
            {
                "Prerequisite Code": [
                    [
                        "We load the code from the previous sections on \nand .",
                        "markdown"
                    ],
                    [
                        "import torch\nfrom torch import nn\nfrom torch.utils.data import \nfrom torchvision import datasets\nfrom torchvision.transforms import \n\n = (\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=()\n)\n\n = (\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=()\n)\n\n = (, batch_size=64)\n = (, batch_size=64)\n\nclass NeuralNetwork():\n    def __init__(self):\n        super(, self).__init__()\n        self.flatten = ()\n        self.linear_relu_stack = (\n            (28*28, 512),\n            (),\n            (512, 512),\n            (),\n            (512, 10),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = ()",
                        "code"
                    ],
                    [
                        "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/26421880 [00:00&lt;?, ?it/s]\n  0%|          | 32768/26421880 [00:00&lt;01:25, 308533.34it/s]\n  0%|          | 65536/26421880 [00:00&lt;01:26, 303989.64it/s]\n  0%|          | 131072/26421880 [00:00&lt;00:59, 440126.28it/s]\n  1%|          | 229376/26421880 [00:00&lt;00:41, 623857.36it/s]\n  2%|1         | 491520/26421880 [00:00&lt;00:20, 1266466.02it/s]\n  4%|3         | 950272/26421880 [00:00&lt;00:11, 2269763.66it/s]\n  7%|7         | 1933312/26421880 [00:00&lt;00:05, 4471185.06it/s]\n 15%|#4        | 3833856/26421880 [00:00&lt;00:02, 8607269.59it/s]\n 26%|##6       | 6946816/26421880 [00:00&lt;00:01, 14841015.69it/s]\n 37%|###6      | 9732096/26421880 [00:01&lt;00:00, 18064398.08it/s]\n 49%|####8     | 12877824/26421880 [00:01&lt;00:00, 21318997.08it/s]\n 59%|#####9    | 15695872/26421880 [00:01&lt;00:00, 22636137.26it/s]\n 71%|#######1  | 18808832/26421880 [00:01&lt;00:00, 24427652.64it/s]\n 83%|########2 | 21889024/26421880 [00:01&lt;00:00, 25578163.49it/s]\n 95%|#########4| 25001984/26421880 [00:01&lt;00:00, 26420546.75it/s]\n100%|##########| 26421880/26421880 [00:01&lt;00:00, 16087653.96it/s]\nExtracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/29515 [00:00&lt;?, ?it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 266477.70it/s]\n100%|##########| 29515/29515 [00:00&lt;00:00, 265043.97it/s]\nExtracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/4422102 [00:00&lt;?, ?it/s]\n  1%|          | 32768/4422102 [00:00&lt;00:14, 301419.72it/s]\n  1%|1         | 65536/4422102 [00:00&lt;00:14, 299886.54it/s]\n  3%|2         | 131072/4422102 [00:00&lt;00:09, 435724.08it/s]\n  5%|5         | 229376/4422102 [00:00&lt;00:06, 617548.80it/s]\n 11%|#1        | 491520/4422102 [00:00&lt;00:03, 1257879.50it/s]\n 21%|##1       | 950272/4422102 [00:00&lt;00:01, 2253560.03it/s]\n 44%|####3     | 1933312/4422102 [00:00&lt;00:00, 4446841.74it/s]\n 87%|########6 | 3833856/4422102 [00:00&lt;00:00, 8563990.63it/s]\n100%|##########| 4422102/4422102 [00:00&lt;00:00, 5030366.42it/s]\nExtracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/5148 [00:00&lt;?, ?it/s]\n100%|##########| 5148/5148 [00:00&lt;00:00, 24508827.46it/s]\nExtracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw",
                        "code"
                    ]
                ]
            },
            {
                "Hyperparameters": [
                    [
                        "Hyperparameters are adjustable parameters that let you control the model optimization process.\nDifferent hyperparameter values can impact model training and convergence rates\n( about hyperparameter tuning)\n<dl class=\"simple\">\n<dt>We define the following hyperparameters for training:</dt><dd>",
                        "markdown"
                    ],
                    [
                        "<strong>Number of Epochs</strong> - the number times to iterate over the dataset",
                        "markdown"
                    ],
                    [
                        "<strong>Batch Size</strong> - the number of data samples propagated through the network before the parameters are updated",
                        "markdown"
                    ],
                    [
                        "<strong>Learning Rate</strong> - how much to update models parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training.\n\n</dd>\n</dl>",
                        "markdown"
                    ],
                    [
                        "learning_rate = 1e-3\nbatch_size = 64\nepochs = 5",
                        "code"
                    ]
                ]
            },
            {
                "Optimization Loop": [
                    [
                        "Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each\niteration of the optimization loop is called an <strong>epoch</strong>.\n<dl class=\"simple\">\n<dt>Each epoch consists of two main parts:</dt><dd>",
                        "markdown"
                    ],
                    [
                        "<strong>The Train Loop</strong> - iterate over the training dataset and try to converge to optimal parameters.",
                        "markdown"
                    ],
                    [
                        "<strong>The Validation/Test Loop</strong> - iterate over the test dataset to check if model performance is improving.\n\n</dd>\n</dl>",
                        "markdown"
                    ],
                    [
                        "Let\u2019s briefly familiarize ourselves with some of the concepts used in the training loop. Jump ahead to\nsee the  of the optimization loop.",
                        "markdown"
                    ],
                    {
                        "Loss Function": [
                            [
                                "When presented with some training data, our untrained network is likely not to give the correct\nanswer. <strong>Loss function</strong> measures the degree of dissimilarity of obtained result to the target value,\nand it is the loss function that we want to minimize during training. To calculate the loss we make a\nprediction using the inputs of our given data sample and compare it against the true data label value.",
                                "markdown"
                            ],
                            [
                                "Common loss functions include  (Mean Square Error) for regression tasks, and\n (Negative Log Likelihood) for classification.\n combines nn.LogSoftmax and nn.NLLLoss.",
                                "markdown"
                            ],
                            [
                                "We pass our model\u2019s output logits to nn.CrossEntropyLoss, which will normalize the logits and compute the prediction error.",
                                "markdown"
                            ],
                            [
                                "# Initialize the loss function\n = ()",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Optimizer": [
                            [
                                "Optimization is the process of adjusting model parameters to reduce model error in each training step. <strong>Optimization algorithms</strong> define how this process is performed (in this example we use Stochastic Gradient Descent).\nAll optimization logic is encapsulated in  the optimizer object. Here, we use the SGD optimizer; additionally, there are many \navailable in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.",
                                "markdown"
                            ],
                            [
                                "We initialize the optimizer by registering the model\u2019s parameters that need to be trained, and passing in the learning rate hyperparameter.",
                                "markdown"
                            ],
                            [
                                " = ((), lr=learning_rate)\n\n\n<dl class=\"simple\">\n<dt>Inside the training loop, optimization happens in three steps:</dt><dd>",
                                "code"
                            ],
                            [
                                "Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.",
                                "markdown"
                            ],
                            [
                                "Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.",
                                "markdown"
                            ],
                            [
                                "Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass.\n\n</dd>\n</dl>",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Full Implementation": [
                    [
                        "We define train_loop that loops over our optimization code, and test_loop that\nevaluates the model\u2019s performance against our test data.",
                        "markdown"
                    ],
                    [
                        "def train_loop(dataloader, model, , ):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        # Compute prediction and loss\n        pred = model(X)\n        loss = (pred, y)\n\n        # Backpropagation\n        ()\n        loss.backward()\n        ()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n\n\ndef test_loop(dataloader, model, ):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    test_loss, correct = 0, 0\n\n    with ():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += (pred, y).item()\n            correct += (pred.argmax(1) == y).type().sum().item()\n\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")",
                        "code"
                    ],
                    [
                        "We initialize the loss function and optimizer, and pass it to train_loop and test_loop.\nFeel free to increase the number of epochs to track the model\u2019s improving performance.",
                        "markdown"
                    ],
                    [
                        " = ()\n = ((), lr=learning_rate)\n\nepochs = 10\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train_loop(, model, , )\n    test_loop(, model, )\nprint(\"Done!\")",
                        "code"
                    ],
                    [
                        "Epoch 1\n-------------------------------\nloss: 2.306274  [   64/60000]\nloss: 2.285838  [ 6464/60000]\nloss: 2.269943  [12864/60000]\nloss: 2.261921  [19264/60000]\nloss: 2.245953  [25664/60000]\nloss: 2.216744  [32064/60000]\nloss: 2.223908  [38464/60000]\nloss: 2.186029  [44864/60000]\nloss: 2.199185  [51264/60000]\nloss: 2.156675  [57664/60000]\nTest Error:\n Accuracy: 43.2%, Avg loss: 2.147765\n\nEpoch 2\n-------------------------------\nloss: 2.169220  [   64/60000]\nloss: 2.154036  [ 6464/60000]\nloss: 2.094223  [12864/60000]\nloss: 2.105725  [19264/60000]\nloss: 2.057751  [25664/60000]\nloss: 1.993958  [32064/60000]\nloss: 2.032755  [38464/60000]\nloss: 1.943888  [44864/60000]\nloss: 1.964005  [51264/60000]\nloss: 1.880819  [57664/60000]\nTest Error:\n Accuracy: 49.6%, Avg loss: 1.877261\n\nEpoch 3\n-------------------------------\nloss: 1.923636  [   64/60000]\nloss: 1.892730  [ 6464/60000]\nloss: 1.766222  [12864/60000]\nloss: 1.803750  [19264/60000]\nloss: 1.695832  [25664/60000]\nloss: 1.644898  [32064/60000]\nloss: 1.681978  [38464/60000]\nloss: 1.567808  [44864/60000]\nloss: 1.608559  [51264/60000]\nloss: 1.498805  [57664/60000]\nTest Error:\n Accuracy: 58.4%, Avg loss: 1.512764\n\nEpoch 4\n-------------------------------\nloss: 1.588199  [   64/60000]\nloss: 1.555499  [ 6464/60000]\nloss: 1.396250  [12864/60000]\nloss: 1.469044  [19264/60000]\nloss: 1.354479  [25664/60000]\nloss: 1.347111  [32064/60000]\nloss: 1.376084  [38464/60000]\nloss: 1.281405  [44864/60000]\nloss: 1.330694  [51264/60000]\nloss: 1.232292  [57664/60000]\nTest Error:\n Accuracy: 62.6%, Avg loss: 1.253344\n\nEpoch 5\n-------------------------------\nloss: 1.334092  [   64/60000]\nloss: 1.319183  [ 6464/60000]\nloss: 1.146310  [12864/60000]\nloss: 1.253771  [19264/60000]\nloss: 1.136652  [25664/60000]\nloss: 1.153831  [32064/60000]\nloss: 1.188763  [38464/60000]\nloss: 1.104697  [44864/60000]\nloss: 1.157200  [51264/60000]\nloss: 1.075848  [57664/60000]\nTest Error:\n Accuracy: 64.3%, Avg loss: 1.092215\n\nEpoch 6\n-------------------------------\nloss: 1.164995  [   64/60000]\nloss: 1.170451  [ 6464/60000]\nloss: 0.982307  [12864/60000]\nloss: 1.118346  [19264/60000]\nloss: 1.000636  [25664/60000]\nloss: 1.022203  [32064/60000]\nloss: 1.070253  [38464/60000]\nloss: 0.991670  [44864/60000]\nloss: 1.042103  [51264/60000]\nloss: 0.976573  [57664/60000]\nTest Error:\n Accuracy: 65.3%, Avg loss: 0.986656\n\nEpoch 7\n-------------------------------\nloss: 1.046831  [   64/60000]\nloss: 1.072952  [ 6464/60000]\nloss: 0.869186  [12864/60000]\nloss: 1.026576  [19264/60000]\nloss: 0.912367  [25664/60000]\nloss: 0.928342  [32064/60000]\nloss: 0.991693  [38464/60000]\nloss: 0.918040  [44864/60000]\nloss: 0.961682  [51264/60000]\nloss: 0.910086  [57664/60000]\nTest Error:\n Accuracy: 66.7%, Avg loss: 0.914412\n\nEpoch 8\n-------------------------------\nloss: 0.960576  [   64/60000]\nloss: 1.005463  [ 6464/60000]\nloss: 0.788259  [12864/60000]\nloss: 0.960789  [19264/60000]\nloss: 0.852388  [25664/60000]\nloss: 0.859124  [32064/60000]\nloss: 0.936759  [38464/60000]\nloss: 0.868817  [44864/60000]\nloss: 0.903424  [51264/60000]\nloss: 0.862730  [57664/60000]\nTest Error:\n Accuracy: 68.1%, Avg loss: 0.862345\n\nEpoch 9\n-------------------------------\nloss: 0.894723  [   64/60000]\nloss: 0.954783  [ 6464/60000]\nloss: 0.727722  [12864/60000]\nloss: 0.911476  [19264/60000]\nloss: 0.809471  [25664/60000]\nloss: 0.806541  [32064/60000]\nloss: 0.895423  [38464/60000]\nloss: 0.834586  [44864/60000]\nloss: 0.859842  [51264/60000]\nloss: 0.826616  [57664/60000]\nTest Error:\n Accuracy: 69.3%, Avg loss: 0.822960\n\nEpoch 10\n-------------------------------\nloss: 0.842345  [   64/60000]\nloss: 0.914199  [ 6464/60000]\nloss: 0.680164  [12864/60000]\nloss: 0.873395  [19264/60000]\nloss: 0.776440  [25664/60000]\nloss: 0.765965  [32064/60000]\nloss: 0.861726  [38464/60000]\nloss: 0.809308  [44864/60000]\nloss: 0.825955  [51264/60000]\nloss: 0.797490  [57664/60000]\nTest Error:\n Accuracy: 70.6%, Avg loss: 0.791684\n\nDone!",
                        "code"
                    ]
                ]
            },
            {
                "Further Reading": [
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 1 minutes  43.533 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Save and Load the Model": [
            [
                "In this section we will look at how to persist model state with saving, loading and running model predictions.",
                "markdown"
            ],
            [
                "import torch\nimport torchvision.models as models",
                "code"
            ],
            {
                "Saving and Loading Model Weights": [
                    [
                        "PyTorch models store the learned parameters in an internal\nstate dictionary, called state_dict. These can be persisted via the torch.save\nmethod:",
                        "markdown"
                    ],
                    [
                        "model = (pretrained=True)\n((), 'model_weights.pth')",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning:\n\nThe parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning:\n\nArguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /var/lib/jenkins/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n\n  0%|          | 0.00/528M [00:00&lt;?, ?B/s]\n  0%|          | 1.99M/528M [00:00&lt;00:26, 20.7MB/s]\n  1%|1         | 5.68M/528M [00:00&lt;00:18, 28.8MB/s]\n  2%|1         | 10.4M/528M [00:00&lt;00:14, 36.9MB/s]\n  3%|2         | 14.7M/528M [00:00&lt;00:13, 40.0MB/s]\n  4%|3         | 19.0M/528M [00:00&lt;00:12, 41.7MB/s]\n  4%|4         | 23.0M/528M [00:00&lt;00:12, 41.5MB/s]\n  5%|5         | 28.1M/528M [00:00&lt;00:11, 45.2MB/s]\n  7%|6         | 34.3M/528M [00:00&lt;00:10, 51.6MB/s]\n  8%|7         | 39.8M/528M [00:00&lt;00:09, 53.2MB/s]\n  8%|8         | 44.8M/528M [00:01&lt;00:10, 49.2MB/s]\n 10%|9         | 51.4M/528M [00:01&lt;00:09, 54.9MB/s]\n 11%|#         | 56.7M/528M [00:01&lt;00:10, 47.5MB/s]\n 12%|#1        | 61.5M/528M [00:01&lt;00:12, 38.8MB/s]\n 12%|#2        | 65.6M/528M [00:01&lt;00:12, 39.3MB/s]\n 14%|#3        | 71.4M/528M [00:01&lt;00:10, 44.6MB/s]\n 14%|#4        | 76.0M/528M [00:01&lt;00:11, 41.8MB/s]\n 15%|#5        | 81.0M/528M [00:01&lt;00:10, 44.6MB/s]\n 16%|#6        | 86.5M/528M [00:02&lt;00:09, 47.9MB/s]\n 17%|#7        | 91.2M/528M [00:02&lt;00:10, 42.2MB/s]\n 18%|#8        | 95.7M/528M [00:02&lt;00:10, 43.3MB/s]\n 19%|#9        | 101M/528M [00:02&lt;00:09, 45.4MB/s]\n 20%|#9        | 105M/528M [00:02&lt;00:10, 44.2MB/s]\n 21%|##1       | 111M/528M [00:02&lt;00:08, 48.6MB/s]\n 22%|##1       | 116M/528M [00:02&lt;00:08, 48.6MB/s]\n 23%|##3       | 122M/528M [00:02&lt;00:08, 50.3MB/s]\n 24%|##4       | 127M/528M [00:03&lt;00:09, 42.4MB/s]\n 25%|##5       | 133M/528M [00:03&lt;00:08, 47.9MB/s]\n 26%|##6       | 138M/528M [00:03&lt;00:10, 38.2MB/s]\n 27%|##6       | 142M/528M [00:03&lt;00:13, 30.1MB/s]\n 28%|##8       | 148M/528M [00:03&lt;00:10, 36.5MB/s]\n 29%|##9       | 154M/528M [00:03&lt;00:09, 42.3MB/s]\n 30%|###       | 159M/528M [00:03&lt;00:08, 43.4MB/s]\n 31%|###       | 163M/528M [00:03&lt;00:09, 40.4MB/s]\n 32%|###2      | 169M/528M [00:04&lt;00:08, 44.9MB/s]\n 33%|###2      | 174M/528M [00:04&lt;00:08, 43.5MB/s]\n 34%|###3      | 179M/528M [00:04&lt;00:07, 46.5MB/s]\n 35%|###4      | 183M/528M [00:04&lt;00:07, 45.3MB/s]\n 36%|###5      | 188M/528M [00:04&lt;00:10, 35.2MB/s]\n 37%|###6      | 194M/528M [00:04&lt;00:08, 41.3MB/s]\n 38%|###7      | 198M/528M [00:04&lt;00:08, 38.5MB/s]\n 38%|###8      | 202M/528M [00:04&lt;00:08, 39.5MB/s]\n 39%|###9      | 206M/528M [00:05&lt;00:08, 39.7MB/s]\n 40%|###9      | 210M/528M [00:05&lt;00:08, 40.0MB/s]\n 41%|####1     | 217M/528M [00:05&lt;00:06, 47.6MB/s]\n 42%|####2     | 222M/528M [00:05&lt;00:06, 49.2MB/s]\n 43%|####2     | 227M/528M [00:05&lt;00:06, 48.3MB/s]\n 44%|####3     | 232M/528M [00:05&lt;00:06, 49.3MB/s]\n 45%|####4     | 237M/528M [00:05&lt;00:06, 45.0MB/s]\n 46%|####5     | 241M/528M [00:05&lt;00:06, 45.7MB/s]\n 47%|####6     | 246M/528M [00:05&lt;00:06, 47.6MB/s]\n 48%|####7     | 251M/528M [00:06&lt;00:06, 42.3MB/s]\n 49%|####8     | 257M/528M [00:06&lt;00:06, 47.1MB/s]\n 50%|####9     | 262M/528M [00:06&lt;00:05, 49.6MB/s]\n 51%|#####     | 267M/528M [00:06&lt;00:05, 51.4MB/s]\n 52%|#####1    | 273M/528M [00:06&lt;00:04, 53.8MB/s]\n 53%|#####2    | 279M/528M [00:06&lt;00:04, 57.0MB/s]\n 54%|#####3    | 285M/528M [00:06&lt;00:05, 50.3MB/s]\n 55%|#####4    | 290M/528M [00:06&lt;00:05, 47.5MB/s]\n 56%|#####5    | 295M/528M [00:06&lt;00:05, 47.8MB/s]\n 57%|#####6    | 300M/528M [00:07&lt;00:04, 48.7MB/s]\n 58%|#####7    | 304M/528M [00:07&lt;00:04, 47.6MB/s]\n 59%|#####8    | 309M/528M [00:07&lt;00:06, 37.6MB/s]\n 59%|#####9    | 313M/528M [00:07&lt;00:05, 37.9MB/s]\n 60%|######    | 318M/528M [00:07&lt;00:05, 37.3MB/s]\n 61%|######1   | 322M/528M [00:07&lt;00:06, 34.6MB/s]\n 62%|######2   | 328M/528M [00:07&lt;00:05, 41.1MB/s]\n 63%|######3   | 335M/528M [00:07&lt;00:04, 48.9MB/s]\n 65%|######4   | 341M/528M [00:08&lt;00:03, 52.9MB/s]\n 66%|######5   | 347M/528M [00:08&lt;00:03, 55.3MB/s]\n 67%|######6   | 352M/528M [00:08&lt;00:03, 48.1MB/s]\n 68%|######7   | 358M/528M [00:08&lt;00:03, 50.4MB/s]\n 69%|######8   | 364M/528M [00:08&lt;00:03, 50.3MB/s]\n 70%|######9   | 369M/528M [00:08&lt;00:03, 47.6MB/s]\n 71%|#######1  | 375M/528M [00:08&lt;00:03, 52.6MB/s]\n 72%|#######2  | 380M/528M [00:08&lt;00:03, 47.4MB/s]\n 73%|#######2  | 385M/528M [00:09&lt;00:03, 47.8MB/s]\n 74%|#######3  | 390M/528M [00:09&lt;00:03, 43.6MB/s]\n 75%|#######4  | 395M/528M [00:09&lt;00:03, 43.8MB/s]\n 76%|#######5  | 400M/528M [00:09&lt;00:03, 42.7MB/s]\n 77%|#######6  | 404M/528M [00:09&lt;00:03, 42.7MB/s]\n 78%|#######7  | 409M/528M [00:09&lt;00:02, 45.8MB/s]\n 78%|#######8  | 414M/528M [00:09&lt;00:02, 45.8MB/s]\n 79%|#######9  | 418M/528M [00:09&lt;00:03, 36.5MB/s]\n 80%|########  | 423M/528M [00:10&lt;00:02, 40.5MB/s]\n 81%|########1 | 428M/528M [00:10&lt;00:03, 34.5MB/s]\n 82%|########1 | 431M/528M [00:10&lt;00:03, 30.7MB/s]\n 82%|########2 | 434M/528M [00:10&lt;00:03, 29.4MB/s]\n 83%|########2 | 437M/528M [00:10&lt;00:03, 26.6MB/s]\n 83%|########3 | 440M/528M [00:10&lt;00:03, 26.3MB/s]\n 84%|########4 | 446M/528M [00:10&lt;00:02, 34.2MB/s]\n 85%|########5 | 451M/528M [00:10&lt;00:02, 39.3MB/s]\n 86%|########6 | 455M/528M [00:11&lt;00:02, 30.5MB/s]\n 87%|########7 | 460M/528M [00:11&lt;00:02, 34.7MB/s]\n 88%|########7 | 463M/528M [00:11&lt;00:02, 32.6MB/s]\n 88%|########8 | 467M/528M [00:11&lt;00:01, 32.8MB/s]\n 89%|########9 | 471M/528M [00:11&lt;00:01, 35.4MB/s]\n 90%|########9 | 474M/528M [00:11&lt;00:01, 34.2MB/s]\n 91%|######### | 478M/528M [00:11&lt;00:01, 34.6MB/s]\n 91%|#########1| 483M/528M [00:11&lt;00:01, 39.1MB/s]\n 92%|#########2| 487M/528M [00:12&lt;00:01, 33.9MB/s]\n 93%|#########2| 490M/528M [00:12&lt;00:01, 30.3MB/s]\n 94%|#########3| 496M/528M [00:12&lt;00:00, 34.2MB/s]\n 95%|#########4| 500M/528M [00:12&lt;00:00, 35.4MB/s]\n 96%|#########5| 505M/528M [00:12&lt;00:00, 41.3MB/s]\n 97%|#########6| 512M/528M [00:12&lt;00:00, 48.2MB/s]\n 98%|#########7| 516M/528M [00:12&lt;00:00, 46.8MB/s]\n 99%|#########8| 521M/528M [00:12&lt;00:00, 46.6MB/s]\n100%|#########9| 526M/528M [00:13&lt;00:00, 13.3MB/s]\n100%|##########| 528M/528M [00:14&lt;00:00, 39.1MB/s]",
                        "code"
                    ],
                    [
                        "To load model weights, you need to create an instance of the same model first, and then load the parameters\nusing load_state_dict() method.",
                        "markdown"
                    ],
                    [
                        "model = () # we do not specify pretrained=True, i.e. do not load default weights\n(('model_weights.pth'))\n()",
                        "code"
                    ],
                    [
                        "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)",
                        "code"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode. Failing to do this will yield inconsistent inference results.",
                        "markdown"
                    ]
                ]
            },
            {
                "Saving and Loading Models with Shapes": [
                    [
                        "When loading model weights, we needed to instantiate the model class first, because the class\ndefines the structure of a network. We might want to save the structure of this class together with\nthe model, in which case we can pass model (and not model.state_dict()) to the saving function:",
                        "markdown"
                    ],
                    [
                        "(model, 'model.pth')",
                        "code"
                    ],
                    [
                        "We can then load the model like this:",
                        "markdown"
                    ],
                    [
                        "model = ('model.pth')",
                        "code"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "This approach uses Python  module when serializing the model, thus it relies on the actual class definition to be available when loading the model.",
                        "markdown"
                    ]
                ]
            },
            {
                "Related Tutorials": [
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  20.151 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ]
    },
    "Learning PyTorch": {
        "Deep Learning with PyTorch: A 60 Minute Blitz": [
            [
                "<strong>Author</strong>: \n\n<iframe allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/u7x8RXwLKcA\" width=\"560\"></iframe>",
                "markdown"
            ],
            {
                "What is PyTorch?": [
                    [
                        "PyTorch is a Python-based scientific computing package serving two broad purposes:",
                        "markdown"
                    ],
                    [
                        "A replacement for NumPy to use the power of GPUs and other accelerators.",
                        "markdown"
                    ],
                    [
                        "An automatic differentiation library that is useful to implement neural networks.",
                        "markdown"
                    ]
                ]
            },
            {
                "Goal of this tutorial:": [
                    [
                        "Understand PyTorch\u2019s Tensor library and neural networks at a high level.",
                        "markdown"
                    ],
                    [
                        "Train a small neural network to classify images",
                        "markdown"
                    ],
                    [
                        "To run the tutorials below, make sure you have the  and \npackages installed.\n\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg> Tensors",
                        "markdown"
                    ],
                    [
                        "In this tutorial, you will learn the basics of PyTorch tensors.",
                        "markdown"
                    ],
                    [
                        "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg> A Gentle Introduction to torch.autograd",
                        "markdown"
                    ],
                    [
                        "Learn about autograd.",
                        "markdown"
                    ],
                    [
                        "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg> Neural Networks",
                        "markdown"
                    ],
                    [
                        "This tutorial demonstrates how you can train neural networks in PyTorch.",
                        "markdown"
                    ],
                    [
                        "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg> Training a Classifier",
                        "markdown"
                    ],
                    [
                        "Learn how to train an image classifier in PyTorch by using the\nCIFAR10 dataset.",
                        "markdown"
                    ],
                    [
                        "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code",
                        "markdown"
                    ]
                ]
            }
        ],
        "Learning PyTorch with Examples": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                "This is one of our older PyTorch tutorials. You can view our latest\nbeginner content in\n.",
                "markdown"
            ],
            [
                "This tutorial introduces the fundamental concepts of\n through self-contained\nexamples.",
                "markdown"
            ],
            [
                "At its core, PyTorch provides two main features:",
                "markdown"
            ],
            [
                "An n-dimensional Tensor, similar to numpy but can run on GPUs",
                "markdown"
            ],
            [
                "Automatic differentiation for building and training neural networks",
                "markdown"
            ],
            [
                "We will use a problem of fitting \\(y=\\sin(x)\\) with a third order polynomial\nas our running example. The network will have four parameters, and will be trained with\ngradient descent to fit random data by minimizing the Euclidean distance\nbetween the network output and the true output.",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                "You can browse the individual examples at the\n.",
                "markdown"
            ],
            [
                "Table of Contents",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            {
                "": [
                    {
                        "": [
                            [
                                "Before introducing PyTorch, we will first implement the network using\nnumpy.",
                                "markdown"
                            ],
                            [
                                "Numpy provides an n-dimensional array object, and many functions for\nmanipulating these arrays. Numpy is a generic framework for scientific\ncomputing; it does not know anything about computation graphs, or deep\nlearning, or gradients. However we can easily use numpy to fit a\nthird order polynomial to sine function by manually implementing the forward\nand backward passes through the network using numpy operations:",
                                "markdown"
                            ],
                            [
                                "# -*- coding: utf-8 -*-\nimport numpy as np\nimport math\n\n# Create random input and output data\nx = np.linspace(-math.pi, math.pi, 2000)\ny = np.sin(x)\n\n# Randomly initialize weights\na = np.random.randn()\nb = np.random.randn()\nc = np.random.randn()\nd = np.random.randn()\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y\n    # y = a + b x + c x^2 + d x^3\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss\n    loss = np.square(y_pred - y).sum()\n    if t % 100 == 99:\n        print(t, loss)\n\n    # Backprop to compute gradients of a, b, c, d with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_a = grad_y_pred.sum()\n    grad_b = (grad_y_pred * x).sum()\n    grad_c = (grad_y_pred * x ** 2).sum()\n    grad_d = (grad_y_pred * x ** 3).sum()\n\n    # Update weights\n    a -= learning_rate * grad_a\n    b -= learning_rate * grad_b\n    c -= learning_rate * grad_c\n    d -= learning_rate * grad_d\n\nprint(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')",
                                "code"
                            ]
                        ]
                    },
                    {
                        "": [
                            [
                                "Numpy is a great framework, but it cannot utilize GPUs to accelerate its\nnumerical computations. For modern deep neural networks, GPUs often\nprovide speedups of , so\nunfortunately numpy won\u2019t be enough for modern deep learning.",
                                "markdown"
                            ],
                            [
                                "Here we introduce the most fundamental PyTorch concept: the <strong>Tensor</strong>.\nA PyTorch Tensor is conceptually identical to a numpy array: a Tensor is\nan n-dimensional array, and PyTorch provides many functions for\noperating on these Tensors. Behind the scenes, Tensors can keep track of\na computational graph and gradients, but they\u2019re also useful as a\ngeneric tool for scientific computing.",
                                "markdown"
                            ],
                            [
                                "Also unlike numpy, PyTorch Tensors can utilize GPUs to accelerate\ntheir numeric computations. To run a PyTorch Tensor on GPU, you simply\nneed to specify the correct device.",
                                "markdown"
                            ],
                            [
                                "Here we use PyTorch Tensors to fit a third order polynomial to sine function.\nLike the numpy example above we need to manually implement the forward\nand backward passes through the network:",
                                "markdown"
                            ],
                            [
                                "# -*- coding: utf-8 -*-\n\nimport torch\nimport math\n\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n\n# Create random input and output data\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Randomly initialize weights\na = torch.randn((), device=device, dtype=dtype)\nb = torch.randn((), device=device, dtype=dtype)\nc = torch.randn((), device=device, dtype=dtype)\nd = torch.randn((), device=device, dtype=dtype)\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum().item()\n    if t % 100 == 99:\n        print(t, loss)\n\n    # Backprop to compute gradients of a, b, c, d with respect to loss\n    grad_y_pred = 2.0 * (y_pred - y)\n    grad_a = grad_y_pred.sum()\n    grad_b = (grad_y_pred * x).sum()\n    grad_c = (grad_y_pred * x ** 2).sum()\n    grad_d = (grad_y_pred * x ** 3).sum()\n\n    # Update weights using gradient descent\n    a -= learning_rate * grad_a\n    b -= learning_rate * grad_b\n    c -= learning_rate * grad_c\n    d -= learning_rate * grad_d\n\n\nprint(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "": [
                    {
                        "": [
                            [
                                "In the above examples, we had to manually implement both the forward and\nbackward passes of our neural network. Manually implementing the\nbackward pass is not a big deal for a small two-layer network, but can\nquickly get very hairy for large complex networks.",
                                "markdown"
                            ],
                            [
                                "Thankfully, we can use \nto automate the computation of backward passes in neural networks. The\n<strong>autograd</strong> package in PyTorch provides exactly this functionality.\nWhen using autograd, the forward pass of your network will define a\n<strong>computational graph</strong>; nodes in the graph will be Tensors, and edges\nwill be functions that produce output Tensors from input Tensors.\nBackpropagating through this graph then allows you to easily compute\ngradients.",
                                "markdown"
                            ],
                            [
                                "This sounds complicated, it\u2019s pretty simple to use in practice. Each Tensor\nrepresents a node in a computational graph. If x is a Tensor that has\nx.requires_grad=True then x.grad is another Tensor holding the\ngradient of x with respect to some scalar value.",
                                "markdown"
                            ],
                            [
                                "Here we use PyTorch Tensors and autograd to implement our fitting sine wave\nwith third order polynomial example; now we no longer need to manually\nimplement the backward pass through the network:",
                                "markdown"
                            ],
                            [
                                "# -*- coding: utf-8 -*-\nimport torch\nimport math\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n\n# Create Tensors to hold input and outputs.\n# By default, requires_grad=False, which indicates that we do not need to\n# compute gradients with respect to these Tensors during the backward pass.\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Create random Tensors for weights. For a third order polynomial, we need\n# 4 weights: y = a + b x + c x^2 + d x^3\n# Setting requires_grad=True indicates that we want to compute gradients with\n# respect to these Tensors during the backward pass.\na = torch.randn((), device=device, dtype=dtype, requires_grad=True)\nb = torch.randn((), device=device, dtype=dtype, requires_grad=True)\nc = torch.randn((), device=device, dtype=dtype, requires_grad=True)\nd = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n\nlearning_rate = 1e-6\nfor t in range(2000):\n    # Forward pass: compute predicted y using operations on Tensors.\n    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n\n    # Compute and print loss using operations on Tensors.\n    # Now loss is a Tensor of shape (1,)\n    # loss.item() gets the scalar value held in the loss.\n    loss = (y_pred - y).pow(2).sum()\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Use autograd to compute the backward pass. This call will compute the\n    # gradient of loss with respect to all Tensors with requires_grad=True.\n    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n    # the gradient of the loss with respect to a, b, c, d respectively.\n    loss.backward()\n\n    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n    # because weights have requires_grad=True, but we don't need to track this\n    # in autograd.\n    with torch.no_grad():\n        a -= learning_rate * a.grad\n        b -= learning_rate * b.grad\n        c -= learning_rate * c.grad\n        d -= learning_rate * d.grad\n\n        # Manually zero the gradients after updating weights\n        a.grad = None\n        b.grad = None\n        c.grad = None\n        d.grad = None\n\nprint(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')",
                                "code"
                            ]
                        ]
                    },
                    {
                        "": [
                            [
                                "Under the hood, each primitive autograd operator is really two functions\nthat operate on Tensors. The <strong>forward</strong> function computes output\nTensors from input Tensors. The <strong>backward</strong> function receives the\ngradient of the output Tensors with respect to some scalar value, and\ncomputes the gradient of the input Tensors with respect to that same\nscalar value.",
                                "markdown"
                            ],
                            [
                                "In PyTorch we can easily define our own autograd operator by defining a\nsubclass of torch.autograd.Function and implementing the forward\nand backward functions. We can then use our new autograd operator by\nconstructing an instance and calling it like a function, passing\nTensors containing input data.",
                                "markdown"
                            ],
                            [
                                "In this example we define our model as \\(y=a+b P_3(c+dx)\\) instead of\n\\(y=a+bx+cx^2+dx^3\\), where \\(P_3(x)=\\frac{1}{2}\\left(5x^3-3x\\right)\\)\nis the  of degree three. We write our own custom autograd\nfunction for computing forward and backward of \\(P_3\\), and use it to implement\nour model:",
                                "markdown"
                            ],
                            [
                                "# -*- coding: utf-8 -*-\nimport torch\nimport math\n\n\nclass LegendrePolynomial3(torch.autograd.Function):\n    \"\"\"\n    We can implement our own custom autograd Functions by subclassing\n    torch.autograd.Function and implementing the forward and backward passes\n    which operate on Tensors.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input):\n        \"\"\"\n        In the forward pass we receive a Tensor containing the input and return\n        a Tensor containing the output. ctx is a context object that can be used\n        to stash information for backward computation. You can cache arbitrary\n        objects for use in the backward pass using the ctx.save_for_backward method.\n        \"\"\"\n        ctx.save_for_backward(input)\n        return 0.5 * (5 * input ** 3 - 3 * input)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"\n        In the backward pass we receive a Tensor containing the gradient of the loss\n        with respect to the output, and we need to compute the gradient of the loss\n        with respect to the input.\n        \"\"\"\n        input, = ctx.saved_tensors\n        return grad_output * 1.5 * (5 * input ** 2 - 1)\n\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n\n# Create Tensors to hold input and outputs.\n# By default, requires_grad=False, which indicates that we do not need to\n# compute gradients with respect to these Tensors during the backward pass.\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)\n\n# Create random Tensors for weights. For this example, we need\n# 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n# not too far from the correct result to ensure convergence.\n# Setting requires_grad=True indicates that we want to compute gradients with\n# respect to these Tensors during the backward pass.\na = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\nb = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\nc = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\nd = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n\nlearning_rate = 5e-6\nfor t in range(2000):\n    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n    P3 = LegendrePolynomial3.apply\n\n    # Forward pass: compute predicted y using operations; we compute\n    # P3 using our custom autograd operation.\n    y_pred = a + b * P3(c + d * x)\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum()\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Use autograd to compute the backward pass.\n    loss.backward()\n\n    # Update weights using gradient descent\n    with torch.no_grad():\n        a -= learning_rate * a.grad\n        b -= learning_rate * b.grad\n        c -= learning_rate * c.grad\n        d -= learning_rate * d.grad\n\n        # Manually zero the gradients after updating weights\n        a.grad = None\n        b.grad = None\n        c.grad = None\n        d.grad = None\n\nprint(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "": [
                    {
                        "": [
                            [
                                "Computational graphs and autograd are a very powerful paradigm for\ndefining complex operators and automatically taking derivatives; however\nfor large neural networks raw autograd can be a bit too low-level.",
                                "markdown"
                            ],
                            [
                                "When building neural networks we frequently think of arranging the\ncomputation into <strong>layers</strong>, some of which have <strong>learnable parameters</strong>\nwhich will be optimized during learning.",
                                "markdown"
                            ],
                            [
                                "In TensorFlow, packages like\n,\n,\nand  provide higher-level abstractions\nover raw computational graphs that are useful for building neural\nnetworks.",
                                "markdown"
                            ],
                            [
                                "In PyTorch, the nn package serves this same purpose. The nn\npackage defines a set of <strong>Modules</strong>, which are roughly equivalent to\nneural network layers. A Module receives input Tensors and computes\noutput Tensors, but may also hold internal state such as Tensors\ncontaining learnable parameters. The nn package also defines a set\nof useful loss functions that are commonly used when training neural\nnetworks.",
                                "markdown"
                            ],
                            [
                                "In this example we use the nn package to implement our polynomial model\nnetwork:",
                                "markdown"
                            ],
                            [
                                "# -*- coding: utf-8 -*-\nimport torch\nimport math\n\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# For this example, the output y is a linear function of (x, x^2, x^3), so\n# we can consider it as a linear layer neural network. Let's prepare the\n# tensor (x, x^2, x^3).\np = torch.tensor([1, 2, 3])\nxx = x.unsqueeze(-1).pow(p)\n\n# In the above code, x.unsqueeze(-1) has shape (2000, 1), and p has shape\n# (3,), for this case, broadcasting semantics will apply to obtain a tensor\n# of shape (2000, 3) \n\n# Use the nn package to define our model as a sequence of layers. nn.Sequential\n# is a Module which contains other Modules, and applies them in sequence to\n# produce its output. The Linear Module computes output from input using a\n# linear function, and holds internal Tensors for its weight and bias.\n# The Flatten layer flatens the output of the linear layer to a 1D tensor,\n# to match the shape of `y`.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(3, 1),\n    torch.nn.Flatten(0, 1)\n)\n\n# The nn package also contains definitions of popular loss functions; in this\n# case we will use Mean Squared Error (MSE) as our loss function.\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\nlearning_rate = 1e-6\nfor t in range(2000):\n\n    # Forward pass: compute predicted y by passing x to the model. Module objects\n    # override the __call__ operator so you can call them like functions. When\n    # doing so you pass a Tensor of input data to the Module and it produces\n    # a Tensor of output data.\n    y_pred = model(xx)\n\n    # Compute and print loss. We pass Tensors containing the predicted and true\n    # values of y, and the loss function returns a Tensor containing the\n    # loss.\n    loss = loss_fn(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Zero the gradients before running the backward pass.\n    model.zero_grad()\n\n    # Backward pass: compute gradient of the loss with respect to all the learnable\n    # parameters of the model. Internally, the parameters of each Module are stored\n    # in Tensors with requires_grad=True, so this call will compute gradients for\n    # all learnable parameters in the model.\n    loss.backward()\n\n    # Update the weights using gradient descent. Each parameter is a Tensor, so\n    # we can access its gradients like we did before.\n    with torch.no_grad():\n        for param in model.parameters():\n            param -= learning_rate * param.grad\n\n# You can access the first layer of `model` like accessing the first item of a list\nlinear_layer = model[0]\n\n# For linear layer, its parameters are stored as `weight` and `bias`.\nprint(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')",
                                "code"
                            ]
                        ]
                    },
                    {
                        "": [
                            [
                                "Up to this point we have updated the weights of our models by manually\nmutating the Tensors holding learnable parameters with torch.no_grad().\nThis is not a huge burden for simple optimization algorithms like stochastic\ngradient descent, but in practice we often train neural networks using more\nsophisticated optimizers like AdaGrad, RMSProp, Adam, etc.",
                                "markdown"
                            ],
                            [
                                "The optim package in PyTorch abstracts the idea of an optimization\nalgorithm and provides implementations of commonly used optimization\nalgorithms.",
                                "markdown"
                            ],
                            [
                                "In this example we will use the nn package to define our model as\nbefore, but we will optimize the model using the RMSprop algorithm provided\nby the optim package:",
                                "markdown"
                            ],
                            [
                                "# -*- coding: utf-8 -*-\nimport torch\nimport math\n\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# Prepare the input tensor (x, x^2, x^3).\np = torch.tensor([1, 2, 3])\nxx = x.unsqueeze(-1).pow(p)\n\n# Use the nn package to define our model and loss function.\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(3, 1),\n    torch.nn.Flatten(0, 1)\n)\nloss_fn = torch.nn.MSELoss(reduction='sum')\n\n# Use the optim package to define an Optimizer that will update the weights of\n# the model for us. Here we will use RMSprop; the optim package contains many other\n# optimization algorithms. The first argument to the RMSprop constructor tells the\n# optimizer which Tensors it should update.\nlearning_rate = 1e-3\noptimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\nfor t in range(2000):\n    # Forward pass: compute predicted y by passing x to the model.\n    y_pred = model(xx)\n\n    # Compute and print loss.\n    loss = loss_fn(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Before the backward pass, use the optimizer object to zero all of the\n    # gradients for the variables it will update (which are the learnable\n    # weights of the model). This is because by default, gradients are\n    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n    # is called. Checkout docs of torch.autograd.backward for more details.\n    optimizer.zero_grad()\n\n    # Backward pass: compute gradient of the loss with respect to model\n    # parameters\n    loss.backward()\n\n    # Calling the step function on an Optimizer makes an update to its\n    # parameters\n    optimizer.step()\n\n\nlinear_layer = model[0]\nprint(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')",
                                "code"
                            ]
                        ]
                    },
                    {
                        "": [
                            [
                                "Sometimes you will want to specify models that are more complex than a\nsequence of existing Modules; for these cases you can define your own\nModules by subclassing nn.Module and defining a forward which\nreceives input Tensors and produces output Tensors using other\nmodules or other autograd operations on Tensors.",
                                "markdown"
                            ],
                            [
                                "In this example we implement our third order polynomial as a custom Module\nsubclass:",
                                "markdown"
                            ],
                            [
                                "# -*- coding: utf-8 -*-\nimport torch\nimport math\n\n\nclass Polynomial3(torch.nn.Module):\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate four parameters and assign them as\n        member parameters.\n        \"\"\"\n        super().__init__()\n        self.a = torch.nn.Parameter(torch.randn(()))\n        self.b = torch.nn.Parameter(torch.randn(()))\n        self.c = torch.nn.Parameter(torch.randn(()))\n        self.d = torch.nn.Parameter(torch.randn(()))\n\n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Tensor of input data and we must return\n        a Tensor of output data. We can use Modules defined in the constructor as\n        well as arbitrary operators on Tensors.\n        \"\"\"\n        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n\n    def string(self):\n        \"\"\"\n        Just like any class in Python, you can also define custom method on PyTorch modules\n        \"\"\"\n        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# Construct our model by instantiating the class defined above\nmodel = Polynomial3()\n\n# Construct our loss function and an Optimizer. The call to model.parameters()\n# in the SGD constructor will contain the learnable parameters (defined \n# with torch.nn.Parameter) which are members of the model.\ncriterion = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\nfor t in range(2000):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x)\n\n    # Compute and print loss\n    loss = criterion(y_pred, y)\n    if t % 100 == 99:\n        print(t, loss.item())\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(f'Result: {model.string()}')",
                                "code"
                            ]
                        ]
                    },
                    {
                        "": [
                            [
                                "As an example of dynamic graphs and weight sharing, we implement a very\nstrange model: a third-fifth order polynomial that on each forward pass\nchooses a random number between 3 and 5 and uses that many orders, reusing\nthe same weights multiple times to compute the fourth and fifth order.",
                                "markdown"
                            ],
                            [
                                "For this model we can use normal Python flow control to implement the loop,\nand we can implement weight sharing by simply reusing the same parameter multiple\ntimes when defining the forward pass.",
                                "markdown"
                            ],
                            [
                                "We can easily implement this model as a Module subclass:",
                                "markdown"
                            ],
                            [
                                "# -*- coding: utf-8 -*-\nimport random\nimport torch\nimport math\n\n\nclass DynamicNet(torch.nn.Module):\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate five parameters and assign them as members.\n        \"\"\"\n        super().__init__()\n        self.a = torch.nn.Parameter(torch.randn(()))\n        self.b = torch.nn.Parameter(torch.randn(()))\n        self.c = torch.nn.Parameter(torch.randn(()))\n        self.d = torch.nn.Parameter(torch.randn(()))\n        self.e = torch.nn.Parameter(torch.randn(()))\n\n    def forward(self, x):\n        \"\"\"\n        For the forward pass of the model, we randomly choose either 4, 5\n        and reuse the e parameter to compute the contribution of these orders.\n\n        Since each forward pass builds a dynamic computation graph, we can use normal\n        Python control-flow operators like loops or conditional statements when\n        defining the forward pass of the model.\n\n        Here we also see that it is perfectly safe to reuse the same parameter many\n        times when defining a computational graph.\n        \"\"\"\n        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n        for exp in range(4, random.randint(4, 6)):\n            y = y + self.e * x ** exp\n        return y\n\n    def string(self):\n        \"\"\"\n        Just like any class in Python, you can also define custom method on PyTorch modules\n        \"\"\"\n        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n\n\n# Create Tensors to hold input and outputs.\nx = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)\n\n# Construct our model by instantiating the class defined above\nmodel = DynamicNet()\n\n# Construct our loss function and an Optimizer. Training this strange model with\n# vanilla stochastic gradient descent is tough, so we use momentum\ncriterion = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\nfor t in range(30000):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x)\n\n    # Compute and print loss\n    loss = criterion(y_pred, y)\n    if t % 2000 == 1999:\n        print(t, loss.item())\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(f'Result: {model.string()}')",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "": [
                    [
                        "You can browse the above examples here.",
                        "markdown"
                    ],
                    {
                        "": [
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "": [
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "": [
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ]
                        ]
                    }
                ]
            }
        ],
        "What is torch.nn really?": [
            [
                "by Jeremy Howard, . Thanks to Rachel Thomas and Francisco Ingham.",
                "markdown"
            ],
            [
                "We recommend running this tutorial as a notebook, not a script. To download the notebook (.ipynb) file,\nclick the link at the top of the page.",
                "markdown"
            ],
            [
                "PyTorch provides the elegantly designed modules and classes  ,\n ,\n ,\nand \nto help you create and train neural networks.\nIn order to fully utilize their power and customize\nthem for your problem, you need to really understand exactly what they\u2019re\ndoing. To develop this understanding, we will first train basic neural net\non the MNIST data set without using any features from these models; we will\ninitially only use the most basic PyTorch tensor functionality. Then, we will\nincrementally add one feature from torch.nn, torch.optim, Dataset, or\nDataLoader at a time, showing exactly what each piece does, and how it\nworks to make the code either more concise, or more flexible.",
                "markdown"
            ],
            [
                "<strong>This tutorial assumes you already have PyTorch installed, and are familiar\nwith the basics of tensor operations.</strong> (If you\u2019re familiar with Numpy array\noperations, you\u2019ll find the PyTorch tensor operations used here nearly identical).",
                "markdown"
            ],
            {
                "MNIST data setup": [
                    [
                        "We will use the classic  dataset,\nwhich consists of black-and-white images of hand-drawn digits (between 0 and 9).",
                        "markdown"
                    ],
                    [
                        "We will use \nfor dealing with paths (part of the Python 3 standard library), and will\ndownload the dataset using\n. We will only\nimport modules when we use them, so you can see exactly what\u2019s being\nused at each point.",
                        "markdown"
                    ],
                    [
                        "from pathlib import Path\nimport requests\n\nDATA_PATH = Path(\"data\")\nPATH = DATA_PATH / \"mnist\"\n\nPATH.mkdir(parents=True, exist_ok=True)\n\nURL = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\nFILENAME = \"mnist.pkl.gz\"\n\nif not (PATH / FILENAME).exists():\n        content = requests.get(URL + FILENAME).content\n        (PATH / FILENAME).open(\"wb\").write(content)",
                        "code"
                    ],
                    [
                        "This dataset is in numpy array format, and has been stored using pickle,\na python-specific format for serializing data.",
                        "markdown"
                    ],
                    [
                        "import pickle\nimport gzip\n\nwith gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n        ((, ), (, ), _) = pickle.load(f, encoding=\"latin-1\")",
                        "code"
                    ],
                    [
                        "Each image is 28 x 28, and is being stored as a flattened row of length\n784 (=28x28). Let\u2019s take a look at one; we need to reshape it to 2d\nfirst.",
                        "markdown"
                    ],
                    [
                        "from matplotlib import pyplot\nimport numpy as np\n\npyplot.imshow([0].reshape((28, 28)), cmap=\"gray\")\nprint(.shape)\n\n\n<img alt=\"nn tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_nn_tutorial_001.png\" srcset=\"../_images/sphx_glr_nn_tutorial_001.png\"/>",
                        "code"
                    ],
                    [
                        "(50000, 784)",
                        "code"
                    ],
                    [
                        "PyTorch uses torch.tensor, rather than numpy arrays, so we need to\nconvert our data.",
                        "markdown"
                    ],
                    [
                        "import torch\n\n, , ,  = map(\n    , (, , , )\n)\nn, c = .shape\nprint(, )\nprint(.shape)\nprint(.min(), .max())",
                        "code"
                    ],
                    [
                        "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\ntorch.Size([50000, 784])\ntensor(0) tensor(9)",
                        "code"
                    ]
                ]
            },
            {
                "Neural net from scratch (no torch.nn)": [
                    [
                        "Let\u2019s first create a model using nothing but PyTorch tensor operations. We\u2019re assuming\nyou\u2019re already familiar with the basics of neural networks. (If you\u2019re not, you can\nlearn them at ).",
                        "markdown"
                    ],
                    [
                        "PyTorch provides methods to create random or zero-filled tensors, which we will\nuse to create our weights and bias for a simple linear model. These are just regular\ntensors, with one very special addition: we tell PyTorch that they require a\ngradient. This causes PyTorch to record all of the operations done on the tensor,\nso that it can calculate the gradient during back-propagation <em>automatically</em>!",
                        "markdown"
                    ],
                    [
                        "For the weights, we set requires_grad <strong>after</strong> the initialization, since we\ndon\u2019t want that step included in the gradient. (Note that a trailing _ in\nPyTorch signifies that the operation is performed in-place.)",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "We are initializing the weights here with\n\n(by multiplying with 1/sqrt(n)).",
                        "markdown"
                    ],
                    [
                        "import math\n\n = (784, 10) / math.sqrt(784)\n.requires_grad_()\n = (10, requires_grad=True)",
                        "code"
                    ],
                    [
                        "Thanks to PyTorch\u2019s ability to calculate gradients automatically, we can\nuse any standard Python function (or callable object) as a model! So\nlet\u2019s just write a plain matrix multiplication and broadcasted addition\nto create a simple linear model. We also need an activation function, so\nwe\u2019ll write <cite>log_softmax</cite> and use it. Remember: although PyTorch\nprovides lots of pre-written loss functions, activation functions, and\nso forth, you can easily write your own using plain python. PyTorch will\neven create fast GPU or vectorized CPU code for your function\nautomatically.",
                        "markdown"
                    ],
                    [
                        "def log_softmax(x):\n    return x - x.exp().sum(-1).log().unsqueeze(-1)\n\ndef model():\n    return log_softmax( @  + )",
                        "code"
                    ],
                    [
                        "In the above, the @ stands for the matrix multiplication operation. We will call\nour function on one batch of data (in this case, 64 images).  This is\none <em>forward pass</em>.  Note that our predictions won\u2019t be any better than\nrandom at this stage, since we start with random weights.",
                        "markdown"
                    ],
                    [
                        "bs = 64  # batch size\n\n = [0:bs]  # a mini-batch from x\n = ()  # predictions\n[0], .shape\nprint([0], .shape)",
                        "code"
                    ],
                    [
                        "tensor([-2.2877, -2.4242, -2.4204, -2.2774, -2.2933, -2.4228, -1.8680, -2.6085,\n        -2.9591, -1.9041], grad_fn=&lt;SelectBackward0&gt;) torch.Size([64, 10])",
                        "code"
                    ],
                    [
                        "As you see, the preds tensor contains not only the tensor values, but also a\ngradient function. We\u2019ll use this later to do backprop.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s implement negative log-likelihood to use as the loss function\n(again, we can just use standard Python):",
                        "markdown"
                    ],
                    [
                        "def nll(input, target):\n    return -input[range(target.shape[0]), target].mean()\n\nloss_func = nll",
                        "code"
                    ],
                    [
                        "Let\u2019s check our loss with our random model, so we can see if we improve\nafter a backprop pass later.",
                        "markdown"
                    ],
                    [
                        " = [0:bs]\nprint(loss_func(, ))",
                        "code"
                    ],
                    [
                        "tensor(2.3459, grad_fn=&lt;NegBackward0&gt;)",
                        "code"
                    ],
                    [
                        "Let\u2019s also implement a function to calculate the accuracy of our model.\nFor each prediction, if the index with the largest value matches the\ntarget value, then the prediction was correct.",
                        "markdown"
                    ],
                    [
                        "def accuracy(out, ):\n     = (out, dim=1)\n    return ( == ).float().mean()",
                        "code"
                    ],
                    [
                        "Let\u2019s check the accuracy of our random model, so we can see if our\naccuracy improves as our loss improves.",
                        "markdown"
                    ],
                    [
                        "print(accuracy(, ))",
                        "code"
                    ],
                    [
                        "tensor(0.1094)",
                        "code"
                    ],
                    [
                        "We can now run a training loop.  For each iteration, we will:",
                        "markdown"
                    ],
                    [
                        "select a mini-batch of data (of size bs)",
                        "markdown"
                    ],
                    [
                        "use the model to make predictions",
                        "markdown"
                    ],
                    [
                        "calculate the loss",
                        "markdown"
                    ],
                    [
                        "loss.backward() updates the gradients of the model, in this case, weights\nand bias.",
                        "markdown"
                    ],
                    [
                        "We now use these gradients to update the weights and bias.  We do this\nwithin the torch.no_grad() context manager, because we do not want these\nactions to be recorded for our next calculation of the gradient.  You can read\nmore about how PyTorch\u2019s Autograd records operations\n.",
                        "markdown"
                    ],
                    [
                        "We then set the\ngradients to zero, so that we are ready for the next loop.\nOtherwise, our gradients would record a running tally of all the operations\nthat had happened (i.e. loss.backward() <em>adds</em> the gradients to whatever is\nalready stored, rather than replacing them).",
                        "markdown"
                    ],
                    [
                        "Tip",
                        "markdown"
                    ],
                    [
                        "You can use the standard python debugger to step through PyTorch\ncode, allowing you to check the various variable values at each step.\nUncomment set_trace() below to try it out.",
                        "markdown"
                    ],
                    [
                        "from IPython.core.debugger import set_trace\n\nlr = 0.5  # learning rate\nepochs = 2  # how many epochs to train for\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        #         set_trace()\n        start_i = i * bs\n        end_i = start_i + bs\n         = [start_i:end_i]\n         = [start_i:end_i]\n         = ()\n         = loss_func(, )\n\n        ()\n        with ():\n             -=  * lr\n             -=  * lr\n            .zero_()\n            .zero_()",
                        "code"
                    ],
                    [
                        "That\u2019s it: we\u2019ve created and trained a minimal neural network (in this case, a\nlogistic regression, since we have no hidden layers) entirely from scratch!",
                        "markdown"
                    ],
                    [
                        "Let\u2019s check the loss and accuracy and compare those to what we got\nearlier. We expect that the loss will have decreased and accuracy to\nhave increased, and they have.",
                        "markdown"
                    ],
                    [
                        "print(loss_func((), ), accuracy((), ))",
                        "code"
                    ],
                    [
                        "tensor(0.0807, grad_fn=&lt;NegBackward0&gt;) tensor(1.)",
                        "code"
                    ]
                ]
            },
            {
                "Using torch.nn.functional": [
                    [
                        "We will now refactor our code, so that it does the same thing as before, only\nwe\u2019ll start taking advantage of PyTorch\u2019s nn classes to make it more concise\nand flexible. At each step from here, we should be making our code one or more\nof: shorter, more understandable, and/or more flexible.",
                        "markdown"
                    ],
                    [
                        "The first and easiest step is to make our code shorter by replacing our\nhand-written activation and loss functions with those from torch.nn.functional\n(which is generally imported into the namespace F by convention). This module\ncontains all the functions in the torch.nn library (whereas other parts of the\nlibrary contain classes). As well as a wide range of loss and activation\nfunctions, you\u2019ll also find here some convenient functions for creating neural\nnets, such as pooling functions. (There are also functions for doing convolutions,\nlinear layers, etc, but as we\u2019ll see, these are usually better handled using\nother parts of the library.)",
                        "markdown"
                    ],
                    [
                        "If you\u2019re using negative log likelihood loss and log softmax activation,\nthen Pytorch provides a single function F.cross_entropy that combines\nthe two. So we can even remove the activation function from our model.",
                        "markdown"
                    ],
                    [
                        "import torch.nn.functional as F\n\nloss_func = \n\ndef model():\n    return  @  + ",
                        "code"
                    ],
                    [
                        "Note that we no longer call log_softmax in the model function. Let\u2019s\nconfirm that our loss and accuracy are the same as before:",
                        "markdown"
                    ],
                    [
                        "print(loss_func((), ), accuracy((), ))",
                        "code"
                    ],
                    [
                        "tensor(0.0807, grad_fn=&lt;NllLossBackward0&gt;) tensor(1.)",
                        "code"
                    ]
                ]
            },
            {
                "Refactor using nn.Module": [
                    [
                        "Next up, we\u2019ll use nn.Module and nn.Parameter, for a clearer and more\nconcise training loop. We subclass nn.Module (which itself is a class and\nable to keep track of state).  In this case, we want to create a class that\nholds our weights, bias, and method for the forward step.  nn.Module has a\nnumber of attributes and methods (such as .parameters() and .zero_grad())\nwhich we will be using.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "nn.Module (uppercase M) is a PyTorch specific concept, and is a\nclass we\u2019ll be using a lot. nn.Module is not to be confused with the Python\nconcept of a (lowercase m) ,\nwhich is a file of Python code that can be imported.",
                        "markdown"
                    ],
                    [
                        "from torch import nn\n\nclass Mnist_Logistic():\n    def __init__(self):\n        super().__init__()\n        self. = ((784, 10) / math.sqrt(784))\n        self. = ((10))\n\n    def forward(self, ):\n        return  @ self. + self.",
                        "code"
                    ],
                    [
                        "Since we\u2019re now using an object instead of just using a function, we\nfirst have to instantiate our model:",
                        "markdown"
                    ],
                    [
                        " = ()",
                        "code"
                    ],
                    [
                        "Now we can calculate the loss in the same way as before. Note that\nnn.Module objects are used as if they are functions (i.e they are\n<em>callable</em>), but behind the scenes Pytorch will call our forward\nmethod automatically.",
                        "markdown"
                    ],
                    [
                        "print(loss_func((), ))",
                        "code"
                    ],
                    [
                        "tensor(2.4012, grad_fn=&lt;NllLossBackward0&gt;)",
                        "code"
                    ],
                    [
                        "Previously for our training loop we had to update the values for each parameter\nby name, and manually zero out the grads for each parameter separately, like this:",
                        "markdown"
                    ],
                    [
                        "with ():\n     -=  * lr\n     -=  * lr\n    .zero_()\n    .zero_()",
                        "code"
                    ],
                    [
                        "Now we can take advantage of model.parameters() and model.zero_grad() (which\nare both defined by PyTorch for nn.Module) to make those steps more concise\nand less prone to the error of forgetting some of our parameters, particularly\nif we had a more complicated model:",
                        "markdown"
                    ],
                    [
                        "with ():\n    for p in (): p -= p.grad * lr\n    ()",
                        "code"
                    ],
                    [
                        "We\u2019ll wrap our little training loop in a fit function so we can run it\nagain later.",
                        "markdown"
                    ],
                    [
                        "def fit():\n    for epoch in range(epochs):\n        for i in range((n - 1) // bs + 1):\n            start_i = i * bs\n            end_i = start_i + bs\n             = [start_i:end_i]\n             = [start_i:end_i]\n             = ()\n             = loss_func(, )\n\n            ()\n            with ():\n                for p in ():\n                    p -= p.grad * lr\n                ()\n\nfit()",
                        "code"
                    ],
                    [
                        "Let\u2019s double-check that our loss has gone down:",
                        "markdown"
                    ],
                    [
                        "print(loss_func((), ))",
                        "code"
                    ],
                    [
                        "tensor(0.0809, grad_fn=&lt;NllLossBackward0&gt;)",
                        "code"
                    ]
                ]
            },
            {
                "Refactor using nn.Linear": [
                    [
                        "We continue to refactor our code.  Instead of manually defining and\ninitializing self.weights and self.bias, and calculating xb\u00a0 @\nself.weights + self.bias, we will instead use the Pytorch class\n for a\nlinear layer, which does all that for us. Pytorch has many types of\npredefined layers that can greatly simplify our code, and often makes it\nfaster too.",
                        "markdown"
                    ],
                    [
                        "class Mnist_Logistic():\n    def __init__(self):\n        super().__init__()\n        self.lin = (784, 10)\n\n    def forward(self, ):\n        return self.lin()",
                        "code"
                    ],
                    [
                        "We instantiate our model and calculate the loss in the same way as before:",
                        "markdown"
                    ],
                    [
                        " = ()\nprint(loss_func((), ))",
                        "code"
                    ],
                    [
                        "tensor(2.2902, grad_fn=&lt;NllLossBackward0&gt;)",
                        "code"
                    ],
                    [
                        "We are still able to use our same fit method as before.",
                        "markdown"
                    ],
                    [
                        "fit()\n\nprint(loss_func((), ))",
                        "code"
                    ],
                    [
                        "tensor(0.0802, grad_fn=&lt;NllLossBackward0&gt;)",
                        "code"
                    ]
                ]
            },
            {
                "Refactor using optim": [
                    [
                        "Pytorch also has a package with various optimization algorithms, torch.optim.\nWe can use the step method from our optimizer to take a forward step, instead\nof manually updating each parameter.",
                        "markdown"
                    ],
                    [
                        "This will let us replace our previous manually coded optimization step:",
                        "markdown"
                    ],
                    [
                        "with ():\n    for p in (): p -= p.grad * lr\n    ()",
                        "code"
                    ],
                    [
                        "and instead use just:",
                        "markdown"
                    ],
                    [
                        "()\n()",
                        "code"
                    ],
                    [
                        "(optim.zero_grad() resets the gradient to 0 and we need to call it before\ncomputing the gradient for the next minibatch.)",
                        "markdown"
                    ],
                    [
                        "from torch import optim",
                        "code"
                    ],
                    [
                        "We\u2019ll define a little function to create our model and optimizer so we\ncan reuse it in the future.",
                        "markdown"
                    ],
                    [
                        "def get_model():\n     = ()\n    return , ((), lr=lr)\n\n,  = get_model()\nprint(loss_func((), ))\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        start_i = i * bs\n        end_i = start_i + bs\n         = [start_i:end_i]\n         = [start_i:end_i]\n         = ()\n         = loss_func(, )\n\n        ()\n        ()\n        ()\n\nprint(loss_func((), ))",
                        "code"
                    ],
                    [
                        "tensor(2.2155, grad_fn=&lt;NllLossBackward0&gt;)\ntensor(0.0802, grad_fn=&lt;NllLossBackward0&gt;)",
                        "code"
                    ]
                ]
            },
            {
                "Refactor using Dataset": [
                    [
                        "PyTorch has an abstract Dataset class.  A Dataset can be anything that has\na __len__ function (called by Python\u2019s standard len function) and\na __getitem__ function as a way of indexing into it.\n\nwalks through a nice example of creating a custom FacialLandmarkDataset class\nas a subclass of Dataset.",
                        "markdown"
                    ],
                    [
                        "PyTorch\u2019s \nis a Dataset wrapping tensors. By defining a length and way of indexing,\nthis also gives us a way to iterate, index, and slice along the first\ndimension of a tensor. This will make it easier to access both the\nindependent and dependent variables in the same line as we train.",
                        "markdown"
                    ],
                    [
                        "from torch.utils.data import ",
                        "code"
                    ],
                    [
                        "Both x_train and y_train can be combined in a single TensorDataset,\nwhich will be easier to iterate over and slice.",
                        "markdown"
                    ],
                    [
                        " = (, )",
                        "code"
                    ],
                    [
                        "Previously, we had to iterate through minibatches of x and y values separately:",
                        "markdown"
                    ],
                    [
                        " = [start_i:end_i]\n = [start_i:end_i]",
                        "code"
                    ],
                    [
                        "Now, we can do these two steps together:",
                        "markdown"
                    ],
                    [
                        ", = [i*bs : i*bs+bs]",
                        "code"
                    ],
                    [
                        ",  = get_model()\n\nfor epoch in range(epochs):\n    for i in range((n - 1) // bs + 1):\n        ,  = [i * bs: i * bs + bs]\n         = ()\n         = loss_func(, )\n\n        ()\n        ()\n        ()\n\nprint(loss_func((), ))",
                        "code"
                    ],
                    [
                        "tensor(0.0818, grad_fn=&lt;NllLossBackward0&gt;)",
                        "code"
                    ]
                ]
            },
            {
                "Refactor using DataLoader": [
                    [
                        "Pytorch\u2019s DataLoader is responsible for managing batches. You can\ncreate a DataLoader from any Dataset. DataLoader makes it easier\nto iterate over batches. Rather than having to use train_ds[i*bs : i*bs+bs],\nthe DataLoader gives us each minibatch automatically.",
                        "markdown"
                    ],
                    [
                        "from torch.utils.data import \n\n = (, )\ntrain_dl = (, batch_size=bs)",
                        "code"
                    ],
                    [
                        "Previously, our loop iterated over batches (xb, yb) like this:",
                        "markdown"
                    ],
                    [
                        "for i in range((n-1)//bs + 1):\n    , = [i*bs : i*bs+bs]\n     = ()",
                        "code"
                    ],
                    [
                        "Now, our loop is much cleaner, as (xb, yb) are loaded automatically from the data loader:",
                        "markdown"
                    ],
                    [
                        "for , in train_dl:\n     = ()",
                        "code"
                    ],
                    [
                        ",  = get_model()\n\nfor epoch in range(epochs):\n    for ,  in train_dl:\n         = ()\n         = loss_func(, )\n\n        ()\n        ()\n        ()\n\nprint(loss_func((), ))",
                        "code"
                    ],
                    [
                        "tensor(0.0808, grad_fn=&lt;NllLossBackward0&gt;)",
                        "code"
                    ],
                    [
                        "Thanks to Pytorch\u2019s nn.Module, nn.Parameter, Dataset, and DataLoader,\nour training loop is now dramatically smaller and easier to understand. Let\u2019s\nnow try to add the basic features necessary to create effective models in practice.",
                        "markdown"
                    ]
                ]
            },
            {
                "Add validation": [
                    [
                        "In section 1, we were just trying to get a reasonable training loop set up for\nuse on our training data.  In reality, you <strong>always</strong> should also have\na , in order\nto identify if you are overfitting.",
                        "markdown"
                    ],
                    [
                        "Shuffling the training data is\n\nto prevent correlation between batches and overfitting. On the other hand, the\nvalidation loss will be identical whether we shuffle the validation set or not.\nSince shuffling takes extra time, it makes no sense to shuffle the validation data.",
                        "markdown"
                    ],
                    [
                        "We\u2019ll use a batch size for the validation set that is twice as large as\nthat for the training set. This is because the validation set does not\nneed backpropagation and thus takes less memory (it doesn\u2019t need to\nstore the gradients). We take advantage of this to use a larger batch\nsize and compute the loss more quickly.",
                        "markdown"
                    ],
                    [
                        " = (, )\ntrain_dl = (, batch_size=bs, shuffle=True)\n\n = (, )\nvalid_dl = (, batch_size=bs * 2)",
                        "code"
                    ],
                    [
                        "We will calculate and print the validation loss at the end of each epoch.",
                        "markdown"
                    ],
                    [
                        "(Note that we always call model.train() before training, and model.eval()\nbefore inference, because these are used by layers such as nn.BatchNorm2d\nand nn.Dropout to ensure appropriate behaviour for these different phases.)",
                        "markdown"
                    ],
                    [
                        ",  = get_model()\n\nfor epoch in range(epochs):\n    ()\n    for ,  in train_dl:\n         = ()\n         = loss_func(, )\n\n        ()\n        ()\n        ()\n\n    ()\n    with ():\n         = sum(loss_func((), ) for ,  in valid_dl)\n\n    print(epoch,  / len(valid_dl))",
                        "code"
                    ],
                    [
                        "0 tensor(0.2961)\n1 tensor(0.2847)",
                        "code"
                    ]
                ]
            },
            {
                "Create fit() and get_data()": [
                    [
                        "We\u2019ll now do a little refactoring of our own. Since we go through a similar\nprocess twice of calculating the loss for both the training set and the\nvalidation set, let\u2019s make that into its own function, loss_batch, which\ncomputes the loss for one batch.",
                        "markdown"
                    ],
                    [
                        "We pass an optimizer in for the training set, and use it to perform\nbackprop.  For the validation set, we don\u2019t pass an optimizer, so the\nmethod doesn\u2019t perform backprop.",
                        "markdown"
                    ],
                    [
                        "def loss_batch(, loss_func, , , =None):\n     = loss_func((), )\n\n    if  is not None:\n        ()\n        ()\n        ()\n\n    return .item(), len()",
                        "code"
                    ],
                    [
                        "fit runs the necessary operations to train our model and compute the\ntraining and validation losses for each epoch.",
                        "markdown"
                    ],
                    [
                        "import numpy as np\n\ndef fit(epochs, , loss_func, , train_dl, valid_dl):\n    for epoch in range(epochs):\n        ()\n        for ,  in train_dl:\n            loss_batch(, loss_func, , , )\n\n        ()\n        with ():\n            losses, nums = zip(\n                *[loss_batch(, loss_func, , ) for ,  in valid_dl]\n            )\n        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n\n        print(epoch, val_loss)",
                        "code"
                    ],
                    [
                        "get_data returns dataloaders for the training and validation sets.",
                        "markdown"
                    ],
                    [
                        "def get_data(, , bs):\n    return (\n        (, batch_size=bs, shuffle=True),\n        (, batch_size=bs * 2),\n    )",
                        "code"
                    ],
                    [
                        "Now, our whole process of obtaining the data loaders and fitting the\nmodel can be run in 3 lines of code:",
                        "markdown"
                    ],
                    [
                        "train_dl, valid_dl = get_data(, , bs)\n,  = get_model()\nfit(epochs, , loss_func, , train_dl, valid_dl)",
                        "code"
                    ],
                    [
                        "0 0.42506868764162065\n1 0.3510864285647869",
                        "code"
                    ],
                    [
                        "You can use these basic 3 lines of code to train a wide variety of models.\nLet\u2019s see if we can use them to train a convolutional neural network (CNN)!",
                        "markdown"
                    ]
                ]
            },
            {
                "Switch to CNN": [
                    [
                        "We are now going to build our neural network with three convolutional layers.\nBecause none of the functions in the previous section assume anything about\nthe model form, we\u2019ll be able to use them to train a CNN without any modification.",
                        "markdown"
                    ],
                    [
                        "We will use Pytorch\u2019s predefined\n class\nas our convolutional layer. We define a CNN with 3 convolutional layers.\nEach convolution is followed by a ReLU.  At the end, we perform an\naverage pooling.  (Note that view is PyTorch\u2019s version of numpy\u2019s\nreshape)",
                        "markdown"
                    ],
                    [
                        "class Mnist_CNN():\n    def __init__(self):\n        super().__init__()\n        self.conv1 = (1, 16, kernel_size=3, stride=2, padding=1)\n        self.conv2 = (16, 16, kernel_size=3, stride=2, padding=1)\n        self.conv3 = (16, 10, kernel_size=3, stride=2, padding=1)\n\n    def forward(self, ):\n         = .view(-1, 1, 28, 28)\n         = (self.conv1())\n         = (self.conv2())\n         = (self.conv3())\n         = (, 4)\n        return .view(-1, .size(1))\n\nlr = 0.1",
                        "code"
                    ],
                    [
                        " is a variation on\nstochastic gradient descent that takes previous updates into account as well\nand generally leads to faster training.",
                        "markdown"
                    ],
                    [
                        " = ()\n = ((), lr=lr, momentum=0.9)\n\nfit(epochs, , loss_func, , train_dl, valid_dl)",
                        "code"
                    ],
                    [
                        "0 0.34581177258491513\n1 0.291131191277504",
                        "code"
                    ]
                ]
            },
            {
                "nn.Sequential": [
                    [
                        "torch.nn has another handy class we can use to simplify our code:\n .\nA Sequential object runs each of the modules contained within it, in a\nsequential manner. This is a simpler way of writing our neural network.",
                        "markdown"
                    ],
                    [
                        "To take advantage of this, we need to be able to easily define a\n<strong>custom layer</strong> from a given function.  For instance, PyTorch doesn\u2019t\nhave a <cite>view</cite> layer, and we need to create one for our network. Lambda\nwill create a layer that we can then use when defining a network with\nSequential.",
                        "markdown"
                    ],
                    [
                        "class Lambda():\n    def __init__(self, func):\n        super().__init__()\n        self.func = func\n\n    def forward(self, x):\n        return self.func(x)\n\n\ndef preprocess(x):\n    return x.view(-1, 1, 28, 28)",
                        "code"
                    ],
                    [
                        "The model created with Sequential is simply:",
                        "markdown"
                    ],
                    [
                        " = (\n    (preprocess),\n    (1, 16, kernel_size=3, stride=2, padding=1),\n    (),\n    (16, 16, kernel_size=3, stride=2, padding=1),\n    (),\n    (16, 10, kernel_size=3, stride=2, padding=1),\n    (),\n    (4),\n    (lambda x: x.view(x.size(0), -1)),\n)\n\n = ((), lr=lr, momentum=0.9)\n\nfit(epochs, , loss_func, , train_dl, valid_dl)",
                        "code"
                    ],
                    [
                        "0 0.40341693930625916\n1 0.2864704542160034",
                        "code"
                    ]
                ]
            },
            {
                "Wrapping DataLoader\n<dl class=\"simple\">\n<dt>Our CNN is fairly concise, but it only works with MNIST, because:</dt><dd>": [
                    [
                        "It assumes the input is a 28*28 long vector",
                        "markdown"
                    ],
                    [
                        "It assumes that the final CNN grid size is 4*4 (since that\u2019s the average pooling kernel size we used)\n\n</dd>\n</dl>",
                        "markdown"
                    ],
                    [
                        "Let\u2019s get rid of these two assumptions, so our model works with any 2d\nsingle channel image. First, we can remove the initial Lambda layer by\nmoving the data preprocessing into a generator:",
                        "markdown"
                    ],
                    [
                        "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28), y\n\n\nclass WrappedDataLoader:\n    def __init__(self, dl, func):\n        self.dl = dl\n        self.func = func\n\n    def __len__(self):\n        return len(self.dl)\n\n    def __iter__(self):\n        batches = iter(self.dl)\n        for b in batches:\n            yield (self.func(*b))\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
                        "code"
                    ],
                    [
                        "Next, we can replace nn.AvgPool2d with nn.AdaptiveAvgPool2d, which\nallows us to define the size of the <em>output</em> tensor we want, rather than\nthe <em>input</em> tensor we have. As a result, our model will work with any\nsize input.",
                        "markdown"
                    ],
                    [
                        " = (\n    (1, 16, kernel_size=3, stride=2, padding=1),\n    (),\n    (16, 16, kernel_size=3, stride=2, padding=1),\n    (),\n    (16, 10, kernel_size=3, stride=2, padding=1),\n    (),\n    (1),\n    (lambda x: x.view(x.size(0), -1)),\n)\n\n = ((), lr=lr, momentum=0.9)",
                        "code"
                    ],
                    [
                        "Let\u2019s try it out:",
                        "markdown"
                    ],
                    [
                        "fit(epochs, , loss_func, , train_dl, valid_dl)",
                        "code"
                    ],
                    [
                        "0 0.4155500603556633\n1 0.25355035238265994",
                        "code"
                    ]
                ]
            },
            {
                "Using your GPU": [
                    [
                        "If you\u2019re lucky enough to have access to a CUDA-capable GPU (you can\nrent one for about $0.50/hour from most cloud providers) you can\nuse it to speed up your code. First check that your GPU is working in\nPytorch:",
                        "markdown"
                    ],
                    [
                        "print(())",
                        "code"
                    ],
                    [
                        "True",
                        "code"
                    ],
                    [
                        "And then create a device object for it:",
                        "markdown"
                    ],
                    [
                        " = (\n    \"cuda\") if () else (\"cpu\")",
                        "code"
                    ],
                    [
                        "Let\u2019s update preprocess to move batches to the GPU:",
                        "markdown"
                    ],
                    [
                        "def preprocess(x, y):\n    return x.view(-1, 1, 28, 28).to(), y.to()\n\n\ntrain_dl, valid_dl = get_data(, , bs)\ntrain_dl = WrappedDataLoader(train_dl, preprocess)\nvalid_dl = WrappedDataLoader(valid_dl, preprocess)",
                        "code"
                    ],
                    [
                        "Finally, we can move our model to the GPU.",
                        "markdown"
                    ],
                    [
                        "()\n = ((), lr=lr, momentum=0.9)",
                        "code"
                    ],
                    [
                        "You should find it runs faster now:",
                        "markdown"
                    ],
                    [
                        "fit(epochs, , loss_func, , train_dl, valid_dl)",
                        "code"
                    ],
                    [
                        "0 0.20518756081461906\n1 0.1734702249288559",
                        "code"
                    ]
                ]
            },
            {
                "Closing thoughts": [
                    [
                        "We now have a general data pipeline and training loop which you can use for\ntraining many types of models using Pytorch. To see how simple training a model\ncan now be, take a look at the .",
                        "markdown"
                    ],
                    [
                        "Of course, there are many things you\u2019ll want to add, such as data augmentation,\nhyperparameter tuning, monitoring training, transfer learning, and so forth.\nThese features are available in the fastai library, which has been developed\nusing the same design approach shown in this tutorial, providing a natural\nnext step for practitioners looking to take their models further.",
                        "markdown"
                    ],
                    [
                        "We promised at the start of this tutorial we\u2019d explain through example each of\ntorch.nn, torch.optim, Dataset, and DataLoader. So let\u2019s summarize\nwhat we\u2019ve seen:\n<blockquote>",
                        "markdown"
                    ],
                    [
                        "<strong>torch.nn</strong>",
                        "markdown"
                    ],
                    [
                        "Module: creates a callable which behaves like a function, but can also\ncontain state(such as neural net layer weights). It knows what Parameter (s) it\ncontains and can zero all their gradients, loop through them for weight updates, etc.",
                        "markdown"
                    ],
                    [
                        "Parameter: a wrapper for a tensor that tells a Module that it has weights\nthat need updating during backprop. Only tensors with the <cite>requires_grad</cite> attribute set are updated",
                        "markdown"
                    ],
                    [
                        "functional: a module(usually imported into the F namespace by convention)\nwhich contains activation functions, loss functions, etc, as well as non-stateful\nversions of layers such as convolutional and linear layers.",
                        "markdown"
                    ],
                    [
                        "torch.optim: Contains optimizers such as SGD, which update the weights\nof Parameter during the backward step",
                        "markdown"
                    ],
                    [
                        "Dataset: An abstract interface of objects with a __len__ and a __getitem__,\nincluding classes provided with Pytorch such as TensorDataset",
                        "markdown"
                    ],
                    [
                        "DataLoader: Takes any Dataset and creates an iterator which returns batches of data.\n\n</blockquote>",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  39.874 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Visualizing Models, Data, and Training with TensorBoard": [
            [
                "In the ,\nwe show you how to load in data,\nfeed it through a model we define as a subclass of nn.Module,\ntrain this model on training data, and test it on test data.\nTo see what\u2019s happening, we print out some statistics as the model\nis training to get a sense for whether training is progressing.\nHowever, we can do much better than that: PyTorch integrates with\nTensorBoard, a tool designed for visualizing the results of neural\nnetwork training runs. This tutorial illustrates some of its\nfunctionality, using the\n\nwhich can be read into PyTorch using <cite>torchvision.datasets</cite>.",
                "markdown"
            ],
            [
                "In this tutorial, we\u2019ll learn how to:\n<blockquote>",
                "markdown"
            ],
            [
                "Read in data and with appropriate transforms (nearly identical to the prior tutorial).",
                "markdown"
            ],
            [
                "Set up TensorBoard.",
                "markdown"
            ],
            [
                "Write to TensorBoard.",
                "markdown"
            ],
            [
                "Inspect a model architecture using TensorBoard.",
                "markdown"
            ],
            [
                "Use TensorBoard to create interactive versions of the visualizations we created in last tutorial, with less code\n\n</blockquote>",
                "markdown"
            ],
            [
                "Specifically, on point #5, we\u2019ll see:\n<blockquote>",
                "markdown"
            ],
            [
                "A couple of ways to inspect our training data",
                "markdown"
            ],
            [
                "How to track our model\u2019s performance as it trains",
                "markdown"
            ],
            [
                "How to assess our model\u2019s performance once it is trained.\n\n</blockquote>",
                "markdown"
            ],
            [
                "We\u2019ll begin with similar boilerplate code as in the :",
                "markdown"
            ],
            [
                "# imports\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# transforms\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))])\n\n# datasets\ntrainset = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=True,\n    transform=transform)\ntestset = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=False,\n    transform=transform)\n\n# dataloaders\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                        shuffle=True, num_workers=2)\n\n\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                        shuffle=False, num_workers=2)\n\n# constant for classes\nclasses = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n\n# helper function to show an image\n# (used in the `plot_classes_preds` function below)\ndef matplotlib_imshow(img, one_channel=False):\n    if one_channel:\n        img = img.mean(dim=0)\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    if one_channel:\n        plt.imshow(npimg, cmap=\"Greys\")\n    else:\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))",
                "code"
            ],
            [
                "We\u2019ll define a similar model architecture from that tutorial, making only\nminor modifications to account for the fact that the images are now\none channel instead of three and 28x28 instead of 32x32:",
                "markdown"
            ],
            [
                "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 4 * 4)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nnet = Net()",
                "code"
            ],
            [
                "We\u2019ll define the same optimizer and criterion from before:",
                "markdown"
            ],
            [
                "criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)",
                "code"
            ],
            {
                "1. TensorBoard setup": [
                    [
                        "Now we\u2019ll set up TensorBoard, importing tensorboard from torch.utils and defining a\nSummaryWriter, our key object for writing information to TensorBoard.",
                        "markdown"
                    ],
                    [
                        "from torch.utils.tensorboard import SummaryWriter\n\n# default `log_dir` is \"runs\" - we'll be more specific here\nwriter = SummaryWriter('runs/fashion_mnist_experiment_1')",
                        "code"
                    ],
                    [
                        "Note that this line alone creates a runs/fashion_mnist_experiment_1\nfolder.",
                        "markdown"
                    ]
                ]
            },
            {
                "2. Writing to TensorBoard": [
                    [
                        "Now let\u2019s write an image to our TensorBoard - specifically, a grid -\nusing .",
                        "markdown"
                    ],
                    [
                        "# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# create grid of images\nimg_grid = torchvision.utils.make_grid(images)\n\n# show images\nmatplotlib_imshow(img_grid, one_channel=True)\n\n# write to tensorboard\nwriter.add_image('four_fashion_mnist_images', img_grid)",
                        "code"
                    ],
                    [
                        "Now running",
                        "markdown"
                    ],
                    [
                        "tensorboard --logdir=runs",
                        "code"
                    ],
                    [
                        "from the command line and then navigating to \nshould show the following.\n<img alt=\"../_static/img/tensorboard_first_view.png\" src=\"../_static/img/tensorboard_first_view.png\"/>",
                        "markdown"
                    ],
                    [
                        "Now you know how to use TensorBoard! This example, however, could be\ndone in a Jupyter Notebook - where TensorBoard really excels is in\ncreating interactive visualizations. We\u2019ll cover one of those next,\nand several more by the end of the tutorial.",
                        "markdown"
                    ]
                ]
            },
            {
                "3. Inspect the model using TensorBoard": [
                    [
                        "One of TensorBoard\u2019s strengths is its ability to visualize complex model\nstructures. Let\u2019s visualize the model we built.",
                        "markdown"
                    ],
                    [
                        "writer.add_graph(net, images)\nwriter.close()",
                        "code"
                    ],
                    [
                        "Now upon refreshing TensorBoard you should see a \u201cGraphs\u201d tab that\nlooks like this:\n<img alt=\"../_static/img/tensorboard_model_viz.png\" src=\"../_static/img/tensorboard_model_viz.png\"/>",
                        "markdown"
                    ],
                    [
                        "Go ahead and double click on \u201cNet\u201d to see it expand, seeing a\ndetailed view of the individual operations that make up the model.",
                        "markdown"
                    ],
                    [
                        "TensorBoard has a very handy feature for visualizing high dimensional\ndata such as image data in a lower dimensional space; we\u2019ll cover this\nnext.",
                        "markdown"
                    ]
                ]
            },
            {
                "4. Adding a \u201cProjector\u201d to TensorBoard": [
                    [
                        "We can visualize the lower dimensional representation of higher\ndimensional data via the  method",
                        "markdown"
                    ],
                    [
                        "# helper function\ndef select_n_random(data, labels, n=100):\n    '''\n    Selects n random datapoints and their corresponding labels from a dataset\n    '''\n    assert len(data) == len(labels)\n\n    perm = torch.randperm(len(data))\n    return data[perm][:n], labels[perm][:n]\n\n# select random images and their target indices\nimages, labels = select_n_random(trainset.data, trainset.targets)\n\n# get the class labels for each image\nclass_labels = [classes[lab] for lab in labels]\n\n# log embeddings\nfeatures = images.view(-1, 28 * 28)\nwriter.add_embedding(features,\n                    metadata=class_labels,\n                    label_img=images.unsqueeze(1))\nwriter.close()",
                        "code"
                    ],
                    [
                        "Now in the \u201cProjector\u201d tab of TensorBoard, you can see these 100\nimages - each of which is 784 dimensional - projected down into three\ndimensional space. Furthermore, this is interactive: you can click\nand drag to rotate the three dimensional projection. Finally, a couple\nof tips to make the visualization easier to see: select \u201ccolor: label\u201d\non the top left, as well as enabling \u201cnight mode\u201d, which will make the\nimages easier to see since their background is white:\n<img alt=\"../_static/img/tensorboard_projector.png\" src=\"../_static/img/tensorboard_projector.png\"/>",
                        "markdown"
                    ],
                    [
                        "Now we\u2019ve thoroughly inspected our data, let\u2019s show how TensorBoard\ncan make tracking model training and evaluation clearer, starting with\ntraining.",
                        "markdown"
                    ]
                ]
            },
            {
                "5. Tracking model training with TensorBoard": [
                    [
                        "In the previous example, we simply <em>printed</em> the model\u2019s running loss\nevery 2000 iterations. Now, we\u2019ll instead log the running loss to\nTensorBoard, along with a view into the predictions the model is\nmaking via the plot_classes_preds function.",
                        "markdown"
                    ],
                    [
                        "# helper functions\n\ndef images_to_probs(net, images):\n    '''\n    Generates predictions and corresponding probabilities from a trained\n    network and a list of images\n    '''\n    output = net(images)\n    # convert output probabilities to predicted class\n    _, preds_tensor = torch.max(output, 1)\n    preds = np.squeeze(preds_tensor.numpy())\n    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n\n\ndef plot_classes_preds(net, images, labels):\n    '''\n    Generates matplotlib Figure using a trained network, along with images\n    and labels from a batch, that shows the network's top prediction along\n    with its probability, alongside the actual label, coloring this\n    information based on whether the prediction was correct or not.\n    Uses the \"images_to_probs\" function.\n    '''\n    preds, probs = images_to_probs(net, images)\n    # plot the images in the batch, along with predicted and true labels\n    fig = plt.figure(figsize=(12, 48))\n    for idx in np.arange(4):\n        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n        matplotlib_imshow(images[idx], one_channel=True)\n        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n            classes[preds[idx]],\n            probs[idx] * 100.0,\n            classes[labels[idx]]),\n                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n    return fig",
                        "code"
                    ],
                    [
                        "Finally, let\u2019s train the model using the same model training code from\nthe prior tutorial, but writing results to TensorBoard every 1000\nbatches instead of printing to console; this is done using the\n\nfunction.",
                        "markdown"
                    ],
                    [
                        "In addition, as we train, we\u2019ll generate an image showing the model\u2019s\npredictions vs. the actual results on the four images included in that\nbatch.",
                        "markdown"
                    ],
                    [
                        "running_loss = 0.0\nfor epoch in range(1):  # loop over the dataset multiple times\n\n    for i, data in enumerate(trainloader, 0):\n\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 1000 == 999:    # every 1000 mini-batches...\n\n            # ...log the running loss\n            writer.add_scalar('training loss',\n                            running_loss / 1000,\n                            epoch * len(trainloader) + i)\n\n            # ...log a Matplotlib Figure showing the model's predictions on a\n            # random mini-batch\n            writer.add_figure('predictions vs. actuals',\n                            plot_classes_preds(net, inputs, labels),\n                            global_step=epoch * len(trainloader) + i)\n            running_loss = 0.0\nprint('Finished Training')",
                        "code"
                    ],
                    [
                        "You can now look at the scalars tab to see the running loss plotted\nover the 15,000 iterations of training:\n<img alt=\"../_static/img/tensorboard_scalar_runs.png\" src=\"../_static/img/tensorboard_scalar_runs.png\"/>",
                        "markdown"
                    ],
                    [
                        "In addition, we can look at the predictions the model made on\narbitrary batches throughout learning. See the \u201cImages\u201d tab and scroll\ndown under the \u201cpredictions vs. actuals\u201d visualization to see this;\nthis shows us that, for example, after just 3000 training iterations,\nthe model was already able to distinguish between visually distinct\nclasses such as shirts, sneakers, and coats, though it isn\u2019t as\nconfident as it becomes later on in training:\n<img alt=\"../_static/img/tensorboard_images.png\" src=\"../_static/img/tensorboard_images.png\"/>",
                        "markdown"
                    ],
                    [
                        "In the prior tutorial, we looked at per-class accuracy once the model\nhad been trained; here, we\u2019ll use TensorBoard to plot precision-recall\ncurves (good explanation\n)\nfor each class.",
                        "markdown"
                    ]
                ]
            },
            {
                "6. Assessing trained models with TensorBoard": [
                    [
                        "# 1. gets the probability predictions in a test_size x num_classes Tensor\n# 2. gets the preds in a test_size Tensor\n# takes ~10 seconds to run\nclass_probs = []\nclass_label = []\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        output = net(images)\n        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n\n        class_probs.append(class_probs_batch)\n        class_label.append(labels)\n\ntest_probs = torch.cat([torch.stack(batch) for batch in class_probs])\ntest_label = torch.cat(class_label)\n\n# helper function\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()\n\n# plot all the pr curves\nfor i in range(len(classes)):\n    add_pr_curve_tensorboard(i, test_probs, test_label)",
                        "code"
                    ],
                    [
                        "You will now see a \u201cPR Curves\u201d tab that contains the precision-recall\ncurves for each class. Go ahead and poke around; you\u2019ll see that on\nsome classes the model has nearly 100% \u201carea under the curve\u201d,\nwhereas on others this area is lower:\n<img alt=\"../_static/img/tensorboard_pr_curves.png\" src=\"../_static/img/tensorboard_pr_curves.png\"/>",
                        "markdown"
                    ],
                    [
                        "And that\u2019s an intro to TensorBoard and PyTorch\u2019s integration with it.\nOf course, you could do everything TensorBoard does in your Jupyter\nNotebook, but with TensorBoard, you gets visuals that are interactive\nby default.",
                        "markdown"
                    ]
                ]
            }
        ]
    },
    "Image and Video": {
        "TorchVision Object Detection Finetuning Tutorial": [
            [
                "Tip",
                "markdown"
            ],
            [
                "To get the most of this tutorial, we suggest using this\n.\nThis will allow you to experiment with the information presented below.",
                "markdown"
            ],
            [
                "For this tutorial, we will be finetuning a pre-trained  model in the . It contains\n170 images with 345 instances of pedestrians, and we will use it to\nillustrate how to use the new features in torchvision in order to train\nan instance segmentation model on a custom dataset.",
                "markdown"
            ],
            {
                "Defining the Dataset": [
                    [
                        "The reference scripts for training object detection, instance\nsegmentation and person keypoint detection allows for easily supporting\nadding new custom datasets. The dataset should inherit from the standard\ntorch.utils.data.Dataset class, and implement __len__ and\n__getitem__.",
                        "markdown"
                    ],
                    [
                        "The only specificity that we require is that the dataset __getitem__\nshould return:",
                        "markdown"
                    ],
                    [
                        "image: a PIL Image of size (H, W)",
                        "markdown"
                    ],
                    [
                        "target: a dict containing the following fields",
                        "markdown"
                    ],
                    [
                        "boxes (FloatTensor[N, 4]): the coordinates of the N\nbounding boxes in [x0, y0, x1, y1] format, ranging from 0\nto W and 0 to H",
                        "markdown"
                    ],
                    [
                        "labels (Int64Tensor[N]): the label for each bounding box. 0 represents always the background class.",
                        "markdown"
                    ],
                    [
                        "image_id (Int64Tensor[1]): an image identifier. It should be\nunique between all the images in the dataset, and is used during\nevaluation",
                        "markdown"
                    ],
                    [
                        "area (Tensor[N]): The area of the bounding box. This is used\nduring evaluation with the COCO metric, to separate the metric\nscores between small, medium and large boxes.",
                        "markdown"
                    ],
                    [
                        "iscrowd (UInt8Tensor[N]): instances with iscrowd=True will be\nignored during evaluation.",
                        "markdown"
                    ],
                    [
                        "(optionally) masks (UInt8Tensor[N, H, W]): The segmentation\nmasks for each one of the objects",
                        "markdown"
                    ],
                    [
                        "(optionally) keypoints (FloatTensor[N, K, 3]): For each one of\nthe N objects, it contains the K keypoints in\n[x, y, visibility] format, defining the object. visibility=0\nmeans that the keypoint is not visible. Note that for data\naugmentation, the notion of flipping a keypoint is dependent on\nthe data representation, and you should probably adapt\nreferences/detection/transforms.py for your new keypoint\nrepresentation",
                        "markdown"
                    ],
                    [
                        "If your model returns the above methods, they will make it work for both\ntraining and evaluation, and will use the evaluation scripts from\npycocotools which can be installed with pip install pycocotools.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "For Windows, please install pycocotools from  with command",
                        "markdown"
                    ],
                    [
                        "pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI",
                        "markdown"
                    ],
                    [
                        "One note on the labels. The model considers class 0 as background. If your dataset does not contain the background class, you should not have 0 in your labels. For example, assuming you have just two classes, <em>cat</em> and <em>dog</em>, you can define 1 (not 0) to represent <em>cats</em> and 2 to represent <em>dogs</em>. So, for instance, if one of the images has both classes, your labels tensor should look like [1,2].",
                        "markdown"
                    ],
                    [
                        "Additionally, if you want to use aspect ratio grouping during training\n(so that each batch only contains images with similar aspect ratios),\nthen it is recommended to also implement a get_height_and_width\nmethod, which returns the height and the width of the image. If this\nmethod is not provided, we query all elements of the dataset via\n__getitem__ , which loads the image in memory and is slower than if\na custom method is provided.",
                        "markdown"
                    ],
                    {
                        "Writing a custom dataset for PennFudan": [
                            [
                                "Let\u2019s write a dataset for the PennFudan dataset. After , we\nhave the following folder structure:",
                                "markdown"
                            ],
                            [
                                "PennFudanPed/\n  PedMasks/\n    FudanPed00001_mask.png\n    FudanPed00002_mask.png\n    FudanPed00003_mask.png\n    FudanPed00004_mask.png\n    ...\n  PNGImages/\n    FudanPed00001.png\n    FudanPed00002.png\n    FudanPed00003.png\n    FudanPed00004.png",
                                "code"
                            ],
                            [
                                "Here is one example of a pair of images and segmentation masks\n<img alt=\"../_static/img/tv_tutorial/tv_image01.png\" src=\"../_static/img/tv_tutorial/tv_image01.png\"/>\n<img alt=\"../_static/img/tv_tutorial/tv_image02.png\" src=\"../_static/img/tv_tutorial/tv_image02.png\"/>",
                                "markdown"
                            ],
                            [
                                "So each image has a corresponding\nsegmentation mask, where each color correspond to a different instance.\nLet\u2019s write a torch.utils.data.Dataset class for this dataset.",
                                "markdown"
                            ],
                            [
                                "import os\nimport numpy as np\nimport torch\nfrom PIL import Image\n\n\nclass PennFudanDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        # because each color corresponds to a different instance\n        # with 0 being background\n        mask = Image.open(mask_path)\n        # convert the PIL Image into a numpy array\n        mask = np.array(mask)\n        # instances are encoded as different colors\n        obj_ids = np.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = mask == obj_ids[:, None, None]\n\n        # get bounding box coordinates for each mask\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            pos = np.where(masks[i])\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        # convert everything into a torch.Tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)",
                                "code"
                            ],
                            [
                                "That\u2019s all for the dataset. Now let\u2019s define a model that can perform\npredictions on this dataset.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Defining your model": [
                    [
                        "In this tutorial, we will be using , which is based on top of\n. Faster R-CNN is a\nmodel that predicts both bounding boxes and class scores for potential\nobjects in the image.\n<img alt=\"../_static/img/tv_tutorial/tv_image03.png\" src=\"../_static/img/tv_tutorial/tv_image03.png\"/>",
                        "markdown"
                    ],
                    [
                        "Mask R-CNN adds an extra branch\ninto Faster R-CNN, which also predicts segmentation masks for each\ninstance.\n<img alt=\"../_static/img/tv_tutorial/tv_image04.png\" src=\"../_static/img/tv_tutorial/tv_image04.png\"/>",
                        "markdown"
                    ],
                    [
                        "There are two common\nsituations where one might want\nto modify one of the available models in torchvision modelzoo. The first\nis when we want to start from a pre-trained model, and just finetune the\nlast layer. The other is when we want to replace the backbone of the\nmodel with a different one (for faster predictions, for example).",
                        "markdown"
                    ],
                    [
                        "Let\u2019s go see how we would do one or another in the following sections.",
                        "markdown"
                    ],
                    {
                        "1 - Finetuning from a pretrained model": [
                            [
                                "Let\u2019s suppose that you want to start from a model pre-trained on COCO\nand want to finetune it for your particular classes. Here is a possible\nway of doing it:",
                                "markdown"
                            ],
                            [
                                "import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# load a model pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 2  # 1 class (person) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2 - Modifying the model to add a different backbone": [
                            [
                                "import torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\n# load a pre-trained model for classification and return\n# only the features\nbackbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n# FasterRCNN needs to know the number of\n# output channels in a backbone. For mobilenet_v2, it's 1280\n# so we need to add it here\nbackbone.out_channels = 1280\n\n# let's make the RPN generate 5 x 3 anchors per spatial\n# location, with 5 different sizes and 3 different aspect\n# ratios. We have a Tuple[Tuple[int]] because each feature\n# map could potentially have different sizes and\n# aspect ratios\nanchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),))\n\n# let's define what are the feature maps that we will\n# use to perform the region of interest cropping, as well as\n# the size of the crop after rescaling.\n# if your backbone returns a Tensor, featmap_names is expected to\n# be [0]. More generally, the backbone should return an\n# OrderedDict[Tensor], and in featmap_names you can choose which\n# feature maps to use.\nroi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n                                                output_size=7,\n                                                sampling_ratio=2)\n\n# put the pieces together inside a FasterRCNN model\nmodel = FasterRCNN(backbone,\n                   num_classes=2,\n                   rpn_anchor_generator=anchor_generator,\n                   box_roi_pool=roi_pooler)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "An Instance segmentation model for PennFudan Dataset": [
                            [
                                "In our case, we want to fine-tune from a pre-trained model, given that\nour dataset is very small, so we will be following approach number 1.",
                                "markdown"
                            ],
                            [
                                "Here we want to also compute the instance segmentation masks, so we will\nbe using Mask R-CNN:",
                                "markdown"
                            ],
                            [
                                "import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n\ndef get_model_instance_segmentation(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n\n    # get number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                       hidden_layer,\n                                                       num_classes)\n\n    return model",
                                "code"
                            ],
                            [
                                "That\u2019s it, this will make model be ready to be trained and evaluated\non your custom dataset.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Putting everything together": [
                    [
                        "In references/detection/, we have a number of helper functions to\nsimplify training and evaluating detection models. Here, we will use\nreferences/detection/engine.py, references/detection/utils.py\nand references/detection/transforms.py. Just copy everything under\nreferences/detection to your folder and use them here.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s write some helper functions for data augmentation /\ntransformation:",
                        "markdown"
                    ],
                    [
                        "import transforms as T\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(T.PILToTensor())\n    transforms.append(T.ConvertImageDtype(torch.float))\n    if train:\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)",
                        "code"
                    ]
                ]
            },
            {
                "Testing forward() method (Optional)": [
                    [
                        "Before iterating over the dataset, it\u2019s good to see what the model\nexpects during training and inference time on sample data.",
                        "markdown"
                    ],
                    [
                        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\ndataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\ndata_loader = torch.utils.data.DataLoader(\n dataset, batch_size=2, shuffle=True, num_workers=4,\n collate_fn=utils.collate_fn)\n# For Training\nimages,targets = next(iter(data_loader))\nimages = list(image for image in images)\ntargets = [{k: v for k, v in t.items()} for t in targets]\noutput = model(images,targets)   # Returns losses and detections\n# For inference\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\npredictions = model(x)           # Returns predictions",
                        "code"
                    ],
                    [
                        "Let\u2019s now write the main function which performs the training and the\nvalidation:",
                        "markdown"
                    ],
                    [
                        "from engine import train_one_epoch, evaluate\nimport utils\n\n\ndef main():\n    # train on the GPU or on the CPU, if a GPU is not available\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n    # our dataset has two classes only - background and person\n    num_classes = 2\n    # use our dataset and defined transformations\n    dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n    dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n\n    # split the dataset in train and test set\n    indices = torch.randperm(len(dataset)).tolist()\n    dataset = torch.utils.data.Subset(dataset, indices[:-50])\n    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n    # define training and validation data loaders\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=2, shuffle=True, num_workers=4,\n        collate_fn=utils.collate_fn)\n\n    data_loader_test = torch.utils.data.DataLoader(\n        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n        collate_fn=utils.collate_fn)\n\n    # get the model using our helper function\n    model = get_model_instance_segmentation(num_classes)\n\n    # move model to the right device\n    model.to(device)\n\n    # construct an optimizer\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=0.005,\n                                momentum=0.9, weight_decay=0.0005)\n    # and a learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                                   step_size=3,\n                                                   gamma=0.1)\n\n    # let's train it for 10 epochs\n    num_epochs = 10\n\n    for epoch in range(num_epochs):\n        # train for one epoch, printing every 10 iterations\n        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n        # update the learning rate\n        lr_scheduler.step()\n        # evaluate on the test dataset\n        evaluate(model, data_loader_test, device=device)\n\n    print(\"That's it!\")",
                        "code"
                    ],
                    [
                        "You should get as output for the first epoch:",
                        "markdown"
                    ],
                    [
                        "Epoch: [0]  [ 0/60]  eta: 0:01:18  lr: 0.000090  loss: 2.5213 (2.5213)  loss_classifier: 0.8025 (0.8025)  loss_box_reg: 0.2634 (0.2634)  loss_mask: 1.4265 (1.4265)  loss_objectness: 0.0190 (0.0190)  loss_rpn_box_reg: 0.0099 (0.0099)  time: 1.3121  data: 0.3024  max mem: 3485\nEpoch: [0]  [10/60]  eta: 0:00:20  lr: 0.000936  loss: 1.3007 (1.5313)  loss_classifier: 0.3979 (0.4719)  loss_box_reg: 0.2454 (0.2272)  loss_mask: 0.6089 (0.7953)  loss_objectness: 0.0197 (0.0228)  loss_rpn_box_reg: 0.0121 (0.0141)  time: 0.4198  data: 0.0298  max mem: 5081\nEpoch: [0]  [20/60]  eta: 0:00:15  lr: 0.001783  loss: 0.7567 (1.1056)  loss_classifier: 0.2221 (0.3319)  loss_box_reg: 0.2002 (0.2106)  loss_mask: 0.2904 (0.5332)  loss_objectness: 0.0146 (0.0176)  loss_rpn_box_reg: 0.0094 (0.0123)  time: 0.3293  data: 0.0035  max mem: 5081\nEpoch: [0]  [30/60]  eta: 0:00:11  lr: 0.002629  loss: 0.4705 (0.8935)  loss_classifier: 0.0991 (0.2517)  loss_box_reg: 0.1578 (0.1957)  loss_mask: 0.1970 (0.4204)  loss_objectness: 0.0061 (0.0140)  loss_rpn_box_reg: 0.0075 (0.0118)  time: 0.3403  data: 0.0044  max mem: 5081\nEpoch: [0]  [40/60]  eta: 0:00:07  lr: 0.003476  loss: 0.3901 (0.7568)  loss_classifier: 0.0648 (0.2022)  loss_box_reg: 0.1207 (0.1736)  loss_mask: 0.1705 (0.3585)  loss_objectness: 0.0018 (0.0113)  loss_rpn_box_reg: 0.0075 (0.0112)  time: 0.3407  data: 0.0044  max mem: 5081\nEpoch: [0]  [50/60]  eta: 0:00:03  lr: 0.004323  loss: 0.3237 (0.6703)  loss_classifier: 0.0474 (0.1731)  loss_box_reg: 0.1109 (0.1561)  loss_mask: 0.1658 (0.3201)  loss_objectness: 0.0015 (0.0093)  loss_rpn_box_reg: 0.0093 (0.0116)  time: 0.3379  data: 0.0043  max mem: 5081\nEpoch: [0]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2540 (0.6082)  loss_classifier: 0.0309 (0.1526)  loss_box_reg: 0.0463 (0.1405)  loss_mask: 0.1568 (0.2945)  loss_objectness: 0.0012 (0.0083)  loss_rpn_box_reg: 0.0093 (0.0123)  time: 0.3489  data: 0.0042  max mem: 5081\nEpoch: [0] Total time: 0:00:21 (0.3570 s / it)\ncreating index...\nindex created!\nTest:  [ 0/50]  eta: 0:00:19  model_time: 0.2152 (0.2152)  evaluator_time: 0.0133 (0.0133)  time: 0.4000  data: 0.1701  max mem: 5081\nTest:  [49/50]  eta: 0:00:00  model_time: 0.0628 (0.0687)  evaluator_time: 0.0039 (0.0064)  time: 0.0735  data: 0.0022  max mem: 5081\nTest: Total time: 0:00:04 (0.0828 s / it)\nAveraged stats: model_time: 0.0628 (0.0687)  evaluator_time: 0.0039 (0.0064)\nAccumulating evaluation results...\nDONE (t=0.01s).\nAccumulating evaluation results...\nDONE (t=0.01s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.606\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.984\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.780\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.313\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.582\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.612\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.270\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.672\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.672\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.755\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.664\nIoU metric: segm\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.704\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.871\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.325\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.488\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.727\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.316\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.748\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.749\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.673\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.758",
                        "code"
                    ],
                    [
                        "So after one epoch of training, we obtain a COCO-style mAP of 60.6, and\na mask mAP of 70.4.",
                        "markdown"
                    ],
                    [
                        "After training for 10 epochs, I got the following metrics",
                        "markdown"
                    ],
                    [
                        "IoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.799\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.969\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.935\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.349\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.592\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.324\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.844\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.844\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.777\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.870\nIoU metric: segm\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.761\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.969\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.919\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.341\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.464\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.788\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.303\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.799\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.799\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.769\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.818",
                        "code"
                    ],
                    [
                        "But what do the predictions look like? Let\u2019s take one image in the\ndataset and verify\n<img alt=\"../_static/img/tv_tutorial/tv_image05.png\" src=\"../_static/img/tv_tutorial/tv_image05.png\"/>",
                        "markdown"
                    ],
                    [
                        "The trained model predicts 9\ninstances of person in this image, let\u2019s see a couple of them:\n<img alt=\"../_static/img/tv_tutorial/tv_image06.png\" src=\"../_static/img/tv_tutorial/tv_image06.png\"/>\n<img alt=\"../_static/img/tv_tutorial/tv_image07.png\" src=\"../_static/img/tv_tutorial/tv_image07.png\"/>",
                        "markdown"
                    ],
                    [
                        "The results look pretty good!",
                        "markdown"
                    ]
                ]
            },
            {
                "Wrapping up": [
                    [
                        "In this tutorial, you have learned how to create your own training\npipeline for instance segmentation models, on a custom dataset. For\nthat, you wrote a torch.utils.data.Dataset class that returns the\nimages and the ground truth boxes and segmentation masks. You also\nleveraged a Mask R-CNN model pre-trained on COCO train2017 in order to\nperform transfer learning on this new dataset.",
                        "markdown"
                    ],
                    [
                        "For a more complete example, which includes multi-machine / multi-gpu\ntraining, check references/detection/train.py, which is present in\nthe torchvision repo.",
                        "markdown"
                    ],
                    [
                        "You can download a full source file for this tutorial\n.",
                        "markdown"
                    ]
                ]
            }
        ],
        "Transfer Learning for Computer Vision Tutorial": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "In this tutorial, you will learn how to train a convolutional neural network for\nimage classification using transfer learning. You can read more about the transfer\nlearning at ",
                "markdown"
            ],
            [
                "Quoting these notes,\n<blockquote>",
                "markdown"
            ],
            [
                "In practice, very few people train an entire Convolutional Network\nfrom scratch (with random initialization), because it is relatively\nrare to have a dataset of sufficient size. Instead, it is common to\npretrain a ConvNet on a very large dataset (e.g. ImageNet, which\ncontains 1.2 million images with 1000 categories), and then use the\nConvNet either as an initialization or a fixed feature extractor for\nthe task of interest.\n</blockquote>",
                "markdown"
            ],
            [
                "These two major transfer learning scenarios look as follows:",
                "markdown"
            ],
            [
                "<strong>Finetuning the convnet</strong>: Instead of random initialization, we\ninitialize the network with a pretrained network, like the one that is\ntrained on imagenet 1000 dataset. Rest of the training looks as\nusual.",
                "markdown"
            ],
            [
                "<strong>ConvNet as fixed feature extractor</strong>: Here, we will freeze the weights\nfor all of the network except that of the final fully connected\nlayer. This last fully connected layer is replaced with a new one\nwith random weights and only this layer is trained.",
                "markdown"
            ],
            [
                "# License: BSD\n# Author: Sasank Chilamkurthy\n\nfrom __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\n\ncudnn.benchmark = True\nplt.ion()   # interactive mode",
                "code"
            ],
            [
                "&lt;contextlib.ExitStack object at 0x7ff9449a8160&gt;",
                "code"
            ],
            {
                "Load Data": [
                    [
                        "We will use torchvision and torch.utils.data packages for loading the\ndata.",
                        "markdown"
                    ],
                    [
                        "The problem we\u2019re going to solve today is to train a model to classify\n<strong>ants</strong> and <strong>bees</strong>. We have about 120 training images each for ants and bees.\nThere are 75 validation images for each class. Usually, this is a very\nsmall dataset to generalize upon, if trained from scratch. Since we\nare using transfer learning, we should be able to generalize reasonably\nwell.",
                        "markdown"
                    ],
                    [
                        "This dataset is a very small subset of imagenet.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "Download the data from\n\nand extract it to the current directory.",
                        "markdown"
                    ],
                    [
                        "# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': ([\n        (224),\n        (),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': ([\n        (256),\n        (224),\n        (),\n        ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: (os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: (image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].\n\n = (\"cuda:0\" if () else \"cpu\")",
                        "code"
                    ],
                    {
                        "Visualize a few images": [
                            [
                                "Let\u2019s visualize a few training images so as to understand the data\naugmentations.",
                                "markdown"
                            ],
                            [
                                "def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\n,  = next(iter(dataloaders['train']))\n\n# Make a grid from batch\n = ()\n\nimshow(, title=[class_names[x] for x in ])\n\n\n<img alt=\"['bees', 'bees', 'ants', 'bees']\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\" srcset=\"../_images/sphx_glr_transfer_learning_tutorial_001.png\"/>",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Training the model": [
                    [
                        "Now, let\u2019s write a general function to train a model. Here, we will\nillustrate:",
                        "markdown"
                    ],
                    [
                        "Scheduling the learning rate",
                        "markdown"
                    ],
                    [
                        "Saving the best model",
                        "markdown"
                    ],
                    [
                        "In the following, parameter scheduler is an LR scheduler object from\ntorch.optim.lr_scheduler.",
                        "markdown"
                    ],
                    [
                        "def train_model(model, , optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for , labels in dataloaders[phase]:\n                 = .to()\n                labels = labels.to()\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with (phase == 'train'):\n                    outputs = model()\n                    _, preds = (outputs, 1)\n                    loss = (outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * .size(0)\n                running_corrects += (preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc &gt; best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:4f}')\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model",
                        "code"
                    ],
                    {
                        "Visualizing the model predictions": [
                            [
                                "Generic function to display predictions for a few images",
                                "markdown"
                            ],
                            [
                                "def visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with ():\n        for i, (, labels) in enumerate(dataloaders['val']):\n             = .to()\n            labels = labels.to()\n\n            outputs = model()\n            _, preds = (outputs, 1)\n\n            for j in range(.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images//2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title(f'predicted: {class_names[preds[j]]}')\n                imshow(.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Finetuning the convnet": [
                    [
                        "Load a pretrained model and reset final fully connected layer.",
                        "markdown"
                    ],
                    [
                        "model_ft = (pretrained=True)\nnum_ftrs = .in_features\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n = (num_ftrs, 2)\n\nmodel_ft = ()\n\n = ()\n\n# Observe that all parameters are being optimized\n = ((), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\n = (, step_size=7, gamma=0.1)",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning:\n\nThe parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning:\n\nArguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /var/lib/jenkins/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n  0%|          | 0.00/44.7M [00:00&lt;?, ?B/s]\n 21%|##        | 9.28M/44.7M [00:00&lt;00:00, 97.0MB/s]\n 66%|######6   | 29.5M/44.7M [00:00&lt;00:00, 165MB/s]\n100%|##########| 44.7M/44.7M [00:00&lt;00:00, 172MB/s]",
                        "code"
                    ],
                    {
                        "Train and evaluate": [
                            [
                                "It should take around 15-25 min on CPU. On GPU though, it takes less than a\nminute.",
                                "markdown"
                            ],
                            [
                                "model_ft = train_model(model_ft, , , ,\n                       num_epochs=25)",
                                "code"
                            ],
                            [
                                "Epoch 0/24\n----------\ntrain Loss: 0.5726 Acc: 0.7172\nval Loss: 0.1992 Acc: 0.9216\n\nEpoch 1/24\n----------\ntrain Loss: 0.6180 Acc: 0.7418\nval Loss: 0.2637 Acc: 0.8954\n\nEpoch 2/24\n----------\ntrain Loss: 0.6153 Acc: 0.7664\nval Loss: 0.4100 Acc: 0.8235\n\nEpoch 3/24\n----------\ntrain Loss: 0.6259 Acc: 0.7746\nval Loss: 0.2515 Acc: 0.9150\n\nEpoch 4/24\n----------\ntrain Loss: 0.4605 Acc: 0.8361\nval Loss: 0.1442 Acc: 0.9412\n\nEpoch 5/24\n----------\ntrain Loss: 0.5114 Acc: 0.8238\nval Loss: 0.4190 Acc: 0.8693\n\nEpoch 6/24\n----------\ntrain Loss: 0.4694 Acc: 0.8238\nval Loss: 0.5706 Acc: 0.8627\n\nEpoch 7/24\n----------\ntrain Loss: 0.3649 Acc: 0.8402\nval Loss: 0.3057 Acc: 0.9150\n\nEpoch 8/24\n----------\ntrain Loss: 0.3285 Acc: 0.8648\nval Loss: 0.2728 Acc: 0.9216\n\nEpoch 9/24\n----------\ntrain Loss: 0.1921 Acc: 0.9426\nval Loss: 0.3099 Acc: 0.9216\n\nEpoch 10/24\n----------\ntrain Loss: 0.3274 Acc: 0.8648\nval Loss: 0.3046 Acc: 0.8954\n\nEpoch 11/24\n----------\ntrain Loss: 0.3464 Acc: 0.8443\nval Loss: 0.2354 Acc: 0.9346\n\nEpoch 12/24\n----------\ntrain Loss: 0.2984 Acc: 0.8730\nval Loss: 0.2999 Acc: 0.9020\n\nEpoch 13/24\n----------\ntrain Loss: 0.3737 Acc: 0.8525\nval Loss: 0.2356 Acc: 0.9281\n\nEpoch 14/24\n----------\ntrain Loss: 0.3435 Acc: 0.8566\nval Loss: 0.2334 Acc: 0.9477\n\nEpoch 15/24\n----------\ntrain Loss: 0.2781 Acc: 0.8730\nval Loss: 0.2462 Acc: 0.9216\n\nEpoch 16/24\n----------\ntrain Loss: 0.1656 Acc: 0.9385\nval Loss: 0.2380 Acc: 0.9216\n\nEpoch 17/24\n----------\ntrain Loss: 0.2829 Acc: 0.9016\nval Loss: 0.2423 Acc: 0.9346\n\nEpoch 18/24\n----------\ntrain Loss: 0.2433 Acc: 0.8852\nval Loss: 0.2419 Acc: 0.9216\n\nEpoch 19/24\n----------\ntrain Loss: 0.3778 Acc: 0.8279\nval Loss: 0.2537 Acc: 0.9281\n\nEpoch 20/24\n----------\ntrain Loss: 0.2816 Acc: 0.8770\nval Loss: 0.2284 Acc: 0.9281\n\nEpoch 21/24\n----------\ntrain Loss: 0.2312 Acc: 0.8852\nval Loss: 0.2293 Acc: 0.9412\n\nEpoch 22/24\n----------\ntrain Loss: 0.2367 Acc: 0.9139\nval Loss: 0.2405 Acc: 0.9216\n\nEpoch 23/24\n----------\ntrain Loss: 0.3230 Acc: 0.8730\nval Loss: 0.2436 Acc: 0.9281\n\nEpoch 24/24\n----------\ntrain Loss: 0.1921 Acc: 0.9016\nval Loss: 0.2392 Acc: 0.9216\n\nTraining complete in 1m 10s\nBest val Acc: 0.947712",
                                "code"
                            ],
                            [
                                "visualize_model(model_ft)\n\n\n<img alt=\"predicted: bees, predicted: bees, predicted: bees, predicted: ants, predicted: ants, predicted: ants\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_transfer_learning_tutorial_002.png\" srcset=\"../_images/sphx_glr_transfer_learning_tutorial_002.png\"/>",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "ConvNet as fixed feature extractor": [
                    [
                        "Here, we need to freeze all the network except the final layer. We need\nto set requires_grad = False to freeze the parameters so that the\ngradients are not computed in backward().",
                        "markdown"
                    ],
                    [
                        "You can read more about this in the documentation\n.",
                        "markdown"
                    ],
                    [
                        "model_conv = (pretrained=True)\nfor  in ():\n    .requires_grad = False\n\n# Parameters of newly constructed modules have requires_grad=True by default\nnum_ftrs = .in_features\n = (num_ftrs, 2)\n\nmodel_conv = ()\n\n = ()\n\n# Observe that only parameters of final layer are being optimized as\n# opposed to before.\n = ((), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\n = (, step_size=7, gamma=0.1)",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning:\n\nThe parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning:\n\nArguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.",
                        "code"
                    ],
                    {
                        "Train and evaluate": [
                            [
                                "On CPU this will take about half the time compared to previous scenario.\nThis is expected as gradients don\u2019t need to be computed for most of the\nnetwork. However, forward does need to be computed.",
                                "markdown"
                            ],
                            [
                                "model_conv = train_model(model_conv, , ,\n                         , num_epochs=25)",
                                "code"
                            ],
                            [
                                "Epoch 0/24\n----------\ntrain Loss: 0.7290 Acc: 0.5902\nval Loss: 0.2586 Acc: 0.8954\n\nEpoch 1/24\n----------\ntrain Loss: 0.6689 Acc: 0.7295\nval Loss: 0.3950 Acc: 0.8497\n\nEpoch 2/24\n----------\ntrain Loss: 0.4358 Acc: 0.8115\nval Loss: 0.1802 Acc: 0.9412\n\nEpoch 3/24\n----------\ntrain Loss: 0.4473 Acc: 0.7910\nval Loss: 0.2035 Acc: 0.9346\n\nEpoch 4/24\n----------\ntrain Loss: 0.3695 Acc: 0.8156\nval Loss: 0.2172 Acc: 0.9150\n\nEpoch 5/24\n----------\ntrain Loss: 0.4376 Acc: 0.8197\nval Loss: 0.1986 Acc: 0.9412\n\nEpoch 6/24\n----------\ntrain Loss: 0.3912 Acc: 0.8156\nval Loss: 0.1906 Acc: 0.9412\n\nEpoch 7/24\n----------\ntrain Loss: 0.4135 Acc: 0.8238\nval Loss: 0.1573 Acc: 0.9412\n\nEpoch 8/24\n----------\ntrain Loss: 0.3502 Acc: 0.8443\nval Loss: 0.1837 Acc: 0.9412\n\nEpoch 9/24\n----------\ntrain Loss: 0.3378 Acc: 0.8566\nval Loss: 0.1689 Acc: 0.9346\n\nEpoch 10/24\n----------\ntrain Loss: 0.3615 Acc: 0.8402\nval Loss: 0.1795 Acc: 0.9477\n\nEpoch 11/24\n----------\ntrain Loss: 0.2932 Acc: 0.8648\nval Loss: 0.1722 Acc: 0.9346\n\nEpoch 12/24\n----------\ntrain Loss: 0.2824 Acc: 0.8648\nval Loss: 0.1795 Acc: 0.9412\n\nEpoch 13/24\n----------\ntrain Loss: 0.3392 Acc: 0.8525\nval Loss: 0.1894 Acc: 0.9412\n\nEpoch 14/24\n----------\ntrain Loss: 0.3772 Acc: 0.8566\nval Loss: 0.2044 Acc: 0.9412\n\nEpoch 15/24\n----------\ntrain Loss: 0.3700 Acc: 0.8033\nval Loss: 0.2390 Acc: 0.9412\n\nEpoch 16/24\n----------\ntrain Loss: 0.3259 Acc: 0.8443\nval Loss: 0.1807 Acc: 0.9477\n\nEpoch 17/24\n----------\ntrain Loss: 0.3933 Acc: 0.8525\nval Loss: 0.1865 Acc: 0.9477\n\nEpoch 18/24\n----------\ntrain Loss: 0.3499 Acc: 0.8443\nval Loss: 0.1757 Acc: 0.9346\n\nEpoch 19/24\n----------\ntrain Loss: 0.3927 Acc: 0.8033\nval Loss: 0.1748 Acc: 0.9477\n\nEpoch 20/24\n----------\ntrain Loss: 0.2217 Acc: 0.9139\nval Loss: 0.1768 Acc: 0.9477\n\nEpoch 21/24\n----------\ntrain Loss: 0.3716 Acc: 0.8443\nval Loss: 0.1914 Acc: 0.9412\n\nEpoch 22/24\n----------\ntrain Loss: 0.3795 Acc: 0.8074\nval Loss: 0.1907 Acc: 0.9412\n\nEpoch 23/24\n----------\ntrain Loss: 0.3627 Acc: 0.8320\nval Loss: 0.1690 Acc: 0.9412\n\nEpoch 24/24\n----------\ntrain Loss: 0.2867 Acc: 0.8811\nval Loss: 0.1908 Acc: 0.9477\n\nTraining complete in 0m 44s\nBest val Acc: 0.947712",
                                "code"
                            ],
                            [
                                "visualize_model(model_conv)\n\nplt.ioff()\nplt.show()\n\n\n<img alt=\"predicted: ants, predicted: ants, predicted: ants, predicted: ants, predicted: bees, predicted: bees\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_transfer_learning_tutorial_003.png\" srcset=\"../_images/sphx_glr_transfer_learning_tutorial_003.png\"/>",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Further Learning": [
                    [
                        "If you would like to learn more about the applications of transfer learning,\ncheckout our .",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 2 minutes  0.817 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Adversarial Example Generation": [
            [
                "<strong>Author:</strong> ",
                "markdown"
            ],
            [
                "If you are reading this, hopefully you can appreciate how effective some\nmachine learning models are. Research is constantly pushing ML models to\nbe faster, more accurate, and more efficient. However, an often\noverlooked aspect of designing and training models is security and\nrobustness, especially in the face of an adversary who wishes to fool\nthe model.",
                "markdown"
            ],
            [
                "This tutorial will raise your awareness to the security vulnerabilities\nof ML models, and will give insight into the hot topic of adversarial\nmachine learning. You may be surprised to find that adding imperceptible\nperturbations to an image <em>can</em> cause drastically different model\nperformance. Given that this is a tutorial, we will explore the topic\nvia example on an image classifier. Specifically, we will use one of the\nfirst and most popular attack methods, the Fast Gradient Sign Attack\n(FGSM), to fool an MNIST classifier.",
                "markdown"
            ],
            {
                "Threat Model": [
                    [
                        "For context, there are many categories of adversarial attacks, each with\na different goal and assumption of the attacker\u2019s knowledge. However, in\ngeneral the overarching goal is to add the least amount of perturbation\nto the input data to cause the desired misclassification. There are\nseveral kinds of assumptions of the attacker\u2019s knowledge, two of which\nare: <strong>white-box</strong> and <strong>black-box</strong>. A <em>white-box</em> attack assumes the\nattacker has full knowledge and access to the model, including\narchitecture, inputs, outputs, and weights. A <em>black-box</em> attack assumes\nthe attacker only has access to the inputs and outputs of the model, and\nknows nothing about the underlying architecture or weights. There are\nalso several types of goals, including <strong>misclassification</strong> and\n<strong>source/target misclassification</strong>. A goal of <em>misclassification</em> means\nthe adversary only wants the output classification to be wrong but does\nnot care what the new classification is. A <em>source/target\nmisclassification</em> means the adversary wants to alter an image that is\noriginally of a specific source class so that it is classified as a\nspecific target class.",
                        "markdown"
                    ],
                    [
                        "In this case, the FGSM attack is a <em>white-box</em> attack with the goal of\n<em>misclassification</em>. With this background information, we can now\ndiscuss the attack in detail.",
                        "markdown"
                    ]
                ]
            },
            {
                "Fast Gradient Sign Attack": [
                    [
                        "One of the first and most popular adversarial attacks to date is\nreferred to as the <em>Fast Gradient Sign Attack (FGSM)</em> and is described\nby Goodfellow et. al.\u00a0in . The attack is remarkably\npowerful, and yet intuitive. It is designed to attack neural networks by\nleveraging the way they learn, <em>gradients</em>. The idea is simple, rather\nthan working to minimize the loss by adjusting the weights based on the\nbackpropagated gradients, the attack <em>adjusts the input data to maximize\nthe loss</em> based on the same backpropagated gradients. In other words,\nthe attack uses the gradient of the loss w.r.t the input data, then\nadjusts the input data to maximize the loss.",
                        "markdown"
                    ],
                    [
                        "Before we jump into the code, let\u2019s look at the famous\n panda example and extract\nsome notation.\n\n<img alt=\"fgsm_panda_image\" src=\"../_images/fgsm_panda_image.png\"/>",
                        "markdown"
                    ],
                    [
                        "From the figure, \\(\\mathbf{x}\\) is the original input image\ncorrectly classified as a \u201cpanda\u201d, \\(y\\) is the ground truth label\nfor \\(\\mathbf{x}\\), \\(\\mathbf{\\theta}\\) represents the model\nparameters, and \\(J(\\mathbf{\\theta}, \\mathbf{x}, y)\\) is the loss\nthat is used to train the network. The attack backpropagates the\ngradient back to the input data to calculate\n\\(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)\\). Then, it adjusts\nthe input data by a small step (\\(\\epsilon\\) or \\(0.007\\) in the\npicture) in the direction (i.e.\n\\(sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))\\)) that will\nmaximize the loss. The resulting perturbed image, \\(x'\\), is then\n<em>misclassified</em> by the target network as a \u201cgibbon\u201d when it is still\nclearly a \u201cpanda\u201d.",
                        "markdown"
                    ],
                    [
                        "Hopefully now the motivation for this tutorial is clear, so lets jump\ninto the implementation.",
                        "markdown"
                    ],
                    [
                        "from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# NOTE: This is a hack to get around \"User-agent\" limitations when downloading MNIST datasets\n#       see, https://github.com/pytorch/vision/issues/3497 for more information\nfrom six.moves import urllib\nopener = urllib.request.build_opener()\nopener.addheaders = [('User-agent', 'Mozilla/5.0')]\nurllib.request.install_opener(opener)",
                        "code"
                    ]
                ]
            },
            {
                "Implementation": [
                    [
                        "In this section, we will discuss the input parameters for the tutorial,\ndefine the model under attack, then code the attack and run some tests.",
                        "markdown"
                    ],
                    {
                        "Inputs": [
                            [
                                "There are only three inputs for this tutorial, and are defined as\nfollows:",
                                "markdown"
                            ],
                            [
                                "<strong>epsilons</strong> - List of epsilon values to use for the run. It is\nimportant to keep 0 in the list because it represents the model\nperformance on the original test set. Also, intuitively we would\nexpect the larger the epsilon, the more noticeable the perturbations\nbut the more effective the attack in terms of degrading model\naccuracy. Since the data range here is \\([0,1]\\), no epsilon\nvalue should exceed 1.",
                                "markdown"
                            ],
                            [
                                "<strong>pretrained_model</strong> - path to the pretrained MNIST model which was\ntrained with\n.\nFor simplicity, download the pretrained model .",
                                "markdown"
                            ],
                            [
                                "<strong>use_cuda</strong> - boolean flag to use CUDA if desired and available.\nNote, a GPU with CUDA is not critical for this tutorial as a CPU will\nnot take much time.",
                                "markdown"
                            ],
                            [
                                "epsilons = [0, .05, .1, .15, .2, .25, .3]\npretrained_model = \"data/lenet_mnist_model.pth\"\nuse_cuda=True",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Model Under Attack": [
                            [
                                "As mentioned, the model under attack is the same MNIST model from\n.\nYou may train and save your own MNIST model or you can download and use\nthe provided model. The <em>Net</em> definition and test dataloader here have\nbeen copied from the MNIST example. The purpose of this section is to\ndefine the model and dataloader, then initialize the model and load the\npretrained weights.",
                                "markdown"
                            ],
                            [
                                "# LeNet Model definition\nclass Net():\n    def __init__(self):\n        super(, self).__init__()\n        self.conv1 = (1, 10, kernel_size=5)\n        self.conv2 = (10, 20, kernel_size=5)\n        self.conv2_drop = ()\n        self.fc1 = (320, 50)\n        self.fc2 = (50, 10)\n\n    def forward(self, x):\n        x = ((self.conv1(x), 2))\n        x = ((self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = (self.fc1(x))\n        x = (x, training=self.training)\n        x = self.fc2(x)\n        return (x, dim=1)\n\n# MNIST Test dataset and dataloader declaration\n = (\n    ('../data', train=False, download=True, transform=([\n            (),\n            ])),\n        batch_size=1, shuffle=True)\n\n# Define what device we are using\nprint(\"CUDA Available: \",())\n = (\"cuda\" if (use_cuda and ()) else \"cpu\")\n\n# Initialize the network\nmodel = ().to()\n\n# Load the pretrained model\n((pretrained_model, map_location='cpu'))\n\n# Set the model in evaluation mode. In this case this is for the Dropout layers\n()",
                                "code"
                            ],
                            [
                                "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/9912422 [00:00&lt;?, ?it/s]\n 90%|########9 | 8880128/9912422 [00:00&lt;00:00, 88669659.28it/s]\n100%|##########| 9912422/9912422 [00:00&lt;00:00, 94623370.71it/s]\nExtracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/28881 [00:00&lt;?, ?it/s]\n100%|##########| 28881/28881 [00:00&lt;00:00, 26611532.04it/s]\nExtracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/1648877 [00:00&lt;?, ?it/s]\n100%|##########| 1648877/1648877 [00:00&lt;00:00, 25857956.22it/s]\nExtracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/4542 [00:00&lt;?, ?it/s]\n100%|##########| 4542/4542 [00:00&lt;00:00, 30143241.72it/s]\nExtracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nCUDA Available:  True\n\nNet(\n  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n  (fc1): Linear(in_features=320, out_features=50, bias=True)\n  (fc2): Linear(in_features=50, out_features=10, bias=True)\n)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "FGSM Attack": [
                            [
                                "Now, we can define the function that creates the adversarial examples by\nperturbing the original inputs. The fgsm_attack function takes three\ninputs, <em>image</em> is the original clean image (\\(x\\)), <em>epsilon</em> is\nthe pixel-wise perturbation amount (\\(\\epsilon\\)), and <em>data_grad</em>\nis gradient of the loss w.r.t the input image\n(\\(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y)\\)). The function\nthen creates perturbed image as\n\n\\[perturbed\\_image = image + epsilon*sign(data\\_grad) = x + \\epsilon * sign(\\nabla_{x} J(\\mathbf{\\theta}, \\mathbf{x}, y))\n\n\\]",
                                "markdown"
                            ],
                            [
                                "Finally, in order to maintain the original range of the data, the\nperturbed image is clipped to range \\([0,1]\\).",
                                "markdown"
                            ],
                            [
                                "# FGSM attack code\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = (perturbed_image, 0, 1)\n    # Return the perturbed image\n    return perturbed_image",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Testing Function": [
                            [
                                "Finally, the central result of this tutorial comes from the test\nfunction. Each call to this test function performs a full test step on\nthe MNIST test set and reports a final accuracy. However, notice that\nthis function also takes an <em>epsilon</em> input. This is because the\ntest function reports the accuracy of a model that is under attack\nfrom an adversary with strength \\(\\epsilon\\). More specifically, for\neach sample in the test set, the function computes the gradient of the\nloss w.r.t the input data (\\(data\\_grad\\)), creates a perturbed\nimage with fgsm_attack (\\(perturbed\\_data\\)), then checks to see\nif the perturbed example is adversarial. In addition to testing the\naccuracy of the model, the function also saves and returns some\nsuccessful adversarial examples to be visualized later.",
                                "markdown"
                            ],
                            [
                                "def test( model, , , epsilon ):\n\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n\n    # Loop over all examples in test set\n    for data, target in :\n\n        # Send the data and label to the device\n        data, target = data.to(), target.to()\n\n        # Set requires_grad attribute of tensor. Important for Attack\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n\n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = (output, target)\n\n        # Zero all existing gradients\n        ()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        output = model(perturbed_data)\n\n        # Check for success\n        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        if final_pred.item() == target.item():\n            correct += 1\n            # Special case for saving 0 epsilon examples\n            if (epsilon == 0) and (len(adv_examples) &lt; 5):\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n        else:\n            # Save some adv examples for visualization later\n            if len(adv_examples) &lt; 5:\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n\n    # Calculate final accuracy for this epsilon\n    final_acc = correct/float(len())\n    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(), final_acc))\n\n    # Return the accuracy and an adversarial example\n    return final_acc, adv_examples",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Run Attack": [
                            [
                                "The last part of the implementation is to actually run the attack. Here,\nwe run a full test step for each epsilon value in the <em>epsilons</em> input.\nFor each epsilon we also save the final accuracy and some successful\nadversarial examples to be plotted in the coming sections. Notice how\nthe printed accuracies decrease as the epsilon value increases. Also,\nnote the \\(\\epsilon=0\\) case represents the original test accuracy,\nwith no attack.",
                                "markdown"
                            ],
                            [
                                "accuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, , , eps)\n    accuracies.append(acc)\n    examples.append(ex)",
                                "code"
                            ],
                            [
                                "Epsilon: 0      Test Accuracy = 9810 / 10000 = 0.981\nEpsilon: 0.05   Test Accuracy = 9426 / 10000 = 0.9426\nEpsilon: 0.1    Test Accuracy = 8510 / 10000 = 0.851\nEpsilon: 0.15   Test Accuracy = 6826 / 10000 = 0.6826\nEpsilon: 0.2    Test Accuracy = 4301 / 10000 = 0.4301\nEpsilon: 0.25   Test Accuracy = 2082 / 10000 = 0.2082\nEpsilon: 0.3    Test Accuracy = 869 / 10000 = 0.0869",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Results": [
                    {
                        "Accuracy vs Epsilon": [
                            [
                                "The first result is the accuracy versus epsilon plot. As alluded to\nearlier, as epsilon increases we expect the test accuracy to decrease.\nThis is because larger epsilons mean we take a larger step in the\ndirection that will maximize the loss. Notice the trend in the curve is\nnot linear even though the epsilon values are linearly spaced. For\nexample, the accuracy at \\(\\epsilon=0.05\\) is only about 4% lower\nthan \\(\\epsilon=0\\), but the accuracy at \\(\\epsilon=0.2\\) is 25%\nlower than \\(\\epsilon=0.15\\). Also, notice the accuracy of the model\nhits random accuracy for a 10-class classifier between\n\\(\\epsilon=0.25\\) and \\(\\epsilon=0.3\\).",
                                "markdown"
                            ],
                            [
                                "plt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n\n<img alt=\"Accuracy vs Epsilon\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_001.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_001.png\"/>",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Sample Adversarial Examples": [
                            [
                                "Remember the idea of no free lunch? In this case, as epsilon increases\nthe test accuracy decreases <strong>BUT</strong> the perturbations become more easily\nperceptible. In reality, there is a tradeoff between accuracy\ndegredation and perceptibility that an attacker must consider. Here, we\nshow some examples of successful adversarial examples at each epsilon\nvalue. Each row of the plot shows a different epsilon value. The first\nrow is the \\(\\epsilon=0\\) examples which represent the original\n\u201cclean\u201d images with no perturbation. The title of each image shows the\n\u201coriginal classification -&gt; adversarial classification.\u201d Notice, the\nperturbations start to become evident at \\(\\epsilon=0.15\\) and are\nquite evident at \\(\\epsilon=0.3\\). However, in all cases humans are\nstill capable of identifying the correct class despite the added noise.",
                                "markdown"
                            ],
                            [
                                "# Plot several examples of adversarial samples at each epsilon\ncnt = 0\nplt.figure(figsize=(8,10))\nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        plt.title(\"{} -&gt; {}\".format(orig, adv))\n        plt.imshow(ex, cmap=\"gray\")\nplt.tight_layout()\nplt.show()\n\n\n<img alt=\"2 -&gt; 2, 6 -&gt; 6, 4 -&gt; 4, 6 -&gt; 6, 2 -&gt; 2, 8 -&gt; 3, 4 -&gt; 2, 9 -&gt; 5, 7 -&gt; 9, 8 -&gt; 9, 6 -&gt; 0, 1 -&gt; 8, 6 -&gt; 8, 9 -&gt; 8, 8 -&gt; 6, 5 -&gt; 8, 5 -&gt; 2, 4 -&gt; 8, 4 -&gt; 8, 5 -&gt; 8, 3 -&gt; 5, 4 -&gt; 8, 8 -&gt; 2, 8 -&gt; 6, 3 -&gt; 8, 6 -&gt; 1, 1 -&gt; 3, 1 -&gt; 8, 4 -&gt; 8, 8 -&gt; 2, 7 -&gt; 8, 1 -&gt; 2, 0 -&gt; 2, 7 -&gt; 8, 7 -&gt; 8\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_fgsm_tutorial_002.png\" srcset=\"../_images/sphx_glr_fgsm_tutorial_002.png\"/>",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Where to go next?": [
                    [
                        "Hopefully this tutorial gives some insight into the topic of adversarial\nmachine learning. There are many potential directions to go from here.\nThis attack represents the very beginning of adversarial attack research\nand since there have been many subsequent ideas for how to attack and\ndefend ML models from an adversary. In fact, at NIPS 2017 there was an\nadversarial attack and defense competition and many of the methods used\nin the competition are described in this paper: . The work\non defense also leads into the idea of making machine learning models\nmore <em>robust</em> in general, to both naturally perturbed and adversarially\ncrafted inputs.",
                        "markdown"
                    ],
                    [
                        "Another direction to go is adversarial attacks and defense in different\ndomains. Adversarial research is not limited to the image domain, check\nout  attack on\nspeech-to-text models. But perhaps the best way to learn more about\nadversarial machine learning is to get your hands dirty. Try to\nimplement a different attack from the NIPS 2017 competition, and see how\nit differs from FGSM. Then, try to defend the model from your own\nattacks.",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 2 minutes  38.625 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Spatial Transformer Networks Tutorial": [
            [
                "<strong>Author</strong>: \n\n<img alt=\"../_images/FSeq.png\" src=\"../_images/FSeq.png\"/>",
                "markdown"
            ],
            [
                "In this tutorial, you will learn how to augment your network using\na visual attention mechanism called spatial transformer\nnetworks. You can read more about the spatial transformer\nnetworks in the ",
                "markdown"
            ],
            [
                "Spatial transformer networks are a generalization of differentiable\nattention to any spatial transformation. Spatial transformer networks\n(STN for short) allow a neural network to learn how to perform spatial\ntransformations on the input image in order to enhance the geometric\ninvariance of the model.\nFor example, it can crop a region of interest, scale and correct\nthe orientation of an image. It can be a useful mechanism because CNNs\nare not invariant to rotation and scale and more general affine\ntransformations.",
                "markdown"
            ],
            [
                "One of the best things about STN is the ability to simply plug it into\nany existing CNN with very little modification.",
                "markdown"
            ],
            [
                "# License: BSD\n# Author: Ghassen Hamrouni\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.ion()   # interactive mode",
                "code"
            ],
            [
                "&lt;contextlib.ExitStack object at 0x7f2d90743cd0&gt;",
                "code"
            ],
            {
                "Loading the data": [
                    [
                        "In this post we experiment with the classic MNIST dataset. Using a\nstandard convolutional network augmented with a spatial transformer\nnetwork.",
                        "markdown"
                    ],
                    [
                        "from six.moves import urllib\nopener = urllib.request.build_opener()\nopener.addheaders = [('User-agent', 'Mozilla/5.0')]\nurllib.request.install_opener(opener)\n\n = (\"cuda\" if () else \"cpu\")\n\n# Training dataset\n = (\n    (root='.', train=True, download=True,\n                   transform=([\n                       (),\n                       ((0.1307,), (0.3081,))\n                   ])), batch_size=64, shuffle=True, num_workers=4)\n# Test dataset\n = (\n    (root='.', train=False, transform=([\n        (),\n        ((0.1307,), (0.3081,))\n    ])), batch_size=64, shuffle=True, num_workers=4)",
                        "code"
                    ],
                    [
                        "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/9912422 [00:00&lt;?, ?it/s]\n 85%|########4 | 8388608/9912422 [00:00&lt;00:00, 83763738.69it/s]\n100%|##########| 9912422/9912422 [00:00&lt;00:00, 96041985.73it/s]\nExtracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/28881 [00:00&lt;?, ?it/s]\n100%|##########| 28881/28881 [00:00&lt;00:00, 133262589.47it/s]\nExtracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/1648877 [00:00&lt;?, ?it/s]\n100%|##########| 1648877/1648877 [00:00&lt;00:00, 25862694.45it/s]\nExtracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/4542 [00:00&lt;?, ?it/s]\n100%|##########| 4542/4542 [00:00&lt;00:00, 28139628.90it/s]\nExtracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw",
                        "code"
                    ]
                ]
            },
            {
                "Depicting spatial transformer networks": [
                    [
                        "Spatial transformer networks boils down to three main components :",
                        "markdown"
                    ],
                    [
                        "The localization network is a regular CNN which regresses the\ntransformation parameters. The transformation is never learned\nexplicitly from this dataset, instead the network learns automatically\nthe spatial transformations that enhances the global accuracy.",
                        "markdown"
                    ],
                    [
                        "The grid generator generates a grid of coordinates in the input\nimage corresponding to each pixel from the output image.",
                        "markdown"
                    ],
                    [
                        "The sampler uses the parameters of the transformation and applies\nit to the input image.\n\n\n<img alt=\"../_images/stn-arch.png\" src=\"../_images/stn-arch.png\"/>",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "We need the latest version of PyTorch that contains\naffine_grid and grid_sample modules.",
                        "markdown"
                    ],
                    [
                        "class Net():\n    def __init__(self):\n        super(, self).__init__()\n        self.conv1 = (1, 10, kernel_size=5)\n        self.conv2 = (10, 20, kernel_size=5)\n        self.conv2_drop = ()\n        self.fc1 = (320, 50)\n        self.fc2 = (50, 10)\n\n        # Spatial transformer localization-network\n        self.localization = (\n            (1, 8, kernel_size=7),\n            (2, stride=2),\n            (True),\n            (8, 10, kernel_size=5),\n            (2, stride=2),\n            (True)\n        )\n\n        # Regressor for the 3 * 2 affine matrix\n        self.fc_loc = (\n            (10 * 3 * 3, 32),\n            (True),\n            (32, 3 * 2)\n        )\n\n        # Initialize the weights/bias with identity transformation\n        self.fc_loc[2].weight.data.zero_()\n        self.fc_loc[2].bias.data.copy_(([1, 0, 0, 0, 1, 0], dtype=))\n\n    # Spatial transformer network forward function\n    def stn(self, x):\n        xs = self.localization(x)\n        xs = xs.view(-1, 10 * 3 * 3)\n        theta = self.fc_loc(xs)\n        theta = theta.view(-1, 2, 3)\n\n        grid = (theta, x.size())\n        x = (x, grid)\n\n        return x\n\n    def forward(self, x):\n        # transform the input\n        x = self.stn(x)\n\n        # Perform the usual forward pass\n        x = ((self.conv1(x), 2))\n        x = ((self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = (self.fc1(x))\n        x = (x, training=self.training)\n        x = self.fc2(x)\n        return (x, dim=1)\n\n\nmodel = ().to()",
                        "code"
                    ]
                ]
            },
            {
                "Training the model": [
                    [
                        "Now, let\u2019s use the SGD algorithm to train the model. The network is\nlearning the classification task in a supervised way. In the same time\nthe model is learning STN automatically in an end-to-end fashion.",
                        "markdown"
                    ],
                    [
                        " = ((), lr=0.01)\n\n\ndef train(epoch):\n    ()\n    for batch_idx, (data, target) in enumerate():\n        data, target = data.to(), target.to()\n\n        ()\n        output = model(data)\n        loss = (output, target)\n        loss.backward()\n        ()\n        if batch_idx % 500 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(),\n                100. * batch_idx / len(), loss.item()))\n#\n# A simple test procedure to measure the STN performances on MNIST.\n#\n\n\ndef test():\n    with ():\n        ()\n        test_loss = 0\n        correct = 0\n        for data, target in :\n            data, target = data.to(), target.to()\n            output = model(data)\n\n            # sum up batch loss\n            test_loss += (output, target, size_average=False).item()\n            # get the index of the max log-probability\n            pred = output.max(1, keepdim=True)[1]\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n        test_loss /= len()\n        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n              .format(test_loss, correct, len(),\n                      100. * correct / len()))",
                        "code"
                    ]
                ]
            },
            {
                "Visualizing the STN results": [
                    [
                        "Now, we will inspect the results of our learned visual attention\nmechanism.",
                        "markdown"
                    ],
                    [
                        "We define a small helper function in order to visualize the\ntransformations while training.",
                        "markdown"
                    ],
                    [
                        "def convert_image_np(inp):\n    \"\"\"Convert a Tensor to numpy image.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    return inp\n\n# We want to visualize the output of the spatial transformers layer\n# after the training, we visualize a batch of input images and\n# the corresponding transformed batch using STN.\n\n\ndef visualize_stn():\n    with ():\n        # Get a batch of training data\n        data = next(iter())[0].to()\n\n        input_tensor = data.cpu()\n        transformed_input_tensor = model.stn(data).cpu()\n\n        in_grid = convert_image_np(\n            (input_tensor))\n\n        out_grid = convert_image_np(\n            (transformed_input_tensor))\n\n        # Plot the results side-by-side\n        f, axarr = plt.subplots(1, 2)\n        axarr[0].imshow(in_grid)\n        axarr[0].set_title('Dataset Images')\n\n        axarr[1].imshow(out_grid)\n        axarr[1].set_title('Transformed Images')\n\nfor epoch in range(1, 20 + 1):\n    train(epoch)\n    test()\n\n# Visualize the STN transformation on some input batch\nvisualize_stn()\n\nplt.ioff()\nplt.show()\n\n\n<img alt=\"Dataset Images, Transformed Images\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_spatial_transformer_tutorial_001.png\" srcset=\"../_images/sphx_glr_spatial_transformer_tutorial_001.png\"/>",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:4298: UserWarning:\n\nDefault grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:4236: UserWarning:\n\nDefault grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n\nTrain Epoch: 1 [0/60000 (0%)]   Loss: 2.283169\nTrain Epoch: 1 [32000/60000 (53%)]      Loss: 0.815205\n/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning:\n\nsize_average and reduce args will be deprecated, please use reduction='sum' instead.\n\n\nTest set: Average loss: 0.1749, Accuracy: 9508/10000 (95%)\n\nTrain Epoch: 2 [0/60000 (0%)]   Loss: 0.338278\nTrain Epoch: 2 [32000/60000 (53%)]      Loss: 0.422044\n\nTest set: Average loss: 0.1444, Accuracy: 9563/10000 (96%)\n\nTrain Epoch: 3 [0/60000 (0%)]   Loss: 0.489171\nTrain Epoch: 3 [32000/60000 (53%)]      Loss: 0.283197\n\nTest set: Average loss: 0.1098, Accuracy: 9675/10000 (97%)\n\nTrain Epoch: 4 [0/60000 (0%)]   Loss: 0.226815\nTrain Epoch: 4 [32000/60000 (53%)]      Loss: 0.326167\n\nTest set: Average loss: 0.0650, Accuracy: 9817/10000 (98%)\n\nTrain Epoch: 5 [0/60000 (0%)]   Loss: 0.065320\nTrain Epoch: 5 [32000/60000 (53%)]      Loss: 0.154336\n\nTest set: Average loss: 0.0557, Accuracy: 9833/10000 (98%)\n\nTrain Epoch: 6 [0/60000 (0%)]   Loss: 0.081050\nTrain Epoch: 6 [32000/60000 (53%)]      Loss: 0.180084\n\nTest set: Average loss: 0.0575, Accuracy: 9833/10000 (98%)\n\nTrain Epoch: 7 [0/60000 (0%)]   Loss: 0.122517\nTrain Epoch: 7 [32000/60000 (53%)]      Loss: 0.224352\n\nTest set: Average loss: 0.0530, Accuracy: 9848/10000 (98%)\n\nTrain Epoch: 8 [0/60000 (0%)]   Loss: 0.130049\nTrain Epoch: 8 [32000/60000 (53%)]      Loss: 0.136648\n\nTest set: Average loss: 0.0474, Accuracy: 9857/10000 (99%)\n\nTrain Epoch: 9 [0/60000 (0%)]   Loss: 0.093681\nTrain Epoch: 9 [32000/60000 (53%)]      Loss: 0.050301\n\nTest set: Average loss: 0.0454, Accuracy: 9868/10000 (99%)\n\nTrain Epoch: 10 [0/60000 (0%)]  Loss: 0.103837\nTrain Epoch: 10 [32000/60000 (53%)]     Loss: 0.047425\n\nTest set: Average loss: 0.0772, Accuracy: 9784/10000 (98%)\n\nTrain Epoch: 11 [0/60000 (0%)]  Loss: 0.149770\nTrain Epoch: 11 [32000/60000 (53%)]     Loss: 0.025939\n\nTest set: Average loss: 0.0440, Accuracy: 9877/10000 (99%)\n\nTrain Epoch: 12 [0/60000 (0%)]  Loss: 0.211523\nTrain Epoch: 12 [32000/60000 (53%)]     Loss: 0.126906\n\nTest set: Average loss: 0.0396, Accuracy: 9881/10000 (99%)\n\nTrain Epoch: 13 [0/60000 (0%)]  Loss: 0.065644\nTrain Epoch: 13 [32000/60000 (53%)]     Loss: 0.080548\n\nTest set: Average loss: 0.0409, Accuracy: 9865/10000 (99%)\n\nTrain Epoch: 14 [0/60000 (0%)]  Loss: 0.097975\nTrain Epoch: 14 [32000/60000 (53%)]     Loss: 0.199458\n\nTest set: Average loss: 0.0355, Accuracy: 9896/10000 (99%)\n\nTrain Epoch: 15 [0/60000 (0%)]  Loss: 0.163116\nTrain Epoch: 15 [32000/60000 (53%)]     Loss: 0.105829\n\nTest set: Average loss: 0.0371, Accuracy: 9890/10000 (99%)\n\nTrain Epoch: 16 [0/60000 (0%)]  Loss: 0.117419\nTrain Epoch: 16 [32000/60000 (53%)]     Loss: 0.111820\n\nTest set: Average loss: 0.0476, Accuracy: 9860/10000 (99%)\n\nTrain Epoch: 17 [0/60000 (0%)]  Loss: 0.325285\nTrain Epoch: 17 [32000/60000 (53%)]     Loss: 0.079091\n\nTest set: Average loss: 0.0432, Accuracy: 9871/10000 (99%)\n\nTrain Epoch: 18 [0/60000 (0%)]  Loss: 0.111252\nTrain Epoch: 18 [32000/60000 (53%)]     Loss: 0.039592\n\nTest set: Average loss: 0.0437, Accuracy: 9861/10000 (99%)\n\nTrain Epoch: 19 [0/60000 (0%)]  Loss: 0.051505\nTrain Epoch: 19 [32000/60000 (53%)]     Loss: 0.143836\n\nTest set: Average loss: 0.0453, Accuracy: 9866/10000 (99%)\n\nTrain Epoch: 20 [0/60000 (0%)]  Loss: 0.159451\nTrain Epoch: 20 [32000/60000 (53%)]     Loss: 0.101160\n\nTest set: Average loss: 0.0390, Accuracy: 9880/10000 (99%)",
                        "code"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 3 minutes  27.639 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Optimizing Vision Transformer Model for Deployment": [
            [
                ",",
                "markdown"
            ],
            [
                "Vision Transformer models apply the cutting-edge attention-based\ntransformer models, introduced in Natural Language Processing to achieve\nall kinds of the state of the art (SOTA) results, to Computer Vision\ntasks. Facebook Data-efficient Image Transformers \nis a Vision Transformer model trained on ImageNet for image\nclassification.",
                "markdown"
            ],
            [
                "In this tutorial, we will first cover what DeiT is and how to use it,\nthen go through the complete steps of scripting, quantizing, optimizing,\nand using the model in iOS and Android apps. We will also compare the\nperformance of quantized, optimized and non-quantized, non-optimized\nmodels, and show the benefits of applying quantization and optimization\nto the model along the steps.",
                "markdown"
            ],
            {
                "What is DeiT": [
                    [
                        "Convolutional Neural Networks (CNNs) have been the main models for image\nclassification since deep learning took off in 2012, but CNNs typically\nrequire hundreds of millions of images for training to achieve the\nSOTAresults. DeiT is a vision transformer model that requires a lot less\ndata and computing resources for training to compete with the leading\nCNNs in performing image classification, which is made possible by two\nkey components of of DeiT:",
                        "markdown"
                    ],
                    [
                        "Data augmentation that simulates training on a much larger dataset;",
                        "markdown"
                    ],
                    [
                        "Native distillation that allows the transformer network to learn from\na CNN\u2019s output.",
                        "markdown"
                    ],
                    [
                        "DeiT shows that Transformers can be successfully applied to computer\nvision tasks, with limited access to data and resources. For more\ndetails on DeiT, see the \nand .",
                        "markdown"
                    ]
                ]
            },
            {
                "Classifying Images with DeiT": [
                    [
                        "Follow the README at the DeiT repo for detailed information on how to\nclassify images using DeiT, or for a quick test, first install the\nrequired packages:",
                        "markdown"
                    ],
                    [
                        "# pip install torch torchvision timm pandas requests",
                        "code"
                    ],
                    [
                        "To run in Google Colab, uncomment the following line:",
                        "markdown"
                    ],
                    [
                        "# !pip install timm pandas requests",
                        "code"
                    ],
                    [
                        "then run the script below:",
                        "markdown"
                    ],
                    [
                        "from PIL import Image\nimport torch\nimport timm\nimport requests\nimport torchvision.transforms as transforms\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\nprint(torch.__version__)\n# should be 1.8.0\n\n\nmodel = ('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n()\n\n = ([\n    (256, interpolation=3),\n    (224),\n    (),\n    (IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n])\n\n = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\n = ()[None,]\n = model()\n = ()\nprint(.item())",
                        "code"
                    ],
                    [
                        "2.0.0+cu117\nDownloading: \"https://github.com/facebookresearch/deit/zipball/main\" to /var/lib/jenkins/.cache/torch/hub/main.zip\nDownloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /var/lib/jenkins/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n\n  0%|          | 0.00/330M [00:00&lt;?, ?B/s]\n  0%|          | 56.0k/330M [00:00&lt;10:58, 526kB/s]\n  0%|          | 256k/330M [00:00&lt;04:20, 1.33MB/s]\n  0%|          | 1.13M/330M [00:00&lt;01:15, 4.57MB/s]\n  1%|1         | 4.59M/330M [00:00&lt;00:21, 16.0MB/s]\n  4%|3         | 12.4M/330M [00:00&lt;00:09, 36.3MB/s]\n  6%|6         | 19.9M/330M [00:00&lt;00:06, 47.0MB/s]\n  8%|8         | 27.5M/330M [00:00&lt;00:05, 55.1MB/s]\n 11%|#         | 35.3M/330M [00:00&lt;00:05, 59.8MB/s]\n 13%|#2        | 42.9M/330M [00:01&lt;00:04, 62.9MB/s]\n 15%|#5        | 50.7M/330M [00:01&lt;00:04, 64.7MB/s]\n 18%|#7        | 58.4M/330M [00:01&lt;00:04, 67.6MB/s]\n 20%|#9        | 65.9M/330M [00:01&lt;00:04, 68.2MB/s]\n 22%|##2       | 73.6M/330M [00:01&lt;00:03, 68.7MB/s]\n 25%|##4       | 81.4M/330M [00:01&lt;00:03, 69.3MB/s]\n 27%|##6       | 89.0M/330M [00:01&lt;00:03, 68.8MB/s]\n 29%|##9       | 96.5M/330M [00:01&lt;00:03, 66.1MB/s]\n 32%|###1      | 104M/330M [00:01&lt;00:03, 66.9MB/s]\n 34%|###3      | 112M/330M [00:02&lt;00:03, 68.4MB/s]\n 36%|###5      | 118M/330M [00:02&lt;00:03, 65.9MB/s]\n 38%|###8      | 126M/330M [00:02&lt;00:03, 66.8MB/s]\n 41%|####      | 134M/330M [00:02&lt;00:03, 68.1MB/s]\n 43%|####2     | 142M/330M [00:02&lt;00:02, 67.2MB/s]\n 45%|####5     | 149M/330M [00:02&lt;00:02, 67.7MB/s]\n 47%|####7     | 156M/330M [00:02&lt;00:02, 65.5MB/s]\n 49%|####9     | 162M/330M [00:02&lt;00:03, 54.0MB/s]\n 51%|#####     | 168M/330M [00:03&lt;00:03, 54.9MB/s]\n 53%|#####3    | 176M/330M [00:03&lt;00:02, 59.3MB/s]\n 55%|#####5    | 183M/330M [00:03&lt;00:02, 62.6MB/s]\n 58%|#####7    | 191M/330M [00:03&lt;00:02, 63.2MB/s]\n 60%|#####9    | 198M/330M [00:03&lt;00:02, 65.3MB/s]\n 62%|######1   | 204M/330M [00:03&lt;00:02, 60.0MB/s]\n 65%|######4   | 213M/330M [00:03&lt;00:01, 66.8MB/s]\n 67%|######6   | 221M/330M [00:03&lt;00:01, 68.0MB/s]\n 69%|######9   | 229M/330M [00:03&lt;00:01, 69.0MB/s]\n 71%|#######1  | 236M/330M [00:04&lt;00:01, 69.2MB/s]\n 74%|#######3  | 244M/330M [00:04&lt;00:01, 67.8MB/s]\n 76%|#######5  | 251M/330M [00:04&lt;00:01, 67.4MB/s]\n 78%|#######8  | 258M/330M [00:04&lt;00:01, 67.6MB/s]\n 81%|########  | 266M/330M [00:04&lt;00:01, 67.4MB/s]\n 83%|########2 | 274M/330M [00:04&lt;00:00, 68.4MB/s]\n 85%|########5 | 281M/330M [00:04&lt;00:00, 68.4MB/s]\n 87%|########7 | 289M/330M [00:04&lt;00:00, 69.6MB/s]\n 90%|########9 | 297M/330M [00:05&lt;00:00, 69.9MB/s]\n 92%|#########2| 304M/330M [00:05&lt;00:00, 69.0MB/s]\n 94%|#########4| 312M/330M [00:05&lt;00:00, 69.6MB/s]\n 97%|#########6| 319M/330M [00:05&lt;00:00, 69.4MB/s]\n 99%|#########9| 327M/330M [00:05&lt;00:00, 70.3MB/s]\n100%|##########| 330M/330M [00:05&lt;00:00, 63.0MB/s]\n269",
                        "code"
                    ],
                    [
                        "The output should be 269, which, according to the ImageNet list of class\nindex to , maps to \u2018timber\nwolf, grey wolf, gray wolf, Canis lupus\u2019.",
                        "markdown"
                    ],
                    [
                        "Now that we have verified that we can use the DeiT model to classify\nimages, let\u2019s see how to modify the model so it can run on iOS and\nAndroid apps.",
                        "markdown"
                    ]
                ]
            },
            {
                "Scripting DeiT": [
                    [
                        "To use the model on mobile, we first need to script the\nmodel. See the  for a\nquick overview. Run the code below to convert the DeiT model used in the\nprevious step to the TorchScript format that can run on mobile.",
                        "markdown"
                    ],
                    [
                        "model = ('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n()\nscripted_model = (model)\n(\"fbdeit_scripted.pt\")",
                        "code"
                    ],
                    [
                        "Using cache found in /var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main",
                        "code"
                    ],
                    [
                        "The scripted model file fbdeit_scripted.pt of size about 346MB is\ngenerated.",
                        "markdown"
                    ]
                ]
            },
            {
                "Quantizing DeiT": [
                    [
                        "To reduce the trained model size significantly while\nkeeping the inference accuracy about the same, quantization can be\napplied to the model. Thanks to the transformer model used in DeiT, we\ncan easily apply dynamic-quantization to the model, because dynamic\nquantization works best for LSTM and transformer models (see \nfor more details).",
                        "markdown"
                    ],
                    [
                        "Now run the code below:",
                        "markdown"
                    ],
                    [
                        "# Use 'x86' for server inference (the old 'fbgemm' is still available but 'x86' is the recommended default) and 'qnnpack' for mobile inference.\nbackend = \"x86\" # replaced with qnnpack causing much worse inference speed for quantized model on this notebook\n = torch.quantization.get_default_qconfig(backend)\ntorch.backends.quantized.engine = backend\n\nquantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={}, dtype=)\nscripted_quantized_model = (quantized_model)\n(\"fbdeit_scripted_quantized.pt\")",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning:\n\nPlease use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.",
                        "code"
                    ],
                    [
                        "This generates the scripted and quantized version of the model\nfbdeit_quantized_scripted.pt, with size about 89MB, a 74% reduction of\nthe non-quantized model size of 346MB!",
                        "markdown"
                    ],
                    [
                        "You can use the scripted_quantized_model to generate the same\ninference result:",
                        "markdown"
                    ],
                    [
                        " = scripted_quantized_model()\n = ()\nprint(.item())\n# The same output 269 should be printed",
                        "code"
                    ],
                    [
                        "269",
                        "code"
                    ]
                ]
            },
            {
                "Optimizing DeiT": [
                    [
                        "The final step before using the quantized and scripted\nmodel on mobile is to optimize it:",
                        "markdown"
                    ],
                    [
                        "from torch.utils.mobile_optimizer import \noptimized_scripted_quantized_model = (scripted_quantized_model)\n(\"fbdeit_optimized_scripted_quantized.pt\")",
                        "code"
                    ],
                    [
                        "The generated fbdeit_optimized_scripted_quantized.pt file has about the\nsame size as the quantized, scripted, but non-optimized model. The\ninference result remains the same.",
                        "markdown"
                    ],
                    [
                        " = optimized_scripted_quantized_model()\n = ()\nprint(.item())\n# Again, the same output 269 should be printed",
                        "code"
                    ],
                    [
                        "269",
                        "code"
                    ]
                ]
            },
            {
                "Using Lite Interpreter": [
                    [
                        "To see how much model size reduction and inference speed up the Lite\nInterpreter can result in, let\u2019s create the lite version of the model.",
                        "markdown"
                    ],
                    [
                        "optimized_scripted_quantized_model._save_for_lite_interpreter(\"fbdeit_optimized_scripted_quantized_lite.ptl\")\nptl = (\"fbdeit_optimized_scripted_quantized_lite.ptl\")",
                        "code"
                    ],
                    [
                        "Although the lite model size is comparable to the non-lite version, when\nrunning the lite version on mobile, the inference speed up is expected.",
                        "markdown"
                    ]
                ]
            },
            {
                "Comparing Inference Speed": [
                    [
                        "To see how the inference speed differs for the four models - the\noriginal model, the scripted model, the quantized-and-scripted model,\nthe optimized-quantized-and-scripted model - run the code below:",
                        "markdown"
                    ],
                    [
                        "with (use_cuda=False) as :\n     = model()\nwith (use_cuda=False) as :\n     = scripted_model()\nwith (use_cuda=False) as :\n     = scripted_quantized_model()\nwith (use_cuda=False) as :\n     = optimized_scripted_quantized_model()\nwith (use_cuda=False) as :\n     = ptl()\n\nprint(\"original model: {:.2f}ms\".format(/1000))\nprint(\"scripted model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized &amp; optimized model: {:.2f}ms\".format(/1000))\nprint(\"lite model: {:.2f}ms\".format(/1000))",
                        "code"
                    ],
                    [
                        "original model: 309.06ms\nscripted model: 335.36ms\nscripted &amp; quantized model: 240.60ms\nscripted &amp; quantized &amp; optimized model: 201.37ms\nlite model: 206.63ms",
                        "code"
                    ],
                    [
                        "The results running on a Google Colab are:",
                        "markdown"
                    ],
                    [
                        "original model: 1236.69ms\nscripted model: 1226.72ms\nscripted &amp; quantized model: 593.19ms\nscripted &amp; quantized &amp; optimized model: 598.01ms\nlite model: 600.72ms",
                        "code"
                    ],
                    [
                        "The following results summarize the inference time taken by each model\nand the percentage reduction of each model relative to the original\nmodel.",
                        "markdown"
                    ],
                    [
                        "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Model': ['original model','scripted model', 'scripted &amp; quantized model', 'scripted &amp; quantized &amp; optimized model', 'lite model']})\ndf = pd.concat([df, pd.DataFrame([\n    [\"{:.2f}ms\".format(/1000), \"0%\"],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)]],\n    columns=['Inference Time', 'Reduction'])], axis=1)\n\nprint(df)\n\n\"\"\"\n        Model                             Inference Time    Reduction\n0   original model                             1236.69ms           0%\n1   scripted model                             1226.72ms        0.81%\n2   scripted &amp; quantized model                  593.19ms       52.03%\n3   scripted &amp; quantized &amp; optimized model      598.01ms       51.64%\n4   lite model                                  600.72ms       51.43%\n\"\"\"",
                        "code"
                    ],
                    [
                        "                                    Model Inference Time Reduction\n0                          original model       309.06ms        0%\n1                          scripted model       335.36ms    -8.51%\n2              scripted &amp; quantized model       240.60ms    22.15%\n3  scripted &amp; quantized &amp; optimized model       201.37ms    34.84%\n4                              lite model       206.63ms    33.14%\n\n'\\n        Model                             Inference Time    Reduction\\n0\\toriginal model                             1236.69ms           0%\\n1\\tscripted model                             1226.72ms        0.81%\\n2\\tscripted &amp; quantized model                  593.19ms       52.03%\\n3\\tscripted &amp; quantized &amp; optimized model      598.01ms       51.64%\\n4\\tlite model                                  600.72ms       51.43%\\n'",
                        "code"
                    ],
                    {
                        "Learn More": [
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "<strong>Total running time of the script:</strong> ( 0 minutes  26.562 seconds)",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ]
                        ]
                    }
                ]
            }
        ]
    },
    "Audio": {
        "Audio I/O": [
            [
                "This tutorial has been moved to ",
                "markdown"
            ],
            [
                "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_io_tutorial.html'\" http-equiv=\"Refresh\"/>",
                "markdown"
            ]
        ],
        "Audio Resampling": [
            [
                "This tutorial has been moved to \nYou will be redirected in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_resampling_tutorial.html'\" http-equiv=\"Refresh\"/>",
                "markdown"
            ]
        ],
        "Audio Data Augmentation": [
            [
                "This tutorial has been moved to ",
                "markdown"
            ],
            [
                "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_data_augmentation_tutorial.html'\" http-equiv=\"Refresh\"/>",
                "markdown"
            ]
        ],
        "Audio Feature Extractions": [
            [
                "This tutorial has been moved to ",
                "markdown"
            ],
            [
                "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_feature_extractions_tutorial.html'\" http-equiv=\"Refresh\"/>",
                "markdown"
            ]
        ],
        "Audio Feature Augmentation": [
            [
                "This tutorial has been moved to ",
                "markdown"
            ],
            [
                "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/audio_data_augmentation_tutorial.html'\" http-equiv=\"Refresh\"/>",
                "markdown"
            ]
        ],
        "Audio Datasets": [
            [
                "This tutorial has been moved to ",
                "markdown"
            ],
            [
                "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/tutorials/beginner/audio_datasets_tutorial.html'\" http-equiv=\"Refresh\"/>",
                "markdown"
            ]
        ],
        "Speech Recognition with Wav2Vec2": [
            [
                "This tutorial has been moved to ",
                "markdown"
            ],
            [
                "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/speech_recognition_pipeline_tutorial.html'\" http-equiv=\"Refresh\"/>",
                "markdown"
            ]
        ],
        "Text-to-speech with Tacotron2": [
            [
                "This tutorial has been moved to ",
                "markdown"
            ],
            [
                "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/tacotron2_pipeline_tutorial.html'\" http-equiv=\"Refresh\"/>",
                "markdown"
            ]
        ],
        "Forced Alignment with Wav2Vec2": [
            [
                "This tutorial has been moved to ",
                "markdown"
            ],
            [
                "It will redirect in 3 seconds.\n<meta content=\"3; url='https://pytorch.org/audio/stable/tutorials/forced_alignment_tutorial.html'\" http-equiv=\"Refresh\"/>",
                "markdown"
            ]
        ]
    },
    "Text": {
        "Language Modeling with nn.Transformer and TorchText": [
            [
                "This is a tutorial on training a sequence-to-sequence model that uses the\n module.",
                "markdown"
            ],
            [
                "The PyTorch 1.2 release includes a standard transformer module based on the\npaper .\nCompared to Recurrent Neural Networks (RNNs), the transformer model has proven\nto be superior in quality for many sequence-to-sequence tasks while being more\nparallelizable. The nn.Transformer module relies entirely on an attention\nmechanism (implemented as\n)\nto draw global dependencies between input and output. The nn.Transformer\nmodule is highly modularized such that a single component (e.g.,\n)\ncan be easily adapted/composed.\n<img alt=\"../_images/transformer_architecture.jpg\" src=\"../_images/transformer_architecture.jpg\"/>",
                "markdown"
            ],
            {
                "Define the model": [
                    [
                        "In this tutorial, we train a nn.TransformerEncoder model on a\nlanguage modeling task. The language modeling task is to assign a\nprobability for the likelihood of a given word (or a sequence of words)\nto follow a sequence of words. A sequence of tokens are passed to the embedding\nlayer first, followed by a positional encoding layer to account for the order\nof the word (see the next paragraph for more details). The\nnn.TransformerEncoder consists of multiple layers of\n.\nAlong with the input sequence, a square attention mask is required because the\nself-attention layers in nn.TransformerEncoder are only allowed to attend\nthe earlier positions in the sequence. For the language modeling task, any\ntokens on the future positions should be masked. To produce a probability\ndistribution over output words, the output of the nn.TransformerEncoder\nmodel is passed through a linear layer followed by a log-softmax function.",
                        "markdown"
                    ],
                    [
                        "import math\nimport os\nfrom tempfile import TemporaryDirectory\nfrom typing import Tuple\n\nimport torch\nfrom torch import nn, \nimport torch.nn.functional as F\nfrom torch.nn import , \nfrom torch.utils.data import dataset\n\nclass TransformerModel():\n\n    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n                 nlayers: int, dropout: float = 0.5):\n        super().__init__()\n        self.model_type = 'Transformer'\n        self.pos_encoder = (d_model, dropout)\n        encoder_layers = (d_model, nhead, d_hid, dropout)\n        self.transformer_encoder = (encoder_layers, nlayers)\n        self.encoder = (ntoken, d_model)\n        self.d_model = d_model\n        self.decoder = (d_model, ntoken)\n\n        self.init_weights()\n\n    def init_weights(self) -&gt; None:\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src: , src_mask: ) -&gt; :\n        \"\"\"\n        Args:\n            src: Tensor, shape [seq_len, batch_size]\n            src_mask: Tensor, shape [seq_len, seq_len]\n\n        Returns:\n            output Tensor of shape [seq_len, batch_size, ntoken]\n        \"\"\"\n        src = self.encoder(src) * math.sqrt(self.d_model)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, src_mask)\n        output = self.decoder(output)\n        return output\n\n\ndef generate_square_subsequent_mask(sz: int) -&gt; :\n    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n    return ((sz, sz) * float('-inf'), diagonal=1)",
                        "code"
                    ],
                    [
                        "PositionalEncoding module injects some information about the\nrelative or absolute position of the tokens in the sequence. The\npositional encodings have the same dimension as the embeddings so that\nthe two can be summed. Here, we use sine and cosine functions of\ndifferent frequencies.",
                        "markdown"
                    ],
                    [
                        "class PositionalEncoding():\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = (p=dropout)\n\n        position = (max_len).unsqueeze(1)\n        div_term = ((0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = (max_len, 1, d_model)\n        pe[:, 0, 0::2] = (position * div_term)\n        pe[:, 0, 1::2] = (position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: ) -&gt; :\n        \"\"\"\n        Args:\n            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n        \"\"\"\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)",
                        "code"
                    ]
                ]
            },
            {
                "Load and batch data": [
                    [
                        "This tutorial uses torchtext to generate Wikitext-2 dataset.\nTo access torchtext datasets, please install torchdata following instructions at .\n%%\n<blockquote>",
                        "markdown"
                    ],
                    [
                        "%%bash\npip install torchdata\n\n\n</blockquote>",
                        "code"
                    ],
                    [
                        "The vocab object is built based on the train dataset and is used to numericalize\ntokens into tensors. Wikitext-2 represents rare tokens as <cite>&lt;unk&gt;</cite>.",
                        "markdown"
                    ],
                    [
                        "Given a 1-D vector of sequential data, batchify() arranges the data\ninto batch_size columns. If the data does not divide evenly into\nbatch_size columns, then the data is trimmed to fit. For instance, with\nthe alphabet as the data (total length of 26) and batch_size=4, we would\ndivide the alphabet into 4 sequences of length 6:\n\n\\[\\begin{bmatrix}\n\\text{A} &amp; \\text{B} &amp; \\text{C} &amp; \\ldots &amp; \\text{X} &amp; \\text{Y} &amp; \\text{Z}\n\\end{bmatrix}\n\\Rightarrow\n\\begin{bmatrix}\n\\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n\\end{bmatrix}\n\n\\]",
                        "markdown"
                    ],
                    [
                        "Batching enables more parallelizable processing. However, batching means that\nthe model treats each column independently; for example, the dependence of\nG and F can not be learned in the example above.",
                        "markdown"
                    ],
                    [
                        "from torchtext.datasets import \nfrom torchtext.data.utils import \nfrom torchtext.vocab import \n\ntrain_iter = (split='train')\ntokenizer = ('basic_english')\n = (map(tokenizer, train_iter), specials=['&lt;unk&gt;'])\n(['&lt;unk&gt;'])\n\ndef data_process(raw_text_iter: ) -&gt; :\n    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n    data = [((tokenizer(item)), dtype=) for item in raw_text_iter]\n    return (tuple(filter(lambda t: t.numel() &gt; 0, data)))\n\n# train_iter was \"consumed\" by the process of building the vocab,\n# so we have to create it again\ntrain_iter, val_iter, test_iter = ()\n = data_process(train_iter)\n = data_process(val_iter)\n = data_process(test_iter)\n\n = ('cuda' if () else 'cpu')\n\ndef batchify(data: , bsz: int) -&gt; :\n    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n    that wouldn't cleanly fit.\n\n    Args:\n        data: Tensor, shape [N]\n        bsz: int, batch size\n\n    Returns:\n        Tensor of shape [N // bsz, bsz]\n    \"\"\"\n    seq_len = data.size(0) // bsz\n    data = data[:seq_len * bsz]\n    data = data.view(bsz, seq_len).t().contiguous()\n    return data.to()\n\nbatch_size = 20\neval_batch_size = 10\n = batchify(, batch_size)  # shape [seq_len, batch_size]\n = batchify(, eval_batch_size)\n = batchify(, eval_batch_size)",
                        "code"
                    ],
                    {
                        "Functions to generate input and target sequence": [
                            [
                                "get_batch() generates a pair of input-target sequences for\nthe transformer model. It subdivides the source data into chunks of\nlength bptt. For the language modeling task, the model needs the\nfollowing words as Target. For example, with a bptt value of 2,\nwe\u2019d get the following two Variables for i = 0:\n<img alt=\"../_images/transformer_input_target.png\" src=\"../_images/transformer_input_target.png\"/>",
                                "markdown"
                            ],
                            [
                                "It should be noted that the chunks are along dimension 0, consistent\nwith the S dimension in the Transformer model. The batch dimension\nN is along dimension 1.",
                                "markdown"
                            ],
                            [
                                "bptt = 35\ndef get_batch(source: , i: int) -&gt; Tuple[, ]:\n    \"\"\"\n    Args:\n        source: Tensor, shape [full_seq_len, batch_size]\n        i: int\n\n    Returns:\n        tuple (data, target), where data has shape [seq_len, batch_size] and\n        target has shape [seq_len * batch_size]\n    \"\"\"\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].reshape(-1)\n    return data, target",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Initiate an instance": [
                    [
                        "The model hyperparameters are defined below. The vocab size is\nequal to the length of the vocab object.",
                        "markdown"
                    ],
                    [
                        "ntokens = len()  # size of vocabulary\nemsize = 200  # embedding dimension\nd_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 2  # number of heads in nn.MultiheadAttention\ndropout = 0.2  # dropout probability\nmodel = (ntokens, emsize, nhead, d_hid, nlayers, dropout).to()",
                        "code"
                    ]
                ]
            },
            {
                "Run the model": [
                    [
                        "We use \nwith the \n(stochastic gradient descent) optimizer. The learning rate is initially set to\n5.0 and follows a \nschedule. During training, we use \nto prevent gradients from exploding.",
                        "markdown"
                    ],
                    [
                        "import copy\nimport time\n\n = ()\nlr = 5.0  # learning rate\n = ((), lr=lr)\n = (, 1.0, gamma=0.95)\n\ndef train(model: ) -&gt; None:\n    ()  # turn on train mode\n    total_loss = 0.\n    log_interval = 200\n    start_time = time.time()\n    src_mask = generate_square_subsequent_mask(bptt).to()\n\n    num_batches = len() // bptt\n    for batch, i in enumerate(range(0, .size(0) - 1, bptt)):\n        data, targets = get_batch(, i)\n        seq_len = data.size(0)\n        if seq_len != bptt:  # only on last batch\n            src_mask = src_mask[:seq_len, :seq_len]\n        output = model(data, src_mask)\n        loss = (output.view(-1, ntokens), targets)\n\n        ()\n        loss.backward()\n        ((), 0.5)\n        .step()\n\n        total_loss += loss.item()\n        if batch % log_interval == 0 and batch &gt; 0:\n            lr = ()[0]\n            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n            cur_loss = total_loss / log_interval\n            ppl = math.exp(cur_loss)\n            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n            total_loss = 0\n            start_time = time.time()\n\ndef evaluate(model: , eval_data: ) -&gt; float:\n    ()  # turn on evaluation mode\n    total_loss = 0.\n    src_mask = generate_square_subsequent_mask(bptt).to()\n    with ():\n        for i in range(0, eval_data.size(0) - 1, bptt):\n            data, targets = get_batch(eval_data, i)\n            seq_len = data.size(0)\n            if seq_len != bptt:\n                src_mask = src_mask[:seq_len, :seq_len]\n            output = model(data, src_mask)\n            output_flat = output.view(-1, ntokens)\n            total_loss += seq_len * (output_flat, targets).item()\n    return total_loss / (len(eval_data) - 1)",
                        "code"
                    ],
                    [
                        "Loop over epochs. Save the model if the validation loss is the best\nwe\u2019ve seen so far. Adjust the learning rate after each epoch.",
                        "markdown"
                    ],
                    [
                        "best_val_loss = float('inf')\nepochs = 3\n\nwith TemporaryDirectory() as tempdir:\n    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n\n    for epoch in range(1, epochs + 1):\n        epoch_start_time = time.time()\n        train(model)\n        val_loss = evaluate(model, )\n        val_ppl = math.exp(val_loss)\n        elapsed = time.time() - epoch_start_time\n        print('-' * 89)\n        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n            f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n        print('-' * 89)\n\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            ((), best_model_params_path)\n\n        .step()\n    ((best_model_params_path)) # load best model states",
                        "code"
                    ],
                    [
                        "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 31.17 | loss  8.07 | ppl  3197.87\n| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 22.10 | loss  6.87 | ppl   960.80\n| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 22.12 | loss  6.44 | ppl   624.70\n| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 22.06 | loss  6.29 | ppl   541.16\n| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 22.14 | loss  6.19 | ppl   489.94\n| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 22.16 | loss  6.15 | ppl   469.48\n| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 22.10 | loss  6.12 | ppl   453.14\n| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 22.10 | loss  6.11 | ppl   452.01\n| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 22.12 | loss  6.03 | ppl   415.23\n| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 22.07 | loss  6.03 | ppl   414.24\n| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 22.10 | loss  5.90 | ppl   366.14\n| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 22.09 | loss  5.97 | ppl   391.54\n| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 22.08 | loss  5.95 | ppl   384.65\n| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 22.08 | loss  5.89 | ppl   359.97\n-----------------------------------------------------------------------------------------\n| end of epoch   1 | time: 69.80s | valid loss  5.84 | valid ppl   344.17\n-----------------------------------------------------------------------------------------\n| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 22.21 | loss  5.86 | ppl   351.29\n| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 22.08 | loss  5.85 | ppl   346.20\n| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 22.12 | loss  5.67 | ppl   289.41\n| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 22.08 | loss  5.70 | ppl   300.23\n| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 22.08 | loss  5.66 | ppl   286.99\n| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 22.14 | loss  5.69 | ppl   296.29\n| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 22.11 | loss  5.69 | ppl   296.85\n| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 22.11 | loss  5.71 | ppl   303.34\n| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 22.11 | loss  5.66 | ppl   286.84\n| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 22.08 | loss  5.67 | ppl   290.62\n| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 22.13 | loss  5.56 | ppl   258.76\n| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 22.10 | loss  5.65 | ppl   283.83\n| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 22.08 | loss  5.65 | ppl   283.45\n| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 22.05 | loss  5.58 | ppl   264.29\n-----------------------------------------------------------------------------------------\n| end of epoch   2 | time: 67.98s | valid loss  5.65 | valid ppl   285.44\n-----------------------------------------------------------------------------------------\n| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 22.20 | loss  5.60 | ppl   270.05\n| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 22.12 | loss  5.61 | ppl   274.27\n| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 22.12 | loss  5.41 | ppl   224.12\n| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 22.09 | loss  5.48 | ppl   239.00\n| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 22.08 | loss  5.43 | ppl   227.12\n| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 22.08 | loss  5.47 | ppl   237.65\n| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 22.09 | loss  5.49 | ppl   241.54\n| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 22.08 | loss  5.51 | ppl   248.00\n| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 22.09 | loss  5.46 | ppl   235.99\n| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 22.14 | loss  5.48 | ppl   239.26\n| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 22.11 | loss  5.35 | ppl   210.16\n| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 22.14 | loss  5.46 | ppl   234.99\n| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 22.19 | loss  5.47 | ppl   237.56\n| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 22.23 | loss  5.40 | ppl   222.09\n-----------------------------------------------------------------------------------------\n| end of epoch   3 | time: 68.06s | valid loss  5.62 | valid ppl   275.14\n-----------------------------------------------------------------------------------------",
                        "code"
                    ]
                ]
            },
            {
                "Evaluate the best model on the test dataset": [
                    [
                        "test_loss = evaluate(model, )\ntest_ppl = math.exp(test_loss)\nprint('=' * 89)\nprint(f'| End of training | test loss {test_loss:5.2f} | '\n      f'test ppl {test_ppl:8.2f}')\nprint('=' * 89)",
                        "code"
                    ],
                    [
                        "=========================================================================================\n| End of training | test loss  5.52 | test ppl   250.61\n=========================================================================================",
                        "code"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 3 minutes  41.329 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Fast Transformer Inference with Better Transformer": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "This tutorial introduces Better Transformer (BT) as part of the PyTorch 1.12 release.\nIn this tutorial, we show how to use Better Transformer for production\ninference with torchtext.  Better Transformer is a production ready fastpath to\naccelerate deployment of Transformer models with high performance on CPU and GPU.\nThe fastpath feature works transparently for models based either directly on\nPyTorch core nn.module or with torchtext.",
                "markdown"
            ],
            [
                "Models which can be accelerated by Better Transformer fastpath execution are those\nusing the following PyTorch core <cite>torch.nn.module</cite> classes <cite>TransformerEncoder</cite>,\n<cite>TransformerEncoderLayer</cite>, and <cite>MultiHeadAttention</cite>.  In addition, torchtext has\nbeen updated to use the core library modules to benefit from fastpath acceleration.\n(Additional modules may be enabled with fastpath execution in the future.)",
                "markdown"
            ],
            [
                "Better Transformer offers two types of acceleration:",
                "markdown"
            ],
            [
                "Native multihead attention (MHA) implementation for CPU and GPU to improve overall execution efficiency.",
                "markdown"
            ],
            [
                "Exploiting sparsity in NLP inference.  Because of variable input lengths, input\ntokens may contain a large number of padding tokens for which processing may be\nskipped, delivering significant speedups.",
                "markdown"
            ],
            [
                "Fastpath execution is subject to some criteria. Most importantly, the model\nmust be executed in inference mode and operate on input tensors that do not collect\ngradient tape information (e.g., running with torch.no_grad).",
                "markdown"
            ],
            [
                "To follow this example in Google Colab, .",
                "markdown"
            ],
            {
                "Better Transformer Features in This Tutorial": [
                    [
                        "Load pre-trained models (pre-1.12 created without Better Transformer)",
                        "markdown"
                    ],
                    [
                        "Run and benchmark inference on CPU with and without BT fastpath (native MHA only)",
                        "markdown"
                    ],
                    [
                        "Run and benchmark inference on (configurable) DEVICE with and without BT fastpath (native MHA only)",
                        "markdown"
                    ],
                    [
                        "Enable sparsity support",
                        "markdown"
                    ],
                    [
                        "Run and benchmark inference on (configurable) DEVICE with and without BT fastpath (native MHA + sparsity)",
                        "markdown"
                    ]
                ]
            },
            {
                "Additional Information": [
                    [
                        "Additional information about Better Transformer may be found in the PyTorch.Org blog\n.",
                        "markdown"
                    ],
                    [
                        "Setup",
                        "markdown"
                    ],
                    [
                        "1.1 Load pre-trained models",
                        "markdown"
                    ],
                    [
                        "We download the XLM-R model from the pre-defined torchtext models by following the instructions in\n.  We also set the DEVICE to execute\non-accelerator tests.  (Enable GPU execution for your environment as appropriate.)",
                        "markdown"
                    ],
                    [
                        "import torch\nimport torch.nn as nn\n\nprint(f\"torch version: {torch.__version__}\")\n\nDEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nprint(f\"torch cuda available: {torch.cuda.is_available()}\")\n\nimport torch, torchtext\nfrom torchtext.models import RobertaClassificationHead\nfrom torchtext.functional import to_tensor\nxlmr_large = torchtext.models.XLMR_LARGE_ENCODER\nclassifier_head = torchtext.models.RobertaClassificationHead(num_classes=2, input_dim = 1024)\nmodel = xlmr_large.get_model(head=classifier_head)\ntransform = xlmr_large.transform()",
                        "code"
                    ],
                    [
                        "1.2 Dataset Setup",
                        "markdown"
                    ],
                    [
                        "We set up two types of inputs: a small input batch and a big input batch with sparsity.",
                        "markdown"
                    ],
                    [
                        "small_input_batch = [\n               \"Hello world\",\n               \"How are you!\"\n]\nbig_input_batch = [\n               \"Hello world\",\n               \"How are you!\",\n               \"\"\"`Well, Prince, so Genoa and Lucca are now just family estates of the\nBuonapartes. But I warn you, if you don't tell me that this means war,\nif you still try to defend the infamies and horrors perpetrated by\nthat Antichrist- I really believe he is Antichrist- I will have\nnothing more to do with you and you are no longer my friend, no longer\nmy 'faithful slave,' as you call yourself! But how do you do? I see\nI have frightened you- sit down and tell me all the news.`\n\nIt was in July, 1805, and the speaker was the well-known Anna\nPavlovna Scherer, maid of honor and favorite of the Empress Marya\nFedorovna. With these words she greeted Prince Vasili Kuragin, a man\nof high rank and importance, who was the first to arrive at her\nreception. Anna Pavlovna had had a cough for some days. She was, as\nshe said, suffering from la grippe; grippe being then a new word in\nSt. Petersburg, used only by the elite.\"\"\"\n]",
                        "code"
                    ],
                    [
                        "Next, we select either the small or large input batch, preprocess the inputs and test the model.",
                        "markdown"
                    ],
                    [
                        "input_batch=big_input_batch\n\nmodel_input = to_tensor(transform(input_batch), padding_value=1)\noutput = model(model_input)\noutput.shape",
                        "code"
                    ],
                    [
                        "Finally, we set the benchmark iteration count:",
                        "markdown"
                    ],
                    [
                        "ITERATIONS=10",
                        "code"
                    ],
                    [
                        "Execution",
                        "markdown"
                    ],
                    [
                        "2.1  Run and benchmark inference on CPU with and without BT fastpath (native MHA only)",
                        "markdown"
                    ],
                    [
                        "We run the model on CPU, and collect profile information:",
                        "markdown"
                    ],
                    [
                        "The first run uses traditional (\u201cslow path\u201d) execution.",
                        "markdown"
                    ],
                    [
                        "The second run enables BT fastpath execution by putting the model in inference mode using <cite>model.eval()</cite> and disables gradient collection with <cite>torch.no_grad()</cite>.",
                        "markdown"
                    ],
                    [
                        "You can see an improvement (whose magnitude will depend on the CPU model) when the model is executing on CPU.  Notice that the fastpath profile shows most of the execution time\nin the native <cite>TransformerEncoderLayer</cite> implementation <cite>aten::_transformer_encoder_layer_fwd</cite>.",
                        "markdown"
                    ],
                    [
                        "print(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=False) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof)\n\nmodel.eval()\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=False) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof)",
                        "code"
                    ],
                    [
                        "2.2  Run and benchmark inference on (configurable) DEVICE with and without BT fastpath (native MHA only)",
                        "markdown"
                    ],
                    [
                        "We check the BT sparsity setting:",
                        "markdown"
                    ],
                    [
                        "model.encoder.transformer.layers.enable_nested_tensor",
                        "code"
                    ],
                    [
                        "We disable the BT sparsity:",
                        "markdown"
                    ],
                    [
                        "model.encoder.transformer.layers.enable_nested_tensor=False",
                        "code"
                    ],
                    [
                        "We run the model on DEVICE, and collect profile information for native MHA execution on DEVICE:",
                        "markdown"
                    ],
                    [
                        "The first run uses traditional (\u201cslow path\u201d) execution.",
                        "markdown"
                    ],
                    [
                        "The second run enables BT fastpath execution by putting the model in inference mode using <cite>model.eval()</cite>\nand disables gradient collection with <cite>torch.no_grad()</cite>.",
                        "markdown"
                    ],
                    [
                        "When executing on a GPU, you should see a significant speedup, in particular for the small input batch setting:",
                        "markdown"
                    ],
                    [
                        "model.to(DEVICE)\nmodel_input = model_input.to(DEVICE)\n\nprint(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof)\n\nmodel.eval()\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof)",
                        "code"
                    ],
                    [
                        "2.3 Run and benchmark inference on (configurable) DEVICE with and without BT fastpath (native MHA + sparsity)",
                        "markdown"
                    ],
                    [
                        "We enable sparsity support:",
                        "markdown"
                    ],
                    [
                        "model.encoder.transformer.layers.enable_nested_tensor = True",
                        "code"
                    ],
                    [
                        "We run the model on DEVICE, and collect profile information for native MHA and sparsity support execution on DEVICE:",
                        "markdown"
                    ],
                    [
                        "The first run uses traditional (\u201cslow path\u201d) execution.",
                        "markdown"
                    ],
                    [
                        "The second run enables BT fastpath execution by putting the model in inference mode using <cite>model.eval()</cite> and disables gradient collection with <cite>torch.no_grad()</cite>.",
                        "markdown"
                    ],
                    [
                        "When executing on a GPU, you should see a significant speedup, in particular for the large input batch setting which includes sparsity:",
                        "markdown"
                    ],
                    [
                        "model.to(DEVICE)\nmodel_input = model_input.to(DEVICE)\n\nprint(\"slow path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  for i in range(ITERATIONS):\n    output = model(model_input)\nprint(prof)\n\nmodel.eval()\n\nprint(\"fast path:\")\nprint(\"==========\")\nwith torch.autograd.profiler.profile(use_cuda=True) as prof:\n  with torch.no_grad():\n    for i in range(ITERATIONS):\n      output = model(model_input)\nprint(prof)",
                        "code"
                    ]
                ]
            },
            {
                "Summary": [
                    [
                        "In this tutorial, we have introduced fast transformer inference with\nBetter Transformer fastpath execution in torchtext using PyTorch core\nBetter Transformer support for Transformer Encoder models.  We have\ndemonstrated the use of Better Transformer with models trained prior to\nthe availability of BT fastpath execution.  We have demonstrated and\nbenchmarked the use of both BT fastpath execution modes, native MHA execution\nand BT sparsity acceleration.",
                        "markdown"
                    ]
                ]
            }
        ],
        "NLP From Scratch: Classifying Names with a Character-Level RNN": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "We will be building and training a basic character-level RNN to classify\nwords. This tutorial, along with the following two, show how to do\npreprocess data for NLP modeling \u201cfrom scratch\u201d, in particular not using\nmany of the convenience functions of <cite>torchtext</cite>, so you can see how\npreprocessing for NLP modeling works at a low level.",
                "markdown"
            ],
            [
                "A character-level RNN reads words as a series of characters -\noutputting a prediction and \u201chidden state\u201d at each step, feeding its\nprevious hidden state into each next step. We take the final prediction\nto be the output, i.e. which class the word belongs to.",
                "markdown"
            ],
            [
                "Specifically, we\u2019ll train on a few thousand surnames from 18 languages\nof origin, and predict which language a name is from based on the\nspelling:",
                "markdown"
            ],
            [
                "$ python predict.py Hinton\n(-0.47) Scottish\n(-1.52) English\n(-3.57) Irish\n\n$ python predict.py Schmidhuber\n(-0.19) German\n(-2.48) Czech\n(-2.68) Dutch",
                "code"
            ],
            [
                "<strong>Recommended Reading:</strong>",
                "markdown"
            ],
            [
                "I assume you have at least installed PyTorch, know Python, and\nunderstand Tensors:",
                "markdown"
            ],
            [
                " For installation instructions",
                "markdown"
            ],
            [
                " to get started with PyTorch in general",
                "markdown"
            ],
            [
                " for a wide and deep overview",
                "markdown"
            ],
            [
                " if you are former Lua Torch user",
                "markdown"
            ],
            [
                "It would also be useful to know about RNNs and how they work:",
                "markdown"
            ],
            [
                "shows a bunch of real life examples",
                "markdown"
            ],
            [
                "is about LSTMs specifically but also informative about RNNs in\ngeneral",
                "markdown"
            ],
            {
                "Preparing the Data": [
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "Download the data from\n\nand extract it to the current directory.",
                        "markdown"
                    ],
                    [
                        "Included in the data/names directory are 18 text files named as\n\u201c[Language].txt\u201d. Each file contains a bunch of names, one name per\nline, mostly romanized (but we still need to convert from Unicode to\nASCII).",
                        "markdown"
                    ],
                    [
                        "We\u2019ll end up with a dictionary of lists of names per language,\n{language: [names ...]}. The generic variables \u201ccategory\u201d and \u201cline\u201d\n(for language and name in our case) are used for later extensibility.",
                        "markdown"
                    ],
                    [
                        "from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport glob\nimport os\n\ndef findFiles(path): return glob.glob(path)\n\nprint(findFiles('data/names/*.txt'))\n\nimport unicodedata\nimport string\n\nall_letters = string.ascii_letters + \" .,;'\"\nn_letters = len(all_letters)\n\n# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n        and c in all_letters\n    )\n\nprint(unicodeToAscii('\u015alus\u00e0rski'))\n\n# Build the category_lines dictionary, a list of names per language\ncategory_lines = {}\nall_categories = []\n\n# Read a file and split into lines\ndef readLines(filename):\n    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n    return [unicodeToAscii(line) for line in lines]\n\nfor filename in findFiles('data/names/*.txt'):\n    category = os.path.splitext(os.path.basename(filename))[0]\n    all_categories.append(category)\n    lines = readLines(filename)\n    category_lines[category] = lines\n\nn_categories = len(all_categories)",
                        "code"
                    ],
                    [
                        "['data/names/Czech.txt', 'data/names/German.txt', 'data/names/Portuguese.txt', 'data/names/Russian.txt', 'data/names/Irish.txt', 'data/names/French.txt', 'data/names/Korean.txt', 'data/names/Arabic.txt', 'data/names/Vietnamese.txt', 'data/names/Italian.txt', 'data/names/Japanese.txt', 'data/names/English.txt', 'data/names/Polish.txt', 'data/names/Scottish.txt', 'data/names/Chinese.txt', 'data/names/Dutch.txt', 'data/names/Greek.txt', 'data/names/Spanish.txt']\nSlusarski",
                        "code"
                    ],
                    [
                        "Now we have category_lines, a dictionary mapping each category\n(language) to a list of lines (names). We also kept track of\nall_categories (just a list of languages) and n_categories for\nlater reference.",
                        "markdown"
                    ],
                    [
                        "print(category_lines['Italian'][:5])",
                        "code"
                    ],
                    [
                        "['Abandonato', 'Abatangelo', 'Abatantuono', 'Abate', 'Abategiovanni']",
                        "code"
                    ],
                    {
                        "Turning Names into Tensors": [
                            [
                                "Now that we have all the names organized, we need to turn them into\nTensors to make any use of them.",
                                "markdown"
                            ],
                            [
                                "To represent a single letter, we use a \u201cone-hot vector\u201d of size\n&lt;1 x n_letters&gt;. A one-hot vector is filled with 0s except for a 1\nat index of the current letter, e.g. \"b\" = &lt;0 1 0 0 0 ...&gt;.",
                                "markdown"
                            ],
                            [
                                "To make a word we join a bunch of those into a 2D matrix\n&lt;line_length x 1 x n_letters&gt;.",
                                "markdown"
                            ],
                            [
                                "That extra 1 dimension is because PyTorch assumes everything is in\nbatches - we\u2019re just using a batch size of 1 here.",
                                "markdown"
                            ],
                            [
                                "import torch\n\n# Find letter index from all_letters, e.g. \"a\" = 0\ndef letterToIndex(letter):\n    return all_letters.find(letter)\n\n# Just for demonstration, turn a letter into a &lt;1 x n_letters&gt; Tensor\ndef letterToTensor(letter):\n    tensor = (1, n_letters)\n    tensor[0][letterToIndex(letter)] = 1\n    return tensor\n\n# Turn a line into a &lt;line_length x 1 x n_letters&gt;,\n# or an array of one-hot letter vectors\ndef lineToTensor(line):\n    tensor = (len(line), 1, n_letters)\n    for li, letter in enumerate(line):\n        tensor[li][0][letterToIndex(letter)] = 1\n    return tensor\n\nprint(letterToTensor('J'))\n\nprint(lineToTensor('Jones').size())",
                                "code"
                            ],
                            [
                                "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0.]])\ntorch.Size([5, 1, 57])",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Creating the Network": [
                    [
                        "Before autograd, creating a recurrent neural network in Torch involved\ncloning the parameters of a layer over several timesteps. The layers\nheld hidden state and gradients which are now entirely handled by the\ngraph itself. This means you can implement a RNN in a very \u201cpure\u201d way,\nas regular feed-forward layers.",
                        "markdown"
                    ],
                    [
                        "This RNN module (mostly copied from )\nis just 2 linear layers which operate on an input and hidden state, with\na LogSoftmax layer after the output.\n\n<img alt=\"\" src=\"https://i.imgur.com/Z2xbySO.png\"/>",
                        "markdown"
                    ],
                    [
                        "import torch.nn as nn\n\nclass RNN():\n    def __init__(self, input_size, hidden_size, output_size):\n        super(, self).__init__()\n\n        self.hidden_size = hidden_size\n\n        self.i2h = (input_size + hidden_size, hidden_size)\n        self.i2o = (input_size + hidden_size, output_size)\n        self.softmax = (dim=1)\n\n    def forward(self, input, ):\n        combined = ((input, ), 1)\n         = self.i2h(combined)\n         = self.i2o(combined)\n         = self.softmax()\n        return , \n\n    def initHidden(self):\n        return (1, self.hidden_size)\n\nn_hidden = 128\nrnn = (n_letters, n_hidden, n_categories)",
                        "code"
                    ],
                    [
                        "To run a step of this network we need to pass an input (in our case, the\nTensor for the current letter) and a previous hidden state (which we\ninitialize as zeros at first). We\u2019ll get back the output (probability of\neach language) and a next hidden state (which we keep for the next\nstep).",
                        "markdown"
                    ],
                    [
                        "input = letterToTensor('A')\n = (1, n_hidden)\n\n,  = rnn(input, )",
                        "code"
                    ],
                    [
                        "For the sake of efficiency we don\u2019t want to be creating a new Tensor for\nevery step, so we will use lineToTensor instead of\nletterToTensor and use slices. This could be further optimized by\npre-computing batches of Tensors.",
                        "markdown"
                    ],
                    [
                        "input = lineToTensor('Albert')\n = (1, n_hidden)\n\n,  = rnn(input[0], )\nprint()",
                        "code"
                    ],
                    [
                        "tensor([[-2.9694, -2.9119, -2.9363, -2.7759, -2.8583, -2.8755, -2.9201, -2.9122,\n         -2.8318, -2.8369, -2.9570, -2.9842, -2.9026, -2.8321, -2.9084, -2.8505,\n         -2.8649, -2.9244]], grad_fn=&lt;LogSoftmaxBackward0&gt;)",
                        "code"
                    ],
                    [
                        "As you can see the output is a &lt;1 x n_categories&gt; Tensor, where\nevery item is the likelihood of that category (higher is more likely).",
                        "markdown"
                    ]
                ]
            },
            {
                "Training": [
                    {
                        "Preparing for Training": [
                            [
                                "Before going into training we should make a few helper functions. The\nfirst is to interpret the output of the network, which we know to be a\nlikelihood of each category. We can use Tensor.topk to get the index\nof the greatest value:",
                                "markdown"
                            ],
                            [
                                "def categoryFromOutput():\n    top_n, top_i = .topk(1)\n    category_i = top_i[0].item()\n    return all_categories[category_i], category_i\n\nprint(categoryFromOutput())",
                                "code"
                            ],
                            [
                                "('Russian', 3)",
                                "code"
                            ],
                            [
                                "We will also want a quick way to get a training example (a name and its\nlanguage):",
                                "markdown"
                            ],
                            [
                                "import random\n\ndef randomChoice(l):\n    return l[random.randint(0, len(l) - 1)]\n\ndef randomTrainingExample():\n    category = randomChoice(all_categories)\n    line = randomChoice(category_lines[category])\n     = ([all_categories.index(category)], dtype=)\n     = lineToTensor(line)\n    return category, line, , \n\nfor i in range(10):\n    category, line, ,  = randomTrainingExample()\n    print('category =', category, '/ line =', line)",
                                "code"
                            ],
                            [
                                "category = Irish / line = O'Mahony\ncategory = Dutch / line = Schoorl\ncategory = Dutch / line = Snyders\ncategory = Czech / line = Soukup\ncategory = Japanese / line = Takara\ncategory = Dutch / line = Richard\ncategory = Scottish / line = Walker\ncategory = Italian / line = Fonda\ncategory = Vietnamese / line = Dinh\ncategory = Dutch / line = Lauwers",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Training the Network": [
                            [
                                "Now all it takes to train this network is show it a bunch of examples,\nhave it make guesses, and tell it if it\u2019s wrong.",
                                "markdown"
                            ],
                            [
                                "For the loss function nn.NLLLoss is appropriate, since the last\nlayer of the RNN is nn.LogSoftmax.",
                                "markdown"
                            ],
                            [
                                " = ()",
                                "code"
                            ],
                            [
                                "Each loop of training will:",
                                "markdown"
                            ],
                            [
                                "Create input and target tensors",
                                "markdown"
                            ],
                            [
                                "Create a zeroed initial hidden state",
                                "markdown"
                            ],
                            [
                                "Read each letter in and",
                                "markdown"
                            ],
                            [
                                "Keep hidden state for next letter",
                                "markdown"
                            ],
                            [
                                "Compare final output to target",
                                "markdown"
                            ],
                            [
                                "Back-propagate",
                                "markdown"
                            ],
                            [
                                "Return the output and loss",
                                "markdown"
                            ],
                            [
                                "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n\ndef train(, ):\n     = rnn.initHidden()\n\n    ()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    loss = (, )\n    loss.backward()\n\n    # Add parameters' gradients to their values, multiplied by learning rate\n    for p in ():\n        p.data.add_(p.grad.data, alpha=-learning_rate)\n\n    return , loss.item()",
                                "code"
                            ],
                            [
                                "Now we just have to run that with a bunch of examples. Since the\ntrain function returns both the output and loss we can print its\nguesses and also keep track of loss for plotting. Since there are 1000s\nof examples we print only every print_every examples, and take an\naverage of the loss.",
                                "markdown"
                            ],
                            [
                                "import time\nimport math\n\nn_iters = 100000\nprint_every = 5000\nplot_every = 1000\n\n\n\n# Keep track of losses for plotting\ncurrent_loss = 0\nall_losses = []\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\nstart = time.time()\n\nfor iter in range(1, n_iters + 1):\n    category, line, ,  = randomTrainingExample()\n    , loss = train(, )\n    current_loss += loss\n\n    # Print iter number, loss, name and guess\n    if iter % print_every == 0:\n        guess, guess_i = categoryFromOutput()\n        correct = '\u2713' if guess == category else '\u2717 (%s)' % category\n        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n\n    # Add current loss avg to list of losses\n    if iter % plot_every == 0:\n        all_losses.append(current_loss / plot_every)\n        current_loss = 0",
                                "code"
                            ],
                            [
                                "5000 5% (0m 5s) 1.9595 Giordano / Italian \u2713\n10000 10% (0m 11s) 2.1746 Lilly / English \u2713\n15000 15% (0m 18s) 2.2633 Mushanaokoji / Italian \u2717 (Japanese)\n20000 20% (0m 23s) 1.8027 Campbell / Scottish \u2713\n25000 25% (0m 29s) 3.9679 Paulis / Greek \u2717 (German)\n30000 30% (0m 35s) 1.0005 Filipek / Polish \u2713\n35000 35% (0m 41s) 1.6657 Kumiega / Czech \u2717 (Polish)\n40000 40% (0m 48s) 0.2472 Vamvakidis / Greek \u2713\n45000 45% (0m 54s) 1.9403 Tykal / Scottish \u2717 (Czech)\n50000 50% (0m 59s) 3.0146 Meeuwe / French \u2717 (Dutch)\n55000 55% (1m 5s) 0.6135 Sung / Korean \u2713\n60000 60% (1m 11s) 3.2133 Kennedy / English \u2717 (Scottish)\n65000 65% (1m 17s) 0.3967 Fearghal / Irish \u2713\n70000 70% (1m 23s) 2.2125 Scott / English \u2717 (Scottish)\n75000 75% (1m 29s) 0.1403 Kowalski / Polish \u2713\n80000 80% (1m 35s) 1.1492 Silveira / Portuguese \u2713\n85000 85% (1m 41s) 0.0385 Paradjanov / Russian \u2713\n90000 90% (1m 47s) 0.5063 Cameron / Scottish \u2713\n95000 95% (1m 53s) 0.5789 Malone / Irish \u2713\n100000 100% (1m 59s) 1.7323 Baba / Japanese \u2717 (Arabic)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Plotting the Results": [
                            [
                                "Plotting the historical loss from all_losses shows the network\nlearning:",
                                "markdown"
                            ],
                            [
                                "import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nplt.figure()\nplt.plot(all_losses)\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_001.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_001.png\"/>",
                                "code"
                            ],
                            [
                                "[&lt;matplotlib.lines.Line2D object at 0x7f739bf9b640&gt;]",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Evaluating the Results": [
                    [
                        "To see how well the network performs on different categories, we will\ncreate a confusion matrix, indicating for every actual language (rows)\nwhich language the network guesses (columns). To calculate the confusion\nmatrix a bunch of samples are run through the network with\nevaluate(), which is the same as train() minus the backprop.",
                        "markdown"
                    ],
                    [
                        "# Keep track of correct guesses in a confusion matrix\n = (n_categories, n_categories)\nn_confusion = 10000\n\n# Just return an output given a line\ndef evaluate():\n     = rnn.initHidden()\n\n    for i in range(.size()[0]):\n        ,  = rnn([i], )\n\n    return \n\n# Go through a bunch of examples and record which are correctly guessed\nfor i in range(n_confusion):\n    category, line, ,  = randomTrainingExample()\n     = evaluate()\n    guess, guess_i = categoryFromOutput()\n    category_i = all_categories.index(category)\n    [category_i][guess_i] += 1\n\n# Normalize by dividing every row by its sum\nfor i in range(n_categories):\n    [i] = [i] / [i].sum()\n\n# Set up plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(.numpy())\nfig.colorbar(cax)\n\n# Set up axes\nax.set_xticklabels([''] + all_categories, rotation=90)\nax.set_yticklabels([''] + all_categories)\n\n# Force label at every tick\nax.xaxis.set_major_locator(ticker.MultipleLocator(1))\nax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n# sphinx_gallery_thumbnail_number = 2\nplt.show()\n\n\n<img alt=\"char rnn classification tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\" srcset=\"../_images/sphx_glr_char_rnn_classification_tutorial_002.png\"/>",
                        "code"
                    ],
                    [
                        "/var/lib/jenkins/workspace/intermediate_source/char_rnn_classification_tutorial.py:445: UserWarning:\n\nFixedFormatter should only be used together with FixedLocator\n\n/var/lib/jenkins/workspace/intermediate_source/char_rnn_classification_tutorial.py:446: UserWarning:\n\nFixedFormatter should only be used together with FixedLocator",
                        "code"
                    ],
                    [
                        "You can pick out bright spots off the main axis that show which\nlanguages it guesses incorrectly, e.g. Chinese for Korean, and Spanish\nfor Italian. It seems to do very well with Greek, and very poorly with\nEnglish (perhaps because of overlap with other languages).",
                        "markdown"
                    ],
                    {
                        "Running on User Input": [
                            [
                                "def predict(input_line, n_predictions=3):\n    print('\\n&gt; %s' % input_line)\n    with ():\n         = evaluate(lineToTensor(input_line))\n\n        # Get top N categories\n        topv, topi = .topk(n_predictions, 1, True)\n        predictions = []\n\n        for i in range(n_predictions):\n            value = topv[0][i].item()\n            category_index = topi[0][i].item()\n            print('(%.2f) %s' % (value, all_categories[category_index]))\n            predictions.append([value, all_categories[category_index]])\n\npredict('Dovesky')\npredict('Jackson')\npredict('Satoshi')",
                                "code"
                            ],
                            [
                                "&gt; Dovesky\n(-0.99) Polish\n(-1.33) Russian\n(-1.45) Czech\n\n&gt; Jackson\n(-0.15) Scottish\n(-2.29) English\n(-4.74) Russian\n\n&gt; Satoshi\n(-1.12) Japanese\n(-1.47) Italian\n(-2.14) Arabic",
                                "code"
                            ],
                            [
                                "The final versions of the scripts \nsplit the above code into a few files:",
                                "markdown"
                            ],
                            [
                                "data.py (loads files)",
                                "markdown"
                            ],
                            [
                                "model.py (defines the RNN)",
                                "markdown"
                            ],
                            [
                                "train.py (runs training)",
                                "markdown"
                            ],
                            [
                                "predict.py (runs predict() with command line arguments)",
                                "markdown"
                            ],
                            [
                                "server.py (serve prediction as a JSON API with bottle.py)",
                                "markdown"
                            ],
                            [
                                "Run train.py to train and save the network.",
                                "markdown"
                            ],
                            [
                                "Run predict.py with a name to view predictions:",
                                "markdown"
                            ],
                            [
                                "$ python predict.py Hazaki\n(-0.42) Japanese\n(-1.39) Polish\n(-3.51) Czech",
                                "code"
                            ],
                            [
                                "Run server.py and visit  to get JSON\noutput of predictions.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Exercises": [
                    [
                        "Try with a different dataset of line -&gt; category, for example:",
                        "markdown"
                    ],
                    [
                        "Any word -&gt; language",
                        "markdown"
                    ],
                    [
                        "First name -&gt; gender",
                        "markdown"
                    ],
                    [
                        "Character name -&gt; writer",
                        "markdown"
                    ],
                    [
                        "Page title -&gt; blog or subreddit",
                        "markdown"
                    ],
                    [
                        "Get better results with a bigger and/or better shaped network",
                        "markdown"
                    ],
                    [
                        "Add more linear layers",
                        "markdown"
                    ],
                    [
                        "Try the nn.LSTM and nn.GRU layers",
                        "markdown"
                    ],
                    [
                        "Combine multiple of these RNNs as a higher level network",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 2 minutes  6.017 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "NLP From Scratch: Generating Names with a Character-Level RNN": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "This is our second of three tutorials on \u201cNLP From Scratch\u201d.\nIn the <cite>first tutorial &lt;/intermediate/char_rnn_classification_tutorial&gt;</cite>\nwe used a RNN to classify names into their language of origin. This time\nwe\u2019ll turn around and generate names from languages.",
                "markdown"
            ],
            [
                "&gt; python sample.py Russian RUS\nRovakov\nUantov\nShavakov\n\n&gt; python sample.py German GER\nGerren\nEreng\nRosher\n\n&gt; python sample.py Spanish SPA\nSalla\nParer\nAllan\n\n&gt; python sample.py Chinese CHI\nChan\nHang\nIun",
                "code"
            ],
            [
                "We are still hand-crafting a small RNN with a few linear layers. The big\ndifference is instead of predicting a category after reading in all the\nletters of a name, we input a category and output one letter at a time.\nRecurrently predicting characters to form language (this could also be\ndone with words or other higher order constructs) is often referred to\nas a \u201clanguage model\u201d.",
                "markdown"
            ],
            [
                "<strong>Recommended Reading:</strong>",
                "markdown"
            ],
            [
                "I assume you have at least installed PyTorch, know Python, and\nunderstand Tensors:",
                "markdown"
            ],
            [
                " For installation instructions",
                "markdown"
            ],
            [
                " to get started with PyTorch in general",
                "markdown"
            ],
            [
                " for a wide and deep overview",
                "markdown"
            ],
            [
                " if you are former Lua Torch user",
                "markdown"
            ],
            [
                "It would also be useful to know about RNNs and how they work:",
                "markdown"
            ],
            [
                "shows a bunch of real life examples",
                "markdown"
            ],
            [
                "is about LSTMs specifically but also informative about RNNs in\ngeneral",
                "markdown"
            ],
            [
                "I also suggest the previous tutorial, ",
                "markdown"
            ],
            {
                "Preparing the Data": [
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "Download the data from\n\nand extract it to the current directory.",
                        "markdown"
                    ],
                    [
                        "See the last tutorial for more detail of this process. In short, there\nare a bunch of plain text files data/names/[Language].txt with a\nname per line. We split lines into an array, convert Unicode to ASCII,\nand end up with a dictionary {language: [names ...]}.",
                        "markdown"
                    ],
                    [
                        "from __future__ import unicode_literals, print_function, division\nfrom io import open\nimport glob\nimport os\nimport unicodedata\nimport string\n\nall_letters = string.ascii_letters + \" .,;'-\"\nn_letters = len(all_letters) + 1 # Plus EOS marker\n\ndef findFiles(path): return glob.glob(path)\n\n# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n        and c in all_letters\n    )\n\n# Read a file and split into lines\ndef readLines(filename):\n    with open(filename, encoding='utf-8') as some_file:\n        return [unicodeToAscii(line.strip()) for line in some_file]\n\n# Build the category_lines dictionary, a list of lines per category\ncategory_lines = {}\nall_categories = []\nfor filename in findFiles('data/names/*.txt'):\n    category = os.path.splitext(os.path.basename(filename))[0]\n    all_categories.append(category)\n    lines = readLines(filename)\n    category_lines[category] = lines\n\nn_categories = len(all_categories)\n\nif n_categories == 0:\n    raise RuntimeError('Data not found. Make sure that you downloaded data '\n        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n        'the current directory.')\n\nprint('# categories:', n_categories, all_categories)\nprint(unicodeToAscii(\"O'N\u00e9\u00e0l\"))",
                        "code"
                    ],
                    [
                        "# categories: 18 ['Czech', 'German', 'Portuguese', 'Russian', 'Irish', 'French', 'Korean', 'Arabic', 'Vietnamese', 'Italian', 'Japanese', 'English', 'Polish', 'Scottish', 'Chinese', 'Dutch', 'Greek', 'Spanish']\nO'Neal",
                        "code"
                    ]
                ]
            },
            {
                "Creating the Network": [
                    [
                        "This network extends \nwith an extra argument for the category tensor, which is concatenated\nalong with the others. The category tensor is a one-hot vector just like\nthe letter input.",
                        "markdown"
                    ],
                    [
                        "We will interpret the output as the probability of the next letter. When\nsampling, the most likely output letter is used as the next input\nletter.",
                        "markdown"
                    ],
                    [
                        "I added a second linear layer o2o (after combining hidden and\noutput) to give it more muscle to work with. There\u2019s also a dropout\nlayer, which  with a given probability\n(here 0.1) and is usually used to fuzz inputs to prevent overfitting.\nHere we\u2019re using it towards the end of the network to purposely add some\nchaos and increase sampling variety.\n\n<img alt=\"\" src=\"https://i.imgur.com/jzVrf7f.png\"/>",
                        "markdown"
                    ],
                    [
                        "import torch\nimport torch.nn as nn\n\nclass RNN():\n    def __init__(self, input_size, hidden_size, output_size):\n        super(, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.i2h = (n_categories + input_size + hidden_size, hidden_size)\n        self.i2o = (n_categories + input_size + hidden_size, output_size)\n        self.o2o = (hidden_size + output_size, output_size)\n        self.dropout = (0.1)\n        self.softmax = (dim=1)\n\n    def forward(self, category, input, hidden):\n        input_combined = ((category, input, hidden), 1)\n        hidden = self.i2h(input_combined)\n         = self.i2o(input_combined)\n        output_combined = ((hidden, ), 1)\n         = self.o2o(output_combined)\n         = self.dropout()\n         = self.softmax()\n        return , hidden\n\n    def initHidden(self):\n        return (1, self.hidden_size)",
                        "code"
                    ]
                ]
            },
            {
                "Training": [
                    {
                        "Preparing for Training": [
                            [
                                "First of all, helper functions to get random pairs of (category, line):",
                                "markdown"
                            ],
                            [
                                "import random\n\n# Random item from a list\ndef randomChoice(l):\n    return l[random.randint(0, len(l) - 1)]\n\n# Get a random category and random line from that category\ndef randomTrainingPair():\n    category = randomChoice(all_categories)\n    line = randomChoice(category_lines[category])\n    return category, line",
                                "code"
                            ],
                            [
                                "For each timestep (that is, for each letter in a training word) the\ninputs of the network will be\n(category, current letter, hidden state) and the outputs will be\n(next letter, next hidden state). So for each training set, we\u2019ll\nneed the category, a set of input letters, and a set of output/target\nletters.",
                                "markdown"
                            ],
                            [
                                "Since we are predicting the next letter from the current letter for each\ntimestep, the letter pairs are groups of consecutive letters from the\nline - e.g. for \"ABCD&lt;EOS&gt;\" we would create (\u201cA\u201d, \u201cB\u201d), (\u201cB\u201d, \u201cC\u201d),\n(\u201cC\u201d, \u201cD\u201d), (\u201cD\u201d, \u201cEOS\u201d).\n\n<img alt=\"\" src=\"https://i.imgur.com/JH58tXY.png\"/>",
                                "markdown"
                            ],
                            [
                                "The category tensor is a  of size\n&lt;1 x n_categories&gt;. When training we feed it to the network at every\ntimestep - this is a design choice, it could have been included as part\nof initial hidden state or some other strategy.",
                                "markdown"
                            ],
                            [
                                "# One-hot vector for category\ndef categoryTensor(category):\n    li = all_categories.index(category)\n    tensor = (1, n_categories)\n    tensor[0][li] = 1\n    return tensor\n\n# One-hot matrix of first to last letters (not including EOS) for input\ndef inputTensor(line):\n    tensor = (len(line), 1, n_letters)\n    for li in range(len(line)):\n        letter = line[li]\n        tensor[li][0][all_letters.find(letter)] = 1\n    return tensor\n\n# LongTensor of second letter to end (EOS) for target\ndef targetTensor(line):\n    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n    letter_indexes.append(n_letters - 1) # EOS\n    return torch.LongTensor(letter_indexes)",
                                "code"
                            ],
                            [
                                "For convenience during training we\u2019ll make a randomTrainingExample\nfunction that fetches a random (category, line) pair and turns them into\nthe required (category, input, target) tensors.",
                                "markdown"
                            ],
                            [
                                "# Make category, input, and target tensors from a random category, line pair\ndef randomTrainingExample():\n    category, line = randomTrainingPair()\n    category_tensor = categoryTensor(category)\n    input_line_tensor = inputTensor(line)\n    target_line_tensor = targetTensor(line)\n    return category_tensor, input_line_tensor, target_line_tensor",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Training the Network": [
                            [
                                "In contrast to classification, where only the last output is used, we\nare making a prediction at every step, so we are calculating loss at\nevery step.",
                                "markdown"
                            ],
                            [
                                "The magic of autograd allows you to simply sum these losses at each step\nand call backward at the end.",
                                "markdown"
                            ],
                            [
                                " = ()\n\nlearning_rate = 0.0005\n\ndef train(category_tensor, input_line_tensor, target_line_tensor):\n    target_line_tensor.unsqueeze_(-1)\n    hidden = rnn.initHidden()\n\n    ()\n\n    loss = 0\n\n    for i in range(input_line_tensor.size(0)):\n        , hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n        l = (, target_line_tensor[i])\n        loss += l\n\n    loss.backward()\n\n    for p in ():\n        p.data.add_(p.grad.data, alpha=-learning_rate)\n\n    return , loss.item() / input_line_tensor.size(0)",
                                "code"
                            ],
                            [
                                "To keep track of how long training takes I am adding a\ntimeSince(timestamp) function which returns a human readable string:",
                                "markdown"
                            ],
                            [
                                "import time\nimport math\n\ndef timeSince(since):\n    now = time.time()\n    s = now - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)",
                                "code"
                            ],
                            [
                                "Training is business as usual - call train a bunch of times and wait a\nfew minutes, printing the current time and loss every print_every\nexamples, and keeping store of an average loss per plot_every examples\nin all_losses for plotting later.",
                                "markdown"
                            ],
                            [
                                "rnn = (n_letters, 128, n_letters)\n\nn_iters = 100000\nprint_every = 5000\nplot_every = 500\nall_losses = []\ntotal_loss = 0 # Reset every plot_every iters\n\nstart = time.time()\n\nfor iter in range(1, n_iters + 1):\n    , loss = train(*randomTrainingExample())\n    total_loss += loss\n\n    if iter % print_every == 0:\n        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n\n    if iter % plot_every == 0:\n        all_losses.append(total_loss / plot_every)\n        total_loss = 0",
                                "code"
                            ],
                            [
                                "0m 14s (5000 5%) 2.8251\n0m 28s (10000 10%) 2.9402\n0m 42s (15000 15%) 2.8376\n0m 56s (20000 20%) 3.3196\n1m 9s (25000 25%) 3.0596\n1m 23s (30000 30%) 2.5502\n1m 37s (35000 35%) 2.5327\n1m 51s (40000 40%) 2.5966\n2m 5s (45000 45%) 1.8684\n2m 18s (50000 50%) 3.2985\n2m 32s (55000 55%) 2.1817\n2m 46s (60000 60%) 2.1271\n2m 59s (65000 65%) 2.9178\n3m 13s (70000 70%) 1.9024\n3m 27s (75000 75%) 2.7952\n3m 41s (80000 80%) 2.1264\n3m 54s (85000 85%) 1.9758\n4m 8s (90000 90%) 2.5004\n4m 21s (95000 95%) 2.0448\n4m 35s (100000 100%) 2.5346",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Plotting the Losses": [
                            [
                                "Plotting the historical loss from all_losses shows the network\nlearning:",
                                "markdown"
                            ],
                            [
                                "import matplotlib.pyplot as plt\n\nplt.figure()\nplt.plot(all_losses)\n\n\n<img alt=\"char rnn generation tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_char_rnn_generation_tutorial_001.png\" srcset=\"../_images/sphx_glr_char_rnn_generation_tutorial_001.png\"/>",
                                "code"
                            ],
                            [
                                "[&lt;matplotlib.lines.Line2D object at 0x7fb3d4641b70&gt;]",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Sampling the Network": [
                    [
                        "To sample we give the network a letter and ask what the next one is,\nfeed that in as the next letter, and repeat until the EOS token.",
                        "markdown"
                    ],
                    [
                        "Create tensors for input category, starting letter, and empty hidden\nstate",
                        "markdown"
                    ],
                    [
                        "Create a string output_name with the starting letter",
                        "markdown"
                    ],
                    [
                        "Up to a maximum output length,",
                        "markdown"
                    ],
                    [
                        "Feed the current letter to the network",
                        "markdown"
                    ],
                    [
                        "Get the next letter from highest output, and next hidden state",
                        "markdown"
                    ],
                    [
                        "If the letter is EOS, stop here",
                        "markdown"
                    ],
                    [
                        "If a regular letter, add to output_name and continue",
                        "markdown"
                    ],
                    [
                        "Return the final name",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "Rather than having to give it a starting letter, another\nstrategy would have been to include a \u201cstart of string\u201d token in\ntraining and have the network choose its own starting letter.",
                        "markdown"
                    ],
                    [
                        "max_length = 20\n\n# Sample from a category and starting letter\ndef sample(category, start_letter='A'):\n    with ():  # no need to track history in sampling\n        category_tensor = categoryTensor(category)\n        input = inputTensor(start_letter)\n        hidden = rnn.initHidden()\n\n        output_name = start_letter\n\n        for i in range(max_length):\n            , hidden = rnn(category_tensor, input[0], hidden)\n            topv, topi = .topk(1)\n            topi = topi[0][0]\n            if topi == n_letters - 1:\n                break\n            else:\n                letter = all_letters[topi]\n                output_name += letter\n            input = inputTensor(letter)\n\n        return output_name\n\n# Get multiple samples from one category and multiple starting letters\ndef samples(category, start_letters='ABC'):\n    for start_letter in start_letters:\n        print(sample(category, start_letter))\n\nsamples('Russian', 'RUS')\n\nsamples('German', 'GER')\n\nsamples('Spanish', 'SPA')\n\nsamples('Chinese', 'CHI')",
                        "code"
                    ],
                    [
                        "Rovanov\nUarinov\nShakinov\nGaner\nEres\nRoure\nSangera\nParan\nAllan\nChing\nHang\nIun",
                        "code"
                    ]
                ]
            },
            {
                "Exercises": [
                    [
                        "Try with a different dataset of category -&gt; line, for example:",
                        "markdown"
                    ],
                    [
                        "Fictional series -&gt; Character name",
                        "markdown"
                    ],
                    [
                        "Part of speech -&gt; Word",
                        "markdown"
                    ],
                    [
                        "Country -&gt; City",
                        "markdown"
                    ],
                    [
                        "Use a \u201cstart of sentence\u201d token so that sampling can be done without\nchoosing a start letter",
                        "markdown"
                    ],
                    [
                        "Get better results with a bigger and/or better shaped network",
                        "markdown"
                    ],
                    [
                        "Try the nn.LSTM and nn.GRU layers",
                        "markdown"
                    ],
                    [
                        "Combine multiple of these RNNs as a higher level network",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 4 minutes  35.803 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Text classification with the torchtext library": [
            [
                "In this tutorial, we will show how to use the torchtext library to build the dataset for the text classification analysis. Users will have the flexibility to\n<blockquote>",
                "markdown"
            ],
            [
                "Access to the raw data as an iterator",
                "markdown"
            ],
            [
                "Build data processing pipeline to convert the raw text strings into torch.Tensor that can be used to train the model",
                "markdown"
            ],
            [
                "Shuffle and iterate the data with \n\n</blockquote>",
                "markdown"
            ],
            {
                "Access to the raw dataset iterators": [
                    [
                        "The torchtext library provides a few raw dataset iterators, which yield the raw text strings. For example, the AG_NEWS dataset iterators yield the raw data as a tuple of label and text.",
                        "markdown"
                    ],
                    [
                        "To access torchtext datasets, please install torchdata following instructions at .",
                        "markdown"
                    ],
                    [
                        "import torch\nfrom torchtext.datasets import \ntrain_iter = iter((split='train'))",
                        "code"
                    ],
                    [
                        "next(train_iter)\n&gt;&gt;&gt; (3, \"Fears for T N pension after talks Unions representing workers at Turner\nNewall say they are 'disappointed' after talks with stricken parent firm Federal\nMogul.\")\n\nnext(train_iter)\n&gt;&gt;&gt; (4, \"The Race is On: Second Private Team Sets Launch Date for Human\nSpaceflight (SPACE.com) SPACE.com - TORONTO, Canada -- A second\\\\team of\nrocketeers competing for the  #36;10 million Ansari X Prize, a contest\nfor\\\\privately funded suborbital space flight, has officially announced\nthe first\\\\launch date for its manned rocket.\")\n\nnext(train_iter)\n&gt;&gt;&gt; (4, 'Ky. Company Wins Grant to Study Peptides (AP) AP - A company founded\nby a chemistry researcher at the University of Louisville won a grant to develop\na method of producing better peptides, which are short chains of amino acids, the\nbuilding blocks of proteins.')",
                        "code"
                    ]
                ]
            },
            {
                "Prepare data processing pipelines": [
                    [
                        "We have revisited the very basic components of the torchtext library, including vocab, word vectors, tokenizer. Those are the basic data processing building blocks for raw text string.",
                        "markdown"
                    ],
                    [
                        "Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Here we use built in\nfactory function <cite>build_vocab_from_iterator</cite> which accepts iterator that yield list or iterator of tokens. Users can also pass any special symbols to be added to the\nvocabulary.",
                        "markdown"
                    ],
                    [
                        "from torchtext.data.utils import \nfrom torchtext.vocab import \n\ntokenizer = ('basic_english')\ntrain_iter = (split='train')\n\ndef yield_tokens(data_iter):\n    for _, text in data_iter:\n        yield tokenizer(text)\n\n = (yield_tokens(train_iter), specials=[\"&lt;unk&gt;\"])\n([\"&lt;unk&gt;\"])",
                        "code"
                    ],
                    [
                        "The vocabulary block converts a list of tokens into integers.",
                        "markdown"
                    ],
                    [
                        "(['here', 'is', 'an', 'example'])\n&gt;&gt;&gt; [475, 21, 30, 5297]",
                        "code"
                    ],
                    [
                        "Prepare the text processing pipeline with the tokenizer and vocabulary. The text and label pipelines will be used to process the raw data strings from the dataset iterators.",
                        "markdown"
                    ],
                    [
                        "text_pipeline = lambda x: (tokenizer(x))\nlabel_pipeline = lambda x: int(x) - 1",
                        "code"
                    ],
                    [
                        "The text pipeline converts a text string into a list of integers based on the lookup table defined in the vocabulary. The label pipeline converts the label into integers. For example,",
                        "markdown"
                    ],
                    [
                        "text_pipeline('here is the an example')\n&gt;&gt;&gt; [475, 21, 2, 30, 5297]\nlabel_pipeline('10')\n&gt;&gt;&gt; 9",
                        "code"
                    ]
                ]
            },
            {
                "Generate data batch and iterator": [
                    [
                        "is recommended for PyTorch users (a tutorial is ).\nIt works with a map-style dataset that implements the getitem() and len() protocols, and represents a map from indices/keys to data samples. It also works with an iterable dataset with the shuffle argument of False.",
                        "markdown"
                    ],
                    [
                        "Before sending to the model, collate_fn function works on a batch of samples generated from DataLoader. The input to collate_fn is a batch of data with the batch size in DataLoader, and collate_fn processes them according to the data processing pipelines declared previously. Pay attention here and make sure that collate_fn is declared as a top level def. This ensures that the function is available in each worker.",
                        "markdown"
                    ],
                    [
                        "In this example, the text entries in the original data batch input are packed into a list and concatenated as a single tensor for the input of nn.EmbeddingBag. The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries.",
                        "markdown"
                    ],
                    [
                        "from torch.utils.data import \n = (\"cuda\" if () else \"cpu\")\n\ndef collate_batch(batch):\n    label_list, text_list, offsets = [], [], [0]\n    for (_label, _text) in batch:\n         label_list.append(label_pipeline(_label))\n         processed_text = (text_pipeline(_text), dtype=)\n         text_list.append(processed_text)\n         offsets.append(processed_text.size(0))\n    label_list = (label_list, dtype=)\n    offsets = (offsets[:-1]).cumsum(dim=0)\n    text_list = (text_list)\n    return label_list.to(), text_list.to(), offsets.to()\n\ntrain_iter = (split='train')\n = (train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)",
                        "code"
                    ]
                ]
            },
            {
                "Define the model": [
                    [
                        "The model is composed of the  layer plus a linear layer for the classification purpose. nn.EmbeddingBag with the default mode of \u201cmean\u201d computes the mean value of a \u201cbag\u201d of embeddings. Although the text entries here have different lengths, nn.EmbeddingBag module requires no padding here since the text lengths are saved in offsets.",
                        "markdown"
                    ],
                    [
                        "Additionally, since nn.EmbeddingBag accumulates the average across\nthe embeddings on the fly, nn.EmbeddingBag can enhance the\nperformance and memory efficiency to process a sequence of tensors.\n<img alt=\"../_images/text_sentiment_ngrams_model.png\" src=\"../_images/text_sentiment_ngrams_model.png\"/>",
                        "markdown"
                    ],
                    [
                        "from torch import nn\n\nclass TextClassificationModel():\n\n    def __init__(self, vocab_size, embed_dim, num_class):\n        super(, self).__init__()\n        self.embedding = (vocab_size, embed_dim, sparse=False)\n        self.fc = (embed_dim, num_class)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.5\n        self.embedding.weight.data.uniform_(-initrange, initrange)\n        self.fc.weight.data.uniform_(-initrange, initrange)\n        self.fc.bias.data.zero_()\n\n    def forward(self, text, offsets):\n        embedded = self.embedding(text, offsets)\n        return self.fc(embedded)",
                        "code"
                    ]
                ]
            },
            {
                "Initiate an instance": [
                    [
                        "The AG_NEWS dataset has four labels and therefore the number of classes is four.",
                        "markdown"
                    ],
                    [
                        "1 : World\n2 : Sports\n3 : Business\n4 : Sci/Tec",
                        "code"
                    ],
                    [
                        "We build a model with the embedding dimension of 64. The vocab size is equal to the length of the vocabulary instance. The number of classes is equal to the number of labels,",
                        "markdown"
                    ],
                    [
                        "train_iter = (split='train')\nnum_class = len(set([label for (label, text) in train_iter]))\nvocab_size = len()\nemsize = 64\nmodel = (vocab_size, emsize, num_class).to()",
                        "code"
                    ]
                ]
            },
            {
                "Define functions to train the model and evaluate results.": [
                    [
                        "import time\n\ndef train():\n    ()\n    total_acc, total_count = 0, 0\n    log_interval = 500\n    start_time = time.time()\n\n    for idx, (label, text, offsets) in enumerate():\n        ()\n        predicted_label = model(text, offsets)\n        loss = (predicted_label, label)\n        loss.backward()\n        ((), 0.1)\n        .step()\n        total_acc += (predicted_label.argmax(1) == label).sum().item()\n        total_count += label.size(0)\n        if idx % log_interval == 0 and idx &gt; 0:\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches '\n                  '| accuracy {:8.3f}'.format(epoch, idx, len(),\n                                              total_acc/total_count))\n            total_acc, total_count = 0, 0\n            start_time = time.time()\n\ndef evaluate():\n    ()\n    total_acc, total_count = 0, 0\n\n    with ():\n        for idx, (label, text, offsets) in enumerate():\n            predicted_label = model(text, offsets)\n            loss = (predicted_label, label)\n            total_acc += (predicted_label.argmax(1) == label).sum().item()\n            total_count += label.size(0)\n    return total_acc/total_count",
                        "code"
                    ]
                ]
            },
            {
                "Split the dataset and run the model": [
                    [
                        "Since the original AG_NEWS has no valid dataset, we split the training\ndataset into train/valid sets with a split ratio of 0.95 (train) and\n0.05 (valid). Here we use\n\nfunction in PyTorch core library.",
                        "markdown"
                    ],
                    [
                        "criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class.\nIt is useful when training a classification problem with C classes.\n\nimplements stochastic gradient descent method as the optimizer. The initial\nlearning rate is set to 5.0.\n\nis used here to adjust the learning rate through epochs.",
                        "markdown"
                    ],
                    [
                        "from torch.utils.data.dataset import \nfrom torchtext.data.functional import \n# Hyperparameters\nEPOCHS = 10 # epoch\nLR = 5  # learning rate\nBATCH_SIZE = 64 # batch size for training\n\n = ()\n = ((), lr=LR)\n = (, 1.0, gamma=0.1)\ntotal_accu = None\ntrain_iter, test_iter = ()\ntrain_dataset = (train_iter)\ntest_dataset = (test_iter)\nnum_train = int(len(train_dataset) * 0.95)\n,  = \\\n    (train_dataset, [num_train, len(train_dataset) - num_train])\n\n = (, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn=collate_batch)\n = (, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn=collate_batch)\n = (test_dataset, batch_size=BATCH_SIZE,\n                             shuffle=True, collate_fn=collate_batch)\n\nfor epoch in range(1, EPOCHS + 1):\n    epoch_start_time = time.time()\n    train()\n    accu_val = evaluate()\n    if total_accu is not None and total_accu &gt; accu_val:\n      .step()\n    else:\n       total_accu = accu_val\n    print('-' * 59)\n    print('| end of epoch {:3d} | time: {:5.2f}s | '\n          'valid accuracy {:8.3f} '.format(epoch,\n                                           time.time() - epoch_start_time,\n                                           accu_val))\n    print('-' * 59)",
                        "code"
                    ],
                    [
                        "| epoch   1 |   500/ 1782 batches | accuracy    0.689\n| epoch   1 |  1000/ 1782 batches | accuracy    0.856\n| epoch   1 |  1500/ 1782 batches | accuracy    0.877\n-----------------------------------------------------------\n| end of epoch   1 | time: 12.46s | valid accuracy    0.884\n-----------------------------------------------------------\n| epoch   2 |   500/ 1782 batches | accuracy    0.898\n| epoch   2 |  1000/ 1782 batches | accuracy    0.900\n| epoch   2 |  1500/ 1782 batches | accuracy    0.903\n-----------------------------------------------------------\n| end of epoch   2 | time: 10.27s | valid accuracy    0.892\n-----------------------------------------------------------\n| epoch   3 |   500/ 1782 batches | accuracy    0.914\n| epoch   3 |  1000/ 1782 batches | accuracy    0.914\n| epoch   3 |  1500/ 1782 batches | accuracy    0.914\n-----------------------------------------------------------\n| end of epoch   3 | time: 10.33s | valid accuracy    0.891\n-----------------------------------------------------------\n| epoch   4 |   500/ 1782 batches | accuracy    0.929\n| epoch   4 |  1000/ 1782 batches | accuracy    0.930\n| epoch   4 |  1500/ 1782 batches | accuracy    0.930\n-----------------------------------------------------------\n| end of epoch   4 | time: 10.28s | valid accuracy    0.908\n-----------------------------------------------------------\n| epoch   5 |   500/ 1782 batches | accuracy    0.932\n| epoch   5 |  1000/ 1782 batches | accuracy    0.932\n| epoch   5 |  1500/ 1782 batches | accuracy    0.930\n-----------------------------------------------------------\n| end of epoch   5 | time: 10.26s | valid accuracy    0.910\n-----------------------------------------------------------\n| epoch   6 |   500/ 1782 batches | accuracy    0.934\n| epoch   6 |  1000/ 1782 batches | accuracy    0.934\n| epoch   6 |  1500/ 1782 batches | accuracy    0.929\n-----------------------------------------------------------\n| end of epoch   6 | time: 10.20s | valid accuracy    0.910\n-----------------------------------------------------------\n| epoch   7 |   500/ 1782 batches | accuracy    0.934\n| epoch   7 |  1000/ 1782 batches | accuracy    0.933\n| epoch   7 |  1500/ 1782 batches | accuracy    0.936\n-----------------------------------------------------------\n| end of epoch   7 | time: 10.25s | valid accuracy    0.908\n-----------------------------------------------------------\n| epoch   8 |   500/ 1782 batches | accuracy    0.935\n| epoch   8 |  1000/ 1782 batches | accuracy    0.934\n| epoch   8 |  1500/ 1782 batches | accuracy    0.934\n-----------------------------------------------------------\n| end of epoch   8 | time: 10.24s | valid accuracy    0.909\n-----------------------------------------------------------\n| epoch   9 |   500/ 1782 batches | accuracy    0.935\n| epoch   9 |  1000/ 1782 batches | accuracy    0.936\n| epoch   9 |  1500/ 1782 batches | accuracy    0.933\n-----------------------------------------------------------\n| end of epoch   9 | time: 10.25s | valid accuracy    0.909\n-----------------------------------------------------------\n| epoch  10 |   500/ 1782 batches | accuracy    0.933\n| epoch  10 |  1000/ 1782 batches | accuracy    0.936\n| epoch  10 |  1500/ 1782 batches | accuracy    0.935\n-----------------------------------------------------------\n| end of epoch  10 | time: 10.23s | valid accuracy    0.909\n-----------------------------------------------------------",
                        "code"
                    ]
                ]
            },
            {
                "Evaluate the model with test dataset": [
                    [
                        "Checking the results of the test dataset\u2026",
                        "markdown"
                    ],
                    [
                        "print('Checking the results of test dataset.')\naccu_test = evaluate()\nprint('test accuracy {:8.3f}'.format(accu_test))",
                        "code"
                    ],
                    [
                        "Checking the results of test dataset.\ntest accuracy    0.903",
                        "code"
                    ]
                ]
            },
            {
                "Test on a random news": [
                    [
                        "Use the best model so far and test a golf news.",
                        "markdown"
                    ],
                    [
                        "ag_news_label = {1: \"World\",\n                 2: \"Sports\",\n                 3: \"Business\",\n                 4: \"Sci/Tec\"}\n\ndef predict(text, text_pipeline):\n    with ():\n        text = (text_pipeline(text))\n        output = model(text, ([0]))\n        return output.argmax(1).item() + 1\n\nex_text_str = \"MEMPHIS, Tenn. \u2013 Four days ago, Jon Rahm was \\\n    enduring the season\u2019s worst weather conditions on Sunday at The \\\n    Open on his way to a closing 75 at Royal Portrush, which \\\n    considering the wind and the rain was a respectable showing. \\\n    Thursday\u2019s first round at the WGC-FedEx St. Jude Invitational \\\n    was another story. With temperatures in the mid-80s and hardly any \\\n    wind, the Spaniard was 13 strokes better in a flawless round. \\\n    Thanks to his best putting performance on the PGA Tour, Rahm \\\n    finished with an 8-under 62 for a three-stroke lead, which \\\n    was even more impressive considering he\u2019d never played the \\\n    front nine at TPC Southwind.\"\n\nmodel = (\"cpu\")\n\nprint(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])",
                        "code"
                    ],
                    [
                        "This is a Sports news",
                        "code"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 2 minutes  1.494 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Language Translation with nn.Transformer and torchtext": [
            [
                "How to train a translation model from scratch using Transformer.",
                "markdown"
            ],
            [
                "Use torchtext library to access   dataset to train a German to English translation model.\n\n</dd>\n</dl>",
                "markdown"
            ],
            {
                "Data Sourcing and Processing": [
                    [
                        " has utilities for creating datasets that can be easily\niterated through for the purposes of creating a language translation\nmodel. In this example, we show how to use torchtext\u2019s inbuilt datasets,\ntokenize a raw text sentence, build vocabulary, and numericalize tokens into tensor. We will use\n\nthat yields a pair of source-target raw sentences.",
                        "markdown"
                    ],
                    [
                        "To access torchtext datasets, please install torchdata following instructions at .",
                        "markdown"
                    ],
                    [
                        "from torchtext.data.utils import \nfrom torchtext.vocab import \nfrom torchtext.datasets import multi30k, \nfrom typing import Iterable, List\n\n\n# We need to modify the URLs for the dataset since the links to the original dataset are broken\n# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\nmulti30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\nmulti30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n\nSRC_LANGUAGE = 'de'\nTGT_LANGUAGE = 'en'\n\n# Place-holders\ntoken_transform = {}\nvocab_transform = {}\n\n\n# Create source and target language tokenizer. Make sure to install the dependencies.\n# pip install -U torchdata\n# pip install -U spacy\n# python -m spacy download en_core_web_sm\n# python -m spacy download de_core_news_sm\ntoken_transform[SRC_LANGUAGE] = ('spacy', language='de_core_news_sm')\ntoken_transform[TGT_LANGUAGE] = ('spacy', language='en_core_web_sm')\n\n\n# helper function to yield list of tokens\ndef yield_tokens(data_iter: Iterable, language: str) -&gt; List[str]:\n    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n\n    for data_sample in data_iter:\n        yield token_transform[language](data_sample[language_index[language]])\n\n# Define special symbols and indices\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n# Make sure the tokens are in order of their indices to properly insert them in vocab\nspecial_symbols = ['&lt;unk&gt;', '&lt;pad&gt;', '&lt;bos&gt;', '&lt;eos&gt;']\n\nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n    # Training data Iterator\n    train_iter = (split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n    # Create torchtext's Vocab object\n    vocab_transform[ln] = (yield_tokens(train_iter, ln),\n                                                    min_freq=1,\n                                                    specials=special_symbols,\n                                                    special_first=True)\n\n# Set UNK_IDX as the default index. This index is returned when the token is not found.\n# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n  vocab_transform[ln].set_default_index(UNK_IDX)",
                        "code"
                    ]
                ]
            },
            {
                "Seq2Seq Network using Transformer": [
                    [
                        "Transformer is a Seq2Seq model introduced in \npaper for solving machine translation tasks.\nBelow, we will create a Seq2Seq network that uses Transformer. The network\nconsists of three parts. First part is the embedding layer. This layer converts tensor of input indices\ninto corresponding tensor of input embeddings. These embedding are further augmented with positional\nencodings to provide position information of input tokens to the model. The second part is the\nactual  model.\nFinally, the output of the Transformer model is passed through linear layer\nthat gives un-normalized probabilities for each token in the target language.",
                        "markdown"
                    ],
                    [
                        "from torch import \nimport torch\nimport torch.nn as nn\nfrom torch.nn import \nimport math\nDEVICE = ('cuda' if () else 'cpu')\n\n# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\nclass PositionalEncoding():\n    def __init__(self,\n                 emb_size: int,\n                 dropout: float,\n                 maxlen: int = 5000):\n        super(PositionalEncoding, self).__init__()\n        den = (- (0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = (0, maxlen).reshape(maxlen, 1)\n        pos_embedding = ((maxlen, emb_size))\n        pos_embedding[:, 0::2] = (pos * den)\n        pos_embedding[:, 1::2] = (pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = (dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, token_embedding: ):\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n\n# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\nclass TokenEmbedding():\n    def __init__(self, vocab_size: int, emb_size):\n        super(TokenEmbedding, self).__init__()\n        self.embedding = (vocab_size, emb_size)\n        self.emb_size = emb_size\n\n    def forward(self, tokens: ):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n\n# Seq2Seq Network\nclass Seq2SeqTransformer():\n    def __init__(self,\n                 num_encoder_layers: int,\n                 num_decoder_layers: int,\n                 emb_size: int,\n                 nhead: int,\n                 src_vocab_size: int,\n                 tgt_vocab_size: int,\n                 dim_feedforward: int = 512,\n                 dropout: float = 0.1):\n        super(Seq2SeqTransformer, self).__init__()\n        self.transformer = (d_model=emb_size,\n                                       nhead=nhead,\n                                       num_encoder_layers=num_encoder_layers,\n                                       num_decoder_layers=num_decoder_layers,\n                                       dim_feedforward=dim_feedforward,\n                                       dropout=dropout)\n        self.generator = (emb_size, tgt_vocab_size)\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(\n            emb_size, dropout=dropout)\n\n    def forward(self,\n                src: ,\n                trg: ,\n                src_mask: ,\n                tgt_mask: ,\n                src_padding_mask: ,\n                tgt_padding_mask: ,\n                memory_key_padding_mask: ):\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n        return self.generator(outs)\n\n    def encode(self, src: , src_mask: ):\n        return self.transformer.encoder(self.positional_encoding(\n                            self.src_tok_emb(src)), src_mask)\n\n    def decode(self, tgt: , memory: , tgt_mask: ):\n        return self.transformer.decoder(self.positional_encoding(\n                          self.tgt_tok_emb(tgt)), memory,\n                          tgt_mask)",
                        "code"
                    ],
                    [
                        "During training, we need a subsequent word mask that will prevent the model from looking into\nthe future words when making predictions. We will also need masks to hide\nsource and target padding tokens. Below, let\u2019s define a function that will take care of both.",
                        "markdown"
                    ],
                    [
                        "def generate_square_subsequent_mask(sz):\n    mask = ((((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask\n\n\ndef create_mask(src, tgt):\n    src_seq_len = src.shape[0]\n    tgt_seq_len = tgt.shape[0]\n\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    src_mask = ((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n\n    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask",
                        "code"
                    ],
                    [
                        "Let\u2019s now define the parameters of our model and instantiate the same. Below, we also\ndefine our loss function which is the cross-entropy loss and the optmizer used for training.",
                        "markdown"
                    ],
                    [
                        "(0)\n\nSRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\nTGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\nEMB_SIZE = 512\nNHEAD = 8\nFFN_HID_DIM = 512\nBATCH_SIZE = 128\nNUM_ENCODER_LAYERS = 3\nNUM_DECODER_LAYERS = 3\n\ntransformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n\nfor p in transformer.parameters():\n    if p.dim() &gt; 1:\n        (p)\n\ntransformer = transformer.to(DEVICE)\n\nloss_fn = (ignore_index=PAD_IDX)\n\noptimizer = (transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)",
                        "code"
                    ]
                ]
            },
            {
                "Collation": [
                    [
                        "As seen in the Data Sourcing and Processing section, our data iterator yields a pair of raw strings.\nWe need to convert these string pairs into the batched tensors that can be processed by our Seq2Seq network\ndefined previously. Below we define our collate function that converts a batch of raw strings into batch tensors that\ncan be fed directly into our model.",
                        "markdown"
                    ],
                    [
                        "from torch.nn.utils.rnn import \n\n# helper function to club together sequential operations\ndef sequential_transforms(*transforms):\n    def func(txt_input):\n        for transform in transforms:\n            txt_input = transform(txt_input)\n        return txt_input\n    return func\n\n# function to add BOS/EOS and create tensor for input sequence indices\ndef tensor_transform(token_ids: List[int]):\n    return ((([BOS_IDX]),\n                      (token_ids),\n                      ([EOS_IDX])))\n\n# src and tgt language text transforms to convert raw strings into tensors indices\ntext_transform = {}\nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n                                               vocab_transform[ln], #Numericalization\n                                               tensor_transform) # Add BOS/EOS and create tensor\n\n\n# function to collate data samples into batch tensors\ndef collate_fn(batch):\n    src_batch, tgt_batch = [], []\n    for src_sample, tgt_sample in batch:\n        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n\n    src_batch = (src_batch, padding_value=PAD_IDX)\n    tgt_batch = (tgt_batch, padding_value=PAD_IDX)\n    return src_batch, tgt_batch",
                        "code"
                    ],
                    [
                        "Let\u2019s define training and evaluation loop that will be called for each\nepoch.",
                        "markdown"
                    ],
                    [
                        "from torch.utils.data import \n\ndef train_epoch(model, optimizer):\n    model.train()\n    losses = 0\n    train_iter = (split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n    train_dataloader = (train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n\n    for src, tgt in train_dataloader:\n        src = src.to(DEVICE)\n        tgt = tgt.to(DEVICE)\n\n        tgt_input = tgt[:-1, :]\n\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n\n        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n\n        optimizer.zero_grad()\n\n        tgt_out = tgt[1:, :]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        loss.backward()\n\n        optimizer.step()\n        losses += loss.item()\n\n    return losses / len(list(train_dataloader))\n\n\ndef evaluate(model):\n    model.eval()\n    losses = 0\n\n    val_iter = (split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n    val_dataloader = (val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n\n    for src, tgt in val_dataloader:\n        src = src.to(DEVICE)\n        tgt = tgt.to(DEVICE)\n\n        tgt_input = tgt[:-1, :]\n\n        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n\n        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n\n        tgt_out = tgt[1:, :]\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n        losses += loss.item()\n\n    return losses / len(list(val_dataloader))",
                        "code"
                    ],
                    [
                        "Now we have all the ingredients to train our model. Let\u2019s do it!",
                        "markdown"
                    ],
                    [
                        "from timeit import default_timer as timer\nNUM_EPOCHS = 18\n\nfor epoch in range(1, NUM_EPOCHS+1):\n    start_time = timer()\n    train_loss = train_epoch(transformer, optimizer)\n    end_time = timer()\n    val_loss = evaluate(transformer)\n    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n\n\n# function to generate output sequence using greedy algorithm\ndef greedy_decode(model, src, src_mask, max_len, start_symbol):\n    src = src.to(DEVICE)\n    src_mask = src_mask.to(DEVICE)\n\n    memory = model.encode(src, src_mask)\n    ys = (1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n    for i in range(max_len-1):\n        memory = memory.to(DEVICE)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                    .type(torch.bool)).to(DEVICE)\n        out = model.decode(ys, memory, tgt_mask)\n        out = out.transpose(0, 1)\n        prob = model.generator(out[:, -1])\n        _, next_word = (prob, dim=1)\n        next_word = next_word.item()\n\n        ys = ([ys,\n                        (1, 1).type_as(src.data).fill_(next_word)], dim=0)\n        if next_word == EOS_IDX:\n            break\n    return ys\n\n\n# actual function to translate input sentence into target language\ndef translate(model: , src_sentence: str):\n    model.eval()\n    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n    num_tokens = src.shape[0]\n    src_mask = ((num_tokens, num_tokens)).type(torch.bool)\n    tgt_tokens = greedy_decode(\n        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"&lt;bos&gt;\", \"\").replace(\"&lt;eos&gt;\", \"\")",
                        "code"
                    ],
                    [
                        "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))",
                        "code"
                    ]
                ]
            },
            {
                "References": [
                    [
                        "Attention is all you need paper.",
                        "markdown"
                    ],
                    [
                        "The annotated transformer. ",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ]
    },
    "Reinforcement Learning": {
        "Reinforcement Learning (DQN) Tutorial": [
            [
                "</dd>\n</dl>",
                "markdown"
            ],
            [
                "This tutorial shows how to use PyTorch to train a Deep Q Learning (DQN) agent\non the CartPole-v1 task from .",
                "markdown"
            ],
            [
                "<strong>Task</strong>",
                "markdown"
            ],
            [
                "The agent has to decide between two actions - moving the cart left or\nright - so that the pole attached to it stays upright. You can find more\ninformation about the environment and other more challenging environments at\n.\n\n<img alt=\"cartpole\" src=\"../_images/cartpole.gif\"/>",
                "markdown"
            ],
            [
                "cartpole",
                "markdown"
            ],
            [
                "As the agent observes the current state of the environment and chooses\nan action, the environment <em>transitions</em> to a new state, and also\nreturns a reward that indicates the consequences of the action. In this\ntask, rewards are +1 for every incremental timestep and the environment\nterminates if the pole falls over too far or the cart moves more than 2.4\nunits away from center. This means better performing scenarios will run\nfor longer duration, accumulating larger return.",
                "markdown"
            ],
            [
                "The CartPole task is designed so that the inputs to the agent are 4 real\nvalues representing the environment state (position, velocity, etc.).\nWe take these 4 inputs without any scaling and pass them through a\nsmall fully-connected network with 2 outputs, one for each action.\nThe network is trained to predict the expected value for each action,\ngiven the input state. The action with the highest expected value is\nthen chosen.",
                "markdown"
            ],
            [
                "<strong>Packages</strong>",
                "markdown"
            ],
            [
                "First, let\u2019s import needed packages. Firstly, we need\n for the environment,\ninstalled by using <cite>pip</cite>. This is a fork of the original OpenAI\nGym project and maintained by the same team since Gym v0.19.\nIf you are running this in Google colab, run:",
                "markdown"
            ],
            [
                "%%bash\npip3 install gymnasium[classic_control]",
                "code"
            ],
            [
                "We\u2019ll also use the following from PyTorch:",
                "markdown"
            ],
            [
                "neural networks (torch.nn)",
                "markdown"
            ],
            [
                "optimization (torch.optim)",
                "markdown"
            ],
            [
                "automatic differentiation (torch.autograd)",
                "markdown"
            ],
            [
                "import gymnasium as gym\nimport math\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom collections import namedtuple, deque\nfrom itertools import count\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nenv = gym.make(\"CartPole-v1\")\n\n# set up matplotlib\nis_ipython = 'inline' in matplotlib.get_backend()\nif is_ipython:\n    from IPython import display\n\nplt.ion()\n\n# if gpu is to be used\n = (\"cuda\" if () else \"cpu\")",
                "code"
            ],
            {
                "Replay Memory": [
                    [
                        "We\u2019ll be using experience replay memory for training our DQN. It stores\nthe transitions that the agent observes, allowing us to reuse this data\nlater. By sampling from it randomly, the transitions that build up a\nbatch are decorrelated. It has been shown that this greatly stabilizes\nand improves the DQN training procedure.",
                        "markdown"
                    ],
                    [
                        "For this, we\u2019re going to need two classses:",
                        "markdown"
                    ],
                    [
                        "Transition - a named tuple representing a single transition in\nour environment. It essentially maps (state, action) pairs\nto their (next_state, reward) result, with the state being the\nscreen difference image as described later on.",
                        "markdown"
                    ],
                    [
                        "ReplayMemory - a cyclic buffer of bounded size that holds the\ntransitions observed recently. It also implements a .sample()\nmethod for selecting a random batch of transitions for training.",
                        "markdown"
                    ],
                    [
                        "Transition = namedtuple('Transition',\n                        ('state', 'action', 'next_state', 'reward'))\n\n\nclass ReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.memory = deque([], maxlen=capacity)\n\n    def push(self, *args):\n        \"\"\"Save a transition\"\"\"\n        self.memory.append(Transition(*args))\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)",
                        "code"
                    ],
                    [
                        "Now, let\u2019s define our model. But first, let\u2019s quickly recap what a DQN is.",
                        "markdown"
                    ]
                ]
            },
            {
                "DQN algorithm": [
                    [
                        "Our environment is deterministic, so all equations presented here are\nalso formulated deterministically for the sake of simplicity. In the\nreinforcement learning literature, they would also contain expectations\nover stochastic transitions in the environment.",
                        "markdown"
                    ],
                    [
                        "Our aim will be to train a policy that tries to maximize the discounted,\ncumulative reward\n\\(R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t\\), where\n\\(R_{t_0}\\) is also known as the <em>return</em>. The discount,\n\\(\\gamma\\), should be a constant between \\(0\\) and \\(1\\)\nthat ensures the sum converges. A lower \\(\\gamma\\) makes\nrewards from the uncertain far future less important for our agent\nthan the ones in the near future that it can be fairly confident\nabout. It also encourages agents to collect reward closer in time\nthan equivalent rewards that are temporally far away in the future.",
                        "markdown"
                    ],
                    [
                        "The main idea behind Q-learning is that if we had a function\n\\(Q^*: State \\times Action \\rightarrow \\mathbb{R}\\), that could tell\nus what our return would be, if we were to take an action in a given\nstate, then we could easily construct a policy that maximizes our\nrewards:\n\n\\[\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\n\n\\]",
                        "markdown"
                    ],
                    [
                        "However, we don\u2019t know everything about the world, so we don\u2019t have\naccess to \\(Q^*\\). But, since neural networks are universal function\napproximators, we can simply create one and train it to resemble\n\\(Q^*\\).",
                        "markdown"
                    ],
                    [
                        "For our training update rule, we\u2019ll use a fact that every \\(Q\\)\nfunction for some policy obeys the Bellman equation:\n\n\\[Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\n\n\\]",
                        "markdown"
                    ],
                    [
                        "The difference between the two sides of the equality is known as the\ntemporal difference error, \\(\\delta\\):\n\n\\[\\delta = Q(s, a) - (r + \\gamma \\max_a' Q(s', a))\n\n\\]",
                        "markdown"
                    ],
                    [
                        "To minimise this error, we will use the . The Huber loss acts\nlike the mean squared error when the error is small, but like the mean\nabsolute error when the error is large - this makes it more robust to\noutliers when the estimates of \\(Q\\) are very noisy. We calculate\nthis over a batch of transitions, \\(B\\), sampled from the replay\nmemory:\n\n\\[\\mathcal{L} = \\frac{1}{|B|}\\sum_{(s, a, s', r) \\ \\in \\ B} \\mathcal{L}(\\delta)\\]\n\n\\[\\text{where} \\quad \\mathcal{L}(\\delta) = \\begin{cases}\n  \\frac{1}{2}{\\delta^2}  &amp; \\text{for } |\\delta| \\le 1, \\\\\n  |\\delta| - \\frac{1}{2} &amp; \\text{otherwise.}\n\\end{cases}\\]",
                        "markdown"
                    ],
                    {
                        "Q-network": [
                            [
                                "Our model will be a convolutional neural network that takes in the\ndifference between the current and previous screen patches. It has two\noutputs, representing \\(Q(s, \\mathrm{left})\\) and\n\\(Q(s, \\mathrm{right})\\) (where \\(s\\) is the input to the\nnetwork). In effect, the network is trying to predict the <em>expected return</em> of\ntaking each action given the current input.",
                                "markdown"
                            ],
                            [
                                "class DQN():\n\n    def __init__(self, n_observations, n_actions):\n        super(, self).__init__()\n        self.layer1 = (n_observations, 128)\n        self.layer2 = (128, 128)\n        self.layer3 = (128, n_actions)\n\n    # Called with either one element to determine next action, or a batch\n    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n    def forward(self, x):\n        x = (self.layer1(x))\n        x = (self.layer2(x))\n        return self.layer3(x)",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Training": [
                    {
                        "Hyperparameters and utilities": [
                            [
                                "This cell instantiates our model and its optimizer, and defines some\nutilities:",
                                "markdown"
                            ],
                            [
                                "select_action - will select an action accordingly to an epsilon\ngreedy policy. Simply put, we\u2019ll sometimes use our model for choosing\nthe action, and sometimes we\u2019ll just sample one uniformly. The\nprobability of choosing a random action will start at EPS_START\nand will decay exponentially towards EPS_END. EPS_DECAY\ncontrols the rate of the decay.",
                                "markdown"
                            ],
                            [
                                "plot_durations - a helper for plotting the durations of episodes,\nalong with an average over the last 100 episodes (the measure used in\nthe official evaluations). The plot will be underneath the cell\ncontaining the main training loop, and will update after every\nepisode.",
                                "markdown"
                            ],
                            [
                                "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n# GAMMA is the discount factor as mentioned in the previous section\n# EPS_START is the starting value of epsilon\n# EPS_END is the final value of epsilon\n# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n# TAU is the update rate of the target network\n# LR is the learning rate of the AdamW optimizer\nBATCH_SIZE = 128\nGAMMA = 0.99\nEPS_START = 0.9\nEPS_END = 0.05\nEPS_DECAY = 1000\nTAU = 0.005\nLR = 1e-4\n\n# Get number of actions from gym action space\nn_actions = env.action_space.n\n# Get the number of state observations\n, info = env.reset()\nn_observations = len()\n\npolicy_net = (n_observations, n_actions).to()\ntarget_net = (n_observations, n_actions).to()\n(())\n\n = ((), lr=LR, amsgrad=True)\nmemory = ReplayMemory(10000)\n\n\nsteps_done = 0\n\n\ndef select_action():\n    global steps_done\n    sample = random.random()\n    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n        math.exp(-1. * steps_done / EPS_DECAY)\n    steps_done += 1\n    if sample &gt; eps_threshold:\n        with ():\n            # t.max(1) will return the largest column value of each row.\n            # second column on max result is index of where max element was\n            # found, so we pick action with the larger expected reward.\n            return policy_net().max(1)[1].view(1, 1)\n    else:\n        return ([[env.action_space.sample()]], =, dtype=)\n\n\nepisode_durations = []\n\n\ndef plot_durations(show_result=False):\n    plt.figure(1)\n    durations_t = (episode_durations, dtype=)\n    if show_result:\n        plt.title('Result')\n    else:\n        plt.clf()\n        plt.title('Training...')\n    plt.xlabel('Episode')\n    plt.ylabel('Duration')\n    plt.plot(durations_t.numpy())\n    # Take 100 episode averages and plot them too\n    if len(durations_t) &gt;= 100:\n        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n        means = (((99), means))\n        plt.plot(means.numpy())\n\n    plt.pause(0.001)  # pause a bit so that plots are updated\n    if is_ipython:\n        if not show_result:\n            display.display(plt.gcf())\n            display.clear_output(wait=True)\n        else:\n            display.display(plt.gcf())",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Training loop": [
                            [
                                "Finally, the code for training our model.",
                                "markdown"
                            ],
                            [
                                "Here, you can find an optimize_model function that performs a\nsingle step of the optimization. It first samples a batch, concatenates\nall the tensors into a single one, computes \\(Q(s_t, a_t)\\) and\n\\(V(s_{t+1}) = \\max_a Q(s_{t+1}, a)\\), and combines them into our\nloss. By definition we set \\(V(s) = 0\\) if \\(s\\) is a terminal\nstate. We also use a target network to compute \\(V(s_{t+1})\\) for\nadded stability. The target network is updated at every step with a\n controlled by\nthe hyperparameter TAU, which was previously defined.",
                                "markdown"
                            ],
                            [
                                "def optimize_model():\n    if len(memory) &lt; BATCH_SIZE:\n        return\n    transitions = memory.sample(BATCH_SIZE)\n    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n    # detailed explanation). This converts batch-array of Transitions\n    # to Transition of batch-arrays.\n    batch = Transition(*zip(*transitions))\n\n    # Compute a mask of non-final states and concatenate the batch elements\n    # (a final state would've been the one after which simulation ended)\n    non_final_mask = (tuple(map(lambda s: s is not None,\n                                          batch.)), =, dtype=)\n    non_final_next_states = ([s for s in batch.\n                                                if s is not None])\n    state_batch = (batch.)\n    action_batch = (batch.)\n    reward_batch = (batch.)\n\n    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n    # columns of actions taken. These are the actions which would've been taken\n    # for each batch state according to policy_net\n    state_action_values = policy_net(state_batch).gather(1, action_batch)\n\n    # Compute V(s_{t+1}) for all next states.\n    # Expected values of actions for non_final_next_states are computed based\n    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n    # This is merged based on the mask, such that we'll have either the expected\n    # state value or 0 in case the state was final.\n    next_state_values = (BATCH_SIZE, =)\n    with ():\n        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n    # Compute the expected Q values\n    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n\n    # Compute Huber loss\n    criterion = ()\n    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n\n    # Optimize the model\n    ()\n    loss.backward()\n    # In-place gradient clipping\n    ((), 100)\n    ()",
                                "code"
                            ],
                            [
                                "Below, you can find the main training loop. At the beginning we reset\nthe environment and obtain the initial state Tensor. Then, we sample\nan action, execute it, observe the next state and the reward (always\n1), and optimize our model once. When the episode ends (our model\nfails), we restart the loop.",
                                "markdown"
                            ],
                            [
                                "Below, <cite>num_episodes</cite> is set to 600 if a GPU is available, otherwise 50\nepisodes are scheduled so training does not take too long. However, 50\nepisodes is insufficient for to observe good performance on cartpole.\nYou should see the model constantly achieve 500 steps within 600 training\nepisodes. Training RL agents can be a noisy process, so restarting training\ncan produce better results if convergence is not observed.",
                                "markdown"
                            ],
                            [
                                "if ():\n    num_episodes = 600\nelse:\n    num_episodes = 50\n\nfor i_episode in range(num_episodes):\n    # Initialize the environment and get it's state\n    , info = env.reset()\n     = (, dtype=, =).unsqueeze(0)\n    for t in count():\n         = select_action()\n        observation, , terminated, truncated, _ = env.step(.item())\n         = ([], =)\n        done = terminated or truncated\n\n        if terminated:\n             = None\n        else:\n             = (observation, dtype=, =).unsqueeze(0)\n\n        # Store the transition in memory\n        memory.push(, , , )\n\n        # Move to the next state\n         = \n\n        # Perform one step of the optimization (on the policy network)\n        optimize_model()\n\n        # Soft update of the target network's weights\n        # \u03b8\u2032 \u2190 \u03c4 \u03b8 + (1 \u2212\u03c4 )\u03b8\u2032\n        target_net_state_dict = ()\n        policy_net_state_dict = ()\n        for key in policy_net_state_dict:\n            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n        (target_net_state_dict)\n\n        if done:\n            episode_durations.append(t + 1)\n            plot_durations()\n            break\n\nprint('Complete')\nplot_durations(show_result=True)\nplt.ioff()\nplt.show()\n\n\n<img alt=\"Result\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_reinforcement_q_learning_001.png\" srcset=\"../_images/sphx_glr_reinforcement_q_learning_001.png\"/>",
                                "code"
                            ],
                            [
                                "Complete",
                                "code"
                            ],
                            [
                                "Here is the diagram that illustrates the overall resulting data flow.\n\n<img alt=\"../_images/reinforcement_learning_diagram.jpg\" src=\"../_images/reinforcement_learning_diagram.jpg\"/>",
                                "markdown"
                            ],
                            [
                                "Actions are chosen either randomly or based on a policy, getting the next\nstep sample from the gym environment. We record the results in the\nreplay memory and also run optimization step on every iteration.\nOptimization picks a random batch from the replay memory to do training of the\nnew policy. The \u201colder\u201d target_net is also used in optimization to compute the\nexpected Q values. A soft update of its weights are performed at every step.",
                                "markdown"
                            ],
                            [
                                "<strong>Total running time of the script:</strong> ( 9 minutes  58.307 seconds)",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ]
                        ]
                    }
                ]
            }
        ],
        "Reinforcement Learning (PPO) with TorchRL Tutorial": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "This tutorial demonstrates how to use PyTorch and torchrl to train a parametric policy\nnetwork to solve the Inverted Pendulum task from the .\n\n<img alt=\"Inverted pendulum\" src=\"../_images/invpendulum.gif\"/>",
                "markdown"
            ],
            [
                "Inverted pendulum",
                "markdown"
            ],
            [
                "Key learnings:",
                "markdown"
            ],
            [
                "How to create an environment in TorchRL, transform its outputs, and collect data from this env;",
                "markdown"
            ],
            [
                "How to make your classes talk to each other using ;",
                "markdown"
            ],
            [
                "The basics of building your training loop with TorchRL:",
                "markdown"
            ],
            [
                "How to compute the advantage signal for policy gradient methods;",
                "markdown"
            ],
            [
                "How to create a stochastic policy using a probabilistic neural network;",
                "markdown"
            ],
            [
                "How to create a dynamic replay buffer and sample from it without repetition.",
                "markdown"
            ],
            [
                "We will cover six crucial components of TorchRL:",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "If you are running this in Google Colab, make sure you install the following dependencies:",
                "markdown"
            ],
            [
                "!pip3 install torchrl\n!pip3 install gym[mujoco]\n!pip3 install tqdm",
                "code"
            ],
            [
                "Proximal Policy Optimization (PPO) is a policy-gradient algorithm where a\nbatch of data is being collected and directly consumed to train the policy to maximise\nthe expected return given some proximality constraints. You can think of it\nas a sophisticated version of ,\nthe foundational policy-optimization algorithm. For more information, see the\n paper.",
                "markdown"
            ],
            [
                "PPO is usually regarded as a fast and efficient method for online, on-policy\nreinforcement algorithm. TorchRL provides a loss-module that does all the work\nfor you, so that you can rely on this implementation and focus on solving your\nproblem rather than re-inventing the wheel every time you want to train a policy.",
                "markdown"
            ],
            [
                "For completeness, here is a brief overview of what the loss computes, even though\nthis is taken care of by our ClipPPOLoss module\u2014the algorithm works as follows:\n1. we will sample a batch of data by playing the\npolicy in the environment for a given number of steps.\n2. Then, we will perform a given number of optimization steps with random sub-samples of this batch using\na clipped version of the REINFORCE loss.\n3. The clipping will put a pessimistic bound on our loss: lower return estimates will\nbe favored compared to higher ones.\nThe precise formula of the loss is:\n\n\\[L(s,a,\\theta_k,\\theta) = \\min\\left(\n\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}  A^{\\pi_{\\theta_k}}(s,a), \\;\\;\ng(\\epsilon, A^{\\pi_{\\theta_k}}(s,a))\n\\right),\\]",
                "markdown"
            ],
            [
                "There are two components in that loss: in the first part of the minimum operator,\nwe simply compute an importance-weighted version of the REINFORCE loss (for example, a\nREINFORCE loss that we have corrected for the fact that the current policy\nconfiguration lags the one that was used for the data collection).\nThe second part of that minimum operator is a similar loss where we have clipped\nthe ratios when they exceeded or were below a given pair of thresholds.",
                "markdown"
            ],
            [
                "This loss ensures that whether the advantage is positive or negative, policy\nupdates that would produce significant shifts from the previous configuration\nare being discouraged.",
                "markdown"
            ],
            [
                "This tutorial is structured as follows:",
                "markdown"
            ],
            [
                "First, we will define a set of hyperparameters we will be using for training.",
                "markdown"
            ],
            [
                "Next, we will focus on creating our environment, or simulator, using TorchRL\u2019s\nwrappers and transforms.",
                "markdown"
            ],
            [
                "Next, we will design the policy network and the value model,\nwhich is indispensable to the loss function. These modules will be used\nto configure our loss module.",
                "markdown"
            ],
            [
                "Next, we will create the replay buffer and data loader.",
                "markdown"
            ],
            [
                "Finally, we will run our training loop and analyze the results.",
                "markdown"
            ],
            [
                "Throughout this tutorial, we\u2019ll be using the tensordict library.\n is the lingua franca of TorchRL: it helps us abstract\nwhat a module reads and writes and care less about the specific data\ndescription and more about the algorithm itself.",
                "markdown"
            ],
            [
                "from collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom tensordict.nn import \nfrom tensordict.nn.distributions import \nfrom torch import nn\nfrom torchrl.collectors import \nfrom torchrl.data.replay_buffers import \nfrom torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\nfrom torchrl.data.replay_buffers.storages import \nfrom torchrl.envs import (\n    ,\n    ,\n    ,\n    ,\n    ,\n)\nfrom torchrl.envs.libs.gym import \nfrom torchrl.envs.utils import , \nfrom torchrl.modules import , , \nfrom torchrl.objectives import \nfrom torchrl.objectives.value import \nfrom tqdm import tqdm",
                "code"
            ],
            [
                "/opt/conda/lib/python3.10/site-packages/torchrl/_utils.py:259: UserWarning:\n\nGot multiple backends for torchrl.envs.libs.gym._get_gym_envs. Using the last queried (&lt;module 'gymnasium' from '/opt/conda/lib/python3.10/site-packages/gymnasium/__init__.py'&gt; with version 0.27.0).\n\n/opt/conda/lib/python3.10/site-packages/torchrl/_utils.py:259: UserWarning:\n\nGot multiple backends for torchrl.envs.libs.gym._build_gym_env. Using the last queried (&lt;module 'gymnasium' from '/opt/conda/lib/python3.10/site-packages/gymnasium/__init__.py'&gt; with version 0.27.0).\n\n/opt/conda/lib/python3.10/site-packages/torchrl/_utils.py:259: UserWarning:\n\nGot multiple backends for torchrl.envs.libs.gym._set_seed_initial. Using the last queried (&lt;module 'gymnasium' from '/opt/conda/lib/python3.10/site-packages/gymnasium/__init__.py'&gt; with version 0.27.0).\n\n/opt/conda/lib/python3.10/site-packages/torchrl/_utils.py:259: UserWarning:\n\nGot multiple backends for torchrl.envs.libs.gym._set_gym_args. Using the last queried (&lt;module 'gymnasium' from '/opt/conda/lib/python3.10/site-packages/gymnasium/__init__.py'&gt; with version 0.27.0).\n\n/opt/conda/lib/python3.10/site-packages/torchrl/_utils.py:259: UserWarning:\n\nGot multiple backends for torchrl.envs.libs.gym._set_gym_default. Using the last queried (&lt;module 'gymnasium' from '/opt/conda/lib/python3.10/site-packages/gymnasium/__init__.py'&gt; with version 0.27.0).",
                "code"
            ],
            {
                "Define Hyperparameters": [
                    [
                        "We set the hyperparameters for our algorithm. Depending on the resources\navailable, one may choose to execute the policy on GPU or on another\ndevice.\nThe frame_skip will control how for how many frames is a single\naction being executed. The rest of the arguments that count frames\nmust be corrected for this value (since one environment step will\nactually return frame_skip frames).",
                        "markdown"
                    ],
                    [
                        "device = \"cpu\" if not torch.has_cuda else \"cuda:0\"\nnum_cells = 256  # number of cells in each layer\nlr = 3e-4\nmax_grad_norm = 1.0",
                        "code"
                    ],
                    {
                        "Data collection parameters": [
                            [
                                "When collecting data, we will be able to choose how big each batch will be\nby defining a frames_per_batch parameter. We will also define how many\nframes (such as the number of interactions with the simulator) we will allow ourselves to\nuse. In general, the goal of an RL algorithm is to learn to solve the task\nas fast as it can in terms of environment interactions: the lower the total_frames\nthe better.\nWe also define a frame_skip: in some contexts, repeating the same action\nmultiple times over the course of a trajectory may be beneficial as it makes\nthe behavior more consistent and less erratic. However, \u201cskipping\u201d\ntoo many frames will hamper training by reducing the reactivity of the actor\nto observation changes.",
                                "markdown"
                            ],
                            [
                                "When using frame_skip it is good practice to\ncorrect the other frame counts by the number of frames we are grouping\ntogether. If we configure a total count of X frames for training but\nuse a frame_skip of Y, we will be actually collecting XY frames in total\nwhich exceeds our predefined budget.",
                                "markdown"
                            ],
                            [
                                "frame_skip = 1\nframes_per_batch = 1000 // frame_skip\n# For a complete training, bring the number of frames up to 1M\ntotal_frames = 50_000 // frame_skip",
                                "code"
                            ]
                        ]
                    },
                    {
                        "PPO parameters": [
                            [
                                "At each data collection (or batch collection) we will run the optimization\nover a certain number of <em>epochs</em>, each time consuming the entire data we just\nacquired in a nested training loop. Here, the sub_batch_size is different from the\nframes_per_batch here above: recall that we are working with a \u201cbatch of data\u201d\ncoming from our collector, which size is defined by frames_per_batch, and that\nwe will further split in smaller sub-batches during the inner training loop.\nThe size of these sub-batches is controlled by sub_batch_size.",
                                "markdown"
                            ],
                            [
                                "sub_batch_size = 64  # cardinality of the sub-samples gathered from the current data in the inner loop\nnum_epochs = 10  # optimisation steps per batch of data collected\nclip_epsilon = (\n    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n)\ngamma = 0.99\nlmbda = 0.95\nentropy_eps = 1e-4",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Define an environment": [
                    [
                        "In RL, an <em>environment</em> is usually the way we refer to a simulator or a\ncontrol system. Various libraries provide simulation environments for reinforcement\nlearning, including Gymnasium (previously OpenAI Gym), DeepMind control suite, and\nmany others.\nAs a generalistic library, TorchRL\u2019s goal is to provide an interchangeable interface\nto a large panel of RL simulators, allowing you to easily swap one environment\nwith another. For example, creating a wrapped gym environment can be achieved with few characters:",
                        "markdown"
                    ],
                    [
                        " = (\"InvertedDoublePendulum-v4\", device=device, frame_skip=frame_skip)",
                        "code"
                    ],
                    [
                        "There are a few things to notice in this code: first, we created\nthe environment by calling the GymEnv wrapper. If extra keyword arguments\nare passed, they will be transmitted to the gym.make method, hence covering\nthe most common env construction commands.\nAlternatively, one could also directly create a gym environment using gym.make(env_name, **kwargs)\nand wrap it in a <cite>GymWrapper</cite> class.",
                        "markdown"
                    ],
                    [
                        "Also the device argument: for gym, this only controls the device where\ninput action and observered states will be stored, but the execution will always\nbe done on CPU. The reason for this is simply that gym does not support on-device\nexecution, unless specified otherwise. For other libraries, we have control over\nthe execution device and, as much as we can, we try to stay consistent in terms of\nstoring and execution backends.",
                        "markdown"
                    ],
                    {
                        "Transforms": [
                            [
                                "We will append some transforms to our environments to prepare the data for\nthe policy. In Gym, this is usually achieved via wrappers. TorchRL takes a different\napproach, more similar to other pytorch domain libraries, through the use of transforms.\nTo add transforms to an environment, one should simply wrap it in a TransformedEnv\ninstance, and append the sequence of transforms to it. The transformed env will inherit\nthe device and meta-data of the wrapped env, and transform these depending on the sequence\nof transforms it contains.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Normalization": [
                            [
                                "The first to encode is a normalization transform.\nAs a rule of thumbs, it is preferable to have data that loosely\nmatch a unit Gaussian distribution: to obtain this, we will\nrun a certain number of random steps in the environment and compute\nthe summary statistics of these observations.",
                                "markdown"
                            ],
                            [
                                "We\u2019ll append two other transforms: the DoubleToFloat transform will\nconvert double entries to single-precision numbers, ready to be read by the\npolicy. The StepCounter transform will be used to count the steps before\nthe environment is terminated. We will use this measure as a supplementary measure\nof performance.",
                                "markdown"
                            ],
                            [
                                "As we will see later, many of the TorchRL\u2019s classes rely on \nto communicate. You could think of it as a python dictionary with some extra\ntensor features. In practice, this means that many modules we will be working\nwith need to be told what key to read (in_keys) and what key to write\n(out_keys) in the tensordict they will receive. Usually, if out_keys\nis omitted, it is assumed that the in_keys entries will be updated\nin-place. For our transforms, the only entry we are interested in is referred\nto as \"observation\" and our transform layers will be told to modify this\nentry and this entry only:",
                                "markdown"
                            ],
                            [
                                " = (\n    ,\n    (\n        # normalize observations\n        (in_keys=[\"observation\"]),\n        (in_keys=[\"observation\"]),\n        (),\n    ),\n)",
                                "code"
                            ],
                            [
                                "As you may have noticed, we have created a normalization layer but we did not\nset its normalization parameters. To do this, ObservationNorm can\nautomatically gather the summary statistics of our environment:",
                                "markdown"
                            ],
                            [
                                ".transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)",
                                "code"
                            ],
                            [
                                "The ObservationNorm transform has now been populated with a\nlocation and a scale that will be used to normalize the data.",
                                "markdown"
                            ],
                            [
                                "Let us do a little sanity check for the shape of our summary stats:",
                                "markdown"
                            ],
                            [
                                "print(\"normalization constant shape:\", .transform[0].loc.shape)",
                                "code"
                            ],
                            [
                                "normalization constant shape: torch.Size([11])",
                                "code"
                            ],
                            [
                                "An environment is not only defined by its simulator and transforms, but also\nby a series of metadata that describe what can be expected during its\nexecution.\nFor efficiency purposes, TorchRL is quite stringent when it comes to\nenvironment specs, but you can easily check that your environment specs are\nadequate.\nIn our example, the GymWrapper and GymEnv that inherits\nfrom it already take care of setting the proper specs for your env so\nyou should not have to care about this.",
                                "markdown"
                            ],
                            [
                                "Nevertheless, let\u2019s see a concrete example using our transformed\nenvironment by looking at its specs.\nThere are three specs to look at: observation_spec which defines what\nis to be expected when executing an action in the environment,\nreward_spec which indicates the reward domain and finally the\ninput_spec (which contains the action_spec) and which represents\neverything an environment requires to execute a single step.",
                                "markdown"
                            ],
                            [
                                "print(\"observation_spec:\", )\nprint(\"reward_spec:\", )\nprint(\"input_spec:\", )\nprint(\"action_spec (as defined by input_spec):\", )",
                                "code"
                            ],
                            [
                                "observation_spec: CompositeSpec(\n    observation: UnboundedContinuousTensorSpec(\n         shape=torch.Size([11]), space=None, device=cuda:0, dtype=torch.float32, domain=continuous),\n    step_count: UnboundedDiscreteTensorSpec(\n         shape=torch.Size([]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, contiguous=True)), device=cuda:0, dtype=torch.int64, domain=continuous), device=cuda:0, shape=torch.Size([]))\nreward_spec: UnboundedContinuousTensorSpec(\n     shape=torch.Size([1]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)), device=cuda:0, dtype=torch.float32, domain=continuous)\ninput_spec: CompositeSpec(\n    action: BoundedTensorSpec(\n         shape=torch.Size([1]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)), device=cuda:0, dtype=torch.float32, domain=continuous),\n    step_count: UnboundedDiscreteTensorSpec(\n         shape=torch.Size([]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, contiguous=True), maximum=Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, contiguous=True)), device=cuda:0, dtype=torch.int64, domain=continuous), device=cuda:0, shape=torch.Size([]))\naction_spec (as defined by input_spec): BoundedTensorSpec(\n     shape=torch.Size([1]), space=ContinuousBox(minimum=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True), maximum=Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, contiguous=True)), device=cuda:0, dtype=torch.float32, domain=continuous)",
                                "code"
                            ],
                            [
                                "the check_env_specs() function runs a small rollout and compares its output against the environemnt\nspecs. If no error is raised, we can be confident that the specs are properly defined:",
                                "markdown"
                            ],
                            [
                                "()",
                                "code"
                            ],
                            [
                                "check_env_specs succeeded!",
                                "code"
                            ],
                            [
                                "For fun, let\u2019s see what a simple random rollout looks like. You can\ncall <cite>env.rollout(n_steps)</cite> and get an overview of what the environment inputs\nand outputs look like. Actions will automatically be drawn from the action spec\ndomain, so you don\u2019t need to care about designing a random sampler.",
                                "markdown"
                            ],
                            [
                                "Typically, at each step, an RL environment receives an\naction as input, and outputs an observation, a reward and a done state. The\nobservation may be composite, meaning that it could be composed of more than one\ntensor. This is not a problem for TorchRL, since the whole set of observations\nis automatically packed in the output . After executing a rollout\n(ie a sequence of environment steps and random action generations) over a given\nnumber of steps, we will retrieve a  instance with a shape\nthat matches this trajectory length:",
                                "markdown"
                            ],
                            [
                                " = (3)\nprint(\"rollout of three steps:\", )\nprint(\"Shape of the rollout TensorDict:\", )",
                                "code"
                            ],
                            [
                                "rollout of three steps: TensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n        next: TensorDict(\n            fields={\n                done: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n                observation: Tensor(shape=torch.Size([3, 11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n                reward: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n                step_count: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.int64, is_shared=True)},\n            batch_size=torch.Size([3]),\n            device=cuda:0,\n            is_shared=True),\n        observation: Tensor(shape=torch.Size([3, 11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        reward: Tensor(shape=torch.Size([3, 1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        step_count: Tensor(shape=torch.Size([3]), device=cuda:0, dtype=torch.int64, is_shared=True)},\n    batch_size=torch.Size([3]),\n    device=cuda:0,\n    is_shared=True)\nShape of the rollout TensorDict: torch.Size([3])",
                                "code"
                            ],
                            [
                                "Our rollout data has a shape of torch.Size([3])`, which matches the number of steps\nwe ran it for. The ``\"next\" entry points to the data coming after the current step.\nIn most cases, the \"next\"\" data at time <cite>t</cite> matches the data at t+1, but this\nmay not be the case if we are using some specific transformations (e.g. mutli-step).",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Policy": [
                    [
                        "PPO utilizes a stochastic policy to handle exploration. This means that our\nneural network will have to output the parameters of a distribution, rather\nthan a single value corresponding to the action taken.",
                        "markdown"
                    ],
                    [
                        "As the data is continuous, we use a Tanh-Normal distribution to respect the\naction space boundaries. TorchRL provides such distribution, and the only\nthing we need to care about is to build a neural network that outputs the\nright number of parameters for the policy to work with (a location, or mean,\nand a scale):\n\n\\[f_{\\theta}(\\text{observation}) = \\mu_{\\theta}(\\text{observation}), \\sigma^{+}_{\\theta}(\\text{observation})\\]",
                        "markdown"
                    ],
                    [
                        "The only extra-difficulty that is brought up here is to split our output in two\nequal parts and map the second to a scrictly positive space.",
                        "markdown"
                    ],
                    [
                        "We design the policy in three steps:",
                        "markdown"
                    ],
                    [
                        "Define a neural network D_obs -&gt; 2 * D_action. Indeed, our loc (mu) and scale (sigma) both have dimension D_action;",
                        "markdown"
                    ],
                    [
                        "Append a NormalParamExtractor to extract a location and a scale (ie splits the input in two equal parts\n\n<blockquote>",
                        "markdown"
                    ],
                    [
                        "and applies a positive transformation to the scale parameter);\n</blockquote>",
                        "markdown"
                    ],
                    [
                        "Create a probabilistic TensorDictModule that can create this distribution and sample from it.",
                        "markdown"
                    ],
                    [
                        " = (\n    (num_cells, device=device),\n    (),\n    (num_cells, device=device),\n    (),\n    (num_cells, device=device),\n    (),\n    (2 * [-1], device=device),\n    (),\n)",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning:\n\nLazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.",
                        "code"
                    ],
                    [
                        "To enable the policy to \u201ctalk\u201d with the environment through the tensordict\ndata carrier, we wrap the nn.Module in a TensorDictModule. This\nclass will simply ready the in_keys it is provided with and write the\noutputs in-place at the registered out_keys.",
                        "markdown"
                    ],
                    [
                        " = (\n    , in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n)",
                        "code"
                    ],
                    [
                        "We now need to build a distribution out of the location and scale of our\nnormal distribution. To do so, we instruct the ProbabilisticActor\nclass to build a TanhNormal out of the location and scale\nparameters. We also provide the minimum and maximum values of this\ndistribution, which we gather from the environment specs.",
                        "markdown"
                    ],
                    [
                        "The name of the in_keys (and hence the name of the out_keys from\nthe TensorDictModule above) cannot be set to any value one may\nlike, as the TanhNormal distribution constructor will expect the\nloc and scale keyword arguments. That being said,\nProbabilisticActor also accepts Dict[str, str] typed in_keys\nwhere the key-value pair indicates what in_key string should be used for\nevery keyword argument that is to be used.",
                        "markdown"
                    ],
                    [
                        " = (\n    module=,\n    spec=,\n    in_keys=[\"loc\", \"scale\"],\n    distribution_class=,\n    distribution_kwargs={\n        \"min\": ,\n        \"max\": ,\n    },\n    return_log_prob=True,\n    # we'll need the log-prob for the numerator of the importance weights\n)",
                        "code"
                    ]
                ]
            },
            {
                "Value network": [
                    [
                        "The value network is a crucial component of the PPO algorithm, even though it\nwon\u2019t be used at inference time. This module will read the observations and\nreturn an estimation of the discounted return for the following trajectory.\nThis allows us to amortize learning by relying on the some utility estimation\nthat is learnt on-the-fly during training. Our value network share the same\nstructure as the policy, but for simplicity we assign it its own set of\nparameters.",
                        "markdown"
                    ],
                    [
                        " = (\n    (num_cells, device=device),\n    (),\n    (num_cells, device=device),\n    (),\n    (num_cells, device=device),\n    (),\n    (1, device=device),\n)\n\n = (\n    module=,\n    in_keys=[\"observation\"],\n)",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning:\n\nLazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.",
                        "code"
                    ],
                    [
                        "let\u2019s try our policy and value modules. As we said earlier, the usage of\nTensorDictModule makes it possible to directly read the output\nof the environment to run these modules, as they know what information to read\nand where to write it:",
                        "markdown"
                    ],
                    [
                        "print(\"Running policy:\", (()))\nprint(\"Running value:\", (()))",
                        "code"
                    ],
                    [
                        "Running policy: TensorDict(\n    fields={\n        action: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n        loc: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        observation: Tensor(shape=torch.Size([11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        reward: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        sample_log_prob: Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        scale: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        step_count: Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, is_shared=True)},\n    batch_size=torch.Size([]),\n    device=cuda:0,\n    is_shared=True)\nRunning value: TensorDict(\n    fields={\n        done: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.bool, is_shared=True),\n        observation: Tensor(shape=torch.Size([11]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        reward: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        state_value: Tensor(shape=torch.Size([1]), device=cuda:0, dtype=torch.float32, is_shared=True),\n        step_count: Tensor(shape=torch.Size([]), device=cuda:0, dtype=torch.int64, is_shared=True)},\n    batch_size=torch.Size([]),\n    device=cuda:0,\n    is_shared=True)",
                        "code"
                    ]
                ]
            },
            {
                "Data collector": [
                    [
                        "TorchRL provides a set of DataCollector classes. Briefly, these\nclasses execute three operations: reset an environment, compute an action\ngiven the latest observation, execute a step in the environment, and repeat\nthe last two steps until the environment reaches a stop signal (or \"done\"\nstate).",
                        "markdown"
                    ],
                    [
                        "They allow you to control how many frames to collect at each iteration\n(through the frames_per_batch parameter),\nwhen to reset the environment (through the max_frames_per_traj argument),\non which device the policy should be executed, etc. They are also\ndesigned to work efficiently with batched and multiprocessed environments.",
                        "markdown"
                    ],
                    [
                        "The simplest data collector is the SyncDataCollector: it is an\niterator that you can use to get batches of data of a given length, and\nthat will stop once a total number of frames (total_frames) have been\ncollected.\nOther data collectors (MultiSyncDataCollector and\nMultiaSyncDataCollector) will execute the same operations in synchronous\nand asynchronous manner over a set of multiprocessed workers.",
                        "markdown"
                    ],
                    [
                        "As for the policy and environment before, the data collector will return\n instances with a total number of elements that will\nmatch frames_per_batch. Using  to pass data to the\ntraining loop allows you to write dataloading pipelines\nthat are 100% oblivious to the actual specificities of the rollout content.",
                        "markdown"
                    ],
                    [
                        " = (\n    ,\n    ,\n    frames_per_batch=frames_per_batch,\n    total_frames=total_frames,\n    split_trajs=False,\n    device=device,\n)",
                        "code"
                    ]
                ]
            },
            {
                "Replay buffer": [
                    [
                        "Replay buffers are a common building piece of off-policy RL algorithms.\nIn on-policy contexts, a replay buffer is refilled every time a batch of\ndata is collected, and its data is repeatedly consumed for a certain number\nof epochs.",
                        "markdown"
                    ],
                    [
                        "TorchRL\u2019s replay buffers are built using a common container\nReplayBuffer which takes as argument the components of the buffer:\na storage, a writer, a sampler and possibly some transforms. Only the\nstorage (which indicates the replay buffer capacity) is mandatory. We\nalso specify a sampler without repetition to avoid sampling multiple times\nthe same item in one epoch.\nUsing a replay buffer for PPO is not mandatory and we could simply\nsample the sub-batches from the collected batch, but using these classes\nmake it easy for us to build the inner training loop in a reproducible way.",
                        "markdown"
                    ],
                    [
                        " = (\n    storage=(frames_per_batch),\n    sampler=SamplerWithoutReplacement(),\n)",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torchrl/data/replay_buffers/replay_buffers.py:151: UserWarning:\n\nConstructing replay buffer without specifying behaviour is no longer recommended, and will be deprecated in the future.",
                        "code"
                    ]
                ]
            },
            {
                "Loss function": [
                    [
                        "The PPO loss can be directly imported from torchrl for convenience using the\nClipPPOLoss class. This is the easiest way of utilizing PPO:\nit hides away the mathematical operations of PPO and the control flow that\ngoes with it.",
                        "markdown"
                    ],
                    [
                        "PPO requires some \u201cadvantage estimation\u201d to be computed. In short, an advantage\nis a value that reflects an expectancy over the return value while dealing with\nthe bias / variance tradeoff.\nTo compute the advantage, one just needs to (1) build the advantage module, which\nutilizes our value operator, and (2) pass each batch of data through it before each\nepoch.\nThe GAE module will update the input tensordict with new \"advantage\" and\n\"value_target\" entries.\nThe \"value_target\" is a gradient-free tensor that represents the empirical\nvalue that the value network should represent with the input observation.\nBoth of these will be used by ClipPPOLoss to\nreturn the policy and value losses.",
                        "markdown"
                    ],
                    [
                        " = (\n    gamma=gamma, lmbda=lmbda, value_network=, average_gae=True\n)\n\n = (\n    actor=,\n    critic=,\n    advantage_key=\"advantage\",\n    clip_epsilon=clip_epsilon,\n    entropy_bonus=bool(entropy_eps),\n    entropy_coef=entropy_eps,\n    # these keys match by default but we set this for completeness\n    value_target_key=.value_target_key,\n    critic_coef=1.0,\n    gamma=0.99,\n    loss_critic_type=\"smooth_l1\",\n)\n\n = ((), lr)\n = (\n    , total_frames // frames_per_batch, 0.0\n)",
                        "code"
                    ]
                ]
            },
            {
                "Training loop": [
                    [
                        "We now have all the pieces needed to code our training loop.\nThe steps include:",
                        "markdown"
                    ],
                    [
                        "Collect data",
                        "markdown"
                    ],
                    [
                        "Compute advantage",
                        "markdown"
                    ],
                    [
                        "Loop over the collected to compute loss values",
                        "markdown"
                    ],
                    [
                        "Back propagate",
                        "markdown"
                    ],
                    [
                        "Optimize",
                        "markdown"
                    ],
                    [
                        "Repeat",
                        "markdown"
                    ],
                    [
                        "Repeat",
                        "markdown"
                    ],
                    [
                        "Repeat",
                        "markdown"
                    ],
                    [
                        "logs = defaultdict(list)\npbar = tqdm(total=total_frames * frame_skip)\neval_str = \"\"\n\n# We iterate over the collector until it reaches the total number of frames it was\n# designed to collect:\nfor i,  in enumerate():\n    # we now have a batch of data to work with. Let's learn something from it.\n    for _ in range(num_epochs):\n        # We'll need an \"advantage\" signal to make PPO work.\n        # We re-compute it at each epoch as its value depends on the value\n        # network which is updated in the inner loop.\n        ()\n         = (-1)\n        (())\n        for _ in range(frames_per_batch // sub_batch_size):\n            , *_ = (sub_batch_size)\n             = ((device))\n             = (\n                [\"loss_objective\"]\n                + [\"loss_critic\"]\n                + [\"loss_entropy\"]\n            )\n\n            # Optimization: backward, grad clipping and optim step\n            ()\n            # this is not strictly mandatory but it's good practice to keep\n            # your gradient norm bounded\n            ((), max_grad_norm)\n            .step()\n            ()\n\n    logs[\"reward\"].append([\"next\", \"reward\"].mean().item())\n    pbar.update(() * frame_skip)\n    cum_reward_str = (\n        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n    )\n    logs[\"step_count\"].append([\"step_count\"].max().item())\n    stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n    logs[\"lr\"].append(.param_groups[0][\"lr\"])\n    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n    if i % 10 == 0:\n        # We evaluate the policy once every 10 batches of data.\n        # Evaluation is rather simple: execute the policy without exploration\n        # (take the expected value of the action distribution) for a given\n        # number of steps (1000, which is our env horizon).\n        # The ``rollout`` method of the env can take a policy as argument:\n        # it will then execute this policy at each step.\n        with (\"mean\"), ():\n            # execute a rollout with the trained policy\n            eval_rollout = (1000, )\n            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n            logs[\"eval reward (sum)\"].append(\n                eval_rollout[\"next\", \"reward\"].sum().item()\n            )\n            logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n            eval_str = (\n                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n                f\"eval step-count: {logs['eval step_count'][-1]}\"\n            )\n            del eval_rollout\n    pbar.set_description(\", \".join([eval_str, cum_reward_str, stepcount_str, lr_str]))\n\n    # We're also using a learning rate scheduler. Like the gradient clipping,\n    # this is a nice-to-have but nothing necessary for PPO to work.\n    .step()",
                        "code"
                    ],
                    [
                        "  0%|          | 0/50000 [00:00&lt;?, ?it/s]/opt/conda/lib/python3.10/site-packages/torchrl/data/replay_buffers/replay_buffers.py:274: UserWarning:\n\nbatch_size argument in sample has been deprecated. Set the batch_size when constructing the replay buffer instead.\n\n\n  2%|2         | 1000/50000 [00:07&lt;05:53, 138.65it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.0758 (init= 9.0758), step count (max): 13, lr policy:  0.0003:   2%|2         | 1000/50000 [00:07&lt;05:53, 138.65it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.0758 (init= 9.0758), step count (max): 13, lr policy:  0.0003:   4%|4         | 2000/50000 [00:14&lt;05:44, 139.16it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1163 (init= 9.0758), step count (max): 13, lr policy:  0.0003:   4%|4         | 2000/50000 [00:14&lt;05:44, 139.16it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1163 (init= 9.0758), step count (max): 13, lr policy:  0.0003:   6%|6         | 3000/50000 [00:21&lt;05:38, 138.87it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1412 (init= 9.0758), step count (max): 14, lr policy:  0.0003:   6%|6         | 3000/50000 [00:21&lt;05:38, 138.87it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1412 (init= 9.0758), step count (max): 14, lr policy:  0.0003:   8%|8         | 4000/50000 [00:28&lt;05:27, 140.51it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1640 (init= 9.0758), step count (max): 19, lr policy:  0.0003:   8%|8         | 4000/50000 [00:28&lt;05:27, 140.51it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1640 (init= 9.0758), step count (max): 19, lr policy:  0.0003:  10%|#         | 5000/50000 [00:35&lt;05:20, 140.27it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1717 (init= 9.0758), step count (max): 17, lr policy:  0.0003:  10%|#         | 5000/50000 [00:35&lt;05:20, 140.27it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1717 (init= 9.0758), step count (max): 17, lr policy:  0.0003:  12%|#2        | 6000/50000 [00:42&lt;05:14, 139.73it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1535 (init= 9.0758), step count (max): 23, lr policy:  0.0003:  12%|#2        | 6000/50000 [00:42&lt;05:14, 139.73it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1535 (init= 9.0758), step count (max): 23, lr policy:  0.0003:  14%|#4        | 7000/50000 [00:49&lt;05:04, 141.24it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1632 (init= 9.0758), step count (max): 21, lr policy:  0.0003:  14%|#4        | 7000/50000 [00:49&lt;05:04, 141.24it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1632 (init= 9.0758), step count (max): 21, lr policy:  0.0003:  16%|#6        | 8000/50000 [00:56&lt;04:54, 142.65it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1584 (init= 9.0758), step count (max): 18, lr policy:  0.0003:  16%|#6        | 8000/50000 [00:56&lt;04:54, 142.65it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1584 (init= 9.0758), step count (max): 18, lr policy:  0.0003:  18%|#8        | 9000/50000 [01:03&lt;04:48, 142.29it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1305 (init= 9.0758), step count (max): 24, lr policy:  0.0003:  18%|#8        | 9000/50000 [01:03&lt;04:48, 142.29it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1305 (init= 9.0758), step count (max): 24, lr policy:  0.0003:  20%|##        | 10000/50000 [01:10&lt;04:39, 142.86it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1164 (init= 9.0758), step count (max): 17, lr policy:  0.0003:  20%|##        | 10000/50000 [01:10&lt;04:39, 142.86it/s]\neval cumulative reward:  91.8310 (init:  91.8310), eval step-count: 9, average reward= 9.1164 (init= 9.0758), step count (max): 17, lr policy:  0.0003:  22%|##2       | 11000/50000 [01:17&lt;04:32, 142.94it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1362 (init= 9.0758), step count (max): 23, lr policy:  0.0003:  22%|##2       | 11000/50000 [01:17&lt;04:32, 142.94it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1362 (init= 9.0758), step count (max): 23, lr policy:  0.0003:  24%|##4       | 12000/50000 [01:24&lt;04:26, 142.55it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1655 (init= 9.0758), step count (max): 23, lr policy:  0.0003:  24%|##4       | 12000/50000 [01:24&lt;04:26, 142.55it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1655 (init= 9.0758), step count (max): 23, lr policy:  0.0003:  26%|##6       | 13000/50000 [01:31&lt;04:19, 142.33it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1693 (init= 9.0758), step count (max): 24, lr policy:  0.0003:  26%|##6       | 13000/50000 [01:31&lt;04:19, 142.33it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1693 (init= 9.0758), step count (max): 24, lr policy:  0.0003:  28%|##8       | 14000/50000 [01:39&lt;04:18, 139.26it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1620 (init= 9.0758), step count (max): 27, lr policy:  0.0003:  28%|##8       | 14000/50000 [01:39&lt;04:18, 139.26it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1620 (init= 9.0758), step count (max): 27, lr policy:  0.0003:  30%|###       | 15000/50000 [01:46&lt;04:10, 139.46it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1641 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  30%|###       | 15000/50000 [01:46&lt;04:10, 139.46it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1641 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  32%|###2      | 16000/50000 [01:53&lt;04:02, 140.35it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1836 (init= 9.0758), step count (max): 19, lr policy:  0.0002:  32%|###2      | 16000/50000 [01:53&lt;04:02, 140.35it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1836 (init= 9.0758), step count (max): 19, lr policy:  0.0002:  34%|###4      | 17000/50000 [02:00&lt;03:54, 140.51it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1994 (init= 9.0758), step count (max): 19, lr policy:  0.0002:  34%|###4      | 17000/50000 [02:00&lt;03:54, 140.51it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1994 (init= 9.0758), step count (max): 19, lr policy:  0.0002:  36%|###6      | 18000/50000 [02:07&lt;03:47, 140.39it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1935 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  36%|###6      | 18000/50000 [02:07&lt;03:47, 140.39it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1935 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  38%|###8      | 19000/50000 [02:14&lt;03:41, 139.88it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1964 (init= 9.0758), step count (max): 23, lr policy:  0.0002:  38%|###8      | 19000/50000 [02:14&lt;03:41, 139.88it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1964 (init= 9.0758), step count (max): 23, lr policy:  0.0002:  40%|####      | 20000/50000 [02:22&lt;03:33, 140.32it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1813 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  40%|####      | 20000/50000 [02:22&lt;03:33, 140.32it/s]\neval cumulative reward:  101.7014 (init:  91.8310), eval step-count: 10, average reward= 9.1813 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  42%|####2     | 21000/50000 [02:29&lt;03:27, 140.03it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1820 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  42%|####2     | 21000/50000 [02:29&lt;03:27, 140.03it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1820 (init= 9.0758), step count (max): 20, lr policy:  0.0002:  44%|####4     | 22000/50000 [02:36&lt;03:23, 137.86it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1896 (init= 9.0758), step count (max): 24, lr policy:  0.0002:  44%|####4     | 22000/50000 [02:36&lt;03:23, 137.86it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1896 (init= 9.0758), step count (max): 24, lr policy:  0.0002:  46%|####6     | 23000/50000 [02:43&lt;03:13, 139.33it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1771 (init= 9.0758), step count (max): 23, lr policy:  0.0002:  46%|####6     | 23000/50000 [02:43&lt;03:13, 139.33it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1771 (init= 9.0758), step count (max): 23, lr policy:  0.0002:  48%|####8     | 24000/50000 [02:50&lt;03:06, 139.53it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1971 (init= 9.0758), step count (max): 27, lr policy:  0.0002:  48%|####8     | 24000/50000 [02:50&lt;03:06, 139.53it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1971 (init= 9.0758), step count (max): 27, lr policy:  0.0002:  50%|#####     | 25000/50000 [02:57&lt;02:58, 139.98it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1933 (init= 9.0758), step count (max): 23, lr policy:  0.0002:  50%|#####     | 25000/50000 [02:57&lt;02:58, 139.98it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1933 (init= 9.0758), step count (max): 23, lr policy:  0.0002:  52%|#####2    | 26000/50000 [03:04&lt;02:50, 141.12it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1828 (init= 9.0758), step count (max): 20, lr policy:  0.0001:  52%|#####2    | 26000/50000 [03:04&lt;02:50, 141.12it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1828 (init= 9.0758), step count (max): 20, lr policy:  0.0001:  54%|#####4    | 27000/50000 [03:11&lt;02:42, 141.33it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1912 (init= 9.0758), step count (max): 22, lr policy:  0.0001:  54%|#####4    | 27000/50000 [03:11&lt;02:42, 141.33it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1912 (init= 9.0758), step count (max): 22, lr policy:  0.0001:  56%|#####6    | 28000/50000 [03:19&lt;02:35, 141.18it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.2035 (init= 9.0758), step count (max): 34, lr policy:  0.0001:  56%|#####6    | 28000/50000 [03:19&lt;02:35, 141.18it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.2035 (init= 9.0758), step count (max): 34, lr policy:  0.0001:  58%|#####8    | 29000/50000 [03:26&lt;02:28, 141.16it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.2096 (init= 9.0758), step count (max): 24, lr policy:  0.0001:  58%|#####8    | 29000/50000 [03:26&lt;02:28, 141.16it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.2096 (init= 9.0758), step count (max): 24, lr policy:  0.0001:  60%|######    | 30000/50000 [03:33&lt;02:22, 140.15it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1983 (init= 9.0758), step count (max): 22, lr policy:  0.0001:  60%|######    | 30000/50000 [03:33&lt;02:22, 140.15it/s]\neval cumulative reward:  92.0639 (init:  91.8310), eval step-count: 9, average reward= 9.1983 (init= 9.0758), step count (max): 22, lr policy:  0.0001:  62%|######2   | 31000/50000 [03:40&lt;02:16, 139.09it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.1963 (init= 9.0758), step count (max): 24, lr policy:  0.0001:  62%|######2   | 31000/50000 [03:40&lt;02:16, 139.09it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.1963 (init= 9.0758), step count (max): 24, lr policy:  0.0001:  64%|######4   | 32000/50000 [03:47&lt;02:08, 139.72it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2000 (init= 9.0758), step count (max): 22, lr policy:  0.0001:  64%|######4   | 32000/50000 [03:47&lt;02:08, 139.72it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2000 (init= 9.0758), step count (max): 22, lr policy:  0.0001:  66%|######6   | 33000/50000 [03:54&lt;02:01, 140.47it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.1990 (init= 9.0758), step count (max): 23, lr policy:  0.0001:  66%|######6   | 33000/50000 [03:54&lt;02:01, 140.47it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.1990 (init= 9.0758), step count (max): 23, lr policy:  0.0001:  68%|######8   | 34000/50000 [04:01&lt;01:53, 141.00it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2098 (init= 9.0758), step count (max): 24, lr policy:  0.0001:  68%|######8   | 34000/50000 [04:01&lt;01:53, 141.00it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2098 (init= 9.0758), step count (max): 24, lr policy:  0.0001:  70%|#######   | 35000/50000 [04:09&lt;01:47, 139.98it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2150 (init= 9.0758), step count (max): 23, lr policy:  0.0001:  70%|#######   | 35000/50000 [04:09&lt;01:47, 139.98it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2150 (init= 9.0758), step count (max): 23, lr policy:  0.0001:  72%|#######2  | 36000/50000 [04:16&lt;01:39, 141.13it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2132 (init= 9.0758), step count (max): 28, lr policy:  0.0001:  72%|#######2  | 36000/50000 [04:16&lt;01:39, 141.13it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2132 (init= 9.0758), step count (max): 28, lr policy:  0.0001:  74%|#######4  | 37000/50000 [04:23&lt;01:31, 142.07it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2048 (init= 9.0758), step count (max): 34, lr policy:  0.0001:  74%|#######4  | 37000/50000 [04:23&lt;01:31, 142.07it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2048 (init= 9.0758), step count (max): 34, lr policy:  0.0001:  76%|#######6  | 38000/50000 [04:30&lt;01:24, 141.98it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2144 (init= 9.0758), step count (max): 23, lr policy:  0.0000:  76%|#######6  | 38000/50000 [04:30&lt;01:24, 141.98it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2144 (init= 9.0758), step count (max): 23, lr policy:  0.0000:  78%|#######8  | 39000/50000 [04:37&lt;01:18, 140.30it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2123 (init= 9.0758), step count (max): 26, lr policy:  0.0000:  78%|#######8  | 39000/50000 [04:37&lt;01:18, 140.30it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2123 (init= 9.0758), step count (max): 26, lr policy:  0.0000:  80%|########  | 40000/50000 [04:44&lt;01:11, 140.39it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2194 (init= 9.0758), step count (max): 38, lr policy:  0.0000:  80%|########  | 40000/50000 [04:44&lt;01:11, 140.39it/s]\neval cumulative reward:  110.5292 (init:  91.8310), eval step-count: 11, average reward= 9.2194 (init= 9.0758), step count (max): 38, lr policy:  0.0000:  82%|########2 | 41000/50000 [04:51&lt;01:04, 140.17it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2183 (init= 9.0758), step count (max): 34, lr policy:  0.0000:  82%|########2 | 41000/50000 [04:51&lt;01:04, 140.17it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2183 (init= 9.0758), step count (max): 34, lr policy:  0.0000:  84%|########4 | 42000/50000 [04:58&lt;00:56, 140.77it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2236 (init= 9.0758), step count (max): 27, lr policy:  0.0000:  84%|########4 | 42000/50000 [04:58&lt;00:56, 140.77it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2236 (init= 9.0758), step count (max): 27, lr policy:  0.0000:  86%|########6 | 43000/50000 [05:05&lt;00:49, 142.01it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2157 (init= 9.0758), step count (max): 26, lr policy:  0.0000:  86%|########6 | 43000/50000 [05:05&lt;00:49, 142.01it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2157 (init= 9.0758), step count (max): 26, lr policy:  0.0000:  88%|########8 | 44000/50000 [05:12&lt;00:42, 141.30it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2210 (init= 9.0758), step count (max): 24, lr policy:  0.0000:  88%|########8 | 44000/50000 [05:12&lt;00:42, 141.30it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2210 (init= 9.0758), step count (max): 24, lr policy:  0.0000:  90%|######### | 45000/50000 [05:19&lt;00:35, 141.66it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2116 (init= 9.0758), step count (max): 23, lr policy:  0.0000:  90%|######### | 45000/50000 [05:19&lt;00:35, 141.66it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2116 (init= 9.0758), step count (max): 23, lr policy:  0.0000:  92%|#########2| 46000/50000 [05:26&lt;00:28, 142.03it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2216 (init= 9.0758), step count (max): 22, lr policy:  0.0000:  92%|#########2| 46000/50000 [05:26&lt;00:28, 142.03it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2216 (init= 9.0758), step count (max): 22, lr policy:  0.0000:  94%|#########3| 47000/50000 [05:33&lt;00:21, 141.37it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2181 (init= 9.0758), step count (max): 27, lr policy:  0.0000:  94%|#########3| 47000/50000 [05:33&lt;00:21, 141.37it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2181 (init= 9.0758), step count (max): 27, lr policy:  0.0000:  96%|#########6| 48000/50000 [05:41&lt;00:14, 140.31it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2188 (init= 9.0758), step count (max): 25, lr policy:  0.0000:  96%|#########6| 48000/50000 [05:41&lt;00:14, 140.31it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2188 (init= 9.0758), step count (max): 25, lr policy:  0.0000:  98%|#########8| 49000/50000 [05:48&lt;00:07, 140.94it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2166 (init= 9.0758), step count (max): 28, lr policy:  0.0000:  98%|#########8| 49000/50000 [05:48&lt;00:07, 140.94it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2166 (init= 9.0758), step count (max): 28, lr policy:  0.0000: 100%|##########| 50000/50000 [05:55&lt;00:00, 141.35it/s]\neval cumulative reward:  185.3627 (init:  91.8310), eval step-count: 19, average reward= 9.2224 (init= 9.0758), step count (max): 28, lr policy:  0.0000: 100%|##########| 50000/50000 [05:55&lt;00:00, 141.35it/s]",
                        "code"
                    ]
                ]
            },
            {
                "Results": [
                    [
                        "Before the 1M step cap is reached, the algorithm should have reached a max\nstep count of 1000 steps, which is the maximum number of steps before the\ntrajectory is truncated.",
                        "markdown"
                    ],
                    [
                        "plt.figure(figsize=(10, 10))\nplt.subplot(2, 2, 1)\nplt.plot(logs[\"reward\"])\nplt.title(\"training rewards (average)\")\nplt.subplot(2, 2, 2)\nplt.plot(logs[\"step_count\"])\nplt.title(\"Max step count (training)\")\nplt.subplot(2, 2, 3)\nplt.plot(logs[\"eval reward (sum)\"])\nplt.title(\"Return (test)\")\nplt.subplot(2, 2, 4)\nplt.plot(logs[\"eval step_count\"])\nplt.title(\"Max step count (test)\")\nplt.show()\n\n\n<img alt=\"training rewards (average), Max step count (training), Return (test), Max step count (test)\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_reinforcement_ppo_001.png\" srcset=\"../_images/sphx_glr_reinforcement_ppo_001.png\"/>",
                        "code"
                    ]
                ]
            },
            {
                "Conclusion and next steps": [
                    [
                        "In this tutorial, we have learned:",
                        "markdown"
                    ],
                    [
                        "How to create and customize an environment with torchrl;",
                        "markdown"
                    ],
                    [
                        "How to write a model and a loss function;",
                        "markdown"
                    ],
                    [
                        "How to set up a typical training loop.",
                        "markdown"
                    ],
                    [
                        "If you want to experiment with this tutorial a bit more, you can apply the following modifications:",
                        "markdown"
                    ],
                    [
                        "From an efficiency perspective,\nwe could run several simulations in parallel to speed up data collection.\nCheck  for further information.",
                        "markdown"
                    ],
                    [
                        "From a logging perspective, one could add a torchrl.record.VideoRecorder transform to\nthe environment after asking for rendering to get a visual rendering of the\ninverted pendulum in action. Check torchrl.record to\nknow more.",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 5 minutes  59.034 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Train a Mario-playing RL Agent": [
            [
                "Authors: , , , .",
                "markdown"
            ],
            [
                "This tutorial walks you through the fundamentals of Deep Reinforcement\nLearning. At the end, you will implement an AI-powered Mario (using\n) that\ncan play the game by itself.",
                "markdown"
            ],
            [
                "Although no prior knowledge of RL is necessary for this tutorial, you\ncan familiarize yourself with these RL\n,\nand have this handy\n\nas your companion. The full code is available\n.\n\n<img alt=\"mario\" src=\"../_images/mario.gif\"/>",
                "markdown"
            ],
            [
                "%%bash\npip install gym-super-mario-bros==7.4.0",
                "code"
            ],
            [
                "import torch\nfrom torch import nn\nfrom torchvision import transforms as T\nfrom PIL import Image\nimport numpy as np\nfrom pathlib import Path\nfrom collections import deque\nimport random, datetime, os, copy\n\n# Gym is an OpenAI toolkit for RL\nimport gym\nfrom gym.spaces import Box\nfrom gym.wrappers import FrameStack\n\n# NES Emulator for OpenAI Gym\nfrom nes_py.wrappers import JoypadSpace\n\n# Super Mario environment for OpenAI Gym\nimport gym_super_mario_bros",
                "code"
            ],
            {
                "RL Definitions": [
                    [
                        "<strong>Environment</strong> The world that an agent interacts with and learns from.",
                        "markdown"
                    ],
                    [
                        "<strong>Action</strong> \\(a\\) : How the Agent responds to the Environment. The\nset of all possible Actions is called <em>action-space</em>.",
                        "markdown"
                    ],
                    [
                        "<strong>State</strong> \\(s\\) : The current characteristic of the Environment. The\nset of all possible States the Environment can be in is called\n<em>state-space</em>.",
                        "markdown"
                    ],
                    [
                        "<strong>Reward</strong> \\(r\\) : Reward is the key feedback from Environment to\nAgent. It is what drives the Agent to learn and to change its future\naction. An aggregation of rewards over multiple time steps is called\n<strong>Return</strong>.",
                        "markdown"
                    ],
                    [
                        "<strong>Optimal Action-Value function</strong> \\(Q^*(s,a)\\) : Gives the expected\nreturn if you start in state \\(s\\), take an arbitrary action\n\\(a\\), and then for each future time step take the action that\nmaximizes returns. \\(Q\\) can be said to stand for the \u201cquality\u201d of\nthe action in a state. We try to approximate this function.",
                        "markdown"
                    ]
                ]
            },
            {
                "Environment": [
                    {
                        "Initialize Environment": [
                            [
                                "In Mario, the environment consists of tubes, mushrooms and other\ncomponents.",
                                "markdown"
                            ],
                            [
                                "When Mario makes an action, the environment responds with the changed\n(next) state, reward and other info.",
                                "markdown"
                            ],
                            [
                                "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\nif gym.__version__ &lt; '0.26':\n    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\nelse:\n    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)\n\n# Limit the action-space to\n#   0. walk right\n#   1. jump right\nenv = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n\nenv.reset()\nnext_state, reward, done, trunc, info = env.step(action=0)\nprint(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")",
                                "code"
                            ],
                            [
                                "/opt/conda/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning:\n\nWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\n\n/opt/conda/lib/python3.10/site-packages/gym/envs/registration.py:627: UserWarning:\n\nWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\n\n(240, 256, 3),\n 0.0,\n False,\n {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Preprocess Environment": [
                            [
                                "Environment data is returned to the agent in next_state. As you saw\nabove, each state is represented by a [3, 240, 256] size array.\nOften that is more information than our agent needs; for instance,\nMario\u2019s actions do not depend on the color of the pipes or the sky!",
                                "markdown"
                            ],
                            [
                                "We use <strong>Wrappers</strong> to preprocess environment data before sending it to\nthe agent.",
                                "markdown"
                            ],
                            [
                                "GrayScaleObservation is a common wrapper to transform an RGB image\nto grayscale; doing so reduces the size of the state representation\nwithout losing useful information. Now the size of each state:\n[1, 240, 256]",
                                "markdown"
                            ],
                            [
                                "ResizeObservation downsamples each observation into a square image.\nNew size: [1, 84, 84]",
                                "markdown"
                            ],
                            [
                                "SkipFrame is a custom wrapper that inherits from gym.Wrapper and\nimplements the step() function. Because consecutive frames don\u2019t\nvary much, we can skip n-intermediate frames without losing much\ninformation. The n-th frame aggregates rewards accumulated over each\nskipped frame.",
                                "markdown"
                            ],
                            [
                                "FrameStack is a wrapper that allows us to squash consecutive frames\nof the environment into a single observation point to feed to our\nlearning model. This way, we can identify if Mario was landing or\njumping based on the direction of his movement in the previous several\nframes.",
                                "markdown"
                            ],
                            [
                                "class SkipFrame(gym.Wrapper):\n    def __init__(self, env, skip):\n        \"\"\"Return only every `skip`-th frame\"\"\"\n        super().__init__(env)\n        self._skip = skip\n\n    def step(self, action):\n        \"\"\"Repeat action, and sum reward\"\"\"\n        total_reward = 0.0\n        for i in range(self._skip):\n            # Accumulate reward and repeat the same action\n            obs, reward, done, trunk, info = self.env.step(action)\n            total_reward += reward\n            if done:\n                break\n        return obs, total_reward, done, trunk, info\n\n\nclass GrayScaleObservation(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        obs_shape = self.observation_space.shape[:2]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n\n    def permute_orientation(self, observation):\n        # permute [H, W, C] array to [C, H, W] tensor\n        observation = np.transpose(observation, (2, 0, 1))\n        observation = (observation.copy(), dtype=)\n        return observation\n\n    def observation(self, observation):\n        observation = self.permute_orientation(observation)\n        transform = ()\n        observation = transform(observation)\n        return observation\n\n\nclass ResizeObservation(gym.ObservationWrapper):\n    def __init__(self, env, shape):\n        super().__init__(env)\n        if isinstance(shape, int):\n            self.shape = (shape, shape)\n        else:\n            self.shape = tuple(shape)\n\n        obs_shape = self.shape + self.observation_space.shape[2:]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n\n    def observation(self, observation):\n        transforms = (\n            [(self.shape), (0, 255)]\n        )\n        observation = transforms(observation).squeeze(0)\n        return observation\n\n\n# Apply Wrappers to environment\nenv = SkipFrame(env, skip=4)\nenv = GrayScaleObservation(env)\nenv = ResizeObservation(env, shape=84)\nif gym.__version__ &lt; '0.26':\n    env = FrameStack(env, num_stack=4, new_step_api=True)\nelse:\n    env = FrameStack(env, num_stack=4)",
                                "code"
                            ],
                            [
                                "After applying the above wrappers to the environment, the final wrapped\nstate consists of 4 gray-scaled consecutive frames stacked together, as\nshown above in the image on the left. Each time Mario makes an action,\nthe environment responds with a state of this structure. The structure\nis represented by a 3-D array of size [4, 84, 84].\n\n<img alt=\"picture\" src=\"../_images/mario_env.png\"/>",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Agent": [
                    [
                        "We create a class Mario to represent our agent in the game. Mario\nshould be able to:",
                        "markdown"
                    ],
                    [
                        "<strong>Act</strong> according to the optimal action policy based on the current\nstate (of the environment).",
                        "markdown"
                    ],
                    [
                        "<strong>Remember</strong> experiences. Experience = (current state, current\naction, reward, next state). Mario <em>caches</em> and later <em>recalls</em> his\nexperiences to update his action policy.",
                        "markdown"
                    ],
                    [
                        "<strong>Learn</strong> a better action policy over time",
                        "markdown"
                    ],
                    [
                        "class Mario:\n    def __init__():\n        pass\n\n    def act(self, state):\n        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n        pass\n\n    def cache(self, experience):\n        \"\"\"Add the experience to memory\"\"\"\n        pass\n\n    def recall(self):\n        \"\"\"Sample experiences from memory\"\"\"\n        pass\n\n    def learn(self):\n        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n        pass",
                        "code"
                    ],
                    [
                        "In the following sections, we will populate Mario\u2019s parameters and\ndefine his functions.",
                        "markdown"
                    ],
                    {
                        "Act": [
                            [
                                "For any given state, an agent can choose to do the most optimal action\n(<strong>exploit</strong>) or a random action (<strong>explore</strong>).",
                                "markdown"
                            ],
                            [
                                "Mario randomly explores with a chance of self.exploration_rate; when\nhe chooses to exploit, he relies on MarioNet (implemented in\nLearn section) to provide the most optimal action.",
                                "markdown"
                            ],
                            [
                                "class Mario:\n    def __init__(self, state_dim, action_dim, save_dir):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.save_dir = save_dir\n\n        self.device = \"cuda\" if () else \"cpu\"\n\n        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n        self.net = (self.state_dim, self.action_dim).float()\n        self.net = self.net.to(device=self.device)\n\n        self.exploration_rate = 1\n        self.exploration_rate_decay = 0.99999975\n        self.exploration_rate_min = 0.1\n        self.curr_step = 0\n\n        self.save_every = 5e5  # no. of experiences between saving Mario Net\n\n    def act(self, state):\n        \"\"\"\n    Given a state, choose an epsilon-greedy action and update value of step.\n\n    Inputs:\n    state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n    Outputs:\n    action_idx (int): An integer representing which action Mario will perform\n    \"\"\"\n        # EXPLORE\n        if np.random.rand() &lt; self.exploration_rate:\n            action_idx = np.random.randint(self.action_dim)\n\n        # EXPLOIT\n        else:\n            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n            state = (state, device=self.device).unsqueeze(0)\n            action_values = self.net(state, model=\"online\")\n            action_idx = (action_values, axis=1).item()\n\n        # decrease exploration_rate\n        self.exploration_rate *= self.exploration_rate_decay\n        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n\n        # increment step\n        self.curr_step += 1\n        return action_idx",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Cache and Recall": [
                            [
                                "These two functions serve as Mario\u2019s \u201cmemory\u201d process.",
                                "markdown"
                            ],
                            [
                                "cache(): Each time Mario performs an action, he stores the\nexperience to his memory. His experience includes the current\n<em>state</em>, <em>action</em> performed, <em>reward</em> from the action, the <em>next state</em>,\nand whether the game is <em>done</em>.",
                                "markdown"
                            ],
                            [
                                "recall(): Mario randomly samples a batch of experiences from his\nmemory, and uses that to learn the game.",
                                "markdown"
                            ],
                            [
                                "class Mario(Mario):  # subclassing for continuity\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.memory = deque(maxlen=100000)\n        self.batch_size = 32\n\n    def cache(self, state, next_state, action, reward, done):\n        \"\"\"\n        Store the experience to self.memory (replay buffer)\n\n        Inputs:\n        state (LazyFrame),\n        next_state (LazyFrame),\n        action (int),\n        reward (float),\n        done(bool))\n        \"\"\"\n        def first_if_tuple(x):\n            return x[0] if isinstance(x, tuple) else x\n        state = first_if_tuple(state).__array__()\n        next_state = first_if_tuple(next_state).__array__()\n\n        state = (state, device=self.device)\n        next_state = (next_state, device=self.device)\n        action = ([action], device=self.device)\n        reward = ([reward], device=self.device)\n        done = ([done], device=self.device)\n\n        self.memory.append((state, next_state, action, reward, done,))\n\n    def recall(self):\n        \"\"\"\n        Retrieve a batch of experiences from memory\n        \"\"\"\n        batch = random.sample(self.memory, self.batch_size)\n        state, next_state, action, reward, done = map(, zip(*batch))\n        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Learn": [
                            [
                                "Mario uses the \nunder the hood. DDQN uses two ConvNets - \\(Q_{online}\\) and\n\\(Q_{target}\\) - that independently approximate the optimal\naction-value function.",
                                "markdown"
                            ],
                            [
                                "In our implementation, we share feature generator features across\n\\(Q_{online}\\) and \\(Q_{target}\\), but maintain separate FC\nclassifiers for each. \\(\\theta_{target}\\) (the parameters of\n\\(Q_{target}\\)) is frozen to prevent updation by backprop. Instead,\nit is periodically synced with \\(\\theta_{online}\\) (more on this\nlater).",
                                "markdown"
                            ],
                            {
                                "Neural Network": [
                                    [
                                        "class MarioNet():\n    \"\"\"mini cnn structure\n  input -&gt; (conv2d + relu) x 3 -&gt; flatten -&gt; (dense + relu) x 2 -&gt; output\n  \"\"\"\n\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        c, h, w = input_dim\n\n        if h != 84:\n            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n        if w != 84:\n            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n\n        self.online = (\n            (in_channels=c, out_channels=32, kernel_size=8, stride=4),\n            (),\n            (in_channels=32, out_channels=64, kernel_size=4, stride=2),\n            (),\n            (in_channels=64, out_channels=64, kernel_size=3, stride=1),\n            (),\n            (),\n            (3136, 512),\n            (),\n            (512, output_dim),\n        )\n\n        self.target = copy.deepcopy(self.online)\n\n        # Q_target parameters are frozen.\n        for p in self.target.parameters():\n            p.requires_grad = False\n\n    def forward(self, input, model):\n        if model == \"online\":\n            return self.online(input)\n        elif model == \"target\":\n            return self.target(input)",
                                        "code"
                                    ]
                                ]
                            },
                            {
                                "TD Estimate &amp; TD Target": [
                                    [
                                        "Two values are involved in learning:",
                                        "markdown"
                                    ],
                                    [
                                        "<strong>TD Estimate</strong> - the predicted optimal \\(Q^*\\) for a given state\n\\(s\\)\n\n\\[{TD}_e = Q_{online}^*(s,a)\\]",
                                        "markdown"
                                    ],
                                    [
                                        "<strong>TD Target</strong> - aggregation of current reward and the estimated\n\\(Q^*\\) in the next state \\(s'\\)\n\n\\[a' = argmax_{a} Q_{online}(s', a)\\]\n\n\\[{TD}_t = r + \\gamma Q_{target}^*(s',a')\\]",
                                        "markdown"
                                    ],
                                    [
                                        "Because we don\u2019t know what next action \\(a'\\) will be, we use the\naction \\(a'\\) maximizes \\(Q_{online}\\) in the next state\n\\(s'\\).",
                                        "markdown"
                                    ],
                                    [
                                        "Notice we use the\n\ndecorator on td_target() to disable gradient calculations here\n(because we don\u2019t need to backpropagate on \\(\\theta_{target}\\)).",
                                        "markdown"
                                    ],
                                    [
                                        "class Mario(Mario):\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.gamma = 0.9\n\n    def td_estimate(self, state, action):\n        current_Q = self.net(state, model=\"online\")[\n            np.arange(0, self.batch_size), action\n        ]  # Q_online(s,a)\n        return current_Q\n\n    @torch.no_grad()\n    def td_target(self, reward, next_state, done):\n        next_state_Q = self.net(next_state, model=\"online\")\n        best_action = (next_state_Q, axis=1)\n        next_Q = self.net(next_state, model=\"target\")[\n            np.arange(0, self.batch_size), best_action\n        ]\n        return (reward + (1 - done.float()) * self.gamma * next_Q).float()",
                                        "code"
                                    ]
                                ]
                            },
                            {
                                "Updating the model": [
                                    [
                                        "As Mario samples inputs from his replay buffer, we compute \\(TD_t\\)\nand \\(TD_e\\) and backpropagate this loss down \\(Q_{online}\\) to\nupdate its parameters \\(\\theta_{online}\\) (\\(\\alpha\\) is the\nlearning rate lr passed to the optimizer)\n\n\\[\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)\\]",
                                        "markdown"
                                    ],
                                    [
                                        "\\(\\theta_{target}\\) does not update through backpropagation.\nInstead, we periodically copy \\(\\theta_{online}\\) to\n\\(\\theta_{target}\\)\n\n\\[\\theta_{target} \\leftarrow \\theta_{online}\\]",
                                        "markdown"
                                    ],
                                    [
                                        "class Mario(Mario):\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.optimizer = (self.net.parameters(), lr=0.00025)\n        self.loss_fn = ()\n\n    def update_Q_online(self, td_estimate, td_target):\n        loss = self.loss_fn(td_estimate, td_target)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    def sync_Q_target(self):\n        self.net.target.load_state_dict(self.net.online.state_dict())",
                                        "code"
                                    ]
                                ]
                            },
                            {
                                "Save checkpoint": [
                                    [
                                        "class Mario(Mario):\n    def save(self):\n        save_path = (\n            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n        )\n        (\n            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n            save_path,\n        )\n        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")",
                                        "code"
                                    ]
                                ]
                            },
                            {
                                "Putting it all together": [
                                    [
                                        "class Mario(Mario):\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.burnin = 1e4  # min. experiences before training\n        self.learn_every = 3  # no. of experiences between updates to Q_online\n        self.sync_every = 1e4  # no. of experiences between Q_target &amp; Q_online sync\n\n    def learn(self):\n        if self.curr_step % self.sync_every == 0:\n            self.sync_Q_target()\n\n        if self.curr_step % self.save_every == 0:\n            self.save()\n\n        if self.curr_step &lt; self.burnin:\n            return None, None\n\n        if self.curr_step % self.learn_every != 0:\n            return None, None\n\n        # Sample from memory\n        state, next_state, action, reward, done = self.recall()\n\n        # Get TD Estimate\n        td_est = self.td_estimate(state, action)\n\n        # Get TD Target\n        td_tgt = self.td_target(reward, next_state, done)\n\n        # Backpropagate loss through Q_online\n        loss = self.update_Q_online(td_est, td_tgt)\n\n        return (td_est.mean().item(), loss)",
                                        "code"
                                    ]
                                ]
                            }
                        ]
                    },
                    {
                        "Logging": [
                            [
                                "import numpy as np\nimport time, datetime\nimport matplotlib.pyplot as plt\n\n\nclass MetricLogger:\n    def __init__(self, save_dir):\n        self.save_log = save_dir / \"log\"\n        with open(self.save_log, \"w\") as f:\n            f.write(\n                f\"{'Episode':&gt;8}{'Step':&gt;8}{'Epsilon':&gt;10}{'MeanReward':&gt;15}\"\n                f\"{'MeanLength':&gt;15}{'MeanLoss':&gt;15}{'MeanQValue':&gt;15}\"\n                f\"{'TimeDelta':&gt;15}{'Time':&gt;20}\\n\"\n            )\n        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n\n        # History metrics\n        self.ep_rewards = []\n        self.ep_lengths = []\n        self.ep_avg_losses = []\n        self.ep_avg_qs = []\n\n        # Moving averages, added for every call to record()\n        self.moving_avg_ep_rewards = []\n        self.moving_avg_ep_lengths = []\n        self.moving_avg_ep_avg_losses = []\n        self.moving_avg_ep_avg_qs = []\n\n        # Current episode metric\n        self.init_episode()\n\n        # Timing\n        self.record_time = time.time()\n\n    def log_step(self, reward, loss, q):\n        self.curr_ep_reward += reward\n        self.curr_ep_length += 1\n        if loss:\n            self.curr_ep_loss += loss\n            self.curr_ep_q += q\n            self.curr_ep_loss_length += 1\n\n    def log_episode(self):\n        \"Mark end of episode\"\n        self.ep_rewards.append(self.curr_ep_reward)\n        self.ep_lengths.append(self.curr_ep_length)\n        if self.curr_ep_loss_length == 0:\n            ep_avg_loss = 0\n            ep_avg_q = 0\n        else:\n            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n        self.ep_avg_losses.append(ep_avg_loss)\n        self.ep_avg_qs.append(ep_avg_q)\n\n        self.init_episode()\n\n    def init_episode(self):\n        self.curr_ep_reward = 0.0\n        self.curr_ep_length = 0\n        self.curr_ep_loss = 0.0\n        self.curr_ep_q = 0.0\n        self.curr_ep_loss_length = 0\n\n    def record(self, episode, epsilon, step):\n        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n        self.moving_avg_ep_rewards.append(mean_ep_reward)\n        self.moving_avg_ep_lengths.append(mean_ep_length)\n        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n\n        last_record_time = self.record_time\n        self.record_time = time.time()\n        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n\n        print(\n            f\"Episode {episode} - \"\n            f\"Step {step} - \"\n            f\"Epsilon {epsilon} - \"\n            f\"Mean Reward {mean_ep_reward} - \"\n            f\"Mean Length {mean_ep_length} - \"\n            f\"Mean Loss {mean_ep_loss} - \"\n            f\"Mean Q Value {mean_ep_q} - \"\n            f\"Time Delta {time_since_last_record} - \"\n            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n        )\n\n        with open(self.save_log, \"a\") as f:\n            f.write(\n                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n                f\"{time_since_last_record:15.3f}\"\n                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):&gt;20}\\n\"\n            )\n\n        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n            plt.savefig(getattr(self, f\"{metric}_plot\"))\n            plt.clf()",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Let\u2019s play!": [
                    [
                        "In this example we run the training loop for 10 episodes, but for Mario to truly learn the ways of\nhis world, we suggest running the loop for at least 40,000 episodes!",
                        "markdown"
                    ],
                    [
                        "use_cuda = ()\nprint(f\"Using CUDA: {use_cuda}\")\nprint()\n\nsave_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\nsave_dir.mkdir(parents=True)\n\nmario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n\nlogger = MetricLogger(save_dir)\n\nepisodes = 10\nfor e in range(episodes):\n\n    state = env.reset()\n\n    # Play the game!\n    while True:\n\n        # Run agent on the state\n        action = mario.act(state)\n\n        # Agent performs action\n        next_state, reward, done, trunc, info = env.step(action)\n\n        # Remember\n        mario.cache(state, next_state, action, reward, done)\n\n        # Learn\n        q, loss = mario.learn()\n\n        # Logging\n        logger.log_step(reward, loss, q)\n\n        # Update state\n        state = next_state\n\n        # Check if end of game\n        if done or info[\"flag_get\"]:\n            break\n\n    logger.log_episode()\n\n    if e % 20 == 0:\n        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)\n\n\n<img alt=\"mario rl tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_mario_rl_tutorial_001.png\" srcset=\"../_images/sphx_glr_mario_rl_tutorial_001.png\"/>",
                        "code"
                    ],
                    [
                        "Using CUDA: True\n\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning:\n\nThe default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n\nEpisode 0 - Step 40 - Epsilon 0.9999900000487484 - Mean Reward 231.0 - Mean Length 40.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.401 - Time 2023-03-17T21:26:25\n/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning:\n\nThe default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).",
                        "code"
                    ]
                ]
            },
            {
                "Conclusion": [
                    [
                        "In this tutorial, we saw how we can use PyTorch to train a game-playing AI. You can use the same methods\nto train an AI to play any of the games at the . Hope you enjoyed this tutorial, feel free to reach us at\n!",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  31.286 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ]
    },
    "Deploying PyTorch Models in Production": {
        "Deploying PyTorch in Python via a REST API with Flask": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "In this tutorial, we will deploy a PyTorch model using Flask and expose a\nREST API for model inference. In particular, we will deploy a pretrained\nDenseNet 121 model which detects the image.",
                "markdown"
            ],
            [
                "Tip",
                "markdown"
            ],
            [
                "All the code used here is released under MIT license and is available on .",
                "markdown"
            ],
            [
                "This represents the first in a series of tutorials on deploying PyTorch models\nin production. Using Flask in this way is by far the easiest way to start\nserving your PyTorch models, but it will not work for a use case\nwith high performance requirements. For that:\n<blockquote>",
                "markdown"
            ],
            [
                "If you\u2019re already familiar with TorchScript, you can jump straight into our\n tutorial.",
                "markdown"
            ],
            [
                "If you first need a refresher on TorchScript, check out our\n tutorial.\n\n</blockquote>",
                "markdown"
            ],
            {
                "API Definition": [
                    [
                        "We will first define our API endpoints, the request and response types. Our\nAPI endpoint will be at /predict which takes HTTP POST requests with a\nfile parameter which contains the image. The response will be of JSON\nresponse containing the prediction:",
                        "markdown"
                    ],
                    [
                        "{\"class_id\": \"n02124075\", \"class_name\": \"Egyptian_cat\"}",
                        "code"
                    ]
                ]
            },
            {
                "Dependencies": [
                    [
                        "Install the required dependencies by running the following command:",
                        "markdown"
                    ],
                    [
                        "$ pip install Flask==2.0.1 torchvision==0.10.0",
                        "code"
                    ]
                ]
            },
            {
                "Simple Web Server": [
                    [
                        "Following is a simple webserver, taken from Flask\u2019s documentation",
                        "markdown"
                    ],
                    [
                        "from flask import Flask\napp = Flask(__name__)\n\n\n@app.route('/')\ndef hello():\n    return 'Hello World!'",
                        "code"
                    ],
                    [
                        "Save the above snippet in a file called app.py and you can now run a\nFlask development server by typing:",
                        "markdown"
                    ],
                    [
                        "$ FLASK_ENV=development FLASK_APP=app.py flask run",
                        "code"
                    ],
                    [
                        "When you visit http://localhost:5000/ in your web browser, you will be\ngreeted with Hello World! text",
                        "markdown"
                    ],
                    [
                        "We will make slight changes to the above snippet, so that it suits our API\ndefinition. First, we will rename the method to predict. We will update\nthe endpoint path to /predict. Since the image files will be sent via\nHTTP POST requests, we will update it so that it also accepts only POST\nrequests:",
                        "markdown"
                    ],
                    [
                        "@app.route('/predict', methods=['POST'])\ndef predict():\n    return 'Hello World!'",
                        "code"
                    ],
                    [
                        "We will also change the response type, so that it returns a JSON response\ncontaining ImageNet class id and name. The updated app.py file will\nbe now:",
                        "markdown"
                    ],
                    [
                        "from flask import Flask, jsonify\napp = Flask(__name__)\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    return jsonify({'class_id': 'IMAGE_NET_XXX', 'class_name': 'Cat'})",
                        "code"
                    ]
                ]
            },
            {
                "Inference": [
                    [
                        "In the next sections we will focus on writing the inference code. This will\ninvolve two parts, one where we prepare the image so that it can be fed\nto DenseNet and next, we will write the code to get the actual prediction\nfrom the model.",
                        "markdown"
                    ],
                    {
                        "Preparing the image": [
                            [
                                "DenseNet model requires the image to be of 3 channel RGB image of size\n224 x 224. We will also normalise the image tensor with the required mean\nand standard deviation values. You can read more about it\n.",
                                "markdown"
                            ],
                            [
                                "We will use transforms from torchvision library and build a\ntransform pipeline, which transforms our images as required. You\ncan read more about transforms .",
                                "markdown"
                            ],
                            [
                                "import io\n\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\ndef transform_image(image_bytes):\n    my_transforms = ([(255),\n                                        (224),\n                                        (),\n                                        (\n                                            [0.485, 0.456, 0.406],\n                                            [0.229, 0.224, 0.225])])\n    image = Image.open(io.BytesIO(image_bytes))\n    return my_transforms(image).unsqueeze(0)",
                                "code"
                            ],
                            [
                                "The above method takes image data in bytes, applies the series of transforms\nand returns a tensor. To test the above method, read an image file in\nbytes mode (first replacing <cite>../_static/img/sample_file.jpeg</cite> with the actual\npath to the file on your computer) and see if you get a tensor back:",
                                "markdown"
                            ],
                            [
                                "with open(\"../_static/img/sample_file.jpeg\", 'rb') as f:\n    image_bytes = f.read()\n    tensor = transform_image(image_bytes=image_bytes)\n    print(tensor)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Prediction": [
                            [
                                "Now will use a pretrained DenseNet 121 model to predict the image class. We\nwill use one from torchvision library, load the model and get an\ninference. While we\u2019ll be using a pretrained model in this example, you can\nuse this same approach for your own models. See more about loading your\nmodels in this .",
                                "markdown"
                            ],
                            [
                                "from torchvision import models\n\n# Make sure to pass `pretrained` as `True` to use the pretrained weights:\nmodel = (pretrained=True)\n# Since we are using our model only for inference, switch to `eval` mode:\nmodel.eval()\n\n\ndef get_prediction(image_bytes):\n    tensor = transform_image(image_bytes=image_bytes)\n    outputs = model.forward(tensor)\n    _, y_hat = outputs.max(1)\n    return y_hat",
                                "code"
                            ],
                            [
                                "The tensor y_hat will contain the index of the predicted class id.\nHowever, we need a human readable class name. For that we need a class id\nto name mapping. Download\n\nas imagenet_class_index.json and remember where you saved it (or, if you\nare following the exact steps in this tutorial, save it in\n<cite>tutorials/_static</cite>). This file contains the mapping of ImageNet class id to\nImageNet class name. We will load this JSON file and get the class name of\nthe predicted index.",
                                "markdown"
                            ],
                            [
                                "import json\n\nimagenet_class_index = json.load(open('../_static/imagenet_class_index.json'))\n\ndef get_prediction(image_bytes):\n    tensor = transform_image(image_bytes=image_bytes)\n    outputs = model.forward(tensor)\n    _, y_hat = outputs.max(1)\n    predicted_idx = str(y_hat.item())\n    return imagenet_class_index[predicted_idx]",
                                "code"
                            ],
                            [
                                "Before using imagenet_class_index dictionary, first we will convert\ntensor value to a string value, since the keys in the\nimagenet_class_index dictionary are strings.\nWe will test our above method:",
                                "markdown"
                            ],
                            [
                                "with open(\"../_static/img/sample_file.jpeg\", 'rb') as f:\n    image_bytes = f.read()\n    print(get_prediction(image_bytes=image_bytes))",
                                "code"
                            ],
                            [
                                "You should get a response like this:",
                                "markdown"
                            ],
                            [
                                "['n02124075', 'Egyptian_cat']",
                                "code"
                            ],
                            [
                                "The first item in array is ImageNet class id and second item is the human\nreadable name.",
                                "markdown"
                            ],
                            [
                                "Note",
                                "markdown"
                            ],
                            [
                                "Did you notice that model variable is not part of get_prediction\nmethod? Or why is model a global variable? Loading a model can be an\nexpensive operation in terms of memory and compute. If we loaded the model in the\nget_prediction method, then it would get unnecessarily loaded every\ntime the method is called. Since, we are building a web server, there\ncould be thousands of requests per second, we should not waste time\nredundantly loading the model for every inference. So, we keep the model\nloaded in memory just once. In\nproduction systems, it\u2019s necessary to be efficient about your use of\ncompute to be able to serve requests at scale, so you should generally\nload your model before serving requests.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Integrating the model in our API Server": [
                    [
                        "In this final part we will add our model to our Flask API server. Since\nour API server is supposed to take an image file, we will update our predict\nmethod to read files from the requests:",
                        "markdown"
                    ],
                    [
                        "from flask import request\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if request.method == 'POST':\n        # we will get the file from the request\n        file = request.files['file']\n        # convert that to bytes\n        img_bytes = file.read()\n        class_id, class_name = get_prediction(image_bytes=img_bytes)\n        return jsonify({'class_id': class_id, 'class_name': class_name})",
                        "code"
                    ],
                    [
                        "The app.py file is now complete. Following is the full version; replace\nthe paths with the paths where you saved your files and it should run:",
                        "markdown"
                    ],
                    [
                        "import io\nimport json\n\nfrom torchvision import models\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom flask import Flask, jsonify, request\n\n\napp = Flask(__name__)\nimagenet_class_index = json.load(open('&lt;PATH/TO/.json/FILE&gt;/imagenet_class_index.json'))\nmodel = (pretrained=True)\nmodel.eval()\n\n\ndef transform_image(image_bytes):\n    my_transforms = ([(255),\n                                        (224),\n                                        (),\n                                        (\n                                            [0.485, 0.456, 0.406],\n                                            [0.229, 0.224, 0.225])])\n    image = Image.open(io.BytesIO(image_bytes))\n    return my_transforms(image).unsqueeze(0)\n\n\ndef get_prediction(image_bytes):\n    tensor = transform_image(image_bytes=image_bytes)\n    outputs = model.forward(tensor)\n    _, y_hat = outputs.max(1)\n    predicted_idx = str(y_hat.item())\n    return imagenet_class_index[predicted_idx]\n\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if request.method == 'POST':\n        file = request.files['file']\n        img_bytes = file.read()\n        class_id, class_name = get_prediction(image_bytes=img_bytes)\n        return jsonify({'class_id': class_id, 'class_name': class_name})\n\n\nif __name__ == '__main__':\n    app.run()",
                        "code"
                    ],
                    [
                        "Let\u2019s test our web server! Run:",
                        "markdown"
                    ],
                    [
                        "$ FLASK_ENV=development FLASK_APP=app.py flask run",
                        "code"
                    ],
                    [
                        "We can use the\n\nlibrary to send a POST request to our app:",
                        "markdown"
                    ],
                    [
                        "import requests\n\nresp = requests.post(\"http://localhost:5000/predict\",\n                     files={\"file\": open('&lt;PATH/TO/.jpg/FILE&gt;/cat.jpg','rb')})",
                        "code"
                    ],
                    [
                        "Printing <cite>resp.json()</cite> will now show the following:",
                        "markdown"
                    ],
                    [
                        "{\"class_id\": \"n02124075\", \"class_name\": \"Egyptian_cat\"}",
                        "code"
                    ]
                ]
            },
            {
                "Next steps": [
                    [
                        "The server we wrote is quite trivial and may not do everything\nyou need for your production application. So, here are some things you\ncan do to make it better:",
                        "markdown"
                    ],
                    [
                        "The endpoint /predict assumes that always there will be a image file\nin the request. This may not hold true for all requests. Our user may\nsend image with a different parameter or send no images at all.",
                        "markdown"
                    ],
                    [
                        "The user may send non-image type files too. Since we are not handling\nerrors, this will break our server. Adding an explicit error handing\npath that will throw an exception would allow us to better handle\nthe bad inputs",
                        "markdown"
                    ],
                    [
                        "Even though the model can recognize a large number of classes of images,\nit may not be able to recognize all images. Enhance the implementation\nto handle cases when the model does not recognize anything in the image.",
                        "markdown"
                    ],
                    [
                        "We run the Flask server in the development mode, which is not suitable for\ndeploying in production. You can check out \nfor deploying a Flask server in production.",
                        "markdown"
                    ],
                    [
                        "You can also add a UI by creating a page with a form which takes the image and\ndisplays the prediction. Check out the \nof a similar project and its .",
                        "markdown"
                    ],
                    [
                        "In this tutorial, we only showed how to build a service that could return predictions for\na single image at a time. We could modify our service to be able to return predictions for\nmultiple images at once. In addition, the \nlibrary automatically queues requests to your service and samples them into mini-batches\nthat can be fed into your model. You can check out .",
                        "markdown"
                    ],
                    [
                        "Finally, we encourage you to check out our other tutorials on deploying PyTorch models\nlinked-to at the top of the page.",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Introduction to TorchScript": [
            [
                "<em>James Reed (jamesreed@fb.com), Michael Suo (suo@fb.com)</em>, rev2",
                "markdown"
            ],
            [
                "This tutorial is an introduction to TorchScript, an intermediate\nrepresentation of a PyTorch model (subclass of nn.Module) that\ncan then be run in a high-performance environment such as C++.",
                "markdown"
            ],
            [
                "In this tutorial we will cover:",
                "markdown"
            ],
            [
                "The basics of model authoring in PyTorch, including:",
                "markdown"
            ],
            [
                "Modules",
                "markdown"
            ],
            [
                "Defining forward functions",
                "markdown"
            ],
            [
                "Composing modules into a hierarchy of modules",
                "markdown"
            ],
            [
                "Specific methods for converting PyTorch modules to TorchScript, our\nhigh-performance deployment runtime",
                "markdown"
            ],
            [
                "Tracing an existing module",
                "markdown"
            ],
            [
                "Using scripting to directly compile a module",
                "markdown"
            ],
            [
                "How to compose both approaches",
                "markdown"
            ],
            [
                "Saving and loading TorchScript modules",
                "markdown"
            ],
            [
                "We hope that after you complete this tutorial, you will proceed to go through\n\nwhich will walk you through an example of actually calling a TorchScript\nmodel from C++.",
                "markdown"
            ],
            [
                "import torch  # This is all you need to use both PyTorch and TorchScript!\nprint(torch.__version__)",
                "code"
            ],
            [
                "2.0.0+cu117",
                "code"
            ],
            {
                "Basics of PyTorch Model Authoring": [
                    [
                        "Let\u2019s start out by defining a simple Module. A Module is the\nbasic unit of composition in PyTorch. It contains:",
                        "markdown"
                    ],
                    [
                        "A constructor, which prepares the module for invocation",
                        "markdown"
                    ],
                    [
                        "A set of Parameters and sub-Modules. These are initialized\nby the constructor and can be used by the module during invocation.",
                        "markdown"
                    ],
                    [
                        "A forward function. This is the code that is run when the module\nis invoked.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s examine a small example:",
                        "markdown"
                    ],
                    [
                        "class MyCell():\n    def __init__(self):\n        super(, self).__init__()\n\n    def forward(self, , ):\n        new_h = ( + )\n        return new_h, new_h\n\nmy_cell = ()\n = (3, 4)\n = (3, 4)\nprint(my_cell(, ))",
                        "code"
                    ],
                    [
                        "(tensor([[0.8423, 0.5403, 0.5722, 0.4816],\n        [0.8413, 0.8463, 0.8283, 0.9122],\n        [0.7446, 0.6997, 0.8749, 0.7332]]), tensor([[0.8423, 0.5403, 0.5722, 0.4816],\n        [0.8413, 0.8463, 0.8283, 0.9122],\n        [0.7446, 0.6997, 0.8749, 0.7332]]))",
                        "code"
                    ],
                    [
                        "So we\u2019ve:",
                        "markdown"
                    ],
                    [
                        "Created a class that subclasses torch.nn.Module.",
                        "markdown"
                    ],
                    [
                        "Defined a constructor. The constructor doesn\u2019t do much, just calls\nthe constructor for super.",
                        "markdown"
                    ],
                    [
                        "Defined a forward function, which takes two inputs and returns\ntwo outputs. The actual contents of the forward function are not\nreally important, but it\u2019s sort of a fake \u2013that\nis\u2013it\u2019s a function that is applied on a loop.",
                        "markdown"
                    ],
                    [
                        "We instantiated the module, and made x and h, which are just 3x4\nmatrices of random values. Then we invoked the cell with\nmy_cell(x, h). This in turn calls our forward function.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s do something a little more interesting:",
                        "markdown"
                    ],
                    [
                        "class MyCell():\n    def __init__(self):\n        super(, self).__init__()\n        self.linear = (4, 4)\n\n    def forward(self, , ):\n        new_h = (self.linear() + )\n        return new_h, new_h\n\nmy_cell = ()\nprint(my_cell)\nprint(my_cell(, ))",
                        "code"
                    ],
                    [
                        "MyCell(\n  (linear): Linear(in_features=4, out_features=4, bias=True)\n)\n(tensor([[0.2829, 0.5454, 0.5981, 0.7620],\n        [0.7984, 0.7317, 0.6353, 0.9266],\n        [0.7826, 0.2063, 0.4779, 0.8727]], grad_fn=&lt;TanhBackward0&gt;), tensor([[0.2829, 0.5454, 0.5981, 0.7620],\n        [0.7984, 0.7317, 0.6353, 0.9266],\n        [0.7826, 0.2063, 0.4779, 0.8727]], grad_fn=&lt;TanhBackward0&gt;))",
                        "code"
                    ],
                    [
                        "We\u2019ve redefined our module MyCell, but this time we\u2019ve added a\nself.linear attribute, and we invoke self.linear in the forward\nfunction.",
                        "markdown"
                    ],
                    [
                        "What exactly is happening here? torch.nn.Linear is a Module from\nthe PyTorch standard library. Just like MyCell, it can be invoked\nusing the call syntax. We are building a hierarchy of Modules.",
                        "markdown"
                    ],
                    [
                        "print on a Module will give a visual representation of the\nModule\u2019s subclass hierarchy. In our example, we can see our\nLinear subclass and its parameters.",
                        "markdown"
                    ],
                    [
                        "By composing Modules in this way, we can succinctly and readably\nauthor models with reusable components.",
                        "markdown"
                    ],
                    [
                        "You may have noticed grad_fn on the outputs. This is a detail of\nPyTorch\u2019s method of automatic differentiation, called\n.\nIn short, this system allows us to compute derivatives through\npotentially complex programs. The design allows for a massive amount of\nflexibility in model authoring.",
                        "markdown"
                    ],
                    [
                        "Now let\u2019s examine said flexibility:",
                        "markdown"
                    ],
                    [
                        "class MyDecisionGate():\n    def forward(self, ):\n        if .sum() &gt; 0:\n            return \n        else:\n            return -\n\nclass MyCell():\n    def __init__(self):\n        super(, self).__init__()\n        self.dg = ()\n        self.linear = (4, 4)\n\n    def forward(self, , ):\n        new_h = (self.dg(self.linear()) + )\n        return new_h, new_h\n\nmy_cell = ()\nprint(my_cell)\nprint(my_cell(, ))",
                        "code"
                    ],
                    [
                        "MyCell(\n  (dg): MyDecisionGate()\n  (linear): Linear(in_features=4, out_features=4, bias=True)\n)\n(tensor([[ 0.8695,  0.4516,  0.7416, -0.0722],\n        [ 0.9652,  0.8120,  0.9017,  0.3150],\n        [ 0.9577,  0.5761,  0.7777, -0.0841]], grad_fn=&lt;TanhBackward0&gt;), tensor([[ 0.8695,  0.4516,  0.7416, -0.0722],\n        [ 0.9652,  0.8120,  0.9017,  0.3150],\n        [ 0.9577,  0.5761,  0.7777, -0.0841]], grad_fn=&lt;TanhBackward0&gt;))",
                        "code"
                    ],
                    [
                        "We\u2019ve once again redefined our MyCell class, but here we\u2019ve defined\nMyDecisionGate. This module utilizes <strong>control flow</strong>. Control flow\nconsists of things like loops and if-statements.",
                        "markdown"
                    ],
                    [
                        "Many frameworks take the approach of computing symbolic derivatives\ngiven a full program representation. However, in PyTorch, we use a\ngradient tape. We record operations as they occur, and replay them\nbackwards in computing derivatives. In this way, the framework does not\nhave to explicitly define derivatives for all constructs in the\nlanguage.\n\n<img alt=\"How autograd works\" src=\"https://github.com/pytorch/pytorch/raw/master/docs/source/_static/img/dynamic_graph.gif\"/>",
                        "markdown"
                    ],
                    [
                        "How autograd works",
                        "markdown"
                    ]
                ]
            },
            {
                "Basics of TorchScript": [
                    [
                        "Now let\u2019s take our running example and see how we can apply TorchScript.",
                        "markdown"
                    ],
                    [
                        "In short, TorchScript provides tools to capture the definition of your\nmodel, even in light of the flexible and dynamic nature of PyTorch.\nLet\u2019s begin by examining what we call <strong>tracing</strong>.",
                        "markdown"
                    ],
                    {
                        "Tracing Modules": [
                            [
                                "class MyCell():\n    def __init__(self):\n        super(, self).__init__()\n        self.linear = (4, 4)\n\n    def forward(self, , ):\n        new_h = (self.linear() + )\n        return new_h, new_h\n\nmy_cell = ()\n,  = (3, 4), (3, 4)\ntraced_cell = (my_cell, (, ))\nprint(traced_cell)\ntraced_cell(, )",
                                "code"
                            ],
                            [
                                "MyCell(\n  original_name=MyCell\n  (linear): Linear(original_name=Linear)\n)\n\n(tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]], grad_fn=&lt;TanhBackward0&gt;), tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]], grad_fn=&lt;TanhBackward0&gt;))",
                                "code"
                            ],
                            [
                                "We\u2019ve rewinded a bit and taken the second version of our MyCell\nclass. As before, we\u2019ve instantiated it, but this time, we\u2019ve called\ntorch.jit.trace, passed in the Module, and passed in <em>example\ninputs</em> the network might see.",
                                "markdown"
                            ],
                            [
                                "What exactly has this done? It has invoked the Module, recorded the\noperations that occured when the Module was run, and created an\ninstance of torch.jit.ScriptModule (of which TracedModule is an\ninstance)",
                                "markdown"
                            ],
                            [
                                "TorchScript records its definitions in an Intermediate Representation\n(or IR), commonly referred to in Deep learning as a <em>graph</em>. We can\nexamine the graph with the .graph property:",
                                "markdown"
                            ],
                            [
                                "print()",
                                "code"
                            ],
                            [
                                "graph(%self.1 : __torch__.MyCell,\n      %x : Float(3, 4, strides=[4, 1], requires_grad=0, device=cpu),\n      %h : Float(3, 4, strides=[4, 1], requires_grad=0, device=cpu)):\n  %linear : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear\"](%self.1)\n  %20 : Tensor = prim::CallMethod[name=\"forward\"](%linear, %x)\n  %11 : int = prim::Constant[value=1]() # /var/lib/jenkins/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:188:0\n  %12 : Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu) = aten::add(%20, %h, %11) # /var/lib/jenkins/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:188:0\n  %13 : Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu) = aten::tanh(%12) # /var/lib/jenkins/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:188:0\n  %14 : (Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu), Float(3, 4, strides=[4, 1], requires_grad=1, device=cpu)) = prim::TupleConstruct(%13, %13)\n  return (%14)",
                                "code"
                            ],
                            [
                                "However, this is a very low-level representation and most of the\ninformation contained in the graph is not useful for end users. Instead,\nwe can use the .code property to give a Python-syntax interpretation\nof the code:",
                                "markdown"
                            ],
                            [
                                "print()",
                                "code"
                            ],
                            [
                                "def forward(self,\n    x: Tensor,\n    h: Tensor) -&gt; Tuple[Tensor, Tensor]:\n  linear = self.linear\n  _0 = torch.tanh(torch.add((linear).forward(x, ), h))\n  return (_0, _0)",
                                "code"
                            ],
                            [
                                "So <strong>why</strong> did we do all this? There are several reasons:",
                                "markdown"
                            ],
                            [
                                "TorchScript code can be invoked in its own interpreter, which is\nbasically a restricted Python interpreter. This interpreter does not\nacquire the Global Interpreter Lock, and so many requests can be\nprocessed on the same instance simultaneously.",
                                "markdown"
                            ],
                            [
                                "This format allows us to save the whole model to disk and load it\ninto another environment, such as in a server written in a language\nother than Python",
                                "markdown"
                            ],
                            [
                                "TorchScript gives us a representation in which we can do compiler\noptimizations on the code to provide more efficient execution",
                                "markdown"
                            ],
                            [
                                "TorchScript allows us to interface with many backend/device runtimes\nthat require a broader view of the program than individual operators.",
                                "markdown"
                            ],
                            [
                                "We can see that invoking traced_cell produces the same results as\nthe Python module:",
                                "markdown"
                            ],
                            [
                                "print(my_cell(, ))\nprint(traced_cell(, ))",
                                "code"
                            ],
                            [
                                "(tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]], grad_fn=&lt;TanhBackward0&gt;), tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]], grad_fn=&lt;TanhBackward0&gt;))\n(tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]],\n       grad_fn=&lt;DifferentiableGraphBackward&gt;), tensor([[ 0.8503,  0.7027, -0.1229,  0.7019],\n        [ 0.4636,  0.3581,  0.4940,  0.0266],\n        [ 0.7999,  0.8178, -0.1399,  0.5714]],\n       grad_fn=&lt;DifferentiableGraphBackward&gt;))",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Using Scripting to Convert Modules": [
                    [
                        "There\u2019s a reason we used version two of our module, and not the one with\nthe control-flow-laden submodule. Let\u2019s examine that now:",
                        "markdown"
                    ],
                    [
                        "class MyDecisionGate():\n    def forward(self, ):\n        if .sum() &gt; 0:\n            return \n        else:\n            return -\n\nclass MyCell():\n    def __init__(self, dg):\n        super(, self).__init__()\n        self.dg = dg\n        self.linear = (4, 4)\n\n    def forward(self, , ):\n        new_h = (self.dg(self.linear()) + )\n        return new_h, new_h\n\nmy_cell = (())\ntraced_cell = (my_cell, (, ))\n\nprint()\nprint()",
                        "code"
                    ],
                    [
                        "/var/lib/jenkins/workspace/beginner_source/Intro_to_TorchScript_tutorial.py:260: TracerWarning:\n\nConverting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n\ndef forward(self,\n    argument_1: Tensor) -&gt; NoneType:\n  return None\n\ndef forward(self,\n    x: Tensor,\n    h: Tensor) -&gt; Tuple[Tensor, Tensor]:\n  dg = self.dg\n  linear = self.linear\n  _0 = (linear).forward(x, )\n  _1 = (dg).forward(_0, )\n  _2 = torch.tanh(torch.add(_0, h))\n  return (_2, _2)",
                        "code"
                    ],
                    [
                        "Looking at the .code output, we can see that the if-else branch\nis nowhere to be found! Why? Tracing does exactly what we said it would:\nrun the code, record the operations <em>that happen</em> and construct a\nScriptModule that does exactly that. Unfortunately, things like control\nflow are erased.",
                        "markdown"
                    ],
                    [
                        "How can we faithfully represent this module in TorchScript? We provide a\n<strong>script compiler</strong>, which does direct analysis of your Python source\ncode to transform it into TorchScript. Let\u2019s convert MyDecisionGate\nusing the script compiler:",
                        "markdown"
                    ],
                    [
                        "scripted_gate = (())\n\nmy_cell = (scripted_gate)\nscripted_cell = (my_cell)\n\nprint()\nprint()",
                        "code"
                    ],
                    [
                        "def forward(self,\n    x: Tensor) -&gt; Tensor:\n  if bool(torch.gt(torch.sum(x), 0)):\n    _0 = x\n  else:\n    _0 = torch.neg(x)\n  return _0\n\ndef forward(self,\n    x: Tensor,\n    h: Tensor) -&gt; Tuple[Tensor, Tensor]:\n  dg = self.dg\n  linear = self.linear\n  _0 = torch.add((dg).forward((linear).forward(x, ), ), h)\n  new_h = torch.tanh(_0)\n  return (new_h, new_h)",
                        "code"
                    ],
                    [
                        "Hooray! We\u2019ve now faithfully captured the behavior of our program in\nTorchScript. Let\u2019s now try running the program:",
                        "markdown"
                    ],
                    [
                        "# New inputs\n,  = (3, 4), (3, 4)\ntraced_cell(, )",
                        "code"
                    ],
                    [
                        "(tensor([[0.8368, 0.6018, 0.0473, 0.7395],\n        [0.7879, 0.7106, 0.5274, 0.9518],\n        [0.7242, 0.6873, 0.5701, 0.7467]], grad_fn=&lt;TanhBackward0&gt;), tensor([[0.8368, 0.6018, 0.0473, 0.7395],\n        [0.7879, 0.7106, 0.5274, 0.9518],\n        [0.7242, 0.6873, 0.5701, 0.7467]], grad_fn=&lt;TanhBackward0&gt;))",
                        "code"
                    ],
                    {
                        "Mixing Scripting and Tracing": [
                            [
                                "Some situations call for using tracing rather than scripting (e.g.\u00a0a\nmodule has many architectural decisions that are made based on constant\nPython values that we would like to not appear in TorchScript). In this\ncase, scripting can be composed with tracing: torch.jit.script will\ninline the code for a traced module, and tracing will inline the code\nfor a scripted module.",
                                "markdown"
                            ],
                            [
                                "An example of the first case:",
                                "markdown"
                            ],
                            [
                                "class MyRNNLoop():\n    def __init__(self):\n        super(, self).__init__()\n        self.cell = ((scripted_gate), (, ))\n\n    def forward(self, xs):\n        , y = (3, 4), (3, 4)\n        for i in range(xs.size(0)):\n            y,  = self.cell(xs[i], )\n        return y, \n\nrnn_loop = (())\nprint()",
                                "code"
                            ],
                            [
                                "def forward(self,\n    xs: Tensor) -&gt; Tuple[Tensor, Tensor]:\n  h = torch.zeros([3, 4])\n  y = torch.zeros([3, 4])\n  y0 = y\n  h0 = h\n  for i in range(torch.size(xs, 0)):\n    cell = self.cell\n    _0 = (cell).forward(torch.select(xs, 0, i), h0, )\n    y1, h1, = _0\n    y0, h0 = y1, h1\n  return (y0, h0)",
                                "code"
                            ],
                            [
                                "And an example of the second case:",
                                "markdown"
                            ],
                            [
                                "class WrapRNN():\n    def __init__(self):\n        super(, self).__init__()\n        self.loop = (())\n\n    def forward(self, xs):\n        y,  = self.loop(xs)\n        return torch.relu(y)\n\ntraced = ((), ((10, 3, 4)))\nprint()",
                                "code"
                            ],
                            [
                                "def forward(self,\n    xs: Tensor) -&gt; Tensor:\n  loop = self.loop\n  _0, y, = (loop).forward(xs, )\n  return torch.relu(y)",
                                "code"
                            ],
                            [
                                "This way, scripting and tracing can be used when the situation calls for\neach of them and used together.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Saving and Loading models": [
                    [
                        "We provide APIs to save and load TorchScript modules to/from disk in an\narchive format. This format includes code, parameters, attributes, and\ndebug information, meaning that the archive is a freestanding\nrepresentation of the model that can be loaded in an entirely separate\nprocess. Let\u2019s save and load our wrapped RNN module:",
                        "markdown"
                    ],
                    [
                        "('wrapped_rnn.pt')\n\nloaded = ('wrapped_rnn.pt')\n\nprint(loaded)\nprint()",
                        "code"
                    ],
                    [
                        "RecursiveScriptModule(\n  original_name=WrapRNN\n  (loop): RecursiveScriptModule(\n    original_name=MyRNNLoop\n    (cell): RecursiveScriptModule(\n      original_name=MyCell\n      (dg): RecursiveScriptModule(original_name=MyDecisionGate)\n      (linear): RecursiveScriptModule(original_name=Linear)\n    )\n  )\n)\ndef forward(self,\n    xs: Tensor) -&gt; Tensor:\n  loop = self.loop\n  _0, y, = (loop).forward(xs, )\n  return torch.relu(y)",
                        "code"
                    ],
                    [
                        "As you can see, serialization preserves the module hierarchy and the\ncode we\u2019ve been examining throughout. The model can also be loaded, for\nexample,  for\npython-free execution.",
                        "markdown"
                    ],
                    {
                        "Further Reading": [
                            [
                                "We\u2019ve completed our tutorial! For a more involved demonstration, check\nout the NeurIPS demo for converting machine translation models using\nTorchScript:",
                                "markdown"
                            ],
                            [
                                "<strong>Total running time of the script:</strong> ( 0 minutes  1.120 seconds)",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ]
                        ]
                    }
                ]
            }
        ],
        "Loading a TorchScript Model in C++": [
            [
                "As its name suggests, the primary interface to PyTorch is the Python\nprogramming language. While Python is a suitable and preferred language for\nmany scenarios requiring dynamism and ease of iteration, there are equally many\nsituations where precisely these properties of Python are unfavorable. One\nenvironment in which the latter often applies is <em>production</em> \u2013 the land of\nlow latencies and strict deployment requirements. For production scenarios, C++\nis very often the language of choice, even if only to bind it into another\nlanguage like Java, Rust or Go. The following paragraphs will outline the path\nPyTorch provides to go from an existing Python model to a serialized\nrepresentation that can be <em>loaded</em> and <em>executed</em> purely from C++, with no\ndependency on Python.",
                "markdown"
            ],
            {
                "Step 1: Converting Your PyTorch Model to Torch Script": [
                    [
                        "A PyTorch model\u2019s journey from Python to C++ is enabled by , a representation of a PyTorch\nmodel that can be understood, compiled and serialized by the Torch Script\ncompiler. If you are starting out from an existing PyTorch model written in the\nvanilla \u201ceager\u201d API, you must first convert your model to Torch Script. In the\nmost common cases, discussed below, this requires only little effort. If you\nalready have a Torch Script module, you can skip to the next section of this\ntutorial.",
                        "markdown"
                    ],
                    [
                        "There exist two ways of converting a PyTorch model to Torch Script. The first\nis known as <em>tracing</em>, a mechanism in which the structure of the model is\ncaptured by evaluating it once using example inputs, and recording the flow of\nthose inputs through the model. This is suitable for models that make limited\nuse of control flow. The second approach is to add explicit annotations to your\nmodel that inform the Torch Script compiler that it may directly parse and\ncompile your model code, subject to the constraints imposed by the Torch Script\nlanguage.",
                        "markdown"
                    ],
                    [
                        "Tip",
                        "markdown"
                    ],
                    [
                        "You can find the complete documentation for both of these methods, as well as\nfurther guidance on which to use, in the official .",
                        "markdown"
                    ],
                    {
                        "Converting to Torch Script via Tracing": [
                            [
                                "To convert a PyTorch model to Torch Script via tracing, you must pass an\ninstance of your model along with an example input to the torch.jit.trace\nfunction. This will produce a torch.jit.ScriptModule object with the trace\nof your model evaluation embedded in the module\u2019s forward method:",
                                "markdown"
                            ],
                            [
                                "import torch\nimport torchvision\n\n# An instance of your model.\nmodel = torchvision.models.resnet18()\n\n# An example input you would normally provide to your model's forward() method.\nexample = torch.rand(1, 3, 224, 224)\n\n# Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\ntraced_script_module = torch.jit.trace(model, example)",
                                "code"
                            ],
                            [
                                "The traced ScriptModule can now be evaluated identically to a regular\nPyTorch module:",
                                "markdown"
                            ],
                            [
                                "In[1]: output = traced_script_module(torch.ones(1, 3, 224, 224))\nIn[2]: output[0, :5]\nOut[2]: tensor([-0.2698, -0.0381,  0.4023, -0.3010, -0.0448], grad_fn=&lt;SliceBackward&gt;)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Converting to Torch Script via Annotation": [
                            [
                                "Under certain circumstances, such as if your model employs particular forms of\ncontrol flow, you may want to write your model in Torch Script directly and\nannotate your model accordingly. For example, say you have the following\nvanilla Pytorch model:",
                                "markdown"
                            ],
                            [
                                "import torch\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, N, M):\n        super(MyModule, self).__init__()\n        self.weight = torch.nn.Parameter(torch.rand(N, M))\n\n    def forward(self, input):\n        if input.sum() &gt; 0:\n          output = self.weight.mv(input)\n        else:\n          output = self.weight + input\n        return output",
                                "code"
                            ],
                            [
                                "Because the forward method of this module uses control flow that is\ndependent on the input, it is not suitable for tracing. Instead, we can convert\nit to a ScriptModule.\nIn order to convert the module to the ScriptModule, one needs to\ncompile the module with torch.jit.script as follows:",
                                "markdown"
                            ],
                            [
                                "class MyModule(torch.nn.Module):\n    def __init__(self, N, M):\n        super(MyModule, self).__init__()\n        self.weight = torch.nn.Parameter(torch.rand(N, M))\n\n    def forward(self, input):\n        if input.sum() &gt; 0:\n          output = self.weight.mv(input)\n        else:\n          output = self.weight + input\n        return output\n\nmy_module = MyModule(10,20)\nsm = torch.jit.script(my_module)",
                                "code"
                            ],
                            [
                                "If you need to exclude some methods in your nn.Module\nbecause they use Python features that TorchScript doesn\u2019t support yet,\nyou could annotate those with @torch.jit.ignore",
                                "markdown"
                            ],
                            [
                                "sm is an instance of\nScriptModule that is ready for serialization.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Step 2: Serializing Your Script Module to a File": [
                    [
                        "Once you have a ScriptModule in your hands, either from tracing or\nannotating a PyTorch model, you are ready to serialize it to a file. Later on,\nyou\u2019ll be able to load the module from this file in C++ and execute it without\nany dependency on Python. Say we want to serialize the ResNet18 model shown\nearlier in the tracing example. To perform this serialization, simply call\n\non the module and pass it a filename:",
                        "markdown"
                    ],
                    [
                        "traced_script_module.save(\"traced_resnet_model.pt\")",
                        "code"
                    ],
                    [
                        "This will produce a traced_resnet_model.pt file in your working directory.\nIf you also would like to serialize sm, call sm.save(\"my_module_model.pt\")\nWe have now officially left the realm of Python and are ready to cross over to the sphere\nof C++.",
                        "markdown"
                    ]
                ]
            },
            {
                "Step 3: Loading Your Script Module in C++": [
                    [
                        "To load your serialized PyTorch model in C++, your application must depend on\nthe PyTorch C++ API \u2013 also known as <em>LibTorch</em>. The LibTorch distribution\nencompasses a collection of shared libraries, header files and CMake build\nconfiguration files. While CMake is not a requirement for depending on\nLibTorch, it is the recommended approach and will be well supported into the\nfuture. For this tutorial, we will be building a minimal C++ application using\nCMake and LibTorch that simply loads and executes a serialized PyTorch model.",
                        "markdown"
                    ],
                    {
                        "A Minimal C++ Application": [
                            [
                                "Let\u2019s begin by discussing the code to load a module. The following will already\ndo:",
                                "markdown"
                            ],
                            [
                                "#include &lt;torch/script.h&gt; // One-stop header.\n\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\nint main(int argc, const char* argv[]) {\n  if (argc != 2) {\n    std::cerr &lt;&lt; \"usage: example-app &lt;path-to-exported-script-module&gt;\\n\";\n    return -1;\n  }\n\n\n  torch::jit::script::Module module;\n  try {\n    // Deserialize the ScriptModule from a file using torch::jit::load().\n    module = torch::jit::load(argv[1]);\n  }\n  catch (const c10::Error&amp; e) {\n    std::cerr &lt;&lt; \"error loading the model\\n\";\n    return -1;\n  }\n\n  std::cout &lt;&lt; \"ok\\n\";\n}",
                                "code"
                            ],
                            [
                                "The &lt;torch/script.h&gt; header encompasses all relevant includes from the\nLibTorch library necessary to run the example. Our application accepts the file\npath to a serialized PyTorch ScriptModule as its only command line argument\nand then proceeds to deserialize the module using the torch::jit::load()\nfunction, which takes this file path as input. In return we receive a torch::jit::script::Module\nobject. We will examine how to execute it in a moment.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Depending on LibTorch and Building the Application": [
                            [
                                "Assume we stored the above code into a file called example-app.cpp. A\nminimal CMakeLists.txt to build it could look as simple as:",
                                "markdown"
                            ],
                            [
                                "cmake_minimum_required(VERSION 3.0 FATAL_ERROR)\nproject(custom_ops)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(example-app example-app.cpp)\ntarget_link_libraries(example-app \"${TORCH_LIBRARIES}\")\nset_property(TARGET example-app PROPERTY CXX_STANDARD 14)",
                                "code"
                            ],
                            [
                                "The last thing we need to build the example application is the LibTorch\ndistribution. You can always grab the latest stable release from the  on the PyTorch website. If you download and unzip\nthe latest archive, you should receive a folder with the following directory\nstructure:",
                                "markdown"
                            ],
                            [
                                "libtorch/\n  bin/\n  include/\n  lib/\n  share/",
                                "code"
                            ],
                            [
                                "The lib/ folder contains the shared libraries you must link against,",
                                "markdown"
                            ],
                            [
                                "The include/ folder contains header files your program will need to include,",
                                "markdown"
                            ],
                            [
                                "The share/ folder contains the necessary CMake configuration to enable the simple find_package(Torch) command above.",
                                "markdown"
                            ],
                            [
                                "Tip",
                                "markdown"
                            ],
                            [
                                "On Windows, debug and release builds are not ABI-compatible. If you plan to\nbuild your project in debug mode, please try the debug version of LibTorch.\nAlso, make sure you specify the correct configuration in the cmake --build .\nline below.",
                                "markdown"
                            ],
                            [
                                "The last step is building the application. For this, assume our example\ndirectory is laid out like this:",
                                "markdown"
                            ],
                            [
                                "example-app/\n  CMakeLists.txt\n  example-app.cpp",
                                "code"
                            ],
                            [
                                "We can now run the following commands to build the application from within the\nexample-app/ folder:",
                                "markdown"
                            ],
                            [
                                "mkdir build\ncd build\ncmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\ncmake --build . --config Release",
                                "code"
                            ],
                            [
                                "where /path/to/libtorch should be the full path to the unzipped LibTorch\ndistribution. If all goes well, it will look something like this:",
                                "markdown"
                            ],
                            [
                                "root@4b5a67132e81:/example-app# mkdir build\nroot@4b5a67132e81:/example-app# cd build\nroot@4b5a67132e81:/example-app/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /example-app/build\nroot@4b5a67132e81:/example-app/build# make\nScanning dependencies of target example-app\n[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o\n[100%] Linking CXX executable example-app\n[100%] Built target example-app",
                                "code"
                            ],
                            [
                                "If we supply the path to the traced ResNet18 model traced_resnet_model.pt  we created earlier\nto the resulting example-app binary, we should be rewarded with a friendly\n\u201cok\u201d. Please note, if try to run this example with my_module_model.pt you will get an error saying that\nyour input is of an incompatible shape. my_module_model.pt expects 1D instead of 4D.",
                                "markdown"
                            ],
                            [
                                "root@4b5a67132e81:/example-app/build# ./example-app &lt;path_to_model&gt;/traced_resnet_model.pt\nok",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Step 4: Executing the Script Module in C++": [
                    [
                        "Having successfully loaded our serialized ResNet18 in C++, we are now just a\ncouple lines of code away from executing it! Let\u2019s add those lines to our C++\napplication\u2019s main() function:",
                        "markdown"
                    ],
                    [
                        "// Create a vector of inputs.\nstd::vector&lt;torch::jit::IValue&gt; inputs;\ninputs.push_back(torch::ones({1, 3, 224, 224}));\n\n// Execute the model and turn its output into a tensor.\nat::Tensor output = module.forward(inputs).toTensor();\nstd::cout &lt;&lt; output.slice(/*dim=*/1, /*start=*/0, /*end=*/5) &lt;&lt; '\\n';",
                        "code"
                    ],
                    [
                        "The first two lines set up the inputs to our model. We create a vector of\ntorch::jit::IValue (a type-erased value type script::Module methods\naccept and return) and add a single input. To create the input tensor, we use\ntorch::ones(), the equivalent to torch.ones in the C++ API.  We then\nrun the script::Module\u2019s forward method, passing it the input vector we\ncreated. In return we get a new IValue, which we convert to a tensor by\ncalling toTensor().",
                        "markdown"
                    ],
                    [
                        "Tip",
                        "markdown"
                    ],
                    [
                        "To learn more about functions like torch::ones and the PyTorch C++ API in\ngeneral, refer to its documentation at . The\nPyTorch C++ API provides near feature parity with the Python API, allowing\nyou to further manipulate and process tensors just like in Python.",
                        "markdown"
                    ],
                    [
                        "In the last line, we print the first five entries of the output. Since we\nsupplied the same input to our model in Python earlier in this tutorial, we\nshould ideally see the same output. Let\u2019s try it out by re-compiling our\napplication and running it with the same serialized model:",
                        "markdown"
                    ],
                    [
                        "root@4b5a67132e81:/example-app/build# make\nScanning dependencies of target example-app\n[ 50%] Building CXX object CMakeFiles/example-app.dir/example-app.cpp.o\n[100%] Linking CXX executable example-app\n[100%] Built target example-app\nroot@4b5a67132e81:/example-app/build# ./example-app traced_resnet_model.pt\n-0.2698 -0.0381  0.4023 -0.3010 -0.0448\n[ Variable[CPUFloatType]{1,5} ]",
                        "code"
                    ],
                    [
                        "For reference, the output in Python previously was:",
                        "markdown"
                    ],
                    [
                        "tensor([-0.2698, -0.0381,  0.4023, -0.3010, -0.0448], grad_fn=&lt;SliceBackward&gt;)",
                        "code"
                    ],
                    [
                        "Looks like a good match!",
                        "markdown"
                    ],
                    [
                        "Tip",
                        "markdown"
                    ],
                    [
                        "To move your model to GPU memory, you can write model.to(at::kCUDA);.\nMake sure the inputs to a model are also living in CUDA memory\nby calling tensor.to(at::kCUDA), which will return a new tensor in CUDA\nmemory.",
                        "markdown"
                    ]
                ]
            },
            {
                "Step 5: Getting Help and Exploring the API": [
                    [
                        "This tutorial has hopefully equipped you with a general understanding of a\nPyTorch model\u2019s path from Python to C++. With the concepts described in this\ntutorial, you should be able to go from a vanilla, \u201ceager\u201d PyTorch model, to a\ncompiled ScriptModule in Python, to a serialized file on disk and \u2013 to\nclose the loop \u2013 to an executable script::Module in C++.",
                        "markdown"
                    ],
                    [
                        "Of course, there are many concepts we did not cover. For example, you may find\nyourself wanting to extend your ScriptModule with a custom operator\nimplemented in C++ or CUDA, and executing this custom operator inside your\nScriptModule loaded in your pure C++ production environment. The good news\nis: this is possible, and well supported! For now, you can explore  folder\nfor examples, and we will follow up with a tutorial shortly. In the time being,\nthe following links may be generally helpful:",
                        "markdown"
                    ],
                    [
                        "The Torch Script reference: ",
                        "markdown"
                    ],
                    [
                        "The PyTorch C++ API documentation: ",
                        "markdown"
                    ],
                    [
                        "The PyTorch Python API documentation: ",
                        "markdown"
                    ],
                    [
                        "As always, if you run into any problems or have questions, you can use our\n or  to get in touch.",
                        "markdown"
                    ]
                ]
            }
        ],
        "(optional) Exporting a Model from PyTorch to ONNX and Running it using ONNX Runtime": [
            [
                "In this tutorial, we describe how to convert a model defined\nin PyTorch into the ONNX format and then run it with ONNX Runtime.",
                "markdown"
            ],
            [
                "ONNX Runtime is a performance-focused engine for ONNX models,\nwhich inferences efficiently across multiple platforms and hardware\n(Windows, Linux, and Mac and on both CPUs and GPUs).\nONNX Runtime has proved to considerably increase performance over\nmultiple models as explained ",
                "markdown"
            ],
            [
                "For this tutorial, you will need to install \nand .\nYou can get binary builds of ONNX and ONNX Runtime with\npip install onnx onnxruntime.\nNote that ONNX Runtime is compatible with Python versions 3.5 to 3.7.",
                "markdown"
            ],
            [
                "NOTE: This tutorial needs PyTorch master branch which can be installed by following\nthe instructions ",
                "markdown"
            ],
            [
                "# Some standard imports\nimport io\nimport numpy as np\n\nfrom torch import nn\nimport torch.utils.model_zoo as model_zoo\nimport torch.onnx",
                "code"
            ],
            [
                "Super-resolution is a way of increasing the resolution of images, videos\nand is widely used in image processing or video editing. For this\ntutorial, we will use a small super-resolution model.",
                "markdown"
            ],
            [
                "First, let\u2019s create a SuperResolution model in PyTorch.\nThis model uses the efficient sub-pixel convolution layer described in\n\nfor increasing the resolution of an image by an upscale factor.\nThe model expects the Y component of the YCbCr of an image as an input, and\noutputs the upscaled Y component in super resolution.",
                "markdown"
            ],
            [
                "comes directly from PyTorch\u2019s examples without modification:",
                "markdown"
            ],
            [
                "# Super Resolution model definition in PyTorch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\nclass SuperResolutionNet():\n    def __init__(self, upscale_factor, inplace=False):\n        super(SuperResolutionNet, self).__init__()\n\n        self.relu = (inplace=inplace)\n        self.conv1 = (1, 64, (5, 5), (1, 1), (2, 2))\n        self.conv2 = (64, 64, (3, 3), (1, 1), (1, 1))\n        self.conv3 = (64, 32, (3, 3), (1, 1), (1, 1))\n        self.conv4 = (32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n        self.pixel_shuffle = (upscale_factor)\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.relu(self.conv3(x))\n        x = self.pixel_shuffle(self.conv4(x))\n        return x\n\n    def _initialize_weights(self):\n        (self.conv1.weight, ('relu'))\n        (self.conv2.weight, ('relu'))\n        (self.conv3.weight, ('relu'))\n        (self.conv4.weight)\n\n# Create the super-resolution model by using the above model definition.\ntorch_model = SuperResolutionNet(upscale_factor=3)",
                "code"
            ],
            [
                "Ordinarily, you would now train this model; however, for this tutorial,\nwe will instead download some pre-trained weights. Note that this model\nwas not trained fully for good accuracy and is used here for\ndemonstration purposes only.",
                "markdown"
            ],
            [
                "It is important to call torch_model.eval() or torch_model.train(False)\nbefore exporting the model, to turn the model to inference mode.\nThis is required since operators like dropout or batchnorm behave\ndifferently in inference and training mode.",
                "markdown"
            ],
            [
                "# Load pretrained model weights\nmodel_url = 'https://s3.amazonaws.com/pytorch/test_data/export/superres_epoch100-44c6958e.pth'\nbatch_size = 1    # just a random number\n\n# Initialize model with the pretrained weights\nmap_location = lambda storage, loc: storage\nif torch.cuda.is_available():\n    map_location = None\ntorch_model.load_state_dict((model_url, map_location=map_location))\n\n# set the model to inference mode\ntorch_model.eval()",
                "code"
            ],
            [
                "Exporting a model in PyTorch works via tracing or scripting. This\ntutorial will use as an example a model exported by tracing.\nTo export a model, we call the torch.onnx.export() function.\nThis will execute the model, recording a trace of what operators\nare used to compute the outputs.\nBecause export runs the model, we need to provide an input\ntensor x. The values in this can be random as long as it is the\nright type and size.\nNote that the input size will be fixed in the exported ONNX graph for\nall the input\u2019s dimensions, unless specified as a dynamic axes.\nIn this example we export the model with an input of batch_size 1,\nbut then specify the first dimension as dynamic in the dynamic_axes\nparameter in torch.onnx.export().\nThe exported model will thus accept inputs of size [batch_size, 1, 224, 224]\nwhere batch_size can be variable.",
                "markdown"
            ],
            [
                "To learn more details about PyTorch\u2019s export interface, check out the\n.",
                "markdown"
            ],
            [
                "# Input to the model\nx = torch.randn(batch_size, 1, 224, 224, requires_grad=True)\ntorch_out = torch_model(x)\n\n# Export the model\n(torch_model,               # model being run\n                  x,                         # model input (or a tuple for multiple inputs)\n                  \"super_resolution.onnx\",   # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  opset_version=10,          # the ONNX version to export the model to\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\n                  input_names = ['input'],   # the model's input names\n                  output_names = ['output'], # the model's output names\n                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n                                'output' : {0 : 'batch_size'}})",
                "code"
            ],
            [
                "We also computed torch_out, the output after of the model,\nwhich we will use to verify that the model we exported computes\nthe same values when run in ONNX Runtime.",
                "markdown"
            ],
            [
                "But before verifying the model\u2019s output with ONNX Runtime, we will check\nthe ONNX model with ONNX\u2019s API.\nFirst, onnx.load(\"super_resolution.onnx\") will load the saved model and\nwill output a onnx.ModelProto structure (a top-level file/container format for bundling a ML model.\nFor more information .).\nThen, onnx.checker.check_model(onnx_model) will verify the model\u2019s structure\nand confirm that the model has a valid schema.\nThe validity of the ONNX graph is verified by checking the model\u2019s\nversion, the graph\u2019s structure, as well as the nodes and their inputs\nand outputs.",
                "markdown"
            ],
            [
                "import onnx\n\nonnx_model = onnx.load(\"super_resolution.onnx\")\nonnx.checker.check_model(onnx_model)",
                "code"
            ],
            [
                "Now let\u2019s compute the output using ONNX Runtime\u2019s Python APIs.\nThis part can normally be done in a separate process or on another\nmachine, but we will continue in the same process so that we can\nverify that ONNX Runtime and PyTorch are computing the same value\nfor the network.",
                "markdown"
            ],
            [
                "In order to run the model with ONNX Runtime, we need to create an\ninference session for the model with the chosen configuration\nparameters (here we use the default config).\nOnce the session is created, we evaluate the model using the run() api.\nThe output of this call is a list containing the outputs of the model\ncomputed by ONNX Runtime.",
                "markdown"
            ],
            [
                "import onnxruntime\n\nort_session = onnxruntime.InferenceSession(\"super_resolution.onnx\")\n\ndef to_numpy(tensor):\n    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n\n# compute ONNX Runtime output prediction\nort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\nort_outs = ort_session.run(None, ort_inputs)\n\n# compare ONNX Runtime and PyTorch results\nnp.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n\nprint(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")",
                "code"
            ],
            [
                "We should see that the output of PyTorch and ONNX Runtime runs match\nnumerically with the given precision (rtol=1e-03 and atol=1e-05).\nAs a side-note, if they do not match then there is an issue in the\nONNX exporter, so please contact us in that case.",
                "markdown"
            ],
            {
                "Running the model on an image using ONNX Runtime": [
                    [
                        "So far we have exported a model from PyTorch and shown how to load it\nand run it in ONNX Runtime with a dummy tensor as an input.",
                        "markdown"
                    ],
                    [
                        "For this tutorial, we will use a famous cat image used widely which\nlooks like below\n\n<img alt=\"cat\" src=\"../_images/cat_224x224.jpg\"/>",
                        "markdown"
                    ],
                    [
                        "First, let\u2019s load the image, pre-process it using standard PIL\npython library. Note that this preprocessing is the standard practice of\nprocessing data for training/testing neural networks.",
                        "markdown"
                    ],
                    [
                        "We first resize the image to fit the size of the model\u2019s input (224x224).\nThen we split the image into its Y, Cb, and Cr components.\nThese components represent a greyscale image (Y), and\nthe blue-difference (Cb) and red-difference (Cr) chroma components.\nThe Y component being more sensitive to the human eye, we are\ninterested in this component which we will be transforming.\nAfter extracting the Y component, we convert it to a tensor which\nwill be the input of our model.",
                        "markdown"
                    ],
                    [
                        "from PIL import Image\nimport torchvision.transforms as transforms\n\nimg = Image.open(\"./_static/img/cat.jpg\")\n\nresize = ([224, 224])\nimg = resize(img)\n\nimg_ycbcr = img.convert('YCbCr')\nimg_y, img_cb, img_cr = img_ycbcr.split()\n\nto_tensor = ()\nimg_y = to_tensor(img_y)\nimg_y.unsqueeze_(0)",
                        "code"
                    ],
                    [
                        "Now, as a next step, let\u2019s take the tensor representing the\ngreyscale resized cat image and run the super-resolution model in\nONNX Runtime as explained previously.",
                        "markdown"
                    ],
                    [
                        "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(img_y)}\nort_outs = ort_session.run(None, ort_inputs)\nimg_out_y = ort_outs[0]",
                        "code"
                    ],
                    [
                        "At this point, the output of the model is a tensor.\nNow, we\u2019ll process the output of the model to construct back the\nfinal output image from the output tensor, and save the image.\nThe post-processing steps have been adopted from PyTorch\nimplementation of super-resolution model\n.",
                        "markdown"
                    ],
                    [
                        "img_out_y = Image.fromarray(np.uint8((img_out_y[0] * 255.0).clip(0, 255)[0]), mode='L')\n\n# get the output image follow post-processing step from PyTorch implementation\nfinal_img = Image.merge(\n    \"YCbCr\", [\n        img_out_y,\n        img_cb.resize(img_out_y.size, Image.BICUBIC),\n        img_cr.resize(img_out_y.size, Image.BICUBIC),\n    ]).convert(\"RGB\")\n\n# Save the image, we will compare this with the output image from mobile device\nfinal_img.save(\"./_static/img/cat_superres_with_ort.jpg\")\n\n\n\n<img alt=\"output\\_cat\" src=\"../_images/cat_superres_with_ort.jpg\"/>",
                        "code"
                    ],
                    [
                        "ONNX Runtime being a cross platform engine, you can run it across\nmultiple platforms and on both CPUs and GPUs.",
                        "markdown"
                    ],
                    [
                        "ONNX Runtime can also be deployed to the cloud for model inferencing\nusing Azure Machine Learning Services. More information .",
                        "markdown"
                    ],
                    [
                        "More information about ONNX Runtime\u2019s performance .",
                        "markdown"
                    ],
                    [
                        "For more information about ONNX Runtime .",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Real Time Inference on Raspberry Pi 4 (30 fps!)": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "PyTorch has out of the box support for Raspberry Pi 4. This tutorial will guide\nyou on how to setup a Raspberry Pi 4 for running PyTorch and run a MobileNet v2\nclassification model in real time (30 fps+) on the CPU.",
                "markdown"
            ],
            [
                "This was all tested with Raspberry Pi 4 Model B 4GB but should work with the 2GB\nvariant as well as on the 3B with reduced performance.\n<img alt=\"https://user-images.githubusercontent.com/909104/153093710-bc736b6f-69d9-4a50-a3e8-9f2b2c9e04fd.gif\" src=\"https://user-images.githubusercontent.com/909104/153093710-bc736b6f-69d9-4a50-a3e8-9f2b2c9e04fd.gif\"/>",
                "markdown"
            ],
            {
                "Prerequisites": [
                    [
                        "To follow this tutorial you\u2019ll need a Raspberry Pi 4, a camera for it and all\nthe other standard accessories.",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "Heat sinks and Fan (optional but recommended)",
                        "markdown"
                    ],
                    [
                        "5V 3A USB-C Power Supply",
                        "markdown"
                    ],
                    [
                        "SD card (at least 8gb)",
                        "markdown"
                    ],
                    [
                        "SD card read/writer",
                        "markdown"
                    ]
                ]
            },
            {
                "Raspberry Pi 4 Setup": [
                    [
                        "PyTorch only provides pip packages for Arm 64bit (aarch64) so you\u2019ll need to install a 64 bit version of the OS on your Raspberry Pi",
                        "markdown"
                    ],
                    [
                        "You can download the latest arm64 Raspberry Pi OS from  and install it via rpi-imager.",
                        "markdown"
                    ],
                    [
                        "<strong>32-bit Raspberry Pi OS will not work.</strong>\n<img alt=\"https://user-images.githubusercontent.com/909104/152866212-36ce29b1-aba6-4924-8ae6-0a283f1fca14.gif\" src=\"https://user-images.githubusercontent.com/909104/152866212-36ce29b1-aba6-4924-8ae6-0a283f1fca14.gif\"/>",
                        "markdown"
                    ],
                    [
                        "Installation will take at least a few minutes depending on your internet speed and sdcard speed. Once it\u2019s done it should look like:\n<img alt=\"https://user-images.githubusercontent.com/909104/152867425-c005cff0-5f3f-47f1-922d-e0bbb541cd25.png\" src=\"https://user-images.githubusercontent.com/909104/152867425-c005cff0-5f3f-47f1-922d-e0bbb541cd25.png\"/>",
                        "markdown"
                    ],
                    [
                        "Time to put your sdcard in your Raspberry Pi, connect the camera and boot it up.\n<img alt=\"https://user-images.githubusercontent.com/909104/152869862-c239c980-b089-4bd5-84eb-0a1e5cf22df2.png\" src=\"https://user-images.githubusercontent.com/909104/152869862-c239c980-b089-4bd5-84eb-0a1e5cf22df2.png\"/>",
                        "markdown"
                    ],
                    [
                        "Once that boots and you complete the initial setup you\u2019ll need to edit the /boot/config.txt file to enable the camera.",
                        "markdown"
                    ],
                    [
                        "# This enables the extended features such as the camera.\nstart_x=1\n\n# This needs to be at least 128M for the camera processing, if it's bigger you can just leave it as is.\ngpu_mem=128\n\n# You need to commment/remove the existing camera_auto_detect line since this causes issues with OpenCV/V4L2 capture.\n#camera_auto_detect=1",
                        "code"
                    ],
                    [
                        "And then reboot. After you reboot the video4linux2 device /dev/video0 should exist.",
                        "markdown"
                    ]
                ]
            },
            {
                "Installing PyTorch and OpenCV": [
                    [
                        "PyTorch and all the other libraries we need have ARM 64-bit/aarch64 variants so you can just install them via pip and have it work like any other Linux system.",
                        "markdown"
                    ],
                    [
                        "$ pip install torch torchvision torchaudio\n$ pip install opencv-python\n$ pip install numpy --upgrade\n\n\n<img alt=\"https://user-images.githubusercontent.com/909104/152874260-95a7a8bd-0f9b-438a-9c0b-5b67729e233f.png\" src=\"https://user-images.githubusercontent.com/909104/152874260-95a7a8bd-0f9b-438a-9c0b-5b67729e233f.png\"/>",
                        "code"
                    ],
                    [
                        "We can now check that everything installed correctly:",
                        "markdown"
                    ],
                    [
                        "$ python -c \"import torch; print(torch.__version__)\"\n\n\n<img alt=\"https://user-images.githubusercontent.com/909104/152874271-d7057c2d-80fd-4761-aed4-df6c8b7aa99f.png\" src=\"https://user-images.githubusercontent.com/909104/152874271-d7057c2d-80fd-4761-aed4-df6c8b7aa99f.png\"/>",
                        "code"
                    ]
                ]
            },
            {
                "Video Capture": [
                    [
                        "For video capture we\u2019re going to be using OpenCV to stream the video frames\ninstead of the more common picamera. <cite>picamera</cite> isn\u2019t available on 64-bit\nRaspberry Pi OS and it\u2019s much slower than OpenCV. OpenCV directly accesses the\n/dev/video0 device to grab frames.",
                        "markdown"
                    ],
                    [
                        "The model we\u2019re using (MobileNetV2) takes in image sizes of 224x224 so we\ncan request that directly from OpenCV at 36fps. We\u2019re targeting 30fps for the\nmodel but we request a slightly higher framerate than that so there\u2019s always\nenough frames.",
                        "markdown"
                    ],
                    [
                        "import cv2\nfrom PIL import Image\n\ncap = cv2.VideoCapture(0)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)\ncap.set(cv2.CAP_PROP_FPS, 36)",
                        "code"
                    ],
                    [
                        "OpenCV returns a numpy array in BGR so we need to read and do a bit of\nshuffling to get it into the expected RGB format.",
                        "markdown"
                    ],
                    [
                        "ret, image = cap.read()\n# convert opencv output from BGR to RGB\nimage = image[:, :, [2, 1, 0]]",
                        "code"
                    ],
                    [
                        "This data reading and processing takes about 3.5 ms.",
                        "markdown"
                    ]
                ]
            },
            {
                "Image Preprocessing": [
                    [
                        "We need to take the frames and transform them into the format the model expects. This is the same processing as you would do on any machine with the standard torchvision transforms.",
                        "markdown"
                    ],
                    [
                        "from torchvision import transforms\n\npreprocess = transforms.Compose([\n    # convert the frame to a CHW torch tensor for training\n    transforms.ToTensor(),\n    # normalize the colors to the range that mobilenet_v2/3 expect\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(image)\n# The model can handle multiple images simultaneously so we need to add an\n# empty dimension for the batch.\n# [3, 224, 224] -&gt; [1, 3, 224, 224]\ninput_batch = input_tensor.unsqueeze(0)",
                        "code"
                    ]
                ]
            },
            {
                "Model Choices": [
                    [
                        "There\u2019s a number of models you can choose from to use with different performance\ncharacteristics. Not all models provide a qnnpack pretrained variant so for\ntesting purposes you should chose one that does but if you train and quantize\nyour own model you can use any of them.",
                        "markdown"
                    ],
                    [
                        "We\u2019re using mobilenet_v2 for this tutorial since it has good performance and\naccuracy.",
                        "markdown"
                    ],
                    [
                        "Raspberry Pi 4 Benchmark Results:",
                        "markdown"
                    ]
                ]
            },
            {
                "MobileNetV2: Quantization and JIT": [
                    [
                        "For optimal performance we want a model that\u2019s quantized and fused. Quantized\nmeans that it does the computation using int8 which is much more performant than\nthe standard float32 math. Fused means that consecutive operations have been\nfused together into a more performant version where possible. Commonly things\nlike activations (ReLU) can be merged into the layer before (Conv2d)\nduring inference.",
                        "markdown"
                    ],
                    [
                        "The aarch64 version of pytorch requires using the qnnpack engine.",
                        "markdown"
                    ],
                    [
                        "import torch\ntorch.backends.quantized.engine = 'qnnpack'",
                        "code"
                    ],
                    [
                        "For this example we\u2019ll use a prequantized and fused version of MobileNetV2 that\u2019s provided out of the box by torchvision.",
                        "markdown"
                    ],
                    [
                        "from torchvision import models\nnet = models.quantization.mobilenet_v2(pretrained=True, quantize=True)",
                        "code"
                    ],
                    [
                        "We then want to jit the model to reduce Python overhead and fuse any ops. Jit gives us ~30fps instead of ~20fps without it.",
                        "markdown"
                    ],
                    [
                        "net = torch.jit.script(net)",
                        "code"
                    ]
                ]
            },
            {
                "Putting It Together": [
                    [
                        "We can now put all the pieces together and run it:",
                        "markdown"
                    ],
                    [
                        "import time\n\nimport torch\nimport numpy as np\nfrom torchvision import models, transforms\n\nimport cv2\nfrom PIL import Image\n\ntorch.backends.quantized.engine = 'qnnpack'\n\ncap = cv2.VideoCapture(0, cv2.CAP_V4L2)\ncap.set(cv2.CAP_PROP_FRAME_WIDTH, 224)\ncap.set(cv2.CAP_PROP_FRAME_HEIGHT, 224)\ncap.set(cv2.CAP_PROP_FPS, 36)\n\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nnet = models.quantization.mobilenet_v2(pretrained=True, quantize=True)\n# jit model to take it from ~20fps to ~30fps\nnet = torch.jit.script(net)\n\nstarted = time.time()\nlast_logged = time.time()\nframe_count = 0\n\nwith torch.no_grad():\n    while True:\n        # read frame\n        ret, image = cap.read()\n        if not ret:\n            raise RuntimeError(\"failed to read frame\")\n\n        # convert opencv output from BGR to RGB\n        image = image[:, :, [2, 1, 0]]\n        permuted = image\n\n        # preprocess\n        input_tensor = preprocess(image)\n\n        # create a mini-batch as expected by the model\n        input_batch = input_tensor.unsqueeze(0)\n\n        # run model\n        output = net(input_batch)\n        # do something with output ...\n\n        # log model performance\n        frame_count += 1\n        now = time.time()\n        if now - last_logged &gt; 1:\n            print(f\"{frame_count / (now-last_logged)} fps\")\n            last_logged = now\n            frame_count = 0",
                        "code"
                    ],
                    [
                        "Running it shows that we\u2019re hovering at ~30 fps.\n<img alt=\"https://user-images.githubusercontent.com/909104/152892609-7d115705-3ec9-4f8d-beed-a51711503a32.png\" src=\"https://user-images.githubusercontent.com/909104/152892609-7d115705-3ec9-4f8d-beed-a51711503a32.png\"/>",
                        "markdown"
                    ],
                    [
                        "This is with all the default settings in Raspberry Pi OS. If you disabled the UI\nand all the other background services that are enabled by default it\u2019s more\nperformant and stable.",
                        "markdown"
                    ],
                    [
                        "If we check htop we see that we have almost 100% utilization.\n<img alt=\"https://user-images.githubusercontent.com/909104/152892630-f094b84b-19ba-48f6-8632-1b954abc59c7.png\" src=\"https://user-images.githubusercontent.com/909104/152892630-f094b84b-19ba-48f6-8632-1b954abc59c7.png\"/>",
                        "markdown"
                    ],
                    [
                        "To verify that it\u2019s working end to end we can compute the probabilities of the\nclasses and\n\nto print the detections.",
                        "markdown"
                    ],
                    [
                        "top = list(enumerate(output[0].softmax(dim=0)))\ntop.sort(key=lambda x: x[1], reverse=True)\nfor idx, val in top[:10]:\n    print(f\"{val.item()*100:.2f}% {classes[idx]}\")",
                        "code"
                    ],
                    [
                        "mobilenet_v3_large running in real time:\n<img alt=\"https://user-images.githubusercontent.com/909104/153093710-bc736b6f-69d9-4a50-a3e8-9f2b2c9e04fd.gif\" src=\"https://user-images.githubusercontent.com/909104/153093710-bc736b6f-69d9-4a50-a3e8-9f2b2c9e04fd.gif\"/>",
                        "markdown"
                    ],
                    [
                        "Detecting an orange:\n<img alt=\"https://user-images.githubusercontent.com/909104/153092153-d9c08dfe-105b-408a-8e1e-295da8a78c19.jpg\" src=\"https://user-images.githubusercontent.com/909104/153092153-d9c08dfe-105b-408a-8e1e-295da8a78c19.jpg\"/>",
                        "markdown"
                    ],
                    [
                        "Detecting a mug:\n<img alt=\"https://user-images.githubusercontent.com/909104/153092155-4b90002f-a0f3-4267-8d70-e713e7b4d5a0.jpg\" src=\"https://user-images.githubusercontent.com/909104/153092155-4b90002f-a0f3-4267-8d70-e713e7b4d5a0.jpg\"/>",
                        "markdown"
                    ]
                ]
            },
            {
                "Troubleshooting: Performance": [
                    [
                        "PyTorch by default will use all of the cores available. If you have anything\nrunning in the background on the Raspberry Pi it may cause contention with the\nmodel inference causing latency spikes. To alleviate this you can reduce the\nnumber of threads which will reduce the peak latency at a small performance\npenalty.",
                        "markdown"
                    ],
                    [
                        "torch.set_num_threads(2)",
                        "code"
                    ],
                    [
                        "For shufflenet_v2_x1_5 using 2 threads instead of 4 threads\nincreases best case latency to 72 ms from 60 ms but eliminates the\nlatency spikes of 128 ms.",
                        "markdown"
                    ]
                ]
            },
            {
                "Next Steps": [
                    [
                        "You can create your own model or fine tune an existing one. If you fine tune on\none of the models from\n\nmost of the work to fuse and quantize has already been done for you so you can\ndirectly deploy with good performance on a Raspberry Pi.",
                        "markdown"
                    ],
                    [
                        "See more:",
                        "markdown"
                    ],
                    [
                        " for more information on how to quantize and fuse your model.",
                        "markdown"
                    ],
                    [
                        "for how to use transfer learning to fine tune a pre-existing model to your dataset.",
                        "markdown"
                    ]
                ]
            }
        ]
    },
    "Code Transforms with FX": {
        "(beta) Building a Convolution/Batch Norm fuser in FX": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "In this tutorial, we are going to use FX, a toolkit for composable function\ntransformations of PyTorch, to do the following:",
                "markdown"
            ],
            [
                "Find patterns of conv/batch norm in the data dependencies.",
                "markdown"
            ],
            [
                "For the patterns found in 1), fold the batch norm statistics into the convolution weights.",
                "markdown"
            ],
            [
                "Note that this optimization only works for models in inference mode (i.e. <cite>mode.eval()</cite>)",
                "markdown"
            ],
            [
                "We will be building the fuser that exists here:",
                "markdown"
            ],
            [
                "First, let\u2019s get some imports out of the way (we will be using all\nof these later in the code).",
                "markdown"
            ],
            [
                "from typing import Type, Dict, Any, Tuple, Iterable\nimport copy\nimport torch.fx as fx\nimport torch\nimport torch.nn as nn",
                "code"
            ],
            [
                "For this tutorial, we are going to create a model consisting of convolutions\nand batch norms. Note that this model has some tricky components - some of\nthe conv/batch norm patterns are hidden within Sequentials and one of the\nBatchNorms is wrapped in another Module.",
                "markdown"
            ],
            [
                "class WrappedBatchNorm():\n    def __init__(self):\n        super().__init__()\n        self.mod = (1)\n    def forward(self, x):\n        return self.mod(x)\n\nclass M():\n    def __init__(self):\n        super().__init__()\n        self.conv1 = (1, 1, 1)\n        self.bn1 = (1)\n        self.conv2 = (1, 1, 1)\n        self.nested = (\n            (1),\n            (1, 1, 1),\n        )\n        self.wrapped = WrappedBatchNorm()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.conv2(x)\n        x = self.nested(x)\n        x = self.wrapped(x)\n        return x\n\nmodel = M()\n\nmodel.eval()",
                "code"
            ],
            {
                "Fusing Convolution with Batch Norm": [
                    [
                        "One of the primary challenges with trying to automatically fuse convolution\nand batch norm in PyTorch is that PyTorch does not provide an easy way of\naccessing the computational graph. FX resolves this problem by symbolically\ntracing the actual operations called, so that we can track the computations\nthrough the <cite>forward</cite> call, nested within Sequential modules, or wrapped in\nan user-defined module.",
                        "markdown"
                    ],
                    [
                        "traced_model = (model)\nprint(traced_model.graph)",
                        "code"
                    ],
                    [
                        "This gives us a graph representation of our model. Note that both the modules\nhidden within the sequential as well as the wrapped Module have been inlined\ninto the graph. This is the default level of abstraction, but it can be\nconfigured by the pass writer. More information can be found at the FX\noverview ",
                        "markdown"
                    ]
                ]
            },
            {
                "Fusing Convolution with Batch Norm": [
                    [
                        "Unlike some other fusions, fusion of convolution with batch norm does not\nrequire any new operators. Instead, as batch norm during inference\nconsists of a pointwise add and multiply, these operations can be \u201cbaked\u201d\ninto the preceding convolution\u2019s weights. This allows us to remove the batch\nnorm entirely from our model! Read\n for further details. The\ncode here is copied from\n\nclarity purposes.",
                        "markdown"
                    ],
                    [
                        "def fuse_conv_bn_eval(conv, bn):\n    \"\"\"\n    Given a conv Module `A` and an batch_norm module `B`, returns a conv\n    module `C` such that C(x) == B(A(x)) in inference mode.\n    \"\"\"\n    assert(not (conv.training or bn.training)), \"Fusion only for eval!\"\n    fused_conv = copy.deepcopy(conv)\n\n    fused_conv.weight, fused_conv.bias = \\\n        fuse_conv_bn_weights(fused_conv.weight, fused_conv.bias,\n                             bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)\n\n    return fused_conv\n\ndef fuse_conv_bn_weights(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b):\n    if conv_b is None:\n        conv_b = (bn_rm)\n    if bn_w is None:\n        bn_w = (bn_rm)\n    if bn_b is None:\n        bn_b = (bn_rm)\n    bn_var_rsqrt = (bn_rv + bn_eps)\n\n    conv_w = conv_w * (bn_w * bn_var_rsqrt).reshape([-1] + [1] * (len(conv_w.shape) - 1))\n    conv_b = (conv_b - bn_rm) * bn_var_rsqrt * bn_w + bn_b\n\n    return torch.nn.Parameter(conv_w), torch.nn.Parameter(conv_b)",
                        "code"
                    ]
                ]
            },
            {
                "FX Fusion Pass": [
                    [
                        "Now that we have our computational graph as well as a method for fusing\nconvolution and batch norm, all that remains is to iterate over the FX graph\nand apply the desired fusions.",
                        "markdown"
                    ],
                    [
                        "def _parent_name(target : str) -&gt; Tuple[str, str]:\n    \"\"\"\n    Splits a qualname into parent path and last atom.\n    For example, `foo.bar.baz` -&gt; (`foo.bar`, `baz`)\n    \"\"\"\n    *parent, name = target.rsplit('.', 1)\n    return parent[0] if parent else '', name\n\ndef replace_node_module(node: , modules: Dict[str, Any], new_module: ):\n    assert(isinstance(node.target, str))\n    parent_name, name = _parent_name(node.target)\n    setattr(modules[parent_name], name, new_module)\n\n\ndef fuse(model: ) -&gt; :\n    model = copy.deepcopy(model)\n    # The first step of most FX passes is to symbolically trace our model to\n    # obtain a `GraphModule`. This is a representation of our original model\n    # that is functionally identical to our original model, except that we now\n    # also have a graph representation of our forward pass.\n    fx_model:  = (model)\n    modules = dict(fx_model.named_modules())\n\n    # The primary representation for working with FX are the `Graph` and the\n    # `Node`. Each `GraphModule` has a `Graph` associated with it - this\n    # `Graph` is also what generates `GraphModule.code`.\n    # The `Graph` itself is represented as a list of `Node` objects. Thus, to\n    # iterate through all of the operations in our graph, we iterate over each\n    # `Node` in our `Graph`.\n    for node in fx_model.graph.nodes:\n        # The FX IR contains several types of nodes, which generally represent\n        # call sites to modules, functions, or methods. The type of node is\n        # determined by `Node.op`.\n        if node.op != 'call_module': # If our current node isn't calling a Module then we can ignore it.\n            continue\n        # For call sites, `Node.target` represents the module/function/method\n        # that's being called. Here, we check `Node.target` to see if it's a\n        # batch norm module, and then check `Node.args[0].target` to see if the\n        # input `Node` is a convolution.\n        if type(modules[node.target]) is  and type(modules[node.args[0].target]) is :\n            if len(node.args[0].users) &gt; 1:  # Output of conv is used by other nodes\n                continue\n            conv = modules[node.args[0].target]\n            bn = modules[node.target]\n            fused_conv = fuse_conv_bn_eval(conv, bn)\n            replace_node_module(node.args[0], modules, fused_conv)\n            # As we've folded the batch nor into the conv, we need to replace all uses\n            # of the batch norm with the conv.\n            node.replace_all_uses_with(node.args[0])\n            # Now that all uses of the batch norm have been replaced, we can\n            # safely remove the batch norm.\n            fx_model.graph.erase_node(node)\n    fx_model.graph.lint()\n    # After we've modified our graph, we need to recompile our graph in order\n    # to keep the generated code in sync.\n    fx_model.recompile()\n    return fx_model",
                        "code"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "We make some simplifications here for demonstration purposes, such as only\nmatching 2D convolutions. View\n\nfor a more usable pass.",
                        "markdown"
                    ]
                ]
            },
            {
                "Testing out our Fusion Pass": [
                    [
                        "We can now run this fusion pass on our initial toy model and verify that our\nresults are identical. In addition, we can print out the code for our fused\nmodel and verify that there are no more batch norms.",
                        "markdown"
                    ],
                    [
                        "fused_model = fuse(model)\nprint(fused_model.code)\ninp = (5, 1, 1, 1)\n(fused_model(inp), model(inp))",
                        "code"
                    ]
                ]
            },
            {
                "Benchmarking our Fusion on ResNet18": [
                    [
                        "We can test our fusion pass on a larger model like ResNet18 and see how much\nthis pass improves inference performance.",
                        "markdown"
                    ],
                    [
                        "import torchvision.models as models\nimport time\n\nrn18 = ()\nrn18.eval()\n\ninp = (10, 3, 224, 224)\noutput = rn18(inp)\n\ndef benchmark(model, iters=20):\n    for _ in range(10):\n        model(inp)\n    begin = time.time()\n    for _ in range(iters):\n        model(inp)\n    return str(time.time()-begin)\n\nfused_rn18 = fuse(rn18)\nprint(\"Unfused time: \", benchmark(rn18))\nprint(\"Fused time: \", benchmark(fused_rn18))",
                        "code"
                    ],
                    [
                        "As we previously saw, the output of our FX transformation is\n(Torchscriptable) PyTorch code, we can easily <cite>jit.script</cite> the output to try\nand increase our performance even more. In this way, our FX model\ntransformation composes with Torchscript with no issues.",
                        "markdown"
                    ],
                    [
                        "jit_rn18 = (fused_rn18)\nprint(\"jit time: \", benchmark(jit_rn18))\n\n\n############\n# Conclusion\n# ----------\n# As we can see, using FX we can easily write static graph transformations on\n# PyTorch code.\n#\n# Since FX is still in beta, we would be happy to hear any\n# feedback you have about using it. Please feel free to use the\n# PyTorch Forums (https://discuss.pytorch.org/) and the issue tracker\n# (https://github.com/pytorch/pytorch/issues) to provide any feedback\n# you might have.",
                        "code"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "(beta) Building a Simple CPU Performance Profiler with FX": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "In this tutorial, we are going to use FX to do the following:",
                "markdown"
            ],
            [
                "Capture PyTorch Python code in a way that we can inspect and gather\nstatistics about the structure and execution of the code",
                "markdown"
            ],
            [
                "Build out a small class that will serve as a simple performance \u201cprofiler\u201d,\ncollecting runtime statistics about each part of the model from actual\nruns.",
                "markdown"
            ],
            [
                "For this tutorial, we are going to use the torchvision ResNet18 model\nfor demonstration purposes.",
                "markdown"
            ],
            [
                "import torch\nimport torch.fx\nimport torchvision.models as models\n\nrn18 = ()\n()",
                "code"
            ],
            [
                "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)",
                "code"
            ],
            [
                "Now that we have our model, we want to inspect deeper into its\nperformance. That is, for the following invocation, which parts\nof the model are taking the longest?",
                "markdown"
            ],
            [
                "input = (5, 3, 224, 224)\n = rn18(input)",
                "code"
            ],
            [
                "A common way of answering that question is to go through the program\nsource, add code that collects timestamps at various points in the\nprogram, and compare the difference between those timestamps to see\nhow long the regions between the timestamps take.",
                "markdown"
            ],
            [
                "That technique is certainly applicable to PyTorch code, however it\nwould be nicer if we didn\u2019t have to copy over model code and edit it,\nespecially code we haven\u2019t written (like this torchvision model).\nInstead, we are going to use FX to automate this \u201cinstrumentation\u201d\nprocess without needing to modify any source.",
                "markdown"
            ],
            [
                "First, let\u2019s get some imports out of the way (we will be using all\nof these later in the code).",
                "markdown"
            ],
            [
                "import statistics, tabulate, time\nfrom typing import Any, Dict, List\nfrom torch.fx import ",
                "code"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                "tabulate is an external library that is not a dependency of PyTorch.\nWe will be using it to more easily visualize performance data. Please\nmake sure you\u2019ve installed it from your favorite Python package source.",
                "markdown"
            ],
            {
                "Capturing the Model with Symbolic Tracing": [
                    [
                        "Next, we are going to use FX\u2019s symbolic tracing mechanism to capture\nthe definition of our model in a data structure we can manipulate\nand examine.",
                        "markdown"
                    ],
                    [
                        "traced_rn18 = (rn18)\nprint()",
                        "code"
                    ],
                    [
                        "graph():\n    %x : torch.Tensor [#users=1] = placeholder[target=x]\n    %conv1 : [#users=1] = call_module[target=conv1](args = (%x,), kwargs = {})\n    %bn1 : [#users=1] = call_module[target=bn1](args = (%conv1,), kwargs = {})\n    %relu : [#users=1] = call_module[target=relu](args = (%bn1,), kwargs = {})\n    %maxpool : [#users=2] = call_module[target=maxpool](args = (%relu,), kwargs = {})\n    %layer1_0_conv1 : [#users=1] = call_module[target=layer1.0.conv1](args = (%maxpool,), kwargs = {})\n    %layer1_0_bn1 : [#users=1] = call_module[target=layer1.0.bn1](args = (%layer1_0_conv1,), kwargs = {})\n    %layer1_0_relu : [#users=1] = call_module[target=layer1.0.relu](args = (%layer1_0_bn1,), kwargs = {})\n    %layer1_0_conv2 : [#users=1] = call_module[target=layer1.0.conv2](args = (%layer1_0_relu,), kwargs = {})\n    %layer1_0_bn2 : [#users=1] = call_module[target=layer1.0.bn2](args = (%layer1_0_conv2,), kwargs = {})\n    %add : [#users=1] = call_function[target=operator.add](args = (%layer1_0_bn2, %maxpool), kwargs = {})\n    %layer1_0_relu_1 : [#users=2] = call_module[target=layer1.0.relu](args = (%add,), kwargs = {})\n    %layer1_1_conv1 : [#users=1] = call_module[target=layer1.1.conv1](args = (%layer1_0_relu_1,), kwargs = {})\n    %layer1_1_bn1 : [#users=1] = call_module[target=layer1.1.bn1](args = (%layer1_1_conv1,), kwargs = {})\n    %layer1_1_relu : [#users=1] = call_module[target=layer1.1.relu](args = (%layer1_1_bn1,), kwargs = {})\n    %layer1_1_conv2 : [#users=1] = call_module[target=layer1.1.conv2](args = (%layer1_1_relu,), kwargs = {})\n    %layer1_1_bn2 : [#users=1] = call_module[target=layer1.1.bn2](args = (%layer1_1_conv2,), kwargs = {})\n    %add_1 : [#users=1] = call_function[target=operator.add](args = (%layer1_1_bn2, %layer1_0_relu_1), kwargs = {})\n    %layer1_1_relu_1 : [#users=2] = call_module[target=layer1.1.relu](args = (%add_1,), kwargs = {})\n    %layer2_0_conv1 : [#users=1] = call_module[target=layer2.0.conv1](args = (%layer1_1_relu_1,), kwargs = {})\n    %layer2_0_bn1 : [#users=1] = call_module[target=layer2.0.bn1](args = (%layer2_0_conv1,), kwargs = {})\n    %layer2_0_relu : [#users=1] = call_module[target=layer2.0.relu](args = (%layer2_0_bn1,), kwargs = {})\n    %layer2_0_conv2 : [#users=1] = call_module[target=layer2.0.conv2](args = (%layer2_0_relu,), kwargs = {})\n    %layer2_0_bn2 : [#users=1] = call_module[target=layer2.0.bn2](args = (%layer2_0_conv2,), kwargs = {})\n    %layer2_0_downsample_0 : [#users=1] = call_module[target=layer2.0.downsample.0](args = (%layer1_1_relu_1,), kwargs = {})\n    %layer2_0_downsample_1 : [#users=1] = call_module[target=layer2.0.downsample.1](args = (%layer2_0_downsample_0,), kwargs = {})\n    %add_2 : [#users=1] = call_function[target=operator.add](args = (%layer2_0_bn2, %layer2_0_downsample_1), kwargs = {})\n    %layer2_0_relu_1 : [#users=2] = call_module[target=layer2.0.relu](args = (%add_2,), kwargs = {})\n    %layer2_1_conv1 : [#users=1] = call_module[target=layer2.1.conv1](args = (%layer2_0_relu_1,), kwargs = {})\n    %layer2_1_bn1 : [#users=1] = call_module[target=layer2.1.bn1](args = (%layer2_1_conv1,), kwargs = {})\n    %layer2_1_relu : [#users=1] = call_module[target=layer2.1.relu](args = (%layer2_1_bn1,), kwargs = {})\n    %layer2_1_conv2 : [#users=1] = call_module[target=layer2.1.conv2](args = (%layer2_1_relu,), kwargs = {})\n    %layer2_1_bn2 : [#users=1] = call_module[target=layer2.1.bn2](args = (%layer2_1_conv2,), kwargs = {})\n    %add_3 : [#users=1] = call_function[target=operator.add](args = (%layer2_1_bn2, %layer2_0_relu_1), kwargs = {})\n    %layer2_1_relu_1 : [#users=2] = call_module[target=layer2.1.relu](args = (%add_3,), kwargs = {})\n    %layer3_0_conv1 : [#users=1] = call_module[target=layer3.0.conv1](args = (%layer2_1_relu_1,), kwargs = {})\n    %layer3_0_bn1 : [#users=1] = call_module[target=layer3.0.bn1](args = (%layer3_0_conv1,), kwargs = {})\n    %layer3_0_relu : [#users=1] = call_module[target=layer3.0.relu](args = (%layer3_0_bn1,), kwargs = {})\n    %layer3_0_conv2 : [#users=1] = call_module[target=layer3.0.conv2](args = (%layer3_0_relu,), kwargs = {})\n    %layer3_0_bn2 : [#users=1] = call_module[target=layer3.0.bn2](args = (%layer3_0_conv2,), kwargs = {})\n    %layer3_0_downsample_0 : [#users=1] = call_module[target=layer3.0.downsample.0](args = (%layer2_1_relu_1,), kwargs = {})\n    %layer3_0_downsample_1 : [#users=1] = call_module[target=layer3.0.downsample.1](args = (%layer3_0_downsample_0,), kwargs = {})\n    %add_4 : [#users=1] = call_function[target=operator.add](args = (%layer3_0_bn2, %layer3_0_downsample_1), kwargs = {})\n    %layer3_0_relu_1 : [#users=2] = call_module[target=layer3.0.relu](args = (%add_4,), kwargs = {})\n    %layer3_1_conv1 : [#users=1] = call_module[target=layer3.1.conv1](args = (%layer3_0_relu_1,), kwargs = {})\n    %layer3_1_bn1 : [#users=1] = call_module[target=layer3.1.bn1](args = (%layer3_1_conv1,), kwargs = {})\n    %layer3_1_relu : [#users=1] = call_module[target=layer3.1.relu](args = (%layer3_1_bn1,), kwargs = {})\n    %layer3_1_conv2 : [#users=1] = call_module[target=layer3.1.conv2](args = (%layer3_1_relu,), kwargs = {})\n    %layer3_1_bn2 : [#users=1] = call_module[target=layer3.1.bn2](args = (%layer3_1_conv2,), kwargs = {})\n    %add_5 : [#users=1] = call_function[target=operator.add](args = (%layer3_1_bn2, %layer3_0_relu_1), kwargs = {})\n    %layer3_1_relu_1 : [#users=2] = call_module[target=layer3.1.relu](args = (%add_5,), kwargs = {})\n    %layer4_0_conv1 : [#users=1] = call_module[target=layer4.0.conv1](args = (%layer3_1_relu_1,), kwargs = {})\n    %layer4_0_bn1 : [#users=1] = call_module[target=layer4.0.bn1](args = (%layer4_0_conv1,), kwargs = {})\n    %layer4_0_relu : [#users=1] = call_module[target=layer4.0.relu](args = (%layer4_0_bn1,), kwargs = {})\n    %layer4_0_conv2 : [#users=1] = call_module[target=layer4.0.conv2](args = (%layer4_0_relu,), kwargs = {})\n    %layer4_0_bn2 : [#users=1] = call_module[target=layer4.0.bn2](args = (%layer4_0_conv2,), kwargs = {})\n    %layer4_0_downsample_0 : [#users=1] = call_module[target=layer4.0.downsample.0](args = (%layer3_1_relu_1,), kwargs = {})\n    %layer4_0_downsample_1 : [#users=1] = call_module[target=layer4.0.downsample.1](args = (%layer4_0_downsample_0,), kwargs = {})\n    %add_6 : [#users=1] = call_function[target=operator.add](args = (%layer4_0_bn2, %layer4_0_downsample_1), kwargs = {})\n    %layer4_0_relu_1 : [#users=2] = call_module[target=layer4.0.relu](args = (%add_6,), kwargs = {})\n    %layer4_1_conv1 : [#users=1] = call_module[target=layer4.1.conv1](args = (%layer4_0_relu_1,), kwargs = {})\n    %layer4_1_bn1 : [#users=1] = call_module[target=layer4.1.bn1](args = (%layer4_1_conv1,), kwargs = {})\n    %layer4_1_relu : [#users=1] = call_module[target=layer4.1.relu](args = (%layer4_1_bn1,), kwargs = {})\n    %layer4_1_conv2 : [#users=1] = call_module[target=layer4.1.conv2](args = (%layer4_1_relu,), kwargs = {})\n    %layer4_1_bn2 : [#users=1] = call_module[target=layer4.1.bn2](args = (%layer4_1_conv2,), kwargs = {})\n    %add_7 : [#users=1] = call_function[target=operator.add](args = (%layer4_1_bn2, %layer4_0_relu_1), kwargs = {})\n    %layer4_1_relu_1 : [#users=1] = call_module[target=layer4.1.relu](args = (%add_7,), kwargs = {})\n    %avgpool : [#users=1] = call_module[target=avgpool](args = (%layer4_1_relu_1,), kwargs = {})\n    %flatten : [#users=1] = call_function[target=torch.flatten](args = (%avgpool, 1), kwargs = {})\n    %fc : [#users=1] = call_module[target=fc](args = (%flatten,), kwargs = {})\n    return fc",
                        "code"
                    ],
                    [
                        "This gives us a Graph representation of the ResNet18 model. A Graph\nconsists of a series of Nodes connected to each other. Each Node\nrepresents a call-site in the Python code (whether to a function,\na module, or a method) and the edges (represented as args and kwargs\non each node) represent the values passed between these call-sites. More\ninformation about the Graph representation and the rest of FX\u2019s APIs ca\nbe found at the FX documentation .",
                        "markdown"
                    ]
                ]
            },
            {
                "Creating a Profiling Interpreter": [
                    [
                        "Next, we are going to create a class that inherits from torch.fx.Interpreter.\nThough the GraphModule that symbolic_trace produces compiles Python code\nthat is run when you call a GraphModule, an alternative way to run a\nGraphModule is by executing each Node in the Graph one by one. That is\nthe functionality that Interpreter provides: It interprets the graph node-\nby-node.",
                        "markdown"
                    ],
                    [
                        "By inheriting from Interpreter, we can override various functionality and\ninstall the profiling behavior we want. The goal is to have an object to which\nwe can pass a model, invoke the model 1 or more times, then get statistics about\nhow long the model and each part of the model took during those runs.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s define our ProfilingInterpreter class:",
                        "markdown"
                    ],
                    [
                        "class ProfilingInterpreter():\n    def __init__(self, mod : ):\n        # Rather than have the user symbolically trace their model,\n        # we're going to do it in the constructor. As a result, the\n        # user can pass in any ``Module`` without having to worry about\n        # symbolic tracing APIs\n        gm = (mod)\n        super().__init__(gm)\n\n        # We are going to store away two things here:\n        #\n        # 1. A list of total runtimes for ``mod``. In other words, we are\n        #    storing away the time ``mod(...)`` took each time this\n        #    interpreter is called.\n        self.total_runtime_sec : List[float] = []\n        # 2. A map from ``Node`` to a list of times (in seconds) that\n        #    node took to run. This can be seen as similar to (1) but\n        #    for specific sub-parts of the model.\n        self.runtimes_sec : Dict[, List[float]] = {}\n\n    ######################################################################\n    # Next, let's override our first method: ``run()``. ``Interpreter``'s ``run``\n    # method is the top-level entrypoint for execution of the model. We will\n    # want to intercept this so that we can record the total runtime of the\n    # model.\n\n    def run(self, *args) -&gt; Any:\n        # Record the time we started running the model\n        t_start = time.time()\n        # Run the model by delegating back into Interpreter.run()\n        return_val = super().run(*args)\n        # Record the time we finished running the model\n        t_end = time.time()\n        # Store the total elapsed time this model execution took in the\n        # ProfilingInterpreter\n        self.total_runtime_sec.append(t_end - t_start)\n        return return_val\n\n    ######################################################################\n    # Now, let's override ``run_node``. ``Interpreter`` calls ``run_node`` each\n    # time it executes a single node. We will intercept this so that we\n    # can measure and record the time taken for each individual call in\n    # the model.\n\n    def run_node(self, n : ) -&gt; Any:\n        # Record the time we started running the op\n        t_start = time.time()\n        # Run the op by delegating back into Interpreter.run_node()\n        return_val = super().run_node(n)\n        # Record the time we finished running the op\n        t_end = time.time()\n        # If we don't have an entry for this node in our runtimes_sec\n        # data structure, add one with an empty list value.\n        self.runtimes_sec.setdefault(n, [])\n        # Record the total elapsed time for this single invocation\n        # in the runtimes_sec data structure\n        self.runtimes_sec[n].append(t_end - t_start)\n        return return_val\n\n    ######################################################################\n    # Finally, we are going to define a method (one which doesn't override\n    # any ``Interpreter`` method) that provides us a nice, organized view of\n    # the data we have collected.\n\n    def summary(self, should_sort : bool = False) -&gt; str:\n        # Build up a list of summary information for each node\n        node_summaries : List[List[Any]] = []\n        # Calculate the mean runtime for the whole network. Because the\n        # network may have been called multiple times during profiling,\n        # we need to summarize the runtimes. We choose to use the\n        # arithmetic mean for this.\n        mean_total_runtime = statistics.mean(self.total_runtime_sec)\n\n        # For each node, record summary statistics\n        for node, runtimes in self.runtimes_sec.items():\n            # Similarly, compute the mean runtime for ``node``\n            mean_runtime = statistics.mean(runtimes)\n            # For easier understanding, we also compute the percentage\n            # time each node took with respect to the whole network.\n            pct_total = mean_runtime / mean_total_runtime * 100\n            # Record the node's type, name of the node, mean runtime, and\n            # percent runtim\n            node_summaries.append(\n                [node.op, str(node), mean_runtime, pct_total])\n\n        # One of the most important questions to answer when doing performance\n        # profiling is \"Which op(s) took the longest?\". We can make this easy\n        # to see by providing sorting functionality in our summary view\n        if should_sort:\n            node_summaries.sort(key=lambda s: s[2], reverse=True)\n\n        # Use the ``tabulate`` library to create a well-formatted table\n        # presenting our summary information\n        headers : List[str] = [\n            'Op type', 'Op', 'Average runtime (s)', 'Pct total runtime'\n        ]\n        return tabulate.tabulate(node_summaries, headers=headers)",
                        "code"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "We use Python\u2019s time.time function to pull wall clock\ntimestamps and compare them. This is not the most accurate\nway to measure performance, and will only give us a first-\norder approximation. We use this simple technique only for the\npurpose of demonstration in this tutorial.",
                        "markdown"
                    ]
                ]
            },
            {
                "Investigating the Performance of ResNet18": [
                    [
                        "We can now use ProfilingInterpreter to inspect the performance\ncharacteristics of our ResNet18 model;",
                        "markdown"
                    ],
                    [
                        "interp = (rn18)\n(input)\nprint(interp.summary(True))",
                        "code"
                    ],
                    [
                        "Op type        Op                       Average runtime (s)    Pct total runtime\n-------------  ---------------------  ---------------------  -------------------\ncall_module    maxpool                          0.0186694              9.70712\ncall_module    conv1                            0.0158079              8.21929\ncall_module    layer1_1_conv1                   0.0112045              5.82576\ncall_module    layer4_0_conv2                   0.0105081              5.46366\ncall_module    layer1_1_conv2                   0.0103536              5.38333\ncall_module    layer1_0_conv1                   0.0103078              5.35952\ncall_module    layer1_0_conv2                   0.00978041             5.08531\ncall_module    layer4_1_conv1                   0.008847               4.59999\ncall_module    layer4_1_conv2                   0.00849557             4.41726\ncall_module    layer2_1_conv2                   0.00840592             4.37065\ncall_module    layer3_1_conv2                   0.00786257             4.08813\ncall_module    layer3_1_conv1                   0.00758052             3.94148\ncall_module    layer2_0_conv2                   0.00745034             3.8738\ncall_module    layer3_0_conv2                   0.00727534             3.78281\ncall_module    layer2_0_conv1                   0.00721908             3.75355\ncall_module    layer2_1_conv1                   0.00681758             3.54479\ncall_module    bn1                              0.00668931             3.4781\ncall_module    layer4_0_conv1                   0.00650167             3.38054\ncall_module    layer3_0_conv1                   0.00450587             2.34282\ncall_function  add_1                            0.0017252              0.897014\ncall_module    layer1_1_bn2                     0.00153756             0.799454\ncall_function  add                              0.00151062             0.785445\ncall_module    layer2_0_downsample_0            0.00104856             0.5452\ncall_module    layer3_0_downsample_0            0.000977039            0.508011\ncall_module    relu                             0.000867844            0.451234\ncall_module    layer4_0_downsample_0            0.00076437             0.397433\ncall_function  add_3                            0.00070715             0.367682\ncall_module    layer1_0_bn2                     0.000444412            0.231072\ncall_module    layer1_0_bn1                     0.00043869             0.228097\ncall_module    layer1_1_bn1                     0.000427485            0.22227\ncall_module    fc                               0.000384808            0.20008\ncall_module    layer4_1_bn1                     0.000296116            0.153965\ncall_module    layer2_0_bn1                     0.000295162            0.153469\ncall_module    avgpool                          0.00028038             0.145783\ncall_module    layer2_0_bn2                     0.000272751            0.141817\ncall_module    layer2_0_downsample_1            0.000244379            0.127065\ncall_module    layer1_1_relu_1                  0.00023818             0.123842\ncall_module    layer4_0_bn1                     0.000236511            0.122974\ncall_module    layer2_1_bn2                     0.000233173            0.121238\ncall_module    layer2_1_bn1                     0.000229359            0.119255\ncall_module    layer1_0_relu_1                  0.000216007            0.112313\ncall_module    layer1_0_relu                    0.000213623            0.111073\ncall_module    layer3_0_bn1                     0.000210524            0.109462\ncall_module    layer3_0_downsample_1            0.000199795            0.103883\ncall_module    layer3_1_bn1                     0.00019455             0.101156\ncall_module    layer3_1_bn2                     0.000190496            0.0990484\ncall_module    layer3_0_bn2                     0.000186682            0.097065\ncall_function  add_2                            0.000185966            0.0966931\ncall_module    layer1_1_relu                    0.000185013            0.0961972\ncall_module    layer2_0_relu                    0.000171185            0.0890072\ncall_module    layer4_1_bn2                     0.000170231            0.0885114\ncall_module    layer4_0_downsample_1            0.000168324            0.0875196\ncall_module    layer4_0_bn2                     0.00016284             0.0846684\ncall_function  add_4                            0.000138521            0.072024\ncall_module    layer2_1_relu_1                  0.000127316            0.0661976\ncall_module    layer2_1_relu                    0.000125647            0.0653298\ncall_module    layer2_0_relu_1                  0.000118494            0.0616109\ncall_function  add_6                            0.000116587            0.0606191\ncall_function  add_5                            0.00011611             0.0603712\ncall_module    layer3_0_relu_1                  0.000106573            0.0554126\ncall_module    layer3_1_relu                    9.34601e-05            0.0485945\ncall_module    layer3_1_relu_1                  9.29832e-05            0.0483465\ncall_module    layer4_0_relu_1                  9.01222e-05            0.046859\ncall_module    layer4_0_relu                    8.98838e-05            0.046735\ncall_module    layer3_0_relu                    8.63075e-05            0.0448755\ncall_module    layer4_1_relu                    8.24928e-05            0.0428921\ncall_function  add_7                            7.4625e-05             0.0388012\ncall_module    layer4_1_relu_1                  7.43866e-05            0.0386772\ncall_function  flatten                          6.93798e-05            0.036074\noutput         output                           2.86102e-05            0.0148759\nplaceholder    x                                2.5034e-05             0.0130164",
                        "code"
                    ],
                    [
                        "There are two things we should call out here:",
                        "markdown"
                    ],
                    [
                        "MaxPool2d takes up the most time. This is a known issue:",
                        "markdown"
                    ],
                    [
                        "BatchNorm2d also takes up significant time. We can continue this\nline of thinking and optimize this in the Conv-BN Fusion with FX\n.",
                        "markdown"
                    ]
                ]
            },
            {
                "Conclusion": [
                    [
                        "As we can see, using FX we can easily capture PyTorch programs (even\nones we don\u2019t have the source code for!) in a machine-interpretable\nformat and use that for analysis, such as the performance analysis\nwe\u2019ve done here. FX opens up an exiciting world of possibilities for\nworking with PyTorch programs.",
                        "markdown"
                    ],
                    [
                        "Finally, since FX is still in beta, we would be happy to hear any\nfeedback you have about using it. Please feel free to use the\nPyTorch Forums () and the issue tracker\n() to provide any feedback\nyou might have.",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.687 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ]
    },
    "Frontend APIs": {
        "(beta) Channels Last Memory Format in PyTorch": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            {
                "What is Channels Last": [
                    [
                        "Channels last memory format is an alternative way of ordering NCHW tensors in memory preserving dimensions ordering. Channels last tensors ordered in such a way that channels become the densest dimension (aka storing images pixel-per-pixel).",
                        "markdown"
                    ],
                    [
                        "For example, classic (contiguous) storage of NCHW tensor (in our case it is two 4x4 images with 3 color channels) look like this:\n\n<img alt=\"classic_memory_format\" src=\"../_images/classic_memory_format.png\"/>",
                        "markdown"
                    ],
                    [
                        "Channels last memory format orders data differently:\n\n<img alt=\"channels_last_memory_format\" src=\"../_images/channels_last_memory_format.png\"/>",
                        "markdown"
                    ],
                    [
                        "Pytorch supports memory formats (and provides back compatibility with existing models including eager, JIT, and TorchScript) by utilizing  existing strides structure.\nFor example, 10x3x16x16 batch in Channels last format will have strides equal to (768, 1, 48, 3).",
                        "markdown"
                    ],
                    [
                        "Channels last memory format is implemented for 4D NCHW Tensors only.",
                        "markdown"
                    ]
                ]
            },
            {
                "Memory Format API": [
                    [
                        "Here is how to convert tensors between contiguous and channels\nlast memory formats.",
                        "markdown"
                    ],
                    [
                        "Classic PyTorch contiguous tensor",
                        "markdown"
                    ],
                    [
                        "import torch\n\nN, C, H, W = 10, 3, 32, 32\n = (N, C, H, W)\nprint(.stride())  # Ouputs: (3072, 1024, 32, 1)",
                        "code"
                    ],
                    [
                        "(3072, 1024, 32, 1)",
                        "code"
                    ],
                    [
                        "Conversion operator",
                        "markdown"
                    ],
                    [
                        " = .to(memory_format=)\nprint(.shape)  # Outputs: (10, 3, 32, 32) as dimensions order preserved\nprint(.stride())  # Outputs: (3072, 1, 96, 3)",
                        "code"
                    ],
                    [
                        "torch.Size([10, 3, 32, 32])\n(3072, 1, 96, 3)",
                        "code"
                    ],
                    [
                        "Back to contiguous",
                        "markdown"
                    ],
                    [
                        " = .to(memory_format=)\nprint(.stride())  # Outputs: (3072, 1024, 32, 1)",
                        "code"
                    ],
                    [
                        "(3072, 1024, 32, 1)",
                        "code"
                    ],
                    [
                        "Alternative option",
                        "markdown"
                    ],
                    [
                        " = .contiguous(memory_format=)\nprint(.stride())  # Ouputs: (3072, 1, 96, 3)",
                        "code"
                    ],
                    [
                        "(3072, 1, 96, 3)",
                        "code"
                    ],
                    [
                        "Format checks",
                        "markdown"
                    ],
                    [
                        "print(.is_contiguous(memory_format=))  # Ouputs: True",
                        "code"
                    ],
                    [
                        "True",
                        "code"
                    ],
                    [
                        "There are minor difference between the two APIs to and\ncontiguous. We suggest to stick with to when explicitly\nconverting memory format of tensor.",
                        "markdown"
                    ],
                    [
                        "For general cases the two APIs behave the same. However in special\ncases for a 4D tensor with size NCHW when either: C==1 or\nH==1 &amp;&amp; W==1, only to would generate a proper stride to\nrepresent channels last memory format.",
                        "markdown"
                    ],
                    [
                        "This is because in either of the two cases above, the memory format\nof a tensor is ambiguous, i.e. a contiguous tensor with size\nN1HW is both contiguous and channels last in memory storage.\nTherefore, they are already considered as is_contiguous\nfor the given memory format and hence contiguous call becomes a\nno-op and would not update the stride. On the contrary, to\nwould restride tensor with a meaningful stride on dimensions whose\nsizes are 1 in order to properly represent the intended memory\nformat",
                        "markdown"
                    ],
                    [
                        " = (4, 1, 4, 4)\nprint(.is_contiguous(memory_format=))  # Ouputs: True\nprint(.is_contiguous(memory_format=))  # Ouputs: True",
                        "code"
                    ],
                    [
                        "True\nTrue",
                        "code"
                    ],
                    [
                        "Same thing applies to explicit permutation API permute. In\nspecial case where ambiguity could occur, permute does not\nguarantee to produce a stride that properly carry the intended\nmemory format. We suggest to use to with explicit memory format\nto avoid unintended behavior.",
                        "markdown"
                    ],
                    [
                        "And a side note that in the extreme case, where three non-batch\ndimensions are all equal to 1 (C==1 &amp;&amp; H==1 &amp;&amp; W==1),\ncurrent implementation cannot mark a tensor as channels last memory\nformat.",
                        "markdown"
                    ],
                    [
                        "Create as channels last",
                        "markdown"
                    ],
                    [
                        " = (N, C, H, W, memory_format=)\nprint(.stride())  # Ouputs: (3072, 1, 96, 3)",
                        "code"
                    ],
                    [
                        "(3072, 1, 96, 3)",
                        "code"
                    ],
                    [
                        "clone preserves memory format",
                        "markdown"
                    ],
                    [
                        " = .clone()\nprint(.stride())  # Ouputs: (3072, 1, 96, 3)",
                        "code"
                    ],
                    [
                        "(3072, 1, 96, 3)",
                        "code"
                    ],
                    [
                        "to, cuda, float \u2026 preserves memory format",
                        "markdown"
                    ],
                    [
                        "if ():\n     = .cuda()\n    print(.stride())  # Ouputs: (3072, 1, 96, 3)",
                        "code"
                    ],
                    [
                        "(3072, 1, 96, 3)",
                        "code"
                    ],
                    [
                        "empty_like, *_like operators preserves memory format",
                        "markdown"
                    ],
                    [
                        " = ()\nprint(.stride())  # Ouputs: (3072, 1, 96, 3)",
                        "code"
                    ],
                    [
                        "(3072, 1, 96, 3)",
                        "code"
                    ],
                    [
                        "Pointwise operators preserves memory format",
                        "markdown"
                    ],
                    [
                        " =  + \nprint(.stride())  # Ouputs: (3072, 1, 96, 3)",
                        "code"
                    ],
                    [
                        "(3072, 1, 96, 3)",
                        "code"
                    ],
                    [
                        "Conv, Batchnorm modules using cudnn backends support channels last\n(only works for CudNN &gt;= 7.6). Convolution modules, unlike binary\np-wise operator, have channels last as the dominating memory format.\nIFF all inputs are in contiguous memory format, the operator\nproduces output in contiguous memory format. Otherwise, output wil\nbe in channels last memroy format.",
                        "markdown"
                    ],
                    [
                        "if () &gt;= 7603:\n     = (8, 4, 3).cuda().half()\n     = (memory_format=)  # Module parameters need to be channels last\n\n    input = (1, 10, (2, 8, 4, 4), dtype=, requires_grad=True)\n    input = input.to(device=\"cuda\", memory_format=, dtype=)\n\n     = (input)\n    print(.is_contiguous(memory_format=))  # Ouputs: True",
                        "code"
                    ],
                    [
                        "True",
                        "code"
                    ],
                    [
                        "When input tensor reaches a operator without channels last support,\na permutation should automatically apply in the kernel to restore\ncontiguous on input tensor. This introduces overhead and stops the\nchannels last memory format propagation. Nevertheless, it guarantees\ncorrect output.",
                        "markdown"
                    ]
                ]
            },
            {
                "Performance Gains": [
                    [
                        "Channels last memory format optimizations are available on both GPU and CPU.\nOn GPU, the most significant performance gains are observed on NVidia\u2019s\nhardware with Tensor Cores support running on reduced precision\n(torch.float16).\nWe were able to archive over 22% perf gains with channels last\ncomparing to contiguous format, both while utilizing\n\u2018AMP (Automated Mixed Precision)\u2019 training scripts.\nOur scripts uses AMP supplied by NVidia\n.",
                        "markdown"
                    ],
                    [
                        "python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2\u00a0 ./data",
                        "markdown"
                    ],
                    [
                        "# opt_level = O2\n# keep_batchnorm_fp32 = None &lt;class 'NoneType'&gt;\n# loss_scale = None &lt;class 'NoneType'&gt;\n# CUDNN VERSION: 7603\n# =&gt; creating model 'resnet50'\n# Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n# Defaults for this optimization level are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n# Processing user overrides (additional kwargs that are not None)...\n# After processing overrides, optimization options are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n# Epoch: [0][10/125] Time 0.866 (0.866) Speed 230.949 (230.949) Loss 0.6735125184 (0.6735) Prec@1 61.000 (61.000) Prec@5 100.000 (100.000)\n# Epoch: [0][20/125] Time 0.259 (0.562) Speed 773.481 (355.693) Loss 0.6968704462 (0.6852) Prec@1 55.000 (58.000) Prec@5 100.000 (100.000)\n# Epoch: [0][30/125] Time 0.258 (0.461) Speed 775.089 (433.965) Loss 0.7877287269 (0.7194) Prec@1 51.500 (55.833) Prec@5 100.000 (100.000)\n# Epoch: [0][40/125] Time 0.259 (0.410) Speed 771.710 (487.281) Loss 0.8285319805 (0.7467) Prec@1 48.500 (54.000) Prec@5 100.000 (100.000)\n# Epoch: [0][50/125] Time 0.260 (0.380) Speed 770.090 (525.908) Loss 0.7370464802 (0.7447) Prec@1 56.500 (54.500) Prec@5 100.000 (100.000)\n# Epoch: [0][60/125] Time 0.258 (0.360) Speed 775.623 (555.728) Loss 0.7592862844 (0.7472) Prec@1 51.000 (53.917) Prec@5 100.000 (100.000)\n# Epoch: [0][70/125] Time 0.258 (0.345) Speed 774.746 (579.115) Loss 1.9698858261 (0.9218) Prec@1 49.500 (53.286) Prec@5 100.000 (100.000)\n# Epoch: [0][80/125] Time 0.260 (0.335) Speed 770.324 (597.659) Loss 2.2505953312 (1.0879) Prec@1 50.500 (52.938) Prec@5 100.000 (100.000)",
                        "code"
                    ],
                    [
                        "Passing --channels-last true allows running a model in Channels last format with observed 22% perf gain.",
                        "markdown"
                    ],
                    [
                        "python main_amp.py -a resnet50 --b 200 --workers 16 --opt-level O2 --channels-last true ./data",
                        "markdown"
                    ],
                    [
                        "# opt_level = O2\n# keep_batchnorm_fp32 = None &lt;class 'NoneType'&gt;\n# loss_scale = None &lt;class 'NoneType'&gt;\n#\n# CUDNN VERSION: 7603\n#\n# =&gt; creating model 'resnet50'\n# Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n#\n# Defaults for this optimization level are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n# Processing user overrides (additional kwargs that are not None)...\n# After processing overrides, optimization options are:\n# enabled                : True\n# opt_level              : O2\n# cast_model_type        : torch.float16\n# patch_torch_functions  : False\n# keep_batchnorm_fp32    : True\n# master_weights         : True\n# loss_scale             : dynamic\n#\n# Epoch: [0][10/125] Time 0.767 (0.767) Speed 260.785 (260.785) Loss 0.7579724789 (0.7580) Prec@1 53.500 (53.500) Prec@5 100.000 (100.000)\n# Epoch: [0][20/125] Time 0.198 (0.482) Speed 1012.135 (414.716) Loss 0.7007197738 (0.7293) Prec@1 49.000 (51.250) Prec@5 100.000 (100.000)\n# Epoch: [0][30/125] Time 0.198 (0.387) Speed 1010.977 (516.198) Loss 0.7113101482 (0.7233) Prec@1 55.500 (52.667) Prec@5 100.000 (100.000)\n# Epoch: [0][40/125] Time 0.197 (0.340) Speed 1013.023 (588.333) Loss 0.8943189979 (0.7661) Prec@1 54.000 (53.000) Prec@5 100.000 (100.000)\n# Epoch: [0][50/125] Time 0.198 (0.312) Speed 1010.541 (641.977) Loss 1.7113249302 (0.9551) Prec@1 51.000 (52.600) Prec@5 100.000 (100.000)\n# Epoch: [0][60/125] Time 0.198 (0.293) Speed 1011.163 (683.574) Loss 5.8537774086 (1.7716) Prec@1 50.500 (52.250) Prec@5 100.000 (100.000)\n# Epoch: [0][70/125] Time 0.198 (0.279) Speed 1011.453 (716.767) Loss 5.7595844269 (2.3413) Prec@1 46.500 (51.429) Prec@5 100.000 (100.000)\n# Epoch: [0][80/125] Time 0.198 (0.269) Speed 1011.827 (743.883) Loss 2.8196096420 (2.4011) Prec@1 47.500 (50.938) Prec@5 100.000 (100.000)",
                        "code"
                    ],
                    [
                        "The following list of models has the full support of Channels last and showing 8%-35% perf gains on Volta devices:\nalexnet, mnasnet0_5, mnasnet0_75, mnasnet1_0, mnasnet1_3, mobilenet_v2, resnet101, resnet152, resnet18, resnet34, resnet50, resnext50_32x4d, shufflenet_v2_x0_5, shufflenet_v2_x1_0, shufflenet_v2_x1_5, shufflenet_v2_x2_0, squeezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, vgg19_bn, wide_resnet101_2, wide_resnet50_2",
                        "markdown"
                    ],
                    [
                        "The following list of models has the full support of Channels last and showing 26%-76% perf gains on Intel(R) Xeon(R) Ice Lake (or newer) CPUs:\nalexnet, densenet121, densenet161, densenet169, googlenet, inception_v3, mnasnet0_5, mnasnet1_0, resnet101, resnet152, resnet18, resnet34, resnet50, resnext101_32x8d, resnext50_32x4d, shufflenet_v2_x0_5, shufflenet_v2_x1_0, squeezenet1_0, squeezenet1_1, vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19, vgg19_bn, wide_resnet101_2, wide_resnet50_2",
                        "markdown"
                    ]
                ]
            },
            {
                "Converting existing models": [
                    [
                        "Channels last support is not limited by existing models, as any\nmodel can be converted to channels last and propagate format through\nthe graph as soon as input (or certain weight) is formatted\ncorrectly.",
                        "markdown"
                    ],
                    [
                        "# Need to be done once, after model initialization (or load)\n = (memory_format=)  # Replace with your model\n\n# Need to be done for every input\ninput = input.to(memory_format=)  # Replace with your input\n = (input)",
                        "code"
                    ],
                    [
                        "However, not all operators fully converted to support channels last\n(usually returning contiguous output instead). In the example posted\nabove, layers that does not support channels last will stop the\nmemory format propagation. In spite of that, as we have converted the\nmodel to channels last format, that means each convolution layer,\nwhich has its 4 dimensional weight in channels last memory format,\nwill restore channels last memory format and benefit from faster\nkernels.",
                        "markdown"
                    ],
                    [
                        "But operators that does not support channels last does introduce\noverhead by permutation. Optionally, you can investigate and identify\noperators in your model that does not support channels last, if you\nwant to improve the performance of converted model.",
                        "markdown"
                    ],
                    [
                        "That means you need to verify the list of used operators\nagainst supported operators list ,\nor introduce memory format checks into eager execution mode and run your model.",
                        "markdown"
                    ],
                    [
                        "After running the code below, operators will raise an exception if the output of the\noperator doesn\u2019t match the memory format of the input.",
                        "markdown"
                    ],
                    [
                        "def contains_cl(args):\n    for t in args:\n        if isinstance(t, ):\n            if t.is_contiguous(memory_format=) and not t.is_contiguous():\n                return True\n        elif isinstance(t, list) or isinstance(t, tuple):\n            if contains_cl(list(t)):\n                return True\n    return False\n\n\ndef print_inputs(args, indent=\"\"):\n    for t in args:\n        if isinstance(t, ):\n            print(indent, t.stride(), t.shape, t.device, t.dtype)\n        elif isinstance(t, list) or isinstance(t, tuple):\n            print(indent, type(t))\n            print_inputs(list(t), indent=indent + \"    \")\n        else:\n            print(indent, t)\n\n\ndef check_wrapper(fn):\n    name = fn.__name__\n\n    def check_cl(*args, **kwargs):\n        was_cl = contains_cl(args)\n        try:\n            result = fn(*args, **kwargs)\n        except Exception as e:\n            print(\"`{}` inputs are:\".format(name))\n            print_inputs(args)\n            print(\"-------------------\")\n            raise e\n        failed = False\n        if was_cl:\n            if isinstance(result, ):\n                if result.dim() == 4 and not result.is_contiguous(memory_format=):\n                    print(\n                        \"`{}` got channels_last input, but output is not channels_last:\".format(name),\n                        result.shape,\n                        result.stride(),\n                        result.device,\n                        result.dtype,\n                    )\n                    failed = True\n        if failed and True:\n            print(\"`{}` inputs are:\".format(name))\n            print_inputs(args)\n            raise Exception(\"Operator `{}` lost channels_last property\".format(name))\n        return result\n\n    return check_cl\n\n\nold_attrs = dict()\n\n\ndef attribute(m):\n    old_attrs[m] = dict()\n    for i in dir(m):\n        e = getattr(m, i)\n        exclude_functions = [\"is_cuda\", \"has_names\", \"numel\", \"stride\", \"Tensor\", \"is_contiguous\", \"__class__\"]\n        if i not in exclude_functions and not i.startswith(\"_\") and \"__call__\" in dir(e):\n            try:\n                old_attrs[m][i] = e\n                setattr(m, i, check_wrapper(e))\n            except Exception as e:\n                print(i)\n                print(e)\n\n\nattribute()\nattribute(torch.nn.functional)\nattribute(torch)",
                        "code"
                    ],
                    [
                        "If you found an operator that doesn\u2019t support channels last tensors\nand you want to contribute, feel free to use following developers\nguide .",
                        "markdown"
                    ],
                    [
                        "Code below is to recover the attributes of torch.",
                        "markdown"
                    ],
                    [
                        "for (m, attrs) in old_attrs.items():\n    for (k, v) in attrs.items():\n        setattr(m, k, v)",
                        "code"
                    ]
                ]
            },
            {
                "Work to do": [
                    [
                        "There are still many things to do, such as:",
                        "markdown"
                    ],
                    [
                        "Resolving ambiguity of N1HW and NC11 Tensors;",
                        "markdown"
                    ],
                    [
                        "Testing of Distributed Training support;",
                        "markdown"
                    ],
                    [
                        "Improving operators coverage.",
                        "markdown"
                    ],
                    [
                        "If you have feedback and/or suggestions for improvement, please let us\nknow by creating .",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  1.886 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Forward-mode Automatic Differentiation (Beta)": [
            [
                "This tutorial demonstrates how to use forward-mode AD to compute\ndirectional derivatives (or equivalently, Jacobian-vector products).",
                "markdown"
            ],
            [
                "The tutorial below uses some APIs only available in versions &gt;= 1.11\n(or nightly builds).",
                "markdown"
            ],
            [
                "Also note that forward-mode AD is currently in beta. The API is\nsubject to change and operator coverage is still incomplete.",
                "markdown"
            ],
            {
                "Basic Usage": [
                    [
                        "Unlike reverse-mode AD, forward-mode AD computes gradients eagerly\nalongside the forward pass. We can use forward-mode AD to compute a\ndirectional derivative by performing the forward pass as before,\nexcept we first associate our input with another tensor representing\nthe direction of the directional derivative (or equivalently, the v\nin a Jacobian-vector product). When an input, which we call \u201cprimal\u201d, is\nassociated with a \u201cdirection\u201d tensor, which we call \u201ctangent\u201d, the\nresultant new tensor object is called a \u201cdual tensor\u201d for its connection\nto dual numbers[0].",
                        "markdown"
                    ],
                    [
                        "As the forward pass is performed, if any input tensors are dual tensors,\nextra computation is performed to propogate this \u201csensitivity\u201d of the\nfunction.",
                        "markdown"
                    ],
                    [
                        "import torch\nimport torch.autograd.forward_ad as fwAD\n\n = (10, 10)\n = (10, 10)\n\ndef fn(x, ):\n    return x ** 2 +  ** 2\n\n# All forward AD computation must be performed in the context of\n# a ``dual_level`` context. All dual tensors created in such a context\n# will have their tangents destroyed upon exit. This is to ensure that\n# if the output or intermediate results of this computation are reused\n# in a future forward AD computation, their tangents (which are associated\n# with this computation) won't be confused with tangents from the later\n# computation.\nwith ():\n    # To create a dual tensor we associate a tensor, which we call the\n    # primal with another tensor of the same size, which we call the tangent.\n    # If the layout of the tangent is different from that of the primal,\n    # The values of the tangent are copied into a new tensor with the same\n    # metadata as the primal. Otherwise, the tangent itself is used as-is.\n    #\n    # It is also important to note that the dual tensor created by\n    # ``make_dual`` is a view of the primal.\n     = (, )\n    assert (). is \n\n    # To demonstrate the case where the copy of the tangent happens,\n    # we pass in a tangent with a layout different from that of the primal\n     = (, )\n    assert (). is not \n\n    # Tensors that do not have an associated tangent are automatically\n    # considered to have a zero-filled tangent of the same shape.\n     = (10, 10)\n     = fn(, )\n\n    # Unpacking the dual returns a namedtuple with ``primal`` and ``tangent``\n    # as attributes\n     = ().\n\nassert (). is None",
                        "code"
                    ]
                ]
            },
            {
                "Usage with Modules": [
                    [
                        "To use nn.Module with forward AD, replace the parameters of your\nmodel with dual tensors before performing the forward pass. At the\ntime of writing, it is not possible to create dual tensor\nnn.Parameter`s. As a workaround, one must register the dual tensor\nas a non-parameter attribute of the module.",
                        "markdown"
                    ],
                    [
                        "import torch.nn as nn\n\n = (5, 5)\ninput = (16, 5)\n\nparams = {name:  for name,  in ()}\ntangents = {name: () for name,  in params.items()}\n\nwith ():\n    for name,  in params.items():\n        delattr(, name)\n        setattr(, name, (, tangents[name]))\n\n     = (input)\n     = ().",
                        "code"
                    ]
                ]
            },
            {
                "Using the functional Module API (beta)": [
                    [
                        "Another way to use nn.Module with forward AD is to utilize\nthe functional Module API (also known as the stateless Module API).",
                        "markdown"
                    ],
                    [
                        "from torch.func import \n\n# We need a fresh module because the functional call requires the\n# the model to have parameters registered.\n = (5, 5)\n\ndual_params = {}\nwith ():\n    for name,  in params.items():\n        # Using the same ``tangents`` from the above section\n        dual_params[name] = (, tangents[name])\n     = (, dual_params, input)\n     = ().\n\n# Check our results\nassert (, )",
                        "code"
                    ]
                ]
            },
            {
                "Custom autograd Function": [
                    [
                        "Custom Functions also support forward-mode AD. To create custom Function\nsupporting forward-mode AD, register the jvp() static method. It is\npossible, but not mandatory for custom Functions to support both forward\nand backward AD. See the\n\nfor more information.",
                        "markdown"
                    ],
                    [
                        "class Fn():\n    @staticmethod\n    def forward(ctx, foo):\n        result = (foo)\n        # Tensors stored in ctx can be used in the subsequent forward grad\n        # computation.\n        ctx.result = result\n        return result\n\n    @staticmethod\n    def jvp(ctx, gI):\n        gO = gI * ctx.result\n        # If the tensor stored in ctx will not also be used in the backward pass,\n        # one can manually free it using ``del``\n        del ctx.result\n        return gO\n\nfn = Fn.apply\n\n = (10, 10, dtype=, requires_grad=True)\n = (10, 10)\n\nwith ():\n     = (, )\n     = fn()\n     = ().\n\n# It is important to use ``autograd.gradcheck`` to verify that your\n# custom autograd Function computes the gradients correctly. By default,\n# gradcheck only checks the backward-mode (reverse-mode) AD gradients. Specify\n# ``check_forward_ad=True`` to also check forward grads. If you did not\n# implement the backward formula for your function, you can also tell gradcheck\n# to skip the tests that require backward-mode AD by specifying\n# ``check_backward_ad=False``, ``check_undefined_grad=False``, and\n# ``check_batched_grad=False``.\n(Fn.apply, (,), check_forward_ad=True,\n                         check_backward_ad=False, check_undefined_grad=False,\n                         check_batched_grad=False)",
                        "code"
                    ],
                    [
                        "True",
                        "code"
                    ]
                ]
            },
            {
                "Functional API (beta)": [
                    [
                        "We also offer a higher-level functional API in functorch\nfor computing Jacobian-vector products that you may find simpler to use\ndepending on your use case.",
                        "markdown"
                    ],
                    [
                        "The benefit of the functional API is that there isn\u2019t a need to understand\nor use the lower-level dual tensor API and that you can compose it with\nother ;\nthe downside is that it offers you less control.",
                        "markdown"
                    ],
                    [
                        "Note that the remainder of this tutorial will require functorch\n() to run. Please find installation\ninstructions at the specified link.",
                        "markdown"
                    ],
                    [
                        "import functorch as ft\n\n = (10, 10)\n = (10, 10)\n = (10, 10)\n = (10, 10)\n\ndef fn(x, ):\n    return x ** 2 +  ** 2\n\n# Here is a basic example to compute the JVP of the above function.\n# The jvp(func, primals, tangents) returns func(*primals) as well as the\n# computed jvp. Each primal must be associated with a tangent of the same shape.\n,  = ft.(fn, (, ), (, ))\n\n# functorch.jvp requires every primal to be associated with a tangent.\n# If we only want to associate certain inputs to `fn` with tangents,\n# then we'll need to create a new function that captures inputs without tangents:\n = (10, 10)\n = (10, 10)\n = (10, 10)\n\nimport functools\nnew_fn = functools.partial(fn, =)\n,  = ft.(new_fn, (,), (,))",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torch/_functorch/deprecated.py:74: UserWarning:\n\nWe've integrated functorch into PyTorch. As the final step of the integration, functorch.jvp is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch &gt;= 2.3. Please use torch.func.jvp instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html",
                        "code"
                    ]
                ]
            },
            {
                "Using the functional API with Modules": [
                    [
                        "To use nn.Module with functorch.jvp to compute Jacobian-vector products\nwith respect to the model parameters, we need to reformulate the\nnn.Module as a function that accepts both the model parameters and inputs\nto the module.",
                        "markdown"
                    ],
                    [
                        " = (5, 5)\ninput = (16, 5)\ntangents = tuple([() for  in ()])\n\n# Given a torch.nn.Module, ft.make_functional_with_buffers extracts the state\n# (params and buffers) and returns a functional version of the model that\n# can be invoked like a function.\n# That is, the returned ``func`` can be invoked like\n# ``func(params, buffers, input)``.\n# ft.make_functional_with_buffers is analogous to the nn.Modules stateless API\n# that you saw previously and we're working on consolidating the two.\nfunc, params, buffers = ft.make_functional_with_buffers()\n\n# Because jvp requires every input to be associated with a tangent, we need to\n# create a new function that, when given the parameters, produces the output\ndef func_params_only(params):\n    return func(params, buffers, input)\n\n,  = ft.(func_params_only, (params,), (tangents,))",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torch/_functorch/deprecated.py:101: UserWarning:\n\nWe've integrated functorch into PyTorch. As the final step of the integration, functorch.make_functional_with_buffers is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch &gt;= 2.3. Please use torch.func.functional_call instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n\n/opt/conda/lib/python3.10/site-packages/torch/_functorch/deprecated.py:74: UserWarning:\n\nWe've integrated functorch into PyTorch. As the final step of the integration, functorch.jvp is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch &gt;= 2.3. Please use torch.func.jvp instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html",
                        "code"
                    ],
                    [
                        "[0] ",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.161 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Jacobians, Hessians, hvp, vhp, and more: composing function transforms": [
            [
                "Computing jacobians or hessians are useful in a number of non-traditional\ndeep learning models. It is difficult (or annoying) to compute these quantities\nefficiently using PyTorch\u2019s regular autodiff APIs\n(Tensor.backward(), torch.autograd.grad). PyTorch\u2019s\n\n\nprovides ways of computing various higher-order autodiff quantities\nefficiently.",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                "This tutorial requires PyTorch 2.0.0 or later.",
                "markdown"
            ],
            {
                "Computing the Jacobian": [
                    [
                        "import torch\nimport torch.nn.functional as F\nfrom functools import partial\n = (0)",
                        "code"
                    ],
                    [
                        "Let\u2019s start with a function that we\u2019d like to compute the jacobian of.\nThis is a simple linear function with non-linear activation.",
                        "markdown"
                    ],
                    [
                        "def predict(, , ):\n    return (, , ).tanh()",
                        "code"
                    ],
                    [
                        "Let\u2019s add some dummy data: a weight, a bias, and a feature vector x.",
                        "markdown"
                    ],
                    [
                        "D = 16\n = (D, D)\n = (D)\n = (D)  # feature vector",
                        "code"
                    ],
                    [
                        "Let\u2019s think of predict as a function that maps the input x from \\(R^D \\to R^D\\).\nPyTorch Autograd computes vector-Jacobian products. In order to compute the full\nJacobian of this \\(R^D \\to R^D\\) function, we would have to compute it row-by-row\nby using a different unit vector each time.",
                        "markdown"
                    ],
                    [
                        "def compute_jac():\n    jacobian_rows = [(predict(, , ), , vec)[0]\n                     for vec in ]\n    return (jacobian_rows)\n\n = .clone().requires_grad_()\n = (D)\n\n = compute_jac()\n\nprint(.shape)\nprint([0])  # show first row",
                        "code"
                    ],
                    [
                        "torch.Size([16, 16])\ntensor([-0.5956, -0.6096, -0.1326, -0.2295,  0.4490,  0.3661, -0.1672, -1.1190,\n         0.1705, -0.6683,  0.1851,  0.1630,  0.0634,  0.6547,  0.5908, -0.1308])",
                        "code"
                    ],
                    [
                        "Instead of computing the jacobian row-by-row, we can use PyTorch\u2019s\ntorch.vmap function transform to get rid of the for-loop and vectorize the\ncomputation. We can\u2019t directly apply vmap to torch.autograd.grad;\ninstead, PyTorch provides a torch.func.vjp transform that composes with\ntorch.vmap:",
                        "markdown"
                    ],
                    [
                        "from torch.func import , \n\n, vjp_fn = (partial(predict, , ), )\n\n, = (vjp_fn)()\n\n# let's confirm both methods compute the same result\nassert (, )",
                        "code"
                    ],
                    [
                        "In a later tutorial a composition of reverse-mode AD and vmap will give us\nper-sample-gradients.\nIn this tutorial, composing reverse-mode AD and vmap gives us Jacobian\ncomputation!\nVarious compositions of vmap and autodiff transforms can give us different\ninteresting quantities.",
                        "markdown"
                    ],
                    [
                        "PyTorch provides torch.func.jacrev as a convenience function that performs\nthe vmap-vjp composition to compute jacobians. jacrev accepts an argnums\nargument that says which argument we would like to compute Jacobians with\nrespect to.",
                        "markdown"
                    ],
                    [
                        "from torch.func import \n\n = (predict, argnums=2)(, , )\n\n# confirm\nassert (, )",
                        "code"
                    ],
                    [
                        "Let\u2019s compare the performance of the two ways to compute the jacobian.\nThe function transform version is much faster (and becomes even faster the\nmore outputs there are).",
                        "markdown"
                    ],
                    [
                        "In general, we expect that vectorization via vmap can help eliminate overhead\nand give better utilization of your hardware.",
                        "markdown"
                    ],
                    [
                        "vmap does this magic by pushing the outer loop down into the function\u2019s\nprimitive operations in order to obtain better performance.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s make a quick function to evaluate performance and deal with\nmicroseconds and milliseconds measurements:",
                        "markdown"
                    ],
                    [
                        "def get_perf(first, first_descriptor, second, second_descriptor):\n    \"\"\"takes torch.benchmark objects and compares delta of second vs first.\"\"\"\n    faster = second.times[0]\n    slower = first.times[0]\n    gain = (slower-faster)/slower\n    if gain &lt; 0: gain *=-1\n    final_gain = gain*100\n    print(f\" Performance delta: {final_gain:.4f} percent improvement with {second_descriptor} \")",
                        "code"
                    ],
                    [
                        "And then run the performance comparison:",
                        "markdown"
                    ],
                    [
                        "from torch.utils.benchmark import \n\n = (stmt=\"compute_jac(xp)\", globals=globals())\n = (stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n\n = (500)\n = (500)\n\nprint()\nprint()",
                        "code"
                    ],
                    [
                        "&lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f0438039000&gt;\ncompute_jac(xp)\n  1.35 ms\n  1 measurement, 500 runs , 1 thread\n&lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f04380c7e20&gt;\njacrev(predict, argnums=2)(weight, bias, x)\n  625.00 us\n  1 measurement, 500 runs , 1 thread",
                        "code"
                    ],
                    [
                        "Let\u2019s do a relative performance comparison of the above with our get_perf function:",
                        "markdown"
                    ],
                    [
                        "get_perf(, \"without vmap\",  , \"vmap\")",
                        "code"
                    ],
                    [
                        "Performance delta: 53.6071 percent improvement with vmap",
                        "code"
                    ],
                    [
                        "Furthemore, it\u2019s pretty easy to flip the problem around and say we want to\ncompute Jacobians of the parameters to our model (weight, bias) instead of the input",
                        "markdown"
                    ],
                    [
                        "# note the change in input via argnums params of 0,1 to map to weight and bias\n,  = (predict, argnums=(0, 1))(, , )",
                        "code"
                    ]
                ]
            },
            {
                "reverse-mode Jacobian (jacrev) vs forward-mode Jacobian (jacfwd)": [
                    [
                        "We offer two APIs to compute jacobians: jacrev and jacfwd:",
                        "markdown"
                    ],
                    [
                        "jacrev uses reverse-mode AD. As you saw above it is a composition of our\nvjp and vmap transforms.",
                        "markdown"
                    ],
                    [
                        "jacfwd uses forward-mode AD. It is implemented as a composition of our\njvp and vmap transforms.",
                        "markdown"
                    ],
                    [
                        "jacfwd and jacrev can be substituted for each other but they have different\nperformance characteristics.",
                        "markdown"
                    ],
                    [
                        "As a general rule of thumb, if you\u2019re computing the jacobian of an \\(R^N \\to R^M\\)\nfunction, and there are many more outputs than inputs (i.e. \\(M &gt; N\\)) then\njacfwd is preferred, otherwise use jacrev. There are exceptions to this rule,\nbut a non-rigorous argument for this follows:",
                        "markdown"
                    ],
                    [
                        "In reverse-mode AD, we are computing the jacobian row-by-row, while in\nforward-mode AD (which computes Jacobian-vector products), we are computing\nit column-by-column. The Jacobian matrix has M rows and N columns, so if it\nis taller or wider one way we may prefer the method that deals with fewer\nrows or columns.",
                        "markdown"
                    ],
                    [
                        "from torch.func import , ",
                        "code"
                    ],
                    [
                        "First, let\u2019s benchmark with more inputs than outputs:",
                        "markdown"
                    ],
                    [
                        "Din = 32\nDout = 2048\n = (Dout, Din)\n\n = (Dout)\n = (Din)\n\n# remember the general rule about taller vs wider... here we have a taller matrix:\nprint(.shape)\n\n = (stmt=\"jacfwd(predict, argnums=2)(weight, bias, x)\", globals=globals())\n = (stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n\n = (500)\n = (500)\n\nprint(f'jacfwd time: {}')\nprint(f'jacrev time: {}')",
                        "code"
                    ],
                    [
                        "torch.Size([2048, 32])\njacfwd time: &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f04380d2ad0&gt;\njacfwd(predict, argnums=2)(weight, bias, x)\n  1.14 ms\n  1 measurement, 500 runs , 1 thread\njacrev time: &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f04380c71f0&gt;\njacrev(predict, argnums=2)(weight, bias, x)\n  9.52 ms\n  1 measurement, 500 runs , 1 thread",
                        "code"
                    ],
                    [
                        "and then do a relative benchmark:",
                        "markdown"
                    ],
                    [
                        "get_perf(, \"jacfwd\", , \"jacrev\", );",
                        "code"
                    ],
                    [
                        "Performance delta: 738.8406 percent improvement with jacrev",
                        "code"
                    ],
                    [
                        "and now the reverse - more outputs (M) than inputs (N):",
                        "markdown"
                    ],
                    [
                        "Din = 2048\nDout = 32\n = (Dout, Din)\n = (Dout)\n = (Din)\n\n = (stmt=\"jacfwd(predict, argnums=2)(weight, bias, x)\", globals=globals())\n = (stmt=\"jacrev(predict, argnums=2)(weight, bias, x)\", globals=globals())\n\n = (500)\n = (500)\n\nprint(f'jacfwd time: {}')\nprint(f'jacrev time: {}')",
                        "code"
                    ],
                    [
                        "jacfwd time: &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f043804b6d0&gt;\njacfwd(predict, argnums=2)(weight, bias, x)\n  6.24 ms\n  1 measurement, 500 runs , 1 thread\njacrev time: &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f0438069c30&gt;\njacrev(predict, argnums=2)(weight, bias, x)\n  736.54 us\n  1 measurement, 500 runs , 1 thread",
                        "code"
                    ],
                    [
                        "and a relative perf comparison:",
                        "markdown"
                    ],
                    [
                        "get_perf(, \"jacrev\", , \"jacfwd\")",
                        "code"
                    ],
                    [
                        "Performance delta: 747.2533 percent improvement with jacfwd",
                        "code"
                    ]
                ]
            },
            {
                "Hessian computation with functorch.hessian": [
                    [
                        "We offer a convenience API to compute hessians: torch.func.hessiani.\nHessians are the jacobian of the jacobian (or the partial derivative of\nthe partial derivative, aka second order).",
                        "markdown"
                    ],
                    [
                        "This suggests that one can just compose functorch\u2019s jacobian transforms to\ncompute the Hessian.\nIndeed, under the hood, hessian(f) is simply jacfwd(jacrev(f)).",
                        "markdown"
                    ],
                    [
                        "Note: to boost performance: depending on your model, you may also want to\nuse jacfwd(jacfwd(f)) or jacrev(jacrev(f)) instead to compute hessians\nleveraging the rule of thumb above regarding wider vs taller matrices.",
                        "markdown"
                    ],
                    [
                        "from torch.func import \n\n# lets reduce the size in order not to blow out colab. Hessians require\n# significant memory:\nDin = 512\nDout = 32\n = (Dout, Din)\n = (Dout)\n = (Din)\n\n = (predict, argnums=2)(, , )\n = ((predict, argnums=2), argnums=2)(, , )\n = ((predict, argnums=2), argnums=2)(, , )",
                        "code"
                    ],
                    [
                        "Let\u2019s verify we have the same result regardless of using hessian api or\nusing jacfwd(jacfwd())",
                        "markdown"
                    ],
                    [
                        "(, )",
                        "code"
                    ],
                    [
                        "True",
                        "code"
                    ]
                ]
            },
            {
                "Batch Jacobian and Batch Hessian": [
                    [
                        "In the above examples we\u2019ve been operating with a single feature vector.\nIn some cases you might want to take the Jacobian of a batch of outputs\nwith respect to a batch of inputs. That is, given a batch of inputs of\nshape (B, N) and a function that goes from \\(R^N \\to R^M\\), we would like\na Jacobian of shape (B, M, N).",
                        "markdown"
                    ],
                    [
                        "The easiest way to do this is to use vmap:",
                        "markdown"
                    ],
                    [
                        "batch_size = 64\nDin = 31\nDout = 33\n\n = (Dout, Din)\nprint(f\"weight shape = {.shape}\")\n\n = (Dout)\n\n = (batch_size, Din)\n\ncompute_batch_jacobian = ((predict, argnums=2), in_dims=(None, None, 0))\n = compute_batch_jacobian(, , )",
                        "code"
                    ],
                    [
                        "weight shape = torch.Size([33, 31])",
                        "code"
                    ],
                    [
                        "If you have a function that goes from (B, N) -&gt; (B, M) instead and are\ncertain that each input produces an independent output, then it\u2019s also\nsometimes possible to do this without using vmap by summing the outputs\nand then computing the Jacobian of that function:",
                        "markdown"
                    ],
                    [
                        "def predict_with_output_summed(, , ):\n    return predict(, , ).sum(0)\n\n = (predict_with_output_summed, argnums=2)(, , ).movedim(1, 0)\nassert (, )",
                        "code"
                    ],
                    [
                        "If you instead have a function that goes from \\(R^N \\to R^M\\) but inputs that\nare batched, you compose vmap with jacrev to compute batched jacobians:",
                        "markdown"
                    ],
                    [
                        "Finally, batch hessians can be computed similarly. It\u2019s easiest to think\nabout them by using vmap to batch over hessian computation, but in some\ncases the sum trick also works.",
                        "markdown"
                    ],
                    [
                        "compute_batch_hessian = ((predict, argnums=2), in_dims=(None, None, 0))\n\n = compute_batch_hessian(, , )\n.shape",
                        "code"
                    ],
                    [
                        "torch.Size([64, 33, 31, 31])",
                        "code"
                    ]
                ]
            },
            {
                "Computing Hessian-vector products": [
                    [
                        "The naive way to compute a Hessian-vector product (hvp) is to materialize\nthe full Hessian and perform a dot-product with a vector. We can do better:\nit turns out we don\u2019t need to materialize the full Hessian to do this. We\u2019ll\ngo through two (of many) different strategies to compute Hessian-vector products:\n- composing reverse-mode AD with reverse-mode AD\n- composing reverse-mode AD with forward-mode AD",
                        "markdown"
                    ],
                    [
                        "Composing reverse-mode AD with forward-mode AD (as opposed to reverse-mode\nwith reverse-mode) is generally the more memory efficient way to compute a\nhvp because forward-mode AD doesn\u2019t need to construct an Autograd graph and\nsave intermediates for backward:",
                        "markdown"
                    ],
                    [
                        "from torch.func import , , \n\ndef hvp(f, primals, tangents):\n  return ((f), primals, tangents)[1]",
                        "code"
                    ],
                    [
                        "Here\u2019s some sample usage.",
                        "markdown"
                    ],
                    [
                        "def f():\n  return .sin().sum()\n\n = (2048)\n = (2048)\n\n = hvp(f, (,), (,))",
                        "code"
                    ],
                    [
                        "If PyTorch forward-AD does not have coverage for your operations, then we can\ninstead compose reverse-mode AD with reverse-mode AD:",
                        "markdown"
                    ],
                    [
                        "def hvp_revrev(f, primals, tangents):\n  , vjp_fn = ((f), *primals)\n  return vjp_fn(*tangents)\n\nresult_hvp_revrev = hvp_revrev(f, (,), (,))\nassert (, result_hvp_revrev[0])",
                        "code"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  11.558 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Model ensembling": [
            [
                "This tutorial illustrates how to vectorize model ensembling using torch.vmap.",
                "markdown"
            ],
            {
                "What is model ensembling?": [
                    [
                        "Model ensembling combines the predictions from multiple models together.\nTraditionally this is done by running each model on some inputs separately\nand then combining the predictions. However, if you\u2019re running models with\nthe same architecture, then it may be possible to combine them together\nusing torch.vmap. vmap is a function transform that maps functions across\ndimensions of the input tensors. One of its use cases is eliminating\nfor-loops and speeding them up through vectorization.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s demonstrate how to do this using an ensemble of simple MLPs.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "This tutorial requires PyTorch 2.0.0 or later.",
                        "markdown"
                    ],
                    [
                        "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n(0)\n\n# Here's a simple MLP\nclass SimpleMLP():\n    def __init__(self):\n        super(, self).__init__()\n        self.fc1 = (784, 128)\n        self.fc2 = (128, 128)\n        self.fc3 = (128, 10)\n\n    def forward(self, x):\n        x = x.flatten(1)\n        x = self.fc1(x)\n        x = (x)\n        x = self.fc2(x)\n        x = (x)\n        x = self.fc3(x)\n        return x",
                        "code"
                    ],
                    [
                        "Let\u2019s generate a batch of dummy data and pretend that we\u2019re working with\nan MNIST dataset. Thus, the dummy images are 28 by 28, and we have a\nminibatch of size 64. Furthermore, lets say we want to combine the predictions\nfrom 10 different models.",
                        "markdown"
                    ],
                    [
                        "device = 'cuda'\nnum_models = 10\n\n = (100, 64, 1, 28, 28, device=device)\n = (10, (6400,), device=device)\n\nmodels = [().to(device) for _ in range(num_models)]",
                        "code"
                    ],
                    [
                        "We have a couple of options for generating predictions. Maybe we want to\ngive each model a different randomized minibatch of data. Alternatively,\nmaybe we want to run the same minibatch of data through each model (e.g.\nif we were testing the effect of different model initializations).",
                        "markdown"
                    ],
                    [
                        "Option 1: different minibatch for each model",
                        "markdown"
                    ],
                    [
                        " = [:num_models]\npredictions_diff_minibatch_loop = [model() for model,  in zip(models, )]",
                        "code"
                    ],
                    [
                        "Option 2: Same minibatch",
                        "markdown"
                    ],
                    [
                        " = [0]\npredictions2 = [model() for model in models]",
                        "code"
                    ]
                ]
            },
            {
                "Using vmap to vectorize the ensemble": [
                    [
                        "Let\u2019s use vmap to speed up the for-loop. We must first prepare the models\nfor use with vmap.",
                        "markdown"
                    ],
                    [
                        "First, let\u2019s combine the states of the model together by stacking each\nparameter. For example, model[i].fc1.weight has shape [784, 128]; we are\ngoing to stack the .fc1.weight of each of the 10 models to produce a big\nweight of shape [10, 784, 128].",
                        "markdown"
                    ],
                    [
                        "PyTorch offers the torch.func.stack_module_state convenience function to do\nthis.",
                        "markdown"
                    ],
                    [
                        "from torch.func import \n\nparams, buffers = (models)",
                        "code"
                    ],
                    [
                        "Next, we need to define a function to vmap over. The function should,\ngiven parameters and buffers and inputs, run the model using those\nparameters, buffers, and inputs. We\u2019ll use torch.func.functional_call\nto help out:",
                        "markdown"
                    ],
                    [
                        "from torch.func import \nimport copy\n\n# Construct a \"stateless\" version of one of the models. It is \"stateless\" in\n# the sense that the parameters are meta Tensors and do not have storage.\nbase_model = copy.deepcopy(models[0])\nbase_model = ('meta')\n\ndef fmodel(params, buffers, x):\n    return (base_model, (params, buffers), (x,))",
                        "code"
                    ],
                    [
                        "Option 1: get predictions using a different minibatch for each model.",
                        "markdown"
                    ],
                    [
                        "By default, vmap maps a function across the first dimension of all inputs to\nthe passed-in function. After using stack_module_state, each of\nthe params and buffers have an additional dimension of size \u2018num_models\u2019 at\nthe front, and minibatches has a dimension of size \u2018num_models\u2019.",
                        "markdown"
                    ],
                    [
                        "print([p.size(0) for p in params.values()]) # show the leading 'num_models' dimension\n\nassert .shape == (num_models, 64, 1, 28, 28) # verify minibatch has leading dimension of size 'num_models'\n\nfrom torch import \n\n = (fmodel)(params, buffers, )\n\n# verify the vmap predictions match the\nassert (, (predictions_diff_minibatch_loop), atol=1e-3, rtol=1e-5)",
                        "code"
                    ],
                    [
                        "[10, 10, 10, 10, 10, 10]",
                        "code"
                    ],
                    [
                        "Option 2: get predictions using the same minibatch of data.",
                        "markdown"
                    ],
                    [
                        "vmap has an in_dims arg that specifies which dimensions to map over.\nBy using None, we tell vmap we want the same minibatch to apply for all of\nthe 10 models.",
                        "markdown"
                    ],
                    [
                        " = (fmodel, in_dims=(0, 0, None))(params, buffers, )\n\nassert (, (predictions2), atol=1e-3, rtol=1e-5)",
                        "code"
                    ],
                    [
                        "A quick note: there are limitations around what types of functions can be\ntransformed by vmap. The best functions to transform are ones that are pure\nfunctions: a function where the outputs are only determined by the inputs\nthat have no side effects (e.g. mutation). vmap is unable to handle mutation\nof arbitrary Python data structures, but it is able to handle many in-place\nPyTorch operations.",
                        "markdown"
                    ]
                ]
            },
            {
                "Performance": [
                    [
                        "Curious about performance numbers? Here\u2019s how the numbers look.",
                        "markdown"
                    ],
                    [
                        "from torch.utils.benchmark import \n = (\n    stmt=\"[model(minibatch) for model, minibatch in zip(models, minibatches)]\",\n    globals=globals())\n = (\n    stmt=\"vmap(fmodel)(params, buffers, minibatches)\",\n    globals=globals())\nprint(f'Predictions without vmap {(100)}')\nprint(f'Predictions with vmap {(100)}')",
                        "code"
                    ],
                    [
                        "Predictions without vmap &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f739b27db70&gt;\n[model(minibatch) for model, minibatch in zip(models, minibatches)]\n  1.48 ms\n  1 measurement, 100 runs , 1 thread\nPredictions with vmap &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f739b27ded0&gt;\nvmap(fmodel)(params, buffers, minibatches)\n  630.78 us\n  1 measurement, 100 runs , 1 thread",
                        "code"
                    ],
                    [
                        "There\u2019s a large speedup using vmap!",
                        "markdown"
                    ],
                    [
                        "In general, vectorization with vmap should be faster than running a function\nin a for-loop and competitive with manual batching. There are some exceptions\nthough, like if we haven\u2019t implemented the vmap rule for a particular\noperation or if the underlying kernels weren\u2019t optimized for older hardware\n(GPUs). If you see any of these cases, please let us know by opening an issue\non GitHub.",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.843 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Per-sample-gradients": [
            {
                "What is it?": [
                    [
                        "Per-sample-gradient computation is computing the gradient for each and every\nsample in a batch of data. It is a useful quantity in differential privacy,\nmeta-learning, and optimization research.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "This tutorial requires PyTorch 2.0.0 or later.",
                        "markdown"
                    ],
                    [
                        "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n(0)\n\n# Here's a simple CNN and loss function:\n\nclass SimpleCNN():\n    def __init__(self):\n        super(, self).__init__()\n        self.conv1 = (1, 32, 3, 1)\n        self.conv2 = (32, 64, 3, 1)\n        self.fc1 = (9216, 128)\n        self.fc2 = (128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = (x)\n        x = self.conv2(x)\n        x = (x)\n        x = (x, 2)\n        x = (x, 1)\n        x = self.fc1(x)\n        x = (x)\n        x = self.fc2(x)\n        output = (x, dim=1)\n        output = x\n        return output\n\ndef loss_fn(, ):\n    return (, )",
                        "code"
                    ],
                    [
                        "Let\u2019s generate a batch of dummy data and pretend that we\u2019re working with an MNIST dataset.\nThe dummy images are 28 by 28 and we use a minibatch of size 64.",
                        "markdown"
                    ],
                    [
                        "device = 'cuda'\n\nnum_models = 10\nbatch_size = 64\n = (batch_size, 1, 28, 28, device=device)\n\n = (10, (64,), device=device)",
                        "code"
                    ],
                    [
                        "In regular model training, one would forward the minibatch through the model,\nand then call .backward() to compute gradients.  This would generate an\n\u2018average\u2019 gradient of the entire mini-batch:",
                        "markdown"
                    ],
                    [
                        "model = ().to(device=device)\n = model()  # move the entire mini-batch through the model\n\n = loss_fn(, )\n()  # back propogate the 'average' gradient of this mini-batch",
                        "code"
                    ],
                    [
                        "In contrast to the above approach, per-sample-gradient computation is\nequivalent to:",
                        "markdown"
                    ],
                    [
                        "for each individual sample of the data, perform a forward and a backward\npass to get an individual (per-sample) gradient.",
                        "markdown"
                    ],
                    [
                        "def compute_grad(sample, target):\n    sample = sample.unsqueeze(0)  # prepend batch dimension for processing\n    target = target.unsqueeze(0)\n\n    prediction = model(sample)\n     = loss_fn(prediction, target)\n\n    return (, list(()))\n\n\ndef compute_sample_grads(, ):\n    \"\"\" manually process each sample with per sample gradient \"\"\"\n    sample_grads = [compute_grad([i], [i]) for i in range(batch_size)]\n    sample_grads = zip(*sample_grads)\n    sample_grads = [(shards) for shards in sample_grads]\n    return sample_grads\n\nper_sample_grads = compute_sample_grads(, )",
                        "code"
                    ],
                    [
                        "sample_grads[0] is the per-sample-grad for model.conv1.weight.\nmodel.conv1.weight.shape is [32, 1, 3, 3]; notice how there is one\ngradient, per sample, in the batch for a total of 64.",
                        "markdown"
                    ],
                    [
                        "print(per_sample_grads[0].shape)",
                        "code"
                    ],
                    [
                        "torch.Size([64, 32, 1, 3, 3])",
                        "code"
                    ]
                ]
            },
            {
                "Per-sample-grads, <em>the efficient way</em>, using function transforms": [
                    [
                        "We can compute per-sample-gradients efficiently by using function transforms.",
                        "markdown"
                    ],
                    [
                        "The torch.func function transform API transforms over functions.\nOur strategy is to define a function that computes the loss and then apply\ntransforms to construct a function that computes per-sample-gradients.",
                        "markdown"
                    ],
                    [
                        "We\u2019ll use the torch.func.functional_call function to treat an nn.Module\nlike a function.",
                        "markdown"
                    ],
                    [
                        "First, let\u2019s extract the state from model into two dictionaries,\nparameters and buffers. We\u2019ll be detaching them because we won\u2019t use\nregular PyTorch autograd (e.g. Tensor.backward(), torch.autograd.grad).",
                        "markdown"
                    ],
                    [
                        "from torch.func import , , \n\nparams = {k: v.detach() for k, v in ()}\nbuffers = {k: v.detach() for k, v in ()}",
                        "code"
                    ],
                    [
                        "Next, let\u2019s define a function to compute the loss of the model given a\nsingle input rather than a batch of inputs. It is important that this\nfunction accepts the parameters, the input, and the target, because we will\nbe transforming over them.",
                        "markdown"
                    ],
                    [
                        "Note - because the model was originally written to handle batches, we\u2019ll\nuse torch.unsqueeze to add a batch dimension.",
                        "markdown"
                    ],
                    [
                        "def compute_loss(params, buffers, sample, target):\n    batch = sample.unsqueeze(0)\n     = target.unsqueeze(0)\n\n     = (model, (params, buffers), (batch,))\n     = loss_fn(, )\n    return ",
                        "code"
                    ],
                    [
                        "Now, let\u2019s use the grad transform to create a new function that computes\nthe gradient with respect to the first argument of compute_loss\n(i.e. the params).",
                        "markdown"
                    ],
                    [
                        "ft_compute_grad = (compute_loss)",
                        "code"
                    ],
                    [
                        "The ft_compute_grad function computes the gradient for a single\n(sample, target) pair. We can use vmap to get it to compute the gradient\nover an entire batch of samples and targets. Note that\nin_dims=(None, None, 0, 0) because we wish to map ft_compute_grad over\nthe 0th dimension of the data and targets, and use the same params and\nbuffers for each.",
                        "markdown"
                    ],
                    [
                        "ft_compute_sample_grad = (ft_compute_grad, in_dims=(None, None, 0, 0))",
                        "code"
                    ],
                    [
                        "Finally, let\u2019s used our transformed function to compute per-sample-gradients:",
                        "markdown"
                    ],
                    [
                        "ft_per_sample_grads = ft_compute_sample_grad(params, buffers, , )",
                        "code"
                    ],
                    [
                        "we can double check that the results using grad and vmap match the\nresults of hand processing each one individually:",
                        "markdown"
                    ],
                    [
                        "for ,  in zip(per_sample_grads, ft_per_sample_grads.values()):\n    assert (, , atol=3e-3, rtol=1e-5)",
                        "code"
                    ],
                    [
                        "A quick note: there are limitations around what types of functions can be\ntransformed by vmap. The best functions to transform are ones that are pure\nfunctions: a function where the outputs are only determined by the inputs,\nand that have no side effects (e.g. mutation). vmap is unable to handle\nmutation of arbitrary Python data structures, but it is able to handle many\nin-place PyTorch operations.",
                        "markdown"
                    ]
                ]
            },
            {
                "Performance comparison": [
                    [
                        "Curious about how the performance of vmap compares?",
                        "markdown"
                    ],
                    [
                        "Currently the best results are obtained on newer GPU\u2019s such as the A100\n(Ampere) where we\u2019ve seen up to 25x speedups on this example, but here are\nsome results on our build machines:",
                        "markdown"
                    ],
                    [
                        "def get_perf(first, first_descriptor, second, second_descriptor):\n    \"\"\"takes torch.benchmark objects and compares delta of second vs first.\"\"\"\n    second_res = second.times[0]\n    first_res = first.times[0]\n\n    gain = (first_res-second_res)/first_res\n    if gain &lt; 0: gain *=-1\n    final_gain = gain*100\n\n    print(f\"Performance delta: {final_gain:.4f} percent improvement with {first_descriptor} \")\n\nfrom torch.utils.benchmark import \n\n = (stmt=\"compute_sample_grads(data, targets)\", globals=globals())\n = (stmt=\"ft_compute_sample_grad(params, buffers, data, targets)\",globals=globals())\n = (100)\n = (100)\n\nprint(f'Per-sample-grads without vmap {}')\nprint(f'Per-sample-grads with vmap {}')\n\nget_perf(, \"vmap\", , \"no vmap\")",
                        "code"
                    ],
                    [
                        "Per-sample-grads without vmap &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f53a48fc250&gt;\ncompute_sample_grads(data, targets)\n  67.80 ms\n  1 measurement, 100 runs , 1 thread\nPer-sample-grads with vmap &lt;torch.utils.benchmark.utils.common.Measurement object at 0x7f53a491a8f0&gt;\nft_compute_sample_grad(params, buffers, data, targets)\n  7.30 ms\n  1 measurement, 100 runs , 1 thread\nPerformance delta: 828.9354 percent improvement with vmap",
                        "code"
                    ],
                    [
                        "There are other optimized solutions (like in )\nto computing per-sample-gradients in PyTorch that also perform better than\nthe naive method. But it\u2019s cool that composing vmap and grad give us a\nnice speedup.",
                        "markdown"
                    ],
                    [
                        "In general, vectorization with vmap should be faster than running a function\nin a for-loop and competitive with manual batching. There are some exceptions\nthough, like if we haven\u2019t implemented the vmap rule for a particular\noperation or if the underlying kernels weren\u2019t optimized for older hardware\n(GPUs). If you see any of these cases, please let us know by opening an issue\nat on GitHub.",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  8.425 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Using the PyTorch C++ Frontend": [
            [
                "The PyTorch C++ frontend is a pure C++ interface to the PyTorch machine learning\nframework. While the primary interface to PyTorch naturally is Python, this\nPython API sits atop a substantial C++ codebase providing foundational data\nstructures and functionality such as tensors and automatic differentiation. The\nC++ frontend exposes a pure C++11 API that extends this underlying C++ codebase\nwith tools required for machine learning training and inference. This includes a\nbuilt-in collection of common components for neural network modeling; an API to\nextend this collection with custom modules; a library of popular optimization\nalgorithms such as stochastic gradient descent; a parallel data loader with an\nAPI to define and load datasets; serialization routines and more.",
                "markdown"
            ],
            [
                "This tutorial will walk you through an end-to-end example of training a model\nwith the C++ frontend. Concretely, we will be training a  \u2013 a kind of generative model \u2013 to\ngenerate images of MNIST digits. While conceptually a simple example, it should\nbe enough to give you a whirlwind overview of the PyTorch C++ frontend and wet\nyour appetite for training more complex models. We will begin with some\nmotivating words for why you would want to use the C++ frontend to begin with,\nand then dive straight into defining and training our model.",
                "markdown"
            ],
            [
                "Tip",
                "markdown"
            ],
            [
                "Watch  for a quick (and humorous)\npresentation on the C++ frontend.",
                "markdown"
            ],
            [
                "Tip",
                "markdown"
            ],
            [
                " provides a sweeping\noverview of the C++ frontend\u2019s components and design philosophy.",
                "markdown"
            ],
            [
                "Tip",
                "markdown"
            ],
            [
                "Documentation for the PyTorch C++ ecosystem is available at\n. There you can find high level descriptions as\nwell as API-level documentation.",
                "markdown"
            ],
            {
                "Motivation": [
                    [
                        "Before we embark on our exciting journey of GANs and MNIST digits, let\u2019s take a\nstep back and discuss why you would want to use the C++ frontend instead of the\nPython one to begin with. We (the PyTorch team) created the C++ frontend to\nenable research in environments in which Python cannot be used, or is simply not\nthe right tool for the job. Examples for such environments include:",
                        "markdown"
                    ],
                    [
                        "<strong>Low Latency Systems</strong>: You may want to do reinforcement learning research in\na pure C++ game engine with high frames-per-second and low latency\nrequirements. Using a pure C++ library is a much better fit to such an\nenvironment than a Python library. Python may not be tractable at all because\nof the slowness of the Python interpreter.",
                        "markdown"
                    ],
                    [
                        "<strong>Highly Multithreaded Environments</strong>: Due to the Global Interpreter Lock\n(GIL), Python cannot run more than one system thread at a time.\nMultiprocessing is an alternative, but not as scalable and has significant\nshortcomings. C++ has no such constraints and threads are easy to use and\ncreate. Models requiring heavy parallelization, like those used in , can benefit from\nthis.",
                        "markdown"
                    ],
                    [
                        "<strong>Existing C++ Codebases</strong>: You may be the owner of an existing C++\napplication doing anything from serving web pages in a backend server to\nrendering 3D graphics in photo editing software, and wish to integrate\nmachine learning methods into your system. The C++ frontend allows you to\nremain in C++ and spare yourself the hassle of binding back and forth between\nPython and C++, while retaining much of the flexibility and intuitiveness of\nthe traditional PyTorch (Python) experience.",
                        "markdown"
                    ],
                    [
                        "The C++ frontend is not intended to compete with the Python frontend. It is\nmeant to complement it. We know researchers and engineers alike love PyTorch for\nits simplicity, flexibility and intuitive API. Our goal is to make sure you can\ntake advantage of these core design principles in every possible environment,\nincluding the ones described above. If one of these scenarios describes your use\ncase well, or if you are simply interested or curious, follow along as we\nexplore the C++ frontend in detail in the following paragraphs.",
                        "markdown"
                    ],
                    [
                        "Tip",
                        "markdown"
                    ],
                    [
                        "The C++ frontend tries to provide an API as close as possible to that of the\nPython frontend. If you are experienced with the Python frontend and ever ask\nyourself \u201chow do I do X with the C++ frontend?\u201d, write your code the way you\nwould in Python, and more often than not the same functions and methods will\nbe available in C++ as in Python (just remember to replace dots with double\ncolons).",
                        "markdown"
                    ]
                ]
            },
            {
                "Writing a Basic Application": [
                    [
                        "Let\u2019s begin by writing a minimal C++ application to verify that we\u2019re on the\nsame page regarding our setup and build environment. First, you will need to\ngrab a copy of the <em>LibTorch</em> distribution \u2013 our ready-built zip archive that\npackages all relevant headers, libraries and CMake build files required to use\nthe C++ frontend. The LibTorch distribution is available for download on the\n for Linux, MacOS\nand Windows. The rest of this tutorial will assume a basic Ubuntu Linux\nenvironment, however you are free to follow along on MacOS or Windows too.",
                        "markdown"
                    ],
                    [
                        "Tip",
                        "markdown"
                    ],
                    [
                        "The note on  describes the following steps\nin more detail.",
                        "markdown"
                    ],
                    [
                        "Tip",
                        "markdown"
                    ],
                    [
                        "On Windows, debug and release builds are not ABI-compatible. If you plan to\nbuild your project in debug mode, please try the debug version of LibTorch.\nAlso, make sure you specify the correct configuration in the cmake --build .\nline below.",
                        "markdown"
                    ],
                    [
                        "The first step is to download the LibTorch distribution locally, via the link\nretrieved from the PyTorch website. For a vanilla Ubuntu Linux environment, this\nmeans running:",
                        "markdown"
                    ],
                    [
                        "# If you need e.g. CUDA 9.0 support, please replace \"cpu\" with \"cu90\" in the URL below.\nwget https://download.pytorch.org/libtorch/nightly/cpu/libtorch-shared-with-deps-latest.zip\nunzip libtorch-shared-with-deps-latest.zip",
                        "code"
                    ],
                    [
                        "Next, let\u2019s write a tiny C++ file called dcgan.cpp that includes\ntorch/torch.h and for now simply prints out a three by three identity\nmatrix:",
                        "markdown"
                    ],
                    [
                        "#include &lt;torch/torch.h&gt;\n#include &lt;iostream&gt;\n\nint main() {\n  torch::Tensor tensor = torch::eye(3);\n  std::cout &lt;&lt; tensor &lt;&lt; std::endl;\n}",
                        "code"
                    ],
                    [
                        "To build this tiny application as well as our full-fledged training script later\non we\u2019ll use this CMakeLists.txt file:",
                        "markdown"
                    ],
                    [
                        "cmake_minimum_required(VERSION 3.0 FATAL_ERROR)\nproject(dcgan)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(dcgan dcgan.cpp)\ntarget_link_libraries(dcgan \"${TORCH_LIBRARIES}\")\nset_property(TARGET dcgan PROPERTY CXX_STANDARD 14)",
                        "code"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "While CMake is the recommended build system for LibTorch, it is not a hard\nrequirement. You can also use Visual Studio project files, QMake, plain\nMakefiles or any other build environment you feel comfortable with. However,\nwe do not provide out-of-the-box support for this.",
                        "markdown"
                    ],
                    [
                        "Make note of line 4 in the above CMake file: find_package(Torch REQUIRED).\nThis instructs CMake to find the build configuration for the LibTorch library.\nIn order for CMake to know <em>where</em> to find these files, we must set the\nCMAKE_PREFIX_PATH when invoking cmake. Before we do this, let\u2019s agree on\nthe following directory structure for our dcgan application:",
                        "markdown"
                    ],
                    [
                        "dcgan/\n  CMakeLists.txt\n  dcgan.cpp",
                        "code"
                    ],
                    [
                        "Further, I will refer to the path to the unzipped LibTorch distribution as\n/path/to/libtorch. Note that this <strong>must be an absolute path</strong>. In\nparticular, setting CMAKE_PREFIX_PATH to something like ../../libtorch\nwill break in unexpected ways. Instead, write $PWD/../../libtorch to get the\ncorresponding absolute path. Now, we are ready to build our application:",
                        "markdown"
                    ],
                    [
                        "root@fa350df05ecf:/home# mkdir build\nroot@fa350df05ecf:/home# cd build\nroot@fa350df05ecf:/home/build# cmake -DCMAKE_PREFIX_PATH=/path/to/libtorch ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /path/to/libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/build\nroot@fa350df05ecf:/home/build# cmake --build . --config Release\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcgan",
                        "code"
                    ],
                    [
                        "Above, we first created a build folder inside of our dcgan directory,\nentered this folder, ran the cmake command to generate the necessary build\n(Make) files and finally compiled the project successfully by running cmake\n--build . --config Release. We are now all set to execute our minimal binary\nand complete this section on basic project configuration:",
                        "markdown"
                    ],
                    [
                        "root@fa350df05ecf:/home/build# ./dcgan\n1  0  0\n0  1  0\n0  0  1\n[ Variable[CPUFloatType]{3,3} ]",
                        "code"
                    ],
                    [
                        "Looks like an identity matrix to me!",
                        "markdown"
                    ]
                ]
            },
            {
                "Defining the Neural Network Models": [
                    [
                        "Now that we have our basic environment configured, we can dive into the much\nmore interesting parts of this tutorial. First, we will discuss how to define\nand interact with modules in the C++ frontend. We\u2019ll begin with basic,\nsmall-scale example modules and then implement a full-fledged GAN using the\nextensive library of built-in modules provided by the C++ frontend.",
                        "markdown"
                    ],
                    {
                        "Module API Basics": [
                            [
                                "In line with the Python interface, neural networks based on the C++ frontend are\ncomposed of reusable building blocks called <em>modules</em>. There is a base module\nclass from which all other modules are derived. In Python, this class is\ntorch.nn.Module and in C++ it is torch::nn::Module. Besides a\nforward() method that implements the algorithm the module encapsulates, a\nmodule usually contains any of three kinds of sub-objects: parameters, buffers\nand submodules.",
                                "markdown"
                            ],
                            [
                                "Parameters and buffers store state in form of tensors. Parameters record\ngradients, while buffers do not. Parameters are usually the trainable weights of\nyour neural network. Examples of buffers include means and variances for batch\nnormalization. In order to re-use particular blocks of logic and state, the\nPyTorch API allows modules to be nested. A nested module is termed a\n<em>submodule</em>.",
                                "markdown"
                            ],
                            [
                                "Parameters, buffers and submodules must be explicitly registered. Once\nregistered, methods like parameters() or buffers() can be used to\nretrieve a container of all parameters in the entire (nested) module hierarchy.\nSimilarly, methods like to(...), where e.g. to(torch::kCUDA) moves all\nparameters and buffers from CPU to CUDA memory, work on the entire module\nhierarchy.",
                                "markdown"
                            ],
                            {
                                "Defining a Module and Registering Parameters": [
                                    [
                                        "To put these words into code, let\u2019s consider this simple module written in the\nPython interface:",
                                        "markdown"
                                    ],
                                    [
                                        "import torch\n\nclass Net(torch.nn.Module):\n  def __init__(self, N, M):\n    super(Net, self).__init__()\n    self.W = torch.nn.Parameter(torch.randn(N, M))\n    self.b = torch.nn.Parameter(torch.randn(M))\n\n  def forward(self, input):\n    return torch.addmm(self.b, input, self.W)",
                                        "code"
                                    ],
                                    [
                                        "In C++, it would look like this:",
                                        "markdown"
                                    ],
                                    [
                                        "#include &lt;torch/torch.h&gt;\n\nstruct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M) {\n    W = register_parameter(\"W\", torch::randn({N, M}));\n    b = register_parameter(\"b\", torch::randn(M));\n  }\n  torch::Tensor forward(torch::Tensor input) {\n    return torch::addmm(b, input, W);\n  }\n  torch::Tensor W, b;\n};",
                                        "code"
                                    ],
                                    [
                                        "Just like in Python, we define a class called Net (for simplicity here a\nstruct instead of a class) and derive it from the module base class.\nInside the constructor, we create tensors using torch::randn just like we\nuse torch.randn in Python. One interesting difference is how we register the\nparameters. In Python, we wrap the tensors with the torch.nn.Parameter\nclass, while in C++ we have to pass the tensor through the\nregister_parameter method instead. The reason for this is that the Python\nAPI can detect that an attribute is of type torch.nn.Parameter and\nautomatically registers such tensors. In C++, reflection is very limited, so a\nmore traditional (and less magical) approach is provided.",
                                        "markdown"
                                    ]
                                ]
                            },
                            {
                                "Registering Submodules and Traversing the Module Hierarchy": [
                                    [
                                        "In the same way we can register parameters, we can also register submodules. In\nPython, submodules are automatically detected and registered when they are\nassigned as an attribute of a module:",
                                        "markdown"
                                    ],
                                    [
                                        "class Net(torch.nn.Module):\n  def __init__(self, N, M):\n      super(Net, self).__init__()\n      # Registered as a submodule behind the scenes\n      self.linear = torch.nn.Linear(N, M)\n      self.another_bias = torch.nn.Parameter(torch.rand(M))\n\n  def forward(self, input):\n    return self.linear(input) + self.another_bias",
                                        "code"
                                    ],
                                    [
                                        "This allows, for example, to use the parameters() method to recursively\naccess all parameters in our module hierarchy:",
                                        "markdown"
                                    ],
                                    [
                                        "&gt;&gt;&gt; net = Net(4, 5)\n&gt;&gt;&gt; print(list(net.parameters()))\n[Parameter containing:\ntensor([0.0808, 0.8613, 0.2017, 0.5206, 0.5353], requires_grad=True), Parameter containing:\ntensor([[-0.3740, -0.0976, -0.4786, -0.4928],\n        [-0.1434,  0.4713,  0.1735, -0.3293],\n        [-0.3467, -0.3858,  0.1980,  0.1986],\n        [-0.1975,  0.4278, -0.1831, -0.2709],\n        [ 0.3730,  0.4307,  0.3236, -0.0629]], requires_grad=True), Parameter containing:\ntensor([ 0.2038,  0.4638, -0.2023,  0.1230, -0.0516], requires_grad=True)]",
                                        "code"
                                    ],
                                    [
                                        "To register submodules in C++, use the aptly named register_module() method\nto register a module like torch::nn::Linear:",
                                        "markdown"
                                    ],
                                    [
                                        "struct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M)\n      : linear(register_module(\"linear\", torch::nn::Linear(N, M))) {\n    another_bias = register_parameter(\"b\", torch::randn(M));\n  }\n  torch::Tensor forward(torch::Tensor input) {\n    return linear(input) + another_bias;\n  }\n  torch::nn::Linear linear;\n  torch::Tensor another_bias;\n};",
                                        "code"
                                    ],
                                    [
                                        "Tip",
                                        "markdown"
                                    ],
                                    [
                                        "You can find the full list of available built-in modules like\ntorch::nn::Linear, torch::nn::Dropout or torch::nn::Conv2d in the\ndocumentation of the torch::nn namespace .",
                                        "markdown"
                                    ],
                                    [
                                        "One subtlety about the above code is why the submodule was created in the\nconstructor\u2019s initializer list, while the parameter was created inside the\nconstructor body. There is a good reason for this, which we\u2019ll touch upon this\nin the section on the C++ frontend\u2019s <em>ownership model</em> further below. The end\nresult, however, is that we can recursively access our module tree\u2019s parameters\njust like in Python. Calling parameters() returns a\nstd::vector&lt;torch::Tensor&gt;, which we can iterate over:",
                                        "markdown"
                                    ],
                                    [
                                        "int main() {\n  Net net(4, 5);\n  for (const auto&amp; p : net.parameters()) {\n    std::cout &lt;&lt; p &lt;&lt; std::endl;\n  }\n}",
                                        "code"
                                    ],
                                    [
                                        "which prints:",
                                        "markdown"
                                    ],
                                    [
                                        "root@fa350df05ecf:/home/build# ./dcgan\n0.0345\n1.4456\n-0.6313\n-0.3585\n-0.4008\n[ Variable[CPUFloatType]{5} ]\n-0.1647  0.2891  0.0527 -0.0354\n0.3084  0.2025  0.0343  0.1824\n-0.4630 -0.2862  0.2500 -0.0420\n0.3679 -0.1482 -0.0460  0.1967\n0.2132 -0.1992  0.4257  0.0739\n[ Variable[CPUFloatType]{5,4} ]\n0.01 *\n3.6861\n-10.1166\n-45.0333\n7.9983\n-20.0705\n[ Variable[CPUFloatType]{5} ]",
                                        "code"
                                    ],
                                    [
                                        "with three parameters just like in Python. To also see the names of these\nparameters, the C++ API provides a named_parameters() method which returns\nan OrderedDict just like in Python:",
                                        "markdown"
                                    ],
                                    [
                                        "Net net(4, 5);\nfor (const auto&amp; pair : net.named_parameters()) {\n  std::cout &lt;&lt; pair.key() &lt;&lt; \": \" &lt;&lt; pair.value() &lt;&lt; std::endl;\n}",
                                        "code"
                                    ],
                                    [
                                        "which we can execute again to see the output:",
                                        "markdown"
                                    ],
                                    [
                                        "root@fa350df05ecf:/home/build# make &amp;&amp; ./dcgan                                                                                                                                            11:13:48\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcgan\nb: -0.1863\n-0.8611\n-0.1228\n1.3269\n0.9858\n[ Variable[CPUFloatType]{5} ]\nlinear.weight:  0.0339  0.2484  0.2035 -0.2103\n-0.0715 -0.2975 -0.4350 -0.1878\n-0.3616  0.1050 -0.4982  0.0335\n-0.1605  0.4963  0.4099 -0.2883\n0.1818 -0.3447 -0.1501 -0.0215\n[ Variable[CPUFloatType]{5,4} ]\nlinear.bias: -0.0250\n0.0408\n0.3756\n-0.2149\n-0.3636\n[ Variable[CPUFloatType]{5} ]",
                                        "code"
                                    ],
                                    [
                                        "Note",
                                        "markdown"
                                    ],
                                    [
                                        "for torch::nn::Module contains the full list of methods that operate on\nthe module hierarchy.",
                                        "markdown"
                                    ]
                                ]
                            },
                            {
                                "Running the Network in Forward Mode": [
                                    [
                                        "To execute the network in C++, we simply call the forward() method we\ndefined ourselves:",
                                        "markdown"
                                    ],
                                    [
                                        "int main() {\n  Net net(4, 5);\n  std::cout &lt;&lt; net.forward(torch::ones({2, 4})) &lt;&lt; std::endl;\n}",
                                        "code"
                                    ],
                                    [
                                        "which prints something like:",
                                        "markdown"
                                    ],
                                    [
                                        "root@fa350df05ecf:/home/build# ./dcgan\n0.8559  1.1572  2.1069 -0.1247  0.8060\n0.8559  1.1572  2.1069 -0.1247  0.8060\n[ Variable[CPUFloatType]{2,5} ]",
                                        "code"
                                    ]
                                ]
                            },
                            {
                                "Module Ownership": [
                                    [
                                        "At this point, we know how to define a module in C++, register parameters,\nregister submodules, traverse the module hierarchy via methods like\nparameters() and finally run the module\u2019s forward() method. While there\nare many more methods, classes and topics to devour in the C++ API, I will refer\nyou to  for\nthe full menu. We\u2019ll also touch upon some more concepts as we implement the\nDCGAN model and end-to-end training pipeline in just a second. Before we do so,\nlet me briefly touch upon the <em>ownership model</em> the C++ frontend provides for\nsubclasses of torch::nn::Module.",
                                        "markdown"
                                    ],
                                    [
                                        "For this discussion, the ownership model refers to the way modules are stored\nand passed around \u2013 which determines who or what <em>owns</em> a particular module\ninstance. In Python, objects are always allocated dynamically (on the heap) and\nhave reference semantics. This is very easy to work with and straightforward to\nunderstand. In fact, in Python, you can largely forget about where objects live\nand how they get referenced, and focus on getting things done.",
                                        "markdown"
                                    ],
                                    [
                                        "C++, being a lower level language, provides more options in this realm. This\nincreases complexity and heavily influences the design and ergonomics of the C++\nfrontend. In particular, for modules in the C++ frontend, we have the option of\nusing <em>either</em> value semantics <em>or</em> reference semantics. The first case is the\nsimplest and was shown in the examples thus far: module objects are allocated on\nthe stack and when passed to a function, can be either copied, moved (with\nstd::move) or taken by reference or by pointer:",
                                        "markdown"
                                    ],
                                    [
                                        "struct Net : torch::nn::Module { };\n\nvoid a(Net net) { }\nvoid b(Net&amp; net) { }\nvoid c(Net* net) { }\n\nint main() {\n  Net net;\n  a(net);\n  a(std::move(net));\n  b(net);\n  c(&amp;net);\n}",
                                        "code"
                                    ],
                                    [
                                        "For the second case \u2013 reference semantics \u2013 we can use std::shared_ptr.\nThe advantage of reference semantics is that, like in Python, it reduces the\ncognitive overhead of thinking about how modules must be passed to functions and\nhow arguments must be declared (assuming you use shared_ptr everywhere).",
                                        "markdown"
                                    ],
                                    [
                                        "struct Net : torch::nn::Module {};\n\nvoid a(std::shared_ptr&lt;Net&gt; net) { }\n\nint main() {\n  auto net = std::make_shared&lt;Net&gt;();\n  a(net);\n}",
                                        "code"
                                    ],
                                    [
                                        "In our experience, researchers coming from dynamic languages greatly prefer\nreference semantics over value semantics, even though the latter is more\n\u201cnative\u201d to C++. It is also important to note that torch::nn::Module\u2019s\ndesign, in order to stay close to the ergonomics of the Python API, relies on\nshared ownership. For example, take our earlier (here shortened) definition of\nNet:",
                                        "markdown"
                                    ],
                                    [
                                        "struct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M)\n    : linear(register_module(\"linear\", torch::nn::Linear(N, M)))\n  { }\n  torch::nn::Linear linear;\n};",
                                        "code"
                                    ],
                                    [
                                        "In order to use the linear submodule, we want to store it directly in our\nclass. However, we also want the module base class to know about and have access\nto this submodule. For this, it must store a reference to this submodule. At\nthis point, we have already arrived at the need for shared ownership. Both the\ntorch::nn::Module class and concrete Net class require a reference to\nthe submodule. For this reason, the base class stores modules as\nshared_ptrs, and therefore the concrete class must too.",
                                        "markdown"
                                    ],
                                    [
                                        "But wait! I don\u2019t see any mention of shared_ptr in the above code! Why is\nthat? Well, because std::shared_ptr&lt;MyModule&gt; is a hell of a lot to type. To\nkeep our researchers productive, we came up with an elaborate scheme to hide the\nmention of shared_ptr \u2013 a benefit usually reserved for value semantics \u2013\nwhile retaining reference semantics. To understand how this works, we can take a\nlook at a simplified definition of the torch::nn::Linear module in the core\nlibrary (the full definition is ):",
                                        "markdown"
                                    ],
                                    [
                                        "struct LinearImpl : torch::nn::Module {\n  LinearImpl(int64_t in, int64_t out);\n\n  Tensor forward(const Tensor&amp; input);\n\n  Tensor weight, bias;\n};\n\nTORCH_MODULE(Linear);",
                                        "code"
                                    ],
                                    [
                                        "In brief: the module is not called Linear, but LinearImpl. A macro,\nTORCH_MODULE then defines the actual Linear class. This \u201cgenerated\u201d\nclass is effectively a wrapper over a std::shared_ptr&lt;LinearImpl&gt;. It is a\nwrapper instead of a simple typedef so that, among other things, constructors\nstill work as expected, i.e. you can still write torch::nn::Linear(3, 4)\ninstead of std::make_shared&lt;LinearImpl&gt;(3, 4). We call the class created by\nthe macro the module <em>holder</em>. Like with (shared) pointers, you access the\nunderlying object using the arrow operator (like model-&gt;forward(...)). The\nend result is an ownership model that resembles that of the Python API quite\nclosely. Reference semantics become the default, but without the extra typing of\nstd::shared_ptr or std::make_shared. For our Net, using the module\nholder API looks like this:",
                                        "markdown"
                                    ],
                                    [
                                        "struct NetImpl : torch::nn::Module {};\nTORCH_MODULE(Net);\n\nvoid a(Net net) { }\n\nint main() {\n  Net net;\n  a(net);\n}",
                                        "code"
                                    ],
                                    [
                                        "There is one subtle issue that deserves mention here. A default constructed\nstd::shared_ptr is \u201cempty\u201d, i.e. contains a null pointer. What is a default\nconstructed Linear or Net? Well, it\u2019s a tricky choice. We could say it\nshould be an empty (null) std::shared_ptr&lt;LinearImpl&gt;. However, recall that\nLinear(3, 4) is the same as std::make_shared&lt;LinearImpl&gt;(3, 4). This\nmeans that if we had decided that Linear linear; should be a null pointer,\nthen there would be no way to construct a module that does not take any\nconstructor arguments, or defaults all of them. For this reason, in the current\nAPI, a default constructed module holder (like Linear()) invokes the\ndefault constructor of the underlying module (LinearImpl()). If the\nunderlying module does not have a default constructor, you get a compiler error.\nTo instead construct the empty holder, you can pass nullptr to the\nconstructor of the holder.",
                                        "markdown"
                                    ],
                                    [
                                        "In practice, this means you can use submodules either like shown earlier, where\nthe module is registered and constructed in the <em>initializer list</em>:",
                                        "markdown"
                                    ],
                                    [
                                        "struct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M)\n    : linear(register_module(\"linear\", torch::nn::Linear(N, M)))\n  { }\n  torch::nn::Linear linear;\n};",
                                        "code"
                                    ],
                                    [
                                        "or you can first construct the holder with a null pointer and then assign to it\nin the constructor (more familiar for Pythonistas):",
                                        "markdown"
                                    ],
                                    [
                                        "struct Net : torch::nn::Module {\n  Net(int64_t N, int64_t M) {\n    linear = register_module(\"linear\", torch::nn::Linear(N, M));\n  }\n  torch::nn::Linear linear{nullptr}; // construct an empty holder\n};",
                                        "code"
                                    ],
                                    [
                                        "In conclusion: Which ownership model \u2013 which semantics \u2013 should you use? The\nC++ frontend\u2019s API best supports the ownership model provided by module holders.\nThe only disadvantage of this mechanism is one extra line of boilerplate below\nthe module declaration. That said, the simplest model is still the value\nsemantics model shown in the introduction to C++ modules. For small, simple\nscripts, you may get away with it too. But you\u2019ll find sooner or later that, for\ntechnical reasons, it is not always supported. For example, the serialization\nAPI (torch::save and torch::load) only supports module holders (or plain\nshared_ptr). As such, the module holder API is the recommended way of\ndefining modules with the C++ frontend, and we will use this API in this\ntutorial henceforth.",
                                        "markdown"
                                    ]
                                ]
                            }
                        ]
                    },
                    {
                        "Defining the DCGAN Modules": [
                            [
                                "We now have the necessary background and introduction to define the modules for\nthe machine learning task we want to solve in this post. To recap: our task is\nto generate images of digits from the . We want to use a  to solve\nthis task. In particular, we\u2019ll use a  \u2013 one of the first and simplest of its\nkind, but entirely sufficient for this task.",
                                "markdown"
                            ],
                            [
                                "Tip",
                                "markdown"
                            ],
                            [
                                "You can find the full source code presented in this tutorial .",
                                "markdown"
                            ],
                            {
                                "What was a GAN aGAN?": [
                                    [
                                        "A GAN consists of two distinct neural network models: a <em>generator</em> and a\n<em>discriminator</em>. The generator receives samples from a noise distribution, and\nits aim is to transform each noise sample into an image that resembles those of\na target distribution \u2013 in our case the MNIST dataset. The discriminator in\nturn receives either <em>real</em> images from the MNIST dataset, or <em>fake</em> images from\nthe generator. It is asked to emit a probability judging how real (closer to\n1) or fake (closer to 0) a particular image is. Feedback from the\ndiscriminator on how real the images produced by the generator are is used to\ntrain the generator. Feedback on how good of an eye for authenticity the\ndiscriminator has is used to optimize the discriminator. In theory, a delicate\nbalance between the generator and discriminator makes them improve in tandem,\nleading to the generator producing images indistinguishable from the target\ndistribution, fooling the discriminator\u2019s (by then) excellent eye into emitting\na probability of 0.5 for both real and fake images. For us, the end result\nis a machine that receives noise as input and generates realistic images of\ndigits as its output.",
                                        "markdown"
                                    ]
                                ]
                            },
                            {
                                "The Generator Module": [
                                    [
                                        "We begin by defining the generator module, which consists of a series of\ntransposed 2D convolutions, batch normalizations and ReLU activation units.\nWe explicitly pass inputs (in a functional way) between modules in the\nforward() method of a module we define ourselves:",
                                        "markdown"
                                    ],
                                    [
                                        "struct DCGANGeneratorImpl : nn::Module {\n  DCGANGeneratorImpl(int kNoiseSize)\n      : conv1(nn::ConvTranspose2dOptions(kNoiseSize, 256, 4)\n                  .bias(false)),\n        batch_norm1(256),\n        conv2(nn::ConvTranspose2dOptions(256, 128, 3)\n                  .stride(2)\n                  .padding(1)\n                  .bias(false)),\n        batch_norm2(128),\n        conv3(nn::ConvTranspose2dOptions(128, 64, 4)\n                  .stride(2)\n                  .padding(1)\n                  .bias(false)),\n        batch_norm3(64),\n        conv4(nn::ConvTranspose2dOptions(64, 1, 4)\n                  .stride(2)\n                  .padding(1)\n                  .bias(false))\n {\n   // register_module() is needed if we want to use the parameters() method later on\n   register_module(\"conv1\", conv1);\n   register_module(\"conv2\", conv2);\n   register_module(\"conv3\", conv3);\n   register_module(\"conv4\", conv4);\n   register_module(\"batch_norm1\", batch_norm1);\n   register_module(\"batch_norm2\", batch_norm2);\n   register_module(\"batch_norm3\", batch_norm3);\n }\n\n torch::Tensor forward(torch::Tensor x) {\n   x = torch::relu(batch_norm1(conv1(x)));\n   x = torch::relu(batch_norm2(conv2(x)));\n   x = torch::relu(batch_norm3(conv3(x)));\n   x = torch::tanh(conv4(x));\n   return x;\n }\n\n nn::ConvTranspose2d conv1, conv2, conv3, conv4;\n nn::BatchNorm2d batch_norm1, batch_norm2, batch_norm3;\n};\nTORCH_MODULE(DCGANGenerator);\n\nDCGANGenerator generator(kNoiseSize);",
                                        "code"
                                    ],
                                    [
                                        "We can now invoke forward() on the DCGANGenerator to map a noise sample to an image.",
                                        "markdown"
                                    ],
                                    [
                                        "The particular modules chosen, like nn::ConvTranspose2d and nn::BatchNorm2d,\nfollows the structure outlined earlier. The kNoiseSize constant determines\nthe size of the input noise vector and is set to 100. Hyperparameters were,\nof course, found via grad student descent.",
                                        "markdown"
                                    ],
                                    [
                                        "Attention",
                                        "markdown"
                                    ],
                                    [
                                        "No grad students were harmed in the discovery of hyperparameters. They were\nfed Soylent regularly.",
                                        "markdown"
                                    ],
                                    [
                                        "Note",
                                        "markdown"
                                    ],
                                    [
                                        "A brief word on the way options are passed to built-in modules like Conv2d\nin the C++ frontend: Every module has some required options, like the number\nof features for BatchNorm2d. If you only need to configure the required\noptions, you can pass them directly to the module\u2019s constructor, like\nBatchNorm2d(128) or Dropout(0.5) or Conv2d(8, 4, 2) (for input\nchannel count, output channel count, and kernel size). If, however, you need\nto modify other options, which are normally defaulted, such as bias\nfor Conv2d, you need to construct and pass an <em>options</em> object. Every\nmodule in the C++ frontend has an associated options struct, called\nModuleOptions where Module is the name of the module, like\nLinearOptions for Linear. This is what we do for the Conv2d\nmodules above.",
                                        "markdown"
                                    ]
                                ]
                            },
                            {
                                "The Discriminator Module": [
                                    [
                                        "The discriminator is similarly a sequence of convolutions, batch normalizations\nand activations. However, the convolutions are now regular ones instead of\ntransposed, and we use a leaky ReLU with an alpha value of 0.2 instead of a\nvanilla ReLU. Also, the final activation becomes a Sigmoid, which squashes\nvalues into a range between 0 and 1. We can then interpret these squashed values\nas the probabilities the discriminator assigns to images being real.",
                                        "markdown"
                                    ],
                                    [
                                        "To build the discriminator, we will try something different: a <cite>Sequential</cite> module.\nLike in Python, PyTorch here provides two APIs for model definition: a functional one\nwhere inputs are passed through successive functions (e.g. the generator module example),\nand a more object-oriented one where we build a <cite>Sequential</cite> module containing the\nentire model as submodules. Using <cite>Sequential</cite>, the discriminator would look like:",
                                        "markdown"
                                    ],
                                    [
                                        "nn::Sequential discriminator(\n  // Layer 1\n  nn::Conv2d(\n      nn::Conv2dOptions(1, 64, 4).stride(2).padding(1).bias(false)),\n  nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)),\n  // Layer 2\n  nn::Conv2d(\n      nn::Conv2dOptions(64, 128, 4).stride(2).padding(1).bias(false)),\n  nn::BatchNorm2d(128),\n  nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)),\n  // Layer 3\n  nn::Conv2d(\n      nn::Conv2dOptions(128, 256, 4).stride(2).padding(1).bias(false)),\n  nn::BatchNorm2d(256),\n  nn::LeakyReLU(nn::LeakyReLUOptions().negative_slope(0.2)),\n  // Layer 4\n  nn::Conv2d(\n      nn::Conv2dOptions(256, 1, 3).stride(1).padding(0).bias(false)),\n  nn::Sigmoid());",
                                        "code"
                                    ],
                                    [
                                        "Tip",
                                        "markdown"
                                    ],
                                    [
                                        "A Sequential module simply performs function composition. The output of\nthe first submodule becomes the input of the second, the output of the third\nbecomes the input of the fourth and so on.",
                                        "markdown"
                                    ]
                                ]
                            }
                        ]
                    }
                ]
            },
            {
                "Loading Data": [
                    [
                        "Now that we have defined the generator and discriminator model, we need some\ndata we can train these models with. The C++ frontend, like the Python one,\ncomes with a powerful parallel data loader. This data loader can read batches of\ndata from a dataset (which you can define yourself) and provides many\nconfiguration knobs.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "While the Python data loader uses multi-processing, the C++ data loader is truly\nmulti-threaded and does not launch any new processes.",
                        "markdown"
                    ],
                    [
                        "The data loader is part of the C++ frontend\u2019s data api, contained in the\ntorch::data:: namespace. This API consists of a few different components:",
                        "markdown"
                    ],
                    [
                        "The data loader class,",
                        "markdown"
                    ],
                    [
                        "An API for defining datasets,",
                        "markdown"
                    ],
                    [
                        "An API for defining <em>transforms</em>, which can be applied to datasets,",
                        "markdown"
                    ],
                    [
                        "An API for defining <em>samplers</em>, which produce the indices with which datasets are indexed,",
                        "markdown"
                    ],
                    [
                        "A library of existing datasets, transforms and samplers.",
                        "markdown"
                    ],
                    [
                        "For this tutorial, we can use the MNIST dataset that comes with the C++\nfrontend. Let\u2019s instantiate a torch::data::datasets::MNIST for this, and\napply two transformations: First, we normalize the images so that they are in\nthe range of -1 to +1 (from an original range of 0 to 1).\nSecond, we apply the Stack <em>collation</em>, which takes a batch of tensors and\nstacks them into a single tensor along the first dimension:",
                        "markdown"
                    ],
                    [
                        "auto dataset = torch::data::datasets::MNIST(\"./mnist\")\n    .map(torch::data::transforms::Normalize&lt;&gt;(0.5, 0.5))\n    .map(torch::data::transforms::Stack&lt;&gt;());",
                        "code"
                    ],
                    [
                        "Note that the MNIST dataset should be located in the ./mnist directory\nrelative to wherever you execute the training binary from. You can use \nto download the MNIST dataset.",
                        "markdown"
                    ],
                    [
                        "Next, we create a data loader and pass it this dataset. To make a new data\nloader, we use torch::data::make_data_loader, which returns a\nstd::unique_ptr of the correct type (which depends on the type of the\ndataset, the type of the sampler and some other implementation details):",
                        "markdown"
                    ],
                    [
                        "auto data_loader = torch::data::make_data_loader(std::move(dataset));",
                        "code"
                    ],
                    [
                        "The data loader does come with a lot of options. You can inspect the full set\n.\nFor example, to speed up the data loading, we can increase the number of\nworkers. The default number is zero, which means the main thread will be used.\nIf we set workers to 2, two threads will be spawned that load data\nconcurrently. We should also increase the batch size from its default of 1\nto something more reasonable, like 64 (the value of kBatchSize). So\nlet\u2019s create a DataLoaderOptions object and set the appropriate properties:",
                        "markdown"
                    ],
                    [
                        "auto data_loader = torch::data::make_data_loader(\n    std::move(dataset),\n    torch::data::DataLoaderOptions().batch_size(kBatchSize).workers(2));",
                        "code"
                    ],
                    [
                        "We can now write a loop to load batches of data, which we\u2019ll only print to the\nconsole for now:",
                        "markdown"
                    ],
                    [
                        "for (torch::data::Example&lt;&gt;&amp; batch : *data_loader) {\n  std::cout &lt;&lt; \"Batch size: \" &lt;&lt; batch.data.size(0) &lt;&lt; \" | Labels: \";\n  for (int64_t i = 0; i &lt; batch.data.size(0); ++i) {\n    std::cout &lt;&lt; batch.target[i].item&lt;int64_t&gt;() &lt;&lt; \" \";\n  }\n  std::cout &lt;&lt; std::endl;\n}",
                        "code"
                    ],
                    [
                        "The type returned by the data loader in this case is a torch::data::Example.\nThis type is a simple struct with a data field for the data and a target\nfield for the label. Because we applied the Stack collation earlier, the\ndata loader returns only a single such example. If we had not applied the\ncollation, the data loader would yield std::vector&lt;torch::data::Example&lt;&gt;&gt;\ninstead, with one element per example in the batch.",
                        "markdown"
                    ],
                    [
                        "If you rebuild and run this code, you should see something like this:",
                        "markdown"
                    ],
                    [
                        "root@fa350df05ecf:/home/build# make\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcgan\nroot@fa350df05ecf:/home/build# make\n[100%] Built target dcgan\nroot@fa350df05ecf:/home/build# ./dcgan\nBatch size: 64 | Labels: 5 2 6 7 2 1 6 7 0 1 6 2 3 6 9 1 8 4 0 6 5 3 3 0 4 6 6 6 4 0 8 6 0 6 9 2 4 0 2 8 6 3 3 2 9 2 0 1 4 2 3 4 8 2 9 9 3 5 8 0 0 7 9 9\nBatch size: 64 | Labels: 2 2 4 7 1 2 8 8 6 9 0 2 2 9 3 6 1 3 8 0 4 4 8 8 8 9 2 6 4 7 1 5 0 9 7 5 4 3 5 4 1 2 8 0 7 1 9 6 1 6 5 3 4 4 1 2 3 2 3 5 0 1 6 2\nBatch size: 64 | Labels: 4 5 4 2 1 4 8 3 8 3 6 1 5 4 3 6 2 2 5 1 3 1 5 0 8 2 1 5 3 2 4 4 5 9 7 2 8 9 2 0 6 7 4 3 8 3 5 8 8 3 0 5 8 0 8 7 8 5 5 6 1 7 8 0\nBatch size: 64 | Labels: 3 3 7 1 4 1 6 1 0 3 6 4 0 2 5 4 0 4 2 8 1 9 6 5 1 6 3 2 8 9 2 3 8 7 4 5 9 6 0 8 3 0 0 6 4 8 2 5 4 1 8 3 7 8 0 0 8 9 6 7 2 1 4 7\nBatch size: 64 | Labels: 3 0 5 5 9 8 3 9 8 9 5 9 5 0 4 1 2 7 7 2 0 0 5 4 8 7 7 6 1 0 7 9 3 0 6 3 2 6 2 7 6 3 3 4 0 5 8 8 9 1 9 2 1 9 4 4 9 2 4 6 2 9 4 0\nBatch size: 64 | Labels: 9 6 7 5 3 5 9 0 8 6 6 7 8 2 1 9 8 8 1 1 8 2 0 7 1 4 1 6 7 5 1 7 7 4 0 3 2 9 0 6 6 3 4 4 8 1 2 8 6 9 2 0 3 1 2 8 5 6 4 8 5 8 6 2\nBatch size: 64 | Labels: 9 3 0 3 6 5 1 8 6 0 1 9 9 1 6 1 7 7 4 4 4 7 8 8 6 7 8 2 6 0 4 6 8 2 5 3 9 8 4 0 9 9 3 7 0 5 8 2 4 5 6 2 8 2 5 3 7 1 9 1 8 2 2 7\nBatch size: 64 | Labels: 9 1 9 2 7 2 6 0 8 6 8 7 7 4 8 6 1 1 6 8 5 7 9 1 3 2 0 5 1 7 3 1 6 1 0 8 6 0 8 1 0 5 4 9 3 8 5 8 4 8 0 1 2 6 2 4 2 7 7 3 7 4 5 3\nBatch size: 64 | Labels: 8 8 3 1 8 6 4 2 9 5 8 0 2 8 6 6 7 0 9 8 3 8 7 1 6 6 2 7 7 4 5 5 2 1 7 9 5 4 9 1 0 3 1 9 3 9 8 8 5 3 7 5 3 6 8 9 4 2 0 1 2 5 4 7\nBatch size: 64 | Labels: 9 2 7 0 8 4 4 2 7 5 0 0 6 2 0 5 9 5 9 8 8 9 3 5 7 5 4 7 3 0 5 7 6 5 7 1 6 2 8 7 6 3 2 6 5 6 1 2 7 7 0 0 5 9 0 0 9 1 7 8 3 2 9 4\nBatch size: 64 | Labels: 7 6 5 7 7 5 2 2 4 9 9 4 8 7 4 8 9 4 5 7 1 2 6 9 8 5 1 2 3 6 7 8 1 1 3 9 8 7 9 5 0 8 5 1 8 7 2 6 5 1 2 0 9 7 4 0 9 0 4 6 0 0 8 6\n...",
                        "code"
                    ],
                    [
                        "Which means we are successfully able to load data from the MNIST dataset.",
                        "markdown"
                    ]
                ]
            },
            {
                "Writing the Training Loop": [
                    [
                        "Let\u2019s now finish the algorithmic part of our example and implement the delicate\ndance between the generator and discriminator. First, we\u2019ll create two\noptimizers, one for the generator and one for the discriminator. The optimizers\nwe use implement the  algorithm:",
                        "markdown"
                    ],
                    [
                        "torch::optim::Adam generator_optimizer(\n    generator-&gt;parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5));\ntorch::optim::Adam discriminator_optimizer(\n    discriminator-&gt;parameters(), torch::optim::AdamOptions(5e-4).beta1(0.5));",
                        "code"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "As of this writing, the C++ frontend provides optimizers implementing Adagrad,\nAdam, LBFGS, RMSprop and SGD. The  have the\nup-to-date list.",
                        "markdown"
                    ],
                    [
                        "Next, we need to update our training loop. We\u2019ll add an outer loop to exhaust\nthe data loader every epoch and then write the GAN training code:",
                        "markdown"
                    ],
                    [
                        "for (int64_t epoch = 1; epoch &lt;= kNumberOfEpochs; ++epoch) {\n  int64_t batch_index = 0;\n  for (torch::data::Example&lt;&gt;&amp; batch : *data_loader) {\n    // Train discriminator with real images.\n    discriminator-&gt;zero_grad();\n    torch::Tensor real_images = batch.data;\n    torch::Tensor real_labels = torch::empty(batch.data.size(0)).uniform_(0.8, 1.0);\n    torch::Tensor real_output = discriminator-&gt;forward(real_images);\n    torch::Tensor d_loss_real = torch::binary_cross_entropy(real_output, real_labels);\n    d_loss_real.backward();\n\n    // Train discriminator with fake images.\n    torch::Tensor noise = torch::randn({batch.data.size(0), kNoiseSize, 1, 1});\n    torch::Tensor fake_images = generator-&gt;forward(noise);\n    torch::Tensor fake_labels = torch::zeros(batch.data.size(0));\n    torch::Tensor fake_output = discriminator-&gt;forward(fake_images.detach());\n    torch::Tensor d_loss_fake = torch::binary_cross_entropy(fake_output, fake_labels);\n    d_loss_fake.backward();\n\n    torch::Tensor d_loss = d_loss_real + d_loss_fake;\n    discriminator_optimizer.step();\n\n    // Train generator.\n    generator-&gt;zero_grad();\n    fake_labels.fill_(1);\n    fake_output = discriminator-&gt;forward(fake_images);\n    torch::Tensor g_loss = torch::binary_cross_entropy(fake_output, fake_labels);\n    g_loss.backward();\n    generator_optimizer.step();\n\n    std::printf(\n        \"\\r[%2ld/%2ld][%3ld/%3ld] D_loss: %.4f | G_loss: %.4f\",\n        epoch,\n        kNumberOfEpochs,\n        ++batch_index,\n        batches_per_epoch,\n        d_loss.item&lt;float&gt;(),\n        g_loss.item&lt;float&gt;());\n  }\n}",
                        "code"
                    ],
                    [
                        "Above, we first evaluate the discriminator on real images, for which it should\nassign a high probability. For this, we use\ntorch::empty(batch.data.size(0)).uniform_(0.8, 1.0) as the target\nprobabilities.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "We pick random values uniformly distributed between 0.8 and 1.0 instead of 1.0\neverywhere in order to make the discriminator training more robust. This trick\nis called <em>label smoothing</em>.",
                        "markdown"
                    ],
                    [
                        "Before evaluating the discriminator, we zero out the gradients of its\nparameters. After computing the loss, we back-propagate it through the network by\ncalling d_loss.backward() to compute new gradients. We repeat this spiel for\nthe fake images. Instead of using images from the dataset, we let the generator\ncreate fake images for this by feeding it a batch of random noise. We then\nforward those fake images to the discriminator. This time, we want the\ndiscriminator to emit low probabilities, ideally all zeros. Once we have\ncomputed the discriminator loss for both the batch of real and the batch of fake\nimages, we can progress the discriminator\u2019s optimizer by one step in order to\nupdate its parameters.",
                        "markdown"
                    ],
                    [
                        "To train the generator, we again first zero its gradients, and then re-evaluate\nthe discriminator on the fake images. However, this time we want the\ndiscriminator to assign probabilities very close to one, which would indicate\nthat the generator can produce images that fool the discriminator into thinking\nthey are actually real (from the dataset). For this, we fill the fake_labels\ntensor with all ones. We finally step the generator\u2019s optimizer to also update\nits parameters.",
                        "markdown"
                    ],
                    [
                        "We should now be ready to train our model on the CPU. We don\u2019t have any code yet\nto capture state or sample outputs, but we\u2019ll add this in just a moment. For\nnow, let\u2019s just observe that our model is doing <em>something</em> \u2013 we\u2019ll later\nverify based on the generated images whether this something is meaningful.\nRe-building and running should print something like:",
                        "markdown"
                    ],
                    [
                        "root@3c0711f20896:/home/build# make &amp;&amp; ./dcgan\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcga\n[ 1/10][100/938] D_loss: 0.6876 | G_loss: 4.1304\n[ 1/10][200/938] D_loss: 0.3776 | G_loss: 4.3101\n[ 1/10][300/938] D_loss: 0.3652 | G_loss: 4.6626\n[ 1/10][400/938] D_loss: 0.8057 | G_loss: 2.2795\n[ 1/10][500/938] D_loss: 0.3531 | G_loss: 4.4452\n[ 1/10][600/938] D_loss: 0.3501 | G_loss: 5.0811\n[ 1/10][700/938] D_loss: 0.3581 | G_loss: 4.5623\n[ 1/10][800/938] D_loss: 0.6423 | G_loss: 1.7385\n[ 1/10][900/938] D_loss: 0.3592 | G_loss: 4.7333\n[ 2/10][100/938] D_loss: 0.4660 | G_loss: 2.5242\n[ 2/10][200/938] D_loss: 0.6364 | G_loss: 2.0886\n[ 2/10][300/938] D_loss: 0.3717 | G_loss: 3.8103\n[ 2/10][400/938] D_loss: 1.0201 | G_loss: 1.3544\n[ 2/10][500/938] D_loss: 0.4522 | G_loss: 2.6545\n...",
                        "code"
                    ]
                ]
            },
            {
                "Moving to the GPU": [
                    [
                        "While our current script can run just fine on the CPU, we all know convolutions\nare a lot faster on GPU. Let\u2019s quickly discuss how we can move our training onto\nthe GPU. We\u2019ll need to do two things for this: pass a GPU device specification\nto tensors we allocate ourselves, and explicitly copy any other tensors onto the\nGPU via the to() method all tensors and modules in the C++ frontend have.\nThe simplest way to achieve both is to create an instance of torch::Device\nat the top level of our training script, and then pass that device to tensor\nfactory functions like torch::zeros as well as the to() method. We can\nstart by doing this with a CPU device:",
                        "markdown"
                    ],
                    [
                        "// Place this somewhere at the top of your training script.\ntorch::Device device(torch::kCPU);",
                        "code"
                    ],
                    [
                        "New tensor allocations like",
                        "markdown"
                    ],
                    [
                        "torch::Tensor fake_labels = torch::zeros(batch.data.size(0));",
                        "code"
                    ],
                    [
                        "should be updated to take the device as the last argument:",
                        "markdown"
                    ],
                    [
                        "torch::Tensor fake_labels = torch::zeros(batch.data.size(0), device);",
                        "code"
                    ],
                    [
                        "For tensors whose creation is not in our hands, like those coming from the MNIST\ndataset, we must insert explicit to() calls. This means",
                        "markdown"
                    ],
                    [
                        "torch::Tensor real_images = batch.data;",
                        "code"
                    ],
                    [
                        "becomes",
                        "markdown"
                    ],
                    [
                        "torch::Tensor real_images = batch.data.to(device);",
                        "code"
                    ],
                    [
                        "and also our model parameters should be moved to the correct device:",
                        "markdown"
                    ],
                    [
                        "generator-&gt;to(device);\ndiscriminator-&gt;to(device);",
                        "code"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "If a tensor already lives on the device supplied to to(), the call is a\nno-op. No extra copy is made.",
                        "markdown"
                    ],
                    [
                        "At this point, we\u2019ve just made our previous CPU-residing code more explicit.\nHowever, it is now also very easy to change the device to a CUDA device:",
                        "markdown"
                    ],
                    [
                        "torch::Device device(torch::kCUDA)",
                        "code"
                    ],
                    [
                        "And now all tensors will live on the GPU, calling into fast CUDA kernels for all\noperations, without us having to change any downstream code. If we wanted to\nspecify a particular device index, it could be passed as the second argument to\nthe Device constructor. If we wanted different tensors to live on different\ndevices, we could pass separate device instances (for example one on CUDA device\n0 and the other on CUDA device 1). We can even do this configuration\ndynamically, which is often useful to make our training scripts more portable:",
                        "markdown"
                    ],
                    [
                        "torch::Device device = torch::kCPU;\nif (torch::cuda::is_available()) {\n  std::cout &lt;&lt; \"CUDA is available! Training on GPU.\" &lt;&lt; std::endl;\n  device = torch::kCUDA;\n}",
                        "code"
                    ],
                    [
                        "or even",
                        "markdown"
                    ],
                    [
                        "torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU);",
                        "code"
                    ]
                ]
            },
            {
                "Checkpointing and Recovering the Training State": [
                    [
                        "The last augmentation we should make to our training script is to periodically\nsave the state of our model parameters, the state of our optimizers as well as a\nfew generated image samples. If our computer were to crash in the middle of the\ntraining procedure, the first two will allow us to restore the training state.\nFor long-lasting training sessions, this is absolutely essential. Fortunately,\nthe C++ frontend provides an API to serialize and deserialize both model and\noptimizer state, as well as individual tensors.",
                        "markdown"
                    ],
                    [
                        "The core API for this is torch::save(thing,filename) and\ntorch::load(thing,filename), where thing could be a\ntorch::nn::Module subclass or an optimizer instance like the Adam object\nwe have in our training script. Let\u2019s update our training loop to checkpoint the\nmodel and optimizer state at a certain interval:",
                        "markdown"
                    ],
                    [
                        "if (batch_index % kCheckpointEvery == 0) {\n  // Checkpoint the model and optimizer state.\n  torch::save(generator, \"generator-checkpoint.pt\");\n  torch::save(generator_optimizer, \"generator-optimizer-checkpoint.pt\");\n  torch::save(discriminator, \"discriminator-checkpoint.pt\");\n  torch::save(discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\");\n  // Sample the generator and save the images.\n  torch::Tensor samples = generator-&gt;forward(torch::randn({8, kNoiseSize, 1, 1}, device));\n  torch::save((samples + 1.0) / 2.0, torch::str(\"dcgan-sample-\", checkpoint_counter, \".pt\"));\n  std::cout &lt;&lt; \"\\n-&gt; checkpoint \" &lt;&lt; ++checkpoint_counter &lt;&lt; '\\n';\n}",
                        "code"
                    ],
                    [
                        "where kCheckpointEvery is an integer set to something like 100 to\ncheckpoint every 100 batches, and checkpoint_counter is a counter bumped\nevery time we make a checkpoint.",
                        "markdown"
                    ],
                    [
                        "To restore the training state, you can add lines like these after all models and\noptimizers are created, but before the training loop:",
                        "markdown"
                    ],
                    [
                        "torch::optim::Adam generator_optimizer(\n    generator-&gt;parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5));\ntorch::optim::Adam discriminator_optimizer(\n    discriminator-&gt;parameters(), torch::optim::AdamOptions(2e-4).beta1(0.5));\n\nif (kRestoreFromCheckpoint) {\n  torch::load(generator, \"generator-checkpoint.pt\");\n  torch::load(generator_optimizer, \"generator-optimizer-checkpoint.pt\");\n  torch::load(discriminator, \"discriminator-checkpoint.pt\");\n  torch::load(\n      discriminator_optimizer, \"discriminator-optimizer-checkpoint.pt\");\n}\n\nint64_t checkpoint_counter = 0;\nfor (int64_t epoch = 1; epoch &lt;= kNumberOfEpochs; ++epoch) {\n  int64_t batch_index = 0;\n  for (torch::data::Example&lt;&gt;&amp; batch : *data_loader) {",
                        "code"
                    ]
                ]
            },
            {
                "Inspecting Generated Images": [
                    [
                        "Our training script is now complete. We are ready to train our GAN, whether on\nCPU or GPU. To inspect the intermediary output of our training procedure, for\nwhich we added code to periodically save image samples to the\n\"dcgan-sample-xxx.pt\" file, we can write a tiny Python script to load the\ntensors and display them with matplotlib:",
                        "markdown"
                    ],
                    [
                        "from __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\n\nimport matplotlib.pyplot as plt\nimport torch\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-i\", \"--sample-file\", required=True)\nparser.add_argument(\"-o\", \"--out-file\", default=\"out.png\")\nparser.add_argument(\"-d\", \"--dimension\", type=int, default=3)\noptions = parser.parse_args()\n\nmodule = torch.jit.load(options.sample_file)\nimages = list(module.parameters())[0]\n\nfor index in range(options.dimension * options.dimension):\n  image = images[index].detach().cpu().reshape(28, 28).mul(255).to(torch.uint8)\n  array = image.numpy()\n  axis = plt.subplot(options.dimension, options.dimension, 1 + index)\n  plt.imshow(array, cmap=\"gray\")\n  axis.get_xaxis().set_visible(False)\n  axis.get_yaxis().set_visible(False)\n\nplt.savefig(options.out_file)\nprint(\"Saved \", options.out_file)",
                        "code"
                    ],
                    [
                        "Let\u2019s now train our model for around 30 epochs:",
                        "markdown"
                    ],
                    [
                        "root@3c0711f20896:/home/build# make &amp;&amp; ./dcgan                                                                                                                                10:17:57\nScanning dependencies of target dcgan\n[ 50%] Building CXX object CMakeFiles/dcgan.dir/dcgan.cpp.o\n[100%] Linking CXX executable dcgan\n[100%] Built target dcgan\nCUDA is available! Training on GPU.\n[ 1/30][200/938] D_loss: 0.4953 | G_loss: 4.0195\n-&gt; checkpoint 1\n[ 1/30][400/938] D_loss: 0.3610 | G_loss: 4.8148\n-&gt; checkpoint 2\n[ 1/30][600/938] D_loss: 0.4072 | G_loss: 4.36760\n-&gt; checkpoint 3\n[ 1/30][800/938] D_loss: 0.4444 | G_loss: 4.0250\n-&gt; checkpoint 4\n[ 2/30][200/938] D_loss: 0.3761 | G_loss: 3.8790\n-&gt; checkpoint 5\n[ 2/30][400/938] D_loss: 0.3977 | G_loss: 3.3315\n...\n-&gt; checkpoint 120\n[30/30][938/938] D_loss: 0.3610 | G_loss: 3.8084",
                        "code"
                    ],
                    [
                        "And display the images in a plot:",
                        "markdown"
                    ],
                    [
                        "root@3c0711f20896:/home/build# python display.py -i dcgan-sample-100.pt\nSaved out.png",
                        "code"
                    ],
                    [
                        "Which should look something like this:\n\n<img alt=\"digits\" src=\"../_images/digits.png\"/>",
                        "markdown"
                    ],
                    [
                        "Digits! Hooray! Now the ball is in your court: can you improve the model to make\nthe digits look even better?",
                        "markdown"
                    ]
                ]
            },
            {
                "Conclusion": [
                    [
                        "This tutorial has hopefully given you a digestible digest of the PyTorch C++\nfrontend. A machine learning library like PyTorch by necessity has a very broad\nand extensive API. As such, there are many concepts we did not have time or\nspace to discuss here. However, I encourage you to try out the API, and consult\n and in particular the\n section when\nyou get stuck. Also, remember that you can expect the C++ frontend to follow the\ndesign and semantics of the Python frontend whenever we could make this\npossible, so you can leverage this fact to increase your learning rate.",
                        "markdown"
                    ],
                    [
                        "Tip",
                        "markdown"
                    ],
                    [
                        "You can find the full source code presented in this tutorial .",
                        "markdown"
                    ],
                    [
                        "As always, if you run into any problems or have questions, you can use our\n or  to get in touch.",
                        "markdown"
                    ]
                ]
            }
        ],
        "Dynamic Parallelism in TorchScript": [
            [
                "In this tutorial, we introduce the syntax for doing <em>dynamic inter-op parallelism</em>\nin TorchScript. This parallelism has the following properties:",
                "markdown"
            ],
            [
                "dynamic - The number of parallel tasks created and their workload can depend on the control flow of the program.",
                "markdown"
            ],
            [
                "inter-op - The parallelism is concerned with running TorchScript program fragments in parallel. This is distinct from <em>intra-op parallelism</em>, which is concerned with splitting up individual operators and running subsets of the operator\u2019s work in parallel.",
                "markdown"
            ],
            {
                "Basic Syntax": [
                    [
                        "The two important APIs for dynamic parallelism are:",
                        "markdown"
                    ],
                    [
                        "torch.jit.fork(fn : Callable[..., T], *args, **kwargs) -&gt; torch.jit.Future[T]",
                        "markdown"
                    ],
                    [
                        "torch.jit.wait(fut : torch.jit.Future[T]) -&gt; T",
                        "markdown"
                    ],
                    [
                        "A good way to demonstrate how these work is by way of an example:",
                        "markdown"
                    ],
                    [
                        "import torch\n\ndef foo(x):\n    return torch.neg(x)\n\n@torch.jit.script\ndef example(x):\n    # Call `foo` using parallelism:\n    # First, we \"fork\" off a task. This task will run `foo` with argument `x`\n    future = torch.jit.fork(foo, x)\n\n    # Call `foo` normally\n    x_normal = foo(x)\n\n    # Second, we \"wait\" on the task. Since the task may be running in\n    # parallel, we have to \"wait\" for its result to become available.\n    # Notice that by having lines of code between the \"fork()\" and \"wait()\"\n    # call for a given Future, we can overlap computations so that they\n    # run in parallel.\n    x_parallel = torch.jit.wait(future)\n\n    return x_normal, x_parallel\n\nprint(example(torch.ones(1))) # (-1., -1.)",
                        "code"
                    ],
                    [
                        "fork() takes the callable fn and arguments to that callable args\nand kwargs and creates an asynchronous task for the execution of fn.\nfn can be a function, method, or Module instance. fork() returns a\nreference to the value of the result of this execution, called a Future.\nBecause fork returns immediately after creating the async task, fn may\nnot have been executed by the time the line of code after the fork() call\nis executed. Thus, wait() is used to wait for the async task to complete\nand return the value.",
                        "markdown"
                    ],
                    [
                        "These constructs can be used to overlap the execution of statements within a\nfunction (shown in the worked example section) or be composed with other language\nconstructs like loops:",
                        "markdown"
                    ],
                    [
                        "import torch\nfrom typing import List\n\ndef foo(x):\n    return torch.neg(x)\n\n@torch.jit.script\ndef example(x):\n    futures : List[torch.jit.Future[torch.Tensor]] = []\n    for _ in range(100):\n        futures.append(torch.jit.fork(foo, x))\n\n    results = []\n    for future in futures:\n        results.append(torch.jit.wait(future))\n\n    return torch.sum(torch.stack(results))\n\nprint(example(torch.ones([])))",
                        "code"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "When we initialized an empty list of Futures, we needed to add an explicit\ntype annotation to futures. In TorchScript, empty containers default\nto assuming they contain Tensor values, so we annotate the list constructor\n# as being of type List[torch.jit.Future[torch.Tensor]]",
                        "markdown"
                    ],
                    [
                        "This example uses fork() to launch 100 instances of the function foo,\nwaits on the 100 tasks to complete, then sums the results, returning -100.0.",
                        "markdown"
                    ]
                ]
            },
            {
                "Applied Example: Ensemble of Bidirectional LSTMs": [
                    [
                        "Let\u2019s try to apply parallelism to a more realistic example and see what sort\nof performance we can get out of it. First, let\u2019s define the baseline model: an\nensemble of bidirectional LSTM layers.",
                        "markdown"
                    ],
                    [
                        "import torch, time\n\n# In RNN parlance, the dimensions we care about are:\n# # of time-steps (T)\n# Batch size (B)\n# Hidden size/number of \"channels\" (C)\nT, B, C = 50, 50, 1024\n\n# A module that defines a single \"bidirectional LSTM\". This is simply two\n# LSTMs applied to the same sequence, but one in reverse\nclass BidirectionalRecurrentLSTM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cell_f = torch.nn.LSTM(input_size=C, hidden_size=C)\n        self.cell_b = torch.nn.LSTM(input_size=C, hidden_size=C)\n\n    def forward(self, x : torch.Tensor) -&gt; torch.Tensor:\n        # Forward layer\n        output_f, _ = self.cell_f(x)\n\n        # Backward layer. Flip input in the time dimension (dim 0), apply the\n        # layer, then flip the outputs in the time dimension\n        x_rev = torch.flip(x, dims=[0])\n        output_b, _ = self.cell_b(torch.flip(x, dims=[0]))\n        output_b_rev = torch.flip(output_b, dims=[0])\n\n        return torch.cat((output_f, output_b_rev), dim=2)\n\n\n# An \"ensemble\" of `BidirectionalRecurrentLSTM` modules. The modules in the\n# ensemble are run one-by-one on the same input then their results are\n# stacked and summed together, returning the combined result.\nclass LSTMEnsemble(torch.nn.Module):\n    def __init__(self, n_models):\n        super().__init__()\n        self.n_models = n_models\n        self.models = torch.nn.ModuleList([\n            BidirectionalRecurrentLSTM() for _ in range(self.n_models)])\n\n    def forward(self, x : torch.Tensor) -&gt; torch.Tensor:\n        results = []\n        for model in self.models:\n            results.append(model(x))\n        return torch.stack(results).sum(dim=0)\n\n# For a head-to-head comparison to what we're going to do with fork/wait, let's\n# instantiate the model and compile it with TorchScript\nens = torch.jit.script(LSTMEnsemble(n_models=4))\n\n# Normally you would pull this input out of an embedding table, but for the\n# purpose of this demo let's just use random data.\nx = torch.rand(T, B, C)\n\n# Let's run the model once to warm up things like the memory allocator\nens(x)\n\nx = torch.rand(T, B, C)\n\n# Let's see how fast it runs!\ns = time.time()\nens(x)\nprint('Inference took', time.time() - s, ' seconds')",
                        "code"
                    ],
                    [
                        "On my machine, this network runs in 2.05 seconds. We can do a lot better!",
                        "markdown"
                    ]
                ]
            },
            {
                "Parallelizing Forward and Backward Layers": [
                    [
                        "A very simple thing we can do is parallelize the forward and backward layers\nwithin BidirectionalRecurrentLSTM. For this, the structure of the computation\nis static, so we don\u2019t actually even need any loops. Let\u2019s rewrite the forward\nmethod of BidirectionalRecurrentLSTM like so:",
                        "markdown"
                    ],
                    [
                        "def forward(self, x : torch.Tensor) -&gt; torch.Tensor:\n    # Forward layer - fork() so this can run in parallel to the backward\n    # layer\n    future_f = torch.jit.fork(self.cell_f, x)\n\n    # Backward layer. Flip input in the time dimension (dim 0), apply the\n    # layer, then flip the outputs in the time dimension\n    x_rev = torch.flip(x, dims=[0])\n    output_b, _ = self.cell_b(torch.flip(x, dims=[0]))\n    output_b_rev = torch.flip(output_b, dims=[0])\n\n    # Retrieve the output from the forward layer. Note this needs to happen\n    # *after* the stuff we want to parallelize with\n    output_f, _ = torch.jit.wait(future_f)\n\n    return torch.cat((output_f, output_b_rev), dim=2)",
                        "code"
                    ],
                    [
                        "In this example, forward() delegates execution of cell_f to another thread,\nwhile it continues to execute cell_b. This causes the execution of both the\ncells to be overlapped with each other.",
                        "markdown"
                    ],
                    [
                        "Running the script again with this simple modification yields a runtime of\n1.71 seconds for an improvement of 17%!",
                        "markdown"
                    ]
                ]
            },
            {
                "Aside: Visualizing Parallelism": [
                    [
                        "We\u2019re not done optimizing our model but it\u2019s worth introducing the tooling we\nhave for visualizing performance. One important tool is the .",
                        "markdown"
                    ],
                    [
                        "Let\u2019s use the profiler along with the Chrome trace export functionality to\nvisualize the performance of our parallelized model:",
                        "markdown"
                    ],
                    [
                        "with torch.autograd.profiler.profile() as prof:\n    ens(x)\nprof.export_chrome_trace('parallel.json')",
                        "code"
                    ],
                    [
                        "This snippet of code will write out a file named parallel.json. If you\nnavigate Google Chrome to chrome://tracing, click the Load button, and\nload in that JSON file, you should see a timeline like the following:\n<img alt=\"https://i.imgur.com/rm5hdG9.png\" src=\"https://i.imgur.com/rm5hdG9.png\"/>",
                        "markdown"
                    ],
                    [
                        "The horizontal axis of the timeline represents time and the vertical axis\nrepresents threads of execution. As we can see, we are running two lstm\ninstances at a time. This is the result of our hard work parallelizing the\nbidirectional layers!",
                        "markdown"
                    ]
                ]
            },
            {
                "Parallelizing Models in the Ensemble": [
                    [
                        "You may have noticed that there is a further parallelization opportunity in our\ncode: we can also run the models contained in LSTMEnsemble in parallel with\neach other. The way to do that is simple enough, this is how we should change\nthe forward method of LSTMEnsemble:",
                        "markdown"
                    ],
                    [
                        "def forward(self, x : torch.Tensor) -&gt; torch.Tensor:\n    # Launch tasks for each model\n    futures : List[torch.jit.Future[torch.Tensor]] = []\n    for model in self.models:\n        futures.append(torch.jit.fork(model, x))\n\n    # Collect the results from the launched tasks\n    results : List[torch.Tensor] = []\n    for future in futures:\n        results.append(torch.jit.wait(future))\n\n    return torch.stack(results).sum(dim=0)",
                        "code"
                    ],
                    [
                        "Or, if you value brevity, we can use list comprehensions:",
                        "markdown"
                    ],
                    [
                        "def forward(self, x : torch.Tensor) -&gt; torch.Tensor:\n    futures = [torch.jit.fork(model, x) for model in self.models]\n    results = [torch.jit.wait(fut) for fut in futures]\n    return torch.stack(results).sum(dim=0)",
                        "code"
                    ],
                    [
                        "Like described in the intro, we\u2019ve used loops to fork off tasks for each of the\nmodels in our ensemble. We\u2019ve then used another loop to wait for all of the\ntasks to be completed. This provides even more overlap of computation.",
                        "markdown"
                    ],
                    [
                        "With this small update, the script runs in 1.4 seconds, for a total speedup\nof 32%! Pretty good for two lines of code.",
                        "markdown"
                    ],
                    [
                        "We can also use the Chrome tracer again to see where\u2019s going on:\n<img alt=\"https://i.imgur.com/kA0gyQm.png\" src=\"https://i.imgur.com/kA0gyQm.png\"/>",
                        "markdown"
                    ],
                    [
                        "We can now see that all LSTM instances are being run fully in parallel.",
                        "markdown"
                    ]
                ]
            },
            {
                "Conclusion": [
                    [
                        "In this tutorial, we learned about fork() and wait(), the basic APIs\nfor doing dynamic, inter-op parallelism in TorchScript. We saw a few typical\nusage patterns for using these functions to parallelize the execution of\nfunctions, methods, or Modules in TorchScript code. Finally, we worked through\nan example of optimizing a model using this technique and explored the performance\nmeasurement and visualization tooling available in PyTorch.",
                        "markdown"
                    ]
                ]
            }
        ],
        "Autograd in C++ Frontend": [
            [
                "The autograd package is crucial for building highly flexible and dynamic neural\nnetworks in PyTorch. Most of the autograd APIs in PyTorch Python frontend are also available\nin C++ frontend, allowing easy translation of autograd code from Python to C++.",
                "markdown"
            ],
            [
                "In this tutorial explore several examples of doing autograd in PyTorch C++ frontend.\nNote that this tutorial assumes that you already have a basic understanding of\nautograd in Python frontend. If that\u2019s not the case, please first read\n.",
                "markdown"
            ],
            {
                "Basic autograd operations": [
                    [
                        "(Adapted from )",
                        "markdown"
                    ],
                    [
                        "Create a tensor and set torch::requires_grad() to track computation with it",
                        "markdown"
                    ],
                    [
                        "auto x = torch::ones({2, 2}, torch::requires_grad());\nstd::cout &lt;&lt; x &lt;&lt; std::endl;",
                        "code"
                    ],
                    [
                        "Out:",
                        "markdown"
                    ],
                    [
                        "1 1\n1 1\n[ CPUFloatType{2,2} ]",
                        "code"
                    ],
                    [
                        "Do a tensor operation:",
                        "markdown"
                    ],
                    [
                        "auto y = x + 2;\nstd::cout &lt;&lt; y &lt;&lt; std::endl;",
                        "code"
                    ],
                    [
                        "Out:",
                        "markdown"
                    ],
                    [
                        " 3  3\n 3  3\n[ CPUFloatType{2,2} ]",
                        "code"
                    ],
                    [
                        "y was created as a result of an operation, so it has a grad_fn.",
                        "markdown"
                    ],
                    [
                        "std::cout &lt;&lt; y.grad_fn()-&gt;name() &lt;&lt; std::endl;",
                        "code"
                    ],
                    [
                        "Out:",
                        "markdown"
                    ],
                    [
                        "AddBackward1",
                        "code"
                    ],
                    [
                        "Do more operations on y",
                        "markdown"
                    ],
                    [
                        "auto z = y * y * 3;\nauto out = z.mean();\n\nstd::cout &lt;&lt; z &lt;&lt; std::endl;\nstd::cout &lt;&lt; z.grad_fn()-&gt;name() &lt;&lt; std::endl;\nstd::cout &lt;&lt; out &lt;&lt; std::endl;\nstd::cout &lt;&lt; out.grad_fn()-&gt;name() &lt;&lt; std::endl;",
                        "code"
                    ],
                    [
                        "Out:",
                        "markdown"
                    ],
                    [
                        " 27  27\n 27  27\n[ CPUFloatType{2,2} ]\nMulBackward1\n27\n[ CPUFloatType{} ]\nMeanBackward0",
                        "code"
                    ],
                    [
                        ".requires_grad_( ... ) changes an existing tensor\u2019s requires_grad flag in-place.",
                        "markdown"
                    ],
                    [
                        "auto a = torch::randn({2, 2});\na = ((a * 3) / (a - 1));\nstd::cout &lt;&lt; a.requires_grad() &lt;&lt; std::endl;\n\na.requires_grad_(true);\nstd::cout &lt;&lt; a.requires_grad() &lt;&lt; std::endl;\n\nauto b = (a * a).sum();\nstd::cout &lt;&lt; b.grad_fn()-&gt;name() &lt;&lt; std::endl;",
                        "code"
                    ],
                    [
                        "Out:",
                        "markdown"
                    ],
                    [
                        "false\ntrue\nSumBackward0",
                        "code"
                    ],
                    [
                        "Let\u2019s backprop now. Because out contains a single scalar, out.backward()\nis equivalent to out.backward(torch::tensor(1.)).",
                        "markdown"
                    ],
                    [
                        "out.backward();",
                        "code"
                    ],
                    [
                        "Print gradients d(out)/dx",
                        "markdown"
                    ],
                    [
                        "std::cout &lt;&lt; x.grad() &lt;&lt; std::endl;",
                        "code"
                    ],
                    [
                        "Out:",
                        "markdown"
                    ],
                    [
                        " 4.5000  4.5000\n 4.5000  4.5000\n[ CPUFloatType{2,2} ]",
                        "code"
                    ],
                    [
                        "You should have got a matrix of 4.5. For explanations on how we arrive at this value,\nplease see .",
                        "markdown"
                    ],
                    [
                        "Now let\u2019s take a look at an example of vector-Jacobian product:",
                        "markdown"
                    ],
                    [
                        "x = torch::randn(3, torch::requires_grad());\n\ny = x * 2;\nwhile (y.norm().item&lt;double&gt;() &lt; 1000) {\n  y = y * 2;\n}\n\nstd::cout &lt;&lt; y &lt;&lt; std::endl;\nstd::cout &lt;&lt; y.grad_fn()-&gt;name() &lt;&lt; std::endl;",
                        "code"
                    ],
                    [
                        "Out:",
                        "markdown"
                    ],
                    [
                        "-1021.4020\n  314.6695\n -613.4944\n[ CPUFloatType{3} ]\nMulBackward1",
                        "code"
                    ],
                    [
                        "If we want the vector-Jacobian product, pass the vector to backward as argument:",
                        "markdown"
                    ],
                    [
                        "auto v = torch::tensor({0.1, 1.0, 0.0001}, torch::kFloat);\ny.backward(v);\n\nstd::cout &lt;&lt; x.grad() &lt;&lt; std::endl;",
                        "code"
                    ],
                    [
                        "Out:",
                        "markdown"
                    ],
                    [
                        "  102.4000\n 1024.0000\n    0.1024\n[ CPUFloatType{3} ]",
                        "code"
                    ],
                    [
                        "You can also stop autograd from tracking history on tensors that require gradients\neither by putting torch::NoGradGuard in a code block",
                        "markdown"
                    ],
                    [
                        "std::cout &lt;&lt; x.requires_grad() &lt;&lt; std::endl;\nstd::cout &lt;&lt; x.pow(2).requires_grad() &lt;&lt; std::endl;\n\n{\n  torch::NoGradGuard no_grad;\n  std::cout &lt;&lt; x.pow(2).requires_grad() &lt;&lt; std::endl;\n}",
                        "code"
                    ],
                    [
                        "Out:",
                        "markdown"
                    ],
                    [
                        "true\ntrue\nfalse",
                        "code"
                    ],
                    [
                        "Or by using .detach() to get a new tensor with the same content but that does\nnot require gradients:",
                        "markdown"
                    ],
                    [
                        "std::cout &lt;&lt; x.requires_grad() &lt;&lt; std::endl;\ny = x.detach();\nstd::cout &lt;&lt; y.requires_grad() &lt;&lt; std::endl;\nstd::cout &lt;&lt; x.eq(y).all().item&lt;bool&gt;() &lt;&lt; std::endl;",
                        "code"
                    ],
                    [
                        "Out:",
                        "markdown"
                    ],
                    [
                        "true\nfalse\ntrue",
                        "code"
                    ],
                    [
                        "For more information on C++ tensor autograd APIs such as grad / requires_grad /\nis_leaf / backward / detach / detach_ / register_hook / retain_grad,\nplease see .",
                        "markdown"
                    ]
                ]
            },
            {
                "Computing higher-order gradients in C++": [
                    [
                        "One of the applications of higher-order gradients is calculating gradient penalty.\nLet\u2019s see an example of it using torch::autograd::grad:",
                        "markdown"
                    ],
                    [
                        "#include &lt;torch/torch.h&gt;\n\nauto model = torch::nn::Linear(4, 3);\n\nauto input = torch::randn({3, 4}).requires_grad_(true);\nauto output = model(input);\n\n// Calculate loss\nauto target = torch::randn({3, 3});\nauto loss = torch::nn::MSELoss()(output, target);\n\n// Use norm of gradients as penalty\nauto grad_output = torch::ones_like(output);\nauto gradient = torch::autograd::grad({output}, {input}, /*grad_outputs=*/{grad_output}, /*create_graph=*/true)[0];\nauto gradient_penalty = torch::pow((gradient.norm(2, /*dim=*/1) - 1), 2).mean();\n\n// Add gradient penalty to loss\nauto combined_loss = loss + gradient_penalty;\ncombined_loss.backward();\n\nstd::cout &lt;&lt; input.grad() &lt;&lt; std::endl;",
                        "code"
                    ],
                    [
                        "Out:",
                        "markdown"
                    ],
                    [
                        "-0.1042 -0.0638  0.0103  0.0723\n-0.2543 -0.1222  0.0071  0.0814\n-0.1683 -0.1052  0.0355  0.1024\n[ CPUFloatType{3,4} ]",
                        "code"
                    ],
                    [
                        "Please see the documentation for torch::autograd::backward\n()\nand torch::autograd::grad\n()\nfor more information on how to use them.",
                        "markdown"
                    ]
                ]
            },
            {
                "Using custom autograd function in C++": [
                    [
                        "(Adapted from )",
                        "markdown"
                    ],
                    [
                        "Adding a new elementary operation to torch::autograd requires implementing a new torch::autograd::Function\nsubclass for each operation. torch::autograd::Function s are what torch::autograd\nuses to compute the results and gradients, and encode the operation history. Every\nnew function requires you to implement 2 methods: forward and backward, and\nplease see \nfor the detailed requirements.",
                        "markdown"
                    ],
                    [
                        "Below you can find code for a Linear function from torch::nn:",
                        "markdown"
                    ],
                    [
                        "#include &lt;torch/torch.h&gt;\n\nusing namespace torch::autograd;\n\n// Inherit from Function\nclass LinearFunction : public Function&lt;LinearFunction&gt; {\n public:\n  // Note that both forward and backward are static functions\n\n  // bias is an optional argument\n  static torch::Tensor forward(\n      AutogradContext *ctx, torch::Tensor input, torch::Tensor weight, torch::Tensor bias = torch::Tensor()) {\n    ctx-&gt;save_for_backward({input, weight, bias});\n    auto output = input.mm(weight.t());\n    if (bias.defined()) {\n      output += bias.unsqueeze(0).expand_as(output);\n    }\n    return output;\n  }\n\n  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n    auto saved = ctx-&gt;get_saved_variables();\n    auto input = saved[0];\n    auto weight = saved[1];\n    auto bias = saved[2];\n\n    auto grad_output = grad_outputs[0];\n    auto grad_input = grad_output.mm(weight);\n    auto grad_weight = grad_output.t().mm(input);\n    auto grad_bias = torch::Tensor();\n    if (bias.defined()) {\n      grad_bias = grad_output.sum(0);\n    }\n\n    return {grad_input, grad_weight, grad_bias};\n  }\n};",
                        "code"
                    ],
                    [
                        "Then, we can use the LinearFunction in the following way:",
                        "markdown"
                    ],
                    [
                        "auto x = torch::randn({2, 3}).requires_grad_();\nauto weight = torch::randn({4, 3}).requires_grad_();\nauto y = LinearFunction::apply(x, weight);\ny.sum().backward();\n\nstd::cout &lt;&lt; x.grad() &lt;&lt; std::endl;\nstd::cout &lt;&lt; weight.grad() &lt;&lt; std::endl;",
                        "code"
                    ],
                    [
                        "Out:",
                        "markdown"
                    ],
                    [
                        " 0.5314  1.2807  1.4864\n 0.5314  1.2807  1.4864\n[ CPUFloatType{2,3} ]\n 3.7608  0.9101  0.0073\n 3.7608  0.9101  0.0073\n 3.7608  0.9101  0.0073\n 3.7608  0.9101  0.0073\n[ CPUFloatType{4,3} ]",
                        "code"
                    ],
                    [
                        "Here, we give an additional example of a function that is parametrized by non-tensor arguments:",
                        "markdown"
                    ],
                    [
                        "#include &lt;torch/torch.h&gt;\n\nusing namespace torch::autograd;\n\nclass MulConstant : public Function&lt;MulConstant&gt; {\n public:\n  static torch::Tensor forward(AutogradContext *ctx, torch::Tensor tensor, double constant) {\n    // ctx is a context object that can be used to stash information\n    // for backward computation\n    ctx-&gt;saved_data[\"constant\"] = constant;\n    return tensor * constant;\n  }\n\n  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n    // We return as many input gradients as there were arguments.\n    // Gradients of non-tensor arguments to forward must be `torch::Tensor()`.\n    return {grad_outputs[0] * ctx-&gt;saved_data[\"constant\"].toDouble(), torch::Tensor()};\n  }\n};",
                        "code"
                    ],
                    [
                        "Then, we can use the MulConstant in the following way:",
                        "markdown"
                    ],
                    [
                        "auto x = torch::randn({2}).requires_grad_();\nauto y = MulConstant::apply(x, 5.5);\ny.sum().backward();\n\nstd::cout &lt;&lt; x.grad() &lt;&lt; std::endl;",
                        "code"
                    ],
                    [
                        "Out:",
                        "markdown"
                    ],
                    [
                        " 5.5000\n 5.5000\n[ CPUFloatType{2} ]",
                        "code"
                    ],
                    [
                        "For more information on torch::autograd::Function, please see\n.",
                        "markdown"
                    ]
                ]
            },
            {
                "Translating autograd code from Python to C++": [
                    [
                        "On a high level, the easiest way to use autograd in C++ is to have working\nautograd code in Python first, and then translate your autograd code from Python to\nC++ using the following table:",
                        "markdown"
                    ],
                    [
                        "After translation, most of your Python autograd code should just work in C++.\nIf that\u2019s not the case, please file a bug report at \nand we will fix it as soon as possible.",
                        "markdown"
                    ]
                ]
            },
            {
                "Conclusion": [
                    [
                        "You should now have a good overview of PyTorch\u2019s C++ autograd API.\nYou can find the code examples displayed in this note . As always, if you run into any\nproblems or have questions, you can use our \nor  to get in touch.",
                        "markdown"
                    ]
                ]
            }
        ]
    },
    "Extending PyTorch": {
        "Double Backward with Custom Functions": [
            [
                "It is sometimes useful to run backwards twice through backward graph, for\nexample to compute higher-order gradients. It takes an understanding of\nautograd and some care to support double backwards, however. Functions\nthat support performing backward a single time are not necessarily\nequipped to support double backward. In this tutorial we show how to\nwrite a custom autograd function that supports double backward, and\npoint out some things to look out for.",
                "markdown"
            ],
            [
                "When writing a custom autograd function to backward through twice,\nit is important to know when operations performed in a custom function\nare recorded by autograd, when they aren\u2019t, and most importantly, how\n<cite>save_for_backward</cite> works with all of this.",
                "markdown"
            ],
            [
                "Custom functions implicitly affects grad mode in two ways:",
                "markdown"
            ],
            [
                "During forward, autograd does not record any the graph for any\noperations performed within the forward function. When forward\ncompletes, the backward function of the custom function\nbecomes the <cite>grad_fn</cite> of each of the forward\u2019s outputs",
                "markdown"
            ],
            [
                "During backward, autograd records the computation graph used to\ncompute the backward pass if create_graph is specified",
                "markdown"
            ],
            [
                "Next, to understand how <cite>save_for_backward</cite> interacts with the above,\nwe can explore a couple examples:",
                "markdown"
            ],
            {
                "Saving the Inputs": [
                    [
                        "Consider this simple squaring function. It saves an input tensor\nfor backward. Double backward works automatically when autograd\nis able to record operations in the backward pass, so there is usually\nnothing to worry about when we save an input for backward as\nthe input should have grad_fn if it is a function of any tensor\nthat requires grad. This allows the gradients to be properly propagated.",
                        "markdown"
                    ],
                    [
                        "import torch\n\nclass Square(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        # Because we are saving one of the inputs use `save_for_backward`\n        # Save non-tensors and non-inputs/non-outputs directly on ctx\n        ctx.save_for_backward(x)\n        return x**2\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        # A function support double backward automatically if autograd\n        # is able to record the computations performed in backward\n        x, = ctx.saved_tensors\n        return grad_out * 2 * x\n\n# Use double precision because finite differencing method magnifies errors\nx = torch.rand(3, 3, requires_grad=True, dtype=torch.double)\ntorch.autograd.gradcheck(Square.apply, x)\n# Use gradcheck to verify second-order derivatives\ntorch.autograd.gradgradcheck(Square.apply, x)",
                        "code"
                    ],
                    [
                        "We can use torchviz to visualize the graph to see why this works",
                        "markdown"
                    ],
                    [
                        "import torchviz\n\nx = torch.tensor(1., requires_grad=True).clone()\nout = Square.apply(x)\ngrad_x, = torch.autograd.grad(out, x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), {\"grad_x\": grad_x, \"x\": x, \"out\": out})",
                        "code"
                    ],
                    [
                        "We can see that the gradient wrt to x, is itself a function of x (dout/dx = 2x)\nAnd the graph of this function has been properly constructed",
                        "markdown"
                    ]
                ]
            },
            {
                "Saving the Outputs": [
                    [
                        "A slight variation on the previous example is to save an output\ninstead of input. The mechanics are similar because outputs are also\nassociated with a grad_fn.",
                        "markdown"
                    ],
                    [
                        "class Exp(torch.autograd.Function):\n    # Simple case where everything goes well\n    @staticmethod\n    def forward(ctx, x):\n        # This time we save the output\n        result = torch.exp(x)\n        # Note that we should use `save_for_backward` here when\n        # the tensor saved is an ouptut (or an input).\n        ctx.save_for_backward(result)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        result, = ctx.saved_tensors\n        return result * grad_out\n\nx = torch.tensor(1., requires_grad=True, dtype=torch.double).clone()\n# Validate our gradients using gradcheck\ntorch.autograd.gradcheck(Exp.apply, x)\ntorch.autograd.gradgradcheck(Exp.apply, x)",
                        "code"
                    ],
                    [
                        "Use torchviz to visualize the graph:",
                        "markdown"
                    ],
                    [
                        "out = Exp.apply(x)\ngrad_x, = torch.autograd.grad(out, x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), {\"grad_x\": grad_x, \"x\": x, \"out\": out})",
                        "code"
                    ]
                ]
            },
            {
                "Saving Intermediate Results": [
                    [
                        "A more tricky case is when we need to save an intermediate result.\nWe demonstrate this case by implementing:\n\n\\[sinh(x) := \\frac{e^x - e^{-x}}{2}\n\n\\]",
                        "markdown"
                    ],
                    [
                        "Since the derivative of sinh is cosh, it might be useful to reuse\n<cite>exp(x)</cite> and <cite>exp(-x)</cite>, the two intermediate results in forward\nin the backward computation.",
                        "markdown"
                    ],
                    [
                        "Intermediate results should not be directly saved and used in backward though.\nBecause forward is performed in no-grad mode, if an intermediate result\nof the forward pass is used to compute gradients in the backward pass\nthe backward graph of the gradients would not include the operations\nthat computed the intermediate result. This leads to incorrect gradients.",
                        "markdown"
                    ],
                    [
                        "class Sinh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        expx = torch.exp(x)\n        expnegx = torch.exp(-x)\n        ctx.save_for_backward(expx, expnegx)\n        # In order to be able to save the intermediate results, a trick is to\n        # include them as our outputs, so that the backward graph is constructed\n        return (expx - expnegx) / 2, expx, expnegx\n\n    @staticmethod\n    def backward(ctx, grad_out, _grad_out_exp, _grad_out_negexp):\n        expx, expnegx = ctx.saved_tensors\n        grad_input = grad_out * (expx + expnegx) / 2\n        # We cannot skip accumulating these even though we won't use the outputs\n        # directly. They will be used later in the second backward.\n        grad_input += _grad_out_exp * expx\n        grad_input -= _grad_out_negexp * expnegx\n        return grad_input\n\ndef sinh(x):\n    # Create a wrapper that only returns the first output\n    return Sinh.apply(x)[0]\n\nx = torch.rand(3, 3, requires_grad=True, dtype=torch.double)\ntorch.autograd.gradcheck(sinh, x)\ntorch.autograd.gradgradcheck(sinh, x)",
                        "code"
                    ],
                    [
                        "Use torchviz to visualize the graph:",
                        "markdown"
                    ],
                    [
                        "out = sinh(x)\ngrad_x, = torch.autograd.grad(out.sum(), x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out})",
                        "code"
                    ]
                ]
            },
            {
                "Saving Intermediate Results: What not to do": [
                    [
                        "Now we show what happens when we don\u2019t also return our intermediate\nresults as outputs: <cite>grad_x</cite> would not even have a  backward graph\nbecause it is purely a function <cite>exp</cite> and <cite>expnegx</cite>, which don\u2019t\nrequire grad.",
                        "markdown"
                    ],
                    [
                        "class SinhBad(torch.autograd.Function):\n    # This is an example of what NOT to do!\n    @staticmethod\n    def forward(ctx, x):\n        expx = torch.exp(x)\n        expnegx = torch.exp(-x)\n        ctx.expx = expx\n        ctx.expnegx = expnegx\n        return (expx - expnegx) / 2\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        expx = ctx.expx\n        expnegx = ctx.expnegx\n        grad_input = grad_out * (expx + expnegx) / 2\n        return grad_input",
                        "code"
                    ],
                    [
                        "Use torchviz to visualize the graph. Notice that <cite>grad_x</cite> is not\npart of the graph!",
                        "markdown"
                    ],
                    [
                        "out = SinhBad.apply(x)\ngrad_x, = torch.autograd.grad(out.sum(), x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out})",
                        "code"
                    ]
                ]
            },
            {
                "When Backward is not Tracked": [
                    [
                        "Finally, let\u2019s consider an example when it may not be possible for\nautograd to track gradients for a functions backward at all.\nWe can imagine cube_backward to be a function that may require a\nnon-PyTorch library like SciPy or NumPy, or written as a\nC++ extension. The workaround demonstrated here is to create another\ncustom function CubeBackward where you also manually specify the\nbackward of cube_backward!",
                        "markdown"
                    ],
                    [
                        "def cube_forward(x):\n    return x**3\n\ndef cube_backward(grad_out, x):\n    return grad_out * 3 * x**2\n\ndef cube_backward_backward(grad_out, sav_grad_out, x):\n    return grad_out * sav_grad_out * 6 * x\n\ndef cube_backward_backward_grad_out(grad_out, x):\n    return grad_out * 3 * x**2\n\nclass Cube(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return cube_forward(x)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        x, = ctx.saved_tensors\n        return CubeBackward.apply(grad_out, x)\n\nclass CubeBackward(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, grad_out, x):\n        ctx.save_for_backward(x, grad_out)\n        return cube_backward(grad_out, x)\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        x, sav_grad_out = ctx.saved_tensors\n        dx = cube_backward_backward(grad_out, sav_grad_out, x)\n        dgrad_out = cube_backward_backward_grad_out(grad_out, x)\n        return dgrad_out, dx\n\nx = torch.tensor(2., requires_grad=True, dtype=torch.double)\n\ntorch.autograd.gradcheck(Cube.apply, x)\ntorch.autograd.gradgradcheck(Cube.apply, x)",
                        "code"
                    ],
                    [
                        "Use torchviz to visualize the graph:",
                        "markdown"
                    ],
                    [
                        "out = Cube.apply(x)\ngrad_x, = torch.autograd.grad(out, x, create_graph=True)\ntorchviz.make_dot((grad_x, x, out), params={\"grad_x\": grad_x, \"x\": x, \"out\": out})",
                        "code"
                    ],
                    [
                        "To conclude, whether double backward works for your custom function\nsimply depends on whether the backward pass can be tracked by autograd.\nWith the first two examples we show situations where double backward\nworks out of the box. With the third and fourth examples, we demonstrate\ntechniques that enable a backward function to be tracked, when they\notherwise would not be.",
                        "markdown"
                    ]
                ]
            }
        ],
        "Fusing Convolution and Batch Norm using Custom Function": [
            [
                "Fusing adjacent convolution and batch norm layers together is typically an\ninference-time optimization to improve run-time. It is usually achieved\nby eliminating the batch norm layer entirely and updating the weight\nand bias of the preceding convolution [0]. However, this technique is not\napplicable for training models.",
                "markdown"
            ],
            [
                "In this tutorial, we will show a different technique to fuse the two layers\nthat can be applied during training. Rather than improved runtime, the\nobjective of this optimization is to reduce memory usage.",
                "markdown"
            ],
            [
                "The idea behind this optimization is to see that both convolution and\nbatch norm (as well as many other ops) need to save a copy of their input\nduring forward for the backward pass. For large\nbatch sizes, these saved inputs are responsible for most of your memory usage,\nso being able to avoid allocating another input tensor for every\nconvolution batch norm pair can be a significant reduction.",
                "markdown"
            ],
            [
                "In this tutorial, we avoid this extra allocation by combining convolution\nand batch norm into a single layer (as a custom function). In the forward\nof this combined layer, we perform normal convolution and batch norm as-is,\nwith the only difference being that we will only save the inputs to the convolution.\nTo obtain the input of batch norm, which is necessary to backward through\nit, we recompute convolution forward again during the backward pass.",
                "markdown"
            ],
            [
                "It is important to note that the usage of this optimization is situational.\nThough (by avoiding one buffer saved) we always reduce the memory allocated at\nthe end of the forward pass, there are cases when the <em>peak</em> memory allocated\nmay not actually be reduced. See the final section for more details.",
                "markdown"
            ],
            [
                "For simplicity, in this tutorial we hardcode <cite>bias=False</cite>, <cite>stride=1</cite>, <cite>padding=0</cite>, <cite>dilation=1</cite>,\nand <cite>groups=1</cite> for Conv2D. For BatchNorm2D, we hardcode <cite>eps=1e-3</cite>, <cite>momentum=0.1</cite>,\n<cite>affine=False</cite>, and <cite>track_running_statistics=False</cite>. Another small difference\nis that we add epsilon in the denomator outside of the square root in the computation\nof batch norm.",
                "markdown"
            ],
            [
                "[0] ",
                "markdown"
            ],
            {
                "Backward Formula Implementation for Convolution": [
                    [
                        "Implementing a custom function requires us to implement the backward\nourselves. In this case, we need both the backward formulas for Conv2D\nand BatchNorm2D. Eventually we\u2019d chain them together in our unified\nbackward function, but below we first implement them as their own\ncustom functions so we can validate their correctness individually",
                        "markdown"
                    ],
                    [
                        "import torch\nfrom torch.autograd.function import once_differentiable\nimport torch.nn.functional as F\n\ndef convolution_backward(grad_out, , ):\n    grad_input = (.transpose(0, 1), grad_out.transpose(0, 1)).transpose(0, 1)\n    grad_X = (grad_out, )\n    return grad_X, grad_input\n\nclass Conv2D():\n    @staticmethod\n    def forward(ctx, , ):\n        ctx.save_for_backward(, )\n        return (, )\n\n    # Use @once_differentiable by default unless we intend to double backward\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_out):\n        ,  = ctx.saved_tensors\n        return convolution_backward(grad_out, , )",
                        "code"
                    ],
                    [
                        "When testing with gradcheck, it is important to use double precision",
                        "markdown"
                    ],
                    [
                        " = (5, 3, 3, 3, requires_grad=True, dtype=)\n = (10, 3, 7, 7, requires_grad=True, dtype=)\n(Conv2D.apply, (, ))",
                        "code"
                    ],
                    [
                        "True",
                        "code"
                    ]
                ]
            },
            {
                "Backward Formula Implementation for Batch Norm": [
                    [
                        "Batch Norm has two modes: training and eval mode. In training mode\nthe sample statistics are a function of the inputs. In eval mode,\nwe use the saved running statistics, which are not a function of the inputs.\nThis makes non-training mode\u2019s backward significantly simpler. Below\nwe implement and test only the training mode case.",
                        "markdown"
                    ],
                    [
                        "def unsqueeze_all(t):\n    # Helper function to unsqueeze all the dimensions that we reduce over\n    return t[None, :, None, None]\n\ndef batch_norm_backward(grad_out, , sum, sqrt_var, N, eps):\n    # We use the formula: out = (X - mean(X)) / (sqrt(var(X)) + eps)\n    # in batch norm 2d's forward. To simplify our derivation, we follow the\n    # chain rule and compute the gradients as follows before accumulating\n    # them all into a final grad_input.\n    #  1) 'grad of out wrt var(X)' * 'grad of var(X) wrt X'\n    #  2) 'grad of out wrt mean(X)' * 'grad of mean(X) wrt X'\n    #  3) 'grad of out wrt X in the numerator' * 'grad of X wrt X'\n    # We then rewrite the formulas to use as few extra buffers as possible\n    tmp = (( - unsqueeze_all(sum) / N) * grad_out).sum(dim=(0, 2, 3))\n    tmp *= -1\n    d_denom = tmp / (sqrt_var + eps)**2  # d_denom = -num / denom**2\n    # It is useful to delete tensors when you no longer need them with `del`\n    # For example, we could've done `del tmp` here because we won't use it later\n    # In this case, it's not a big difference because tmp only has size of (C,)\n    # The important thing is avoid allocating NCHW-sized tensors unnecessarily\n    d_var = d_denom / (2 * sqrt_var)  # denom = torch.sqrt(var) + eps\n    # Compute d_mean_dx before allocating the final NCHW-sized grad_input buffer\n    d_mean_dx = grad_out / unsqueeze_all(sqrt_var + eps)\n    d_mean_dx = unsqueeze_all(-d_mean_dx.sum(dim=(0, 2, 3)) / N)\n    # d_mean_dx has already been reassigned to a C-sized buffer so no need to worry\n\n    # (1) unbiased_var(x) = ((X - unsqueeze_all(mean))**2).sum(dim=(0, 2, 3)) / (N - 1)\n    grad_input =  * unsqueeze_all(d_var * N)\n    grad_input += unsqueeze_all(-d_var * sum)\n    grad_input *= 2 / ((N - 1) * N)\n    # (2) mean (see above)\n    grad_input += d_mean_dx\n    # (3) Add 'grad_out / &lt;factor&gt;' without allocating an extra buffer\n    grad_input *= unsqueeze_all(sqrt_var + eps)\n    grad_input += grad_out\n    grad_input /= unsqueeze_all(sqrt_var + eps)  # sqrt_var + eps &gt; 0!\n    return grad_input\n\nclass BatchNorm():\n    @staticmethod\n    def forward(ctx, , eps=1e-3):\n        # Don't save keepdim'd values for backward\n        sum = .sum(dim=(0, 2, 3))\n        var = .var(unbiased=True, dim=(0, 2, 3))\n        N = .numel() / .size(1)\n        sqrt_var = (var)\n        ctx.save_for_backward()\n        ctx.eps = eps\n        ctx.sum = sum\n        ctx.N = N\n        ctx.sqrt_var = sqrt_var\n        mean = sum / N\n        denom = sqrt_var + eps\n        out =  - unsqueeze_all(mean)\n        out /= unsqueeze_all(denom)\n        return out\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, grad_out):\n        , = ctx.saved_tensors\n        return batch_norm_backward(grad_out, , ctx.sum, ctx.sqrt_var, ctx.N, ctx.eps)",
                        "code"
                    ],
                    [
                        "Testing with gradcheck",
                        "markdown"
                    ],
                    [
                        " = (1, 2, 3, 4, requires_grad=True, dtype=)\n(BatchNorm.apply, (,), fast_mode=False)",
                        "code"
                    ],
                    [
                        "True",
                        "code"
                    ]
                ]
            },
            {
                "Fusing Convolution and BatchNorm": [
                    [
                        "Now that the bulk of the work has been done, we can combine\nthem together. Note that in (1) we only save a single buffer\nfor backward, but this also means we recompute convolution forward\nin (5). Also see that in (2), (3), (4), and (6), it\u2019s the same\nexact code as the examples above.",
                        "markdown"
                    ],
                    [
                        "class FusedConvBN2DFunction():\n    @staticmethod\n    def forward(ctx, , conv_weight, eps=1e-3):\n        assert .ndim == 4  # N, C, H, W\n        # (1) Only need to save this single buffer for backward!\n        ctx.save_for_backward(, conv_weight)\n\n        # (2) Exact same Conv2D forward from example above\n         = (, conv_weight)\n        # (3) Exact same BatchNorm2D forward from example above\n        sum = .sum(dim=(0, 2, 3))\n        var = .var(unbiased=True, dim=(0, 2, 3))\n        N = .numel() / .size(1)\n        sqrt_var = (var)\n        ctx.eps = eps\n        ctx.sum = sum\n        ctx.N = N\n        ctx.sqrt_var = sqrt_var\n        mean = sum / N\n        denom = sqrt_var + eps\n        # Try to do as many things in-place as possible\n        # Instead of `out = (X - a) / b`, doing `out = X - a; out /= b`\n        # avoids allocating one extra NCHW-sized buffer here\n        out =  - unsqueeze_all(mean)\n        out /= unsqueeze_all(denom)\n        return out\n\n    @staticmethod\n    def backward(ctx, grad_out):\n        , conv_weight, = ctx.saved_tensors\n        # (4) Batch norm backward\n        # (5) We need to recompute conv\n        X_conv_out = (, conv_weight)\n        grad_out = batch_norm_backward(grad_out, X_conv_out, ctx.sum, ctx.sqrt_var,\n                                       ctx.N, ctx.eps)\n        # (6) Conv2d backward\n        grad_X, grad_input = convolution_backward(grad_out, , conv_weight)\n        return grad_X, grad_input, None, None, None, None, None",
                        "code"
                    ],
                    [
                        "The next step is to wrap our functional variant in a stateful\n<cite>nn.Module</cite>",
                        "markdown"
                    ],
                    [
                        "import torch.nn as nn\nimport math\n\nclass FusedConvBN():\n    def __init__(self, in_channels, out_channels, kernel_size, exp_avg_factor=0.1,\n                 eps=1e-3, =None, dtype=None):\n        super(, self).__init__()\n        factory_kwargs = {'device': , 'dtype': dtype}\n        # Conv parameters\n        weight_shape = (out_channels, in_channels, kernel_size, kernel_size)\n        self.conv_weight = ((*weight_shape, **factory_kwargs))\n        # Batch norm parameters\n        num_features = out_channels\n        self.num_features = num_features\n        self.eps = eps\n        # Initialize\n        self.reset_parameters()\n\n    def forward(self, ):\n        return FusedConvBN2DFunction.apply(, self.conv_weight, self.eps)\n\n    def reset_parameters(self) -&gt; None:\n        (self.conv_weight, =math.sqrt(5))",
                        "code"
                    ],
                    [
                        "Use gradcheck to validate the correctness of our backward formula",
                        "markdown"
                    ],
                    [
                        " = (5, 3, 3, 3, requires_grad=True, dtype=)\n = (2, 3, 4, 4, requires_grad=True, dtype=)\n(FusedConvBN2DFunction.apply, (, ))",
                        "code"
                    ],
                    [
                        "True",
                        "code"
                    ]
                ]
            },
            {
                "Testing out our new Layer": [
                    [
                        "Use FusedConvBN to train a basic network\nThe code below is after some light modifications to the example here:",
                        "markdown"
                    ],
                    [
                        "import torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.optim.lr_scheduler import \n\n# Record memory allocated at the end of the forward pass\nmemory_allocated = [[],[]]\n\nclass Net():\n    def __init__(self, fused=True):\n        super(, self).__init__()\n        self.fused = fused\n        if fused:\n            self.convbn1 = (1, 32, 3)\n            self.convbn2 = (32, 64, 3)\n        else:\n            self.conv1 = (1, 32, 3, 1, bias=False)\n            self.bn1 = (32, affine=False, track_running_stats=False)\n            self.conv2 = (32, 64, 3, 1, bias=False)\n            self.bn2 = (64, affine=False, track_running_stats=False)\n        self.fc1 = (9216, 128)\n        self.dropout = (0.5)\n        self.fc2 = (128, 10)\n\n    def forward(self, x):\n        if self.fused:\n            x = self.convbn1(x)\n        else:\n            x = self.conv1(x)\n            x = self.bn1(x)\n        (x)\n        if self.fused:\n            x = self.convbn2(x)\n        else:\n            x = self.conv2(x)\n            x = self.bn2(x)\n        (x)\n        x = (x, 2)\n        (x)\n        x = x.flatten(1)\n        x = self.fc1(x)\n        x = self.dropout(x)\n        (x)\n        x = self.fc2(x)\n        output = (x, dim=1)\n        if fused:\n            memory_allocated[0].append(())\n        else:\n            memory_allocated[1].append(())\n        return output\n\ndef train(model, , , , epoch):\n    ()\n    for batch_idx, (data, target) in enumerate():\n        data, target = data.to(), target.to()\n        ()\n        output = model(data)\n        loss = (output, target)\n        loss.backward()\n        .step()\n        if batch_idx % 2 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(),\n                100. * batch_idx / len(), loss.item()))\n\ndef test(model, , ):\n    ()\n    test_loss = 0\n    correct = 0\n    # Use inference mode instead of no_grad, for free improved test-time performance\n    with ():\n        for data, target in :\n            data, target = data.to(), target.to()\n            output = model(data)\n            # sum up batch loss\n            test_loss += (output, target, reduction='sum').item()\n            # get the index of the max log-probability\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len()\n\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(),\n        100. * correct / len()))\n\nuse_cuda = ()\n = (\"cuda\" if use_cuda else \"cpu\")\ntrain_kwargs = {'batch_size': 2048}\ntest_kwargs = {'batch_size': 2048}\n\nif use_cuda:\n    cuda_kwargs = {'num_workers': 1,\n                   'pin_memory': True,\n                   'shuffle': True}\n    train_kwargs.update(cuda_kwargs)\n    test_kwargs.update(cuda_kwargs)\n\n = ([\n    (),\n    ((0.1307,), (0.3081,))\n])\n = ('../data', train=True, download=True,\n                          =)\n = ('../data', train=False,\n                          =)\n = (, **train_kwargs)\n = (, **test_kwargs)",
                        "code"
                    ],
                    [
                        "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n\n  0%|          | 0/9912422 [00:00&lt;?, ?it/s]\n100%|##########| 9912422/9912422 [00:00&lt;00:00, 100193542.52it/s]\nExtracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n\n  0%|          | 0/28881 [00:00&lt;?, ?it/s]\n100%|##########| 28881/28881 [00:00&lt;00:00, 102831658.59it/s]\nExtracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n\n  0%|          | 0/1648877 [00:00&lt;?, ?it/s]\n100%|##########| 1648877/1648877 [00:00&lt;00:00, 25955584.32it/s]\nExtracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n\n  0%|          | 0/4542 [00:00&lt;?, ?it/s]\n100%|##########| 4542/4542 [00:00&lt;00:00, 23606603.18it/s]\nExtracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw",
                        "code"
                    ]
                ]
            },
            {
                "A Comparison of Memory Usage": [
                    [
                        "If cuda is enabled, print out memory usage for both <cite>fused=True</cite> and <cite>fused=False</cite>\nFor an example run on RTX 3070, CuDNN 8.0.5: fused peak memory: 1.56GB,\nunfused peak memory: 2.68GB",
                        "markdown"
                    ],
                    [
                        "It is important to note that the <em>peak</em> memory usage for this model may vary depending\nthe specific CuDNN convolution algorithm used. For shallower models, it\nmay be possible for the peak memory allocated of the fused model to exceed\nthat of the unfused model! This is because the memory allocated to compute\ncertain CuDNN convolution algorithms can be high enough to \u201chide\u201d the typical peak\nyou would expect to be near the start of the backward pass.",
                        "markdown"
                    ],
                    [
                        "For this reason, we also record and display the memory allocated at the end\nof the forward pass as an approximation, and to demonstrate that we indeed\nallocate one fewer buffer per fused conv-bn pair.",
                        "markdown"
                    ],
                    [
                        "from statistics import mean\n\ntorch.backends.cudnn.enabled = True\n\nif use_cuda:\n    peak_memory_allocated = []\n\n    for fused in (True, False):\n        (123456)\n\n        model = (fused=fused).to()\n         = ((), lr=1.0)\n         = (, step_size=1, gamma=0.7)\n\n        for epoch in range(1):\n            train(model, , , , epoch)\n            test(model, , )\n            .step()\n        peak_memory_allocated.append(())\n        ()\n    print(\"CuDNN version:\", ())\n    print()\n    print(\"Peak memory allocated:\")\n    print(f\"fused: {peak_memory_allocated[0]/1024**3:.2f}GB, unfused: {peak_memory_allocated[1]/1024**3:.2f}GB\")\n    print(\"Memory allocated at end of forward pass:\")\n    print(f\"fused: {mean(memory_allocated[0])/1024**3:.2f}GB, unfused: {mean(memory_allocated[1])/1024**3:.2f}GB\")",
                        "code"
                    ],
                    [
                        "Train Epoch: 0 [0/60000 (0%)]   Loss: 2.352060\nTrain Epoch: 0 [4096/60000 (7%)]        Loss: 7.321198\nTrain Epoch: 0 [8192/60000 (13%)]       Loss: 4.253123\nTrain Epoch: 0 [12288/60000 (20%)]      Loss: 2.916881\nTrain Epoch: 0 [16384/60000 (27%)]      Loss: 2.643569\nTrain Epoch: 0 [20480/60000 (33%)]      Loss: 1.819675\nTrain Epoch: 0 [24576/60000 (40%)]      Loss: 1.546603\nTrain Epoch: 0 [28672/60000 (47%)]      Loss: 1.501737\nTrain Epoch: 0 [32768/60000 (53%)]      Loss: 1.495718\nTrain Epoch: 0 [36864/60000 (60%)]      Loss: 1.421847\nTrain Epoch: 0 [40960/60000 (67%)]      Loss: 1.260746\nTrain Epoch: 0 [45056/60000 (73%)]      Loss: 1.199898\nTrain Epoch: 0 [49152/60000 (80%)]      Loss: 0.951574\nTrain Epoch: 0 [53248/60000 (87%)]      Loss: 0.846121\nTrain Epoch: 0 [57344/60000 (93%)]      Loss: 0.794893\n\nTest set: Average loss: 0.4699, Accuracy: 8564/10000 (86%)\n\nTrain Epoch: 0 [0/60000 (0%)]   Loss: 2.352356\nTrain Epoch: 0 [4096/60000 (7%)]        Loss: 7.323086\nTrain Epoch: 0 [8192/60000 (13%)]       Loss: 4.011035\nTrain Epoch: 0 [12288/60000 (20%)]      Loss: 2.065851\nTrain Epoch: 0 [16384/60000 (27%)]      Loss: 2.220910\nTrain Epoch: 0 [20480/60000 (33%)]      Loss: 1.881398\nTrain Epoch: 0 [24576/60000 (40%)]      Loss: 1.611524\nTrain Epoch: 0 [28672/60000 (47%)]      Loss: 1.692404\nTrain Epoch: 0 [32768/60000 (53%)]      Loss: 1.603679\nTrain Epoch: 0 [36864/60000 (60%)]      Loss: 1.207830\nTrain Epoch: 0 [40960/60000 (67%)]      Loss: 1.220353\nTrain Epoch: 0 [45056/60000 (73%)]      Loss: 0.990351\nTrain Epoch: 0 [49152/60000 (80%)]      Loss: 0.884814\nTrain Epoch: 0 [53248/60000 (87%)]      Loss: 0.895245\nTrain Epoch: 0 [57344/60000 (93%)]      Loss: 0.779024\n\nTest set: Average loss: 0.4244, Accuracy: 8935/10000 (89%)\n\nCuDNN version: 8500\n\nPeak memory allocated:\nfused: 3.36GB, unfused: 2.68GB\nMemory allocated at end of forward pass:\nfused: 0.59GB, unfused: 0.96GB",
                        "code"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  38.242 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Custom C++ and CUDA Extensions": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "PyTorch provides a plethora of operations related to neural networks, arbitrary\ntensor algebra, data wrangling and other purposes. However, you may still find\nyourself in need of a more customized operation. For example, you might want to\nuse a novel activation function you found in a paper, or implement an operation\nyou developed as part of your research.",
                "markdown"
            ],
            [
                "The easiest way of integrating such a custom operation in PyTorch is to write it\nin Python by extending Function and Module as outlined . This gives you the full\npower of automatic differentiation (spares you from writing derivative\nfunctions) as well as the usual expressiveness of Python. However, there may be\ntimes when your operation is better implemented in C++. For example, your code\nmay need to be <em>really</em> fast because it is called very frequently in your model\nor is very expensive even for few calls. Another plausible reason is that it\ndepends on or interacts with other C or C++ libraries. To address such cases,\nPyTorch provides a very easy way of writing custom <em>C++ extensions</em>.",
                "markdown"
            ],
            [
                "C++ extensions are a mechanism we have developed to allow users (you) to create\nPyTorch operators defined <em>out-of-source</em>, i.e. separate from the PyTorch\nbackend. This approach is <em>different</em> from the way native PyTorch operations are\nimplemented. C++ extensions are intended to spare you much of the boilerplate\nassociated with integrating an operation with PyTorch\u2019s backend while providing\nyou with a high degree of flexibility for your PyTorch-based projects.\nNevertheless, once you have defined your operation as a C++ extension, turning\nit into a native PyTorch function is largely a matter of code organization,\nwhich you can tackle after the fact if you decide to contribute your operation\nupstream.",
                "markdown"
            ],
            {
                "Motivation and Example": [
                    [
                        "The rest of this note will walk through a practical example of writing and using\na C++ (and CUDA) extension. If you are being chased or someone will fire you if\nyou don\u2019t get that op done by the end of the day, you can skip this section and\nhead straight to the implementation details in the next section.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s say you\u2019ve come up with a new kind of recurrent unit that you found to\nhave superior properties compared to the state of the art. This recurrent unit\nis similar to an LSTM, but differs in that it lacks a <em>forget gate</em> and uses an\n<em>Exponential Linear Unit</em> (ELU) as its internal activation function. Because\nthis unit never forgets, we\u2019ll call it <em>LLTM</em>, or <em>Long-Long-Term-Memory</em> unit.",
                        "markdown"
                    ],
                    [
                        "The two ways in which LLTMs differ from vanilla LSTMs are significant enough\nthat we can\u2019t configure PyTorch\u2019s LSTMCell for our purposes, so we\u2019ll have to\ncreate a custom cell. The first and easiest approach for this \u2013 and likely in\nall cases a good first step \u2013 is to implement our desired functionality in\nplain PyTorch with Python. For this, we need to subclass\n and implement the forward pass of the LLTM. This would\nlook something like this:",
                        "markdown"
                    ],
                    [
                        "class LLTM(torch.nn.Module):\n    def __init__(self, input_features, state_size):\n        super(LLTM, self).__init__()\n        self.input_features = input_features\n        self.state_size = state_size\n        # 3 * state_size for input gate, output gate and candidate cell gate.\n        # input_features + state_size because we will multiply with [input, h].\n        self.weights = torch.nn.Parameter(\n            torch.empty(3 * state_size, input_features + state_size))\n        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.state_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, +stdv)\n\n    def forward(self, input, state):\n        old_h, old_cell = state\n        X = torch.cat([old_h, input], dim=1)\n\n        # Compute the input, output and candidate cell gates with one MM.\n        gate_weights = F.linear(X, self.weights, self.bias)\n        # Split the combined gate weight matrix into its components.\n        gates = gate_weights.chunk(3, dim=1)\n\n        input_gate = torch.sigmoid(gates[0])\n        output_gate = torch.sigmoid(gates[1])\n        # Here we use an ELU instead of the usual tanh.\n        candidate_cell = F.elu(gates[2])\n\n        # Compute the new cell state.\n        new_cell = old_cell + candidate_cell * input_gate\n        # Compute the new hidden state and output.\n        new_h = torch.tanh(new_cell) * output_gate\n\n        return new_h, new_cell",
                        "code"
                    ],
                    [
                        "which we could then use as expected:",
                        "markdown"
                    ],
                    [
                        "import torch\n\nX = torch.randn(batch_size, input_features)\nh = torch.randn(batch_size, state_size)\nC = torch.randn(batch_size, state_size)\n\nrnn = LLTM(input_features, state_size)\n\nnew_h, new_C = rnn(X, (h, C))",
                        "code"
                    ],
                    [
                        "Naturally, if at all possible and plausible, you should use this approach to\nextend PyTorch. Since PyTorch has highly optimized implementations of its\noperations for CPU <em>and</em> GPU, powered by libraries such as ,  or , PyTorch code like above will often be\nfast enough. However, we can also see why, under certain circumstances, there is\nroom for further performance improvements. The most obvious reason is that\nPyTorch has no knowledge of the <em>algorithm</em> you are implementing. It knows only\nof the individual operations you use to compose your algorithm. As such, PyTorch\nmust execute your operations individually, one after the other. Since each\nindividual call to the implementation (or <em>kernel</em>) of an operation, which may\ninvolve the launch of a CUDA kernel, has a certain amount of overhead, this\noverhead may become significant across many function calls. Furthermore, the\nPython interpreter that is running our code can itself slow down our program.",
                        "markdown"
                    ],
                    [
                        "A definite method of speeding things up is therefore to rewrite parts in C++ (or\nCUDA) and <em>fuse</em> particular groups of operations. Fusing means combining the\nimplementations of many functions into a single function, which profits from\nfewer kernel launches as well as other optimizations we can perform with\nincreased visibility of the global flow of data.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s see how we can use C++ extensions to implement a <em>fused</em> version of the\nLLTM. We\u2019ll begin by writing it in plain C++, using the  library that powers much of PyTorch\u2019s\nbackend, and see how easily it lets us translate our Python code. We\u2019ll then\nspeed things up even more by moving parts of the model to CUDA kernel to benefit\nfrom the massive parallelism GPUs provide.",
                        "markdown"
                    ]
                ]
            },
            {
                "Writing a C++ Extension": [
                    [
                        "C++ extensions come in two flavors: They can be built \u201cahead of time\u201d with\nsetuptools, or \u201cjust in time\u201d via\n. We\u2019ll begin with the first approach and\ndiscuss the latter later.",
                        "markdown"
                    ],
                    {
                        "Building with setuptools": [
                            [
                                "For the \u201cahead of time\u201d flavor, we build our C++ extension by writing a\nsetup.py script that uses setuptools to compile our C++ code. For the LLTM, it\nlooks as simple as this:",
                                "markdown"
                            ],
                            [
                                "from setuptools import setup, Extension\nfrom torch.utils import cpp_extension\n\nsetup(name='lltm_cpp',\n      ext_modules=[cpp_extension.CppExtension('lltm_cpp', ['lltm.cpp'])],\n      cmdclass={'build_ext': cpp_extension.BuildExtension})",
                                "code"
                            ],
                            [
                                "In this code, CppExtension is a convenience wrapper around\nsetuptools.Extension that passes the correct include paths and sets\nthe language of the extension to C++. The equivalent vanilla setuptools\ncode would simply be:",
                                "markdown"
                            ],
                            [
                                "Extension(\n   name='lltm_cpp',\n   sources=['lltm.cpp'],\n   include_dirs=cpp_extension.include_paths(),\n   language='c++')",
                                "code"
                            ],
                            [
                                "BuildExtension performs a number of required configuration steps and\nchecks and also manages mixed compilation in the case of mixed C++/CUDA\nextensions. And that\u2019s all we really need to know about building C++ extensions\nfor now! Let\u2019s now take a look at the implementation of our C++ extension,\nwhich goes into lltm.cpp.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Writing the C++ Op": [
                            [
                                "Let\u2019s start implementing the LLTM in C++! One function we\u2019ll need for the\nbackward pass is the derivative of the sigmoid. This is a small enough piece of\ncode to discuss the overall environment that is available to us when writing C++\nextensions:",
                                "markdown"
                            ],
                            [
                                "#include &lt;torch/extension.h&gt;\n\n#include &lt;iostream&gt;\n\ntorch::Tensor d_sigmoid(torch::Tensor z) {\n  auto s = torch::sigmoid(z);\n  return (1 - s) * s;\n}",
                                "code"
                            ],
                            [
                                "&lt;torch/extension.h&gt; is the one-stop header to include all the necessary PyTorch\nbits to write C++ extensions. It includes:",
                                "markdown"
                            ],
                            [
                                "The ATen library, which is our primary API for tensor computation,",
                                "markdown"
                            ],
                            [
                                ", which is how we create Python bindings for our C++ code,",
                                "markdown"
                            ],
                            [
                                "Headers that manage the details of interaction between ATen and pybind11.",
                                "markdown"
                            ],
                            [
                                "The implementation of d_sigmoid() shows how to use the ATen API.\nPyTorch\u2019s tensor and variable interface is generated automatically from the\nATen library, so we can more or less translate our Python implementation 1:1\ninto C++. Our primary datatype for all computations will be\ntorch::Tensor. Its full API can be inspected . Notice\nalso that we can include &lt;iostream&gt; or <em>any other C or C++ header</em> \u2013 we have\nthe full power of C++11 at our disposal.",
                                "markdown"
                            ],
                            [
                                "Note that CUDA-11.5 nvcc will hit internal compiler error while parsing torch/extension.h on Windows.\nTo workaround the issue, move python binding logic to pure C++ file.\nExample use:",
                                "markdown"
                            ],
                            [
                                "#include &lt;ATen/ATen.h&gt;\nat::Tensor SigmoidAlphaBlendForwardCuda(....)",
                                "code"
                            ],
                            [
                                "Instead of:",
                                "markdown"
                            ],
                            [
                                "#include &lt;torch/extension.h&gt;\ntorch::Tensor SigmoidAlphaBlendForwardCuda(...)",
                                "code"
                            ],
                            [
                                "Currently open issue for nvcc bug .\nComplete workaround code example .",
                                "markdown"
                            ],
                            {
                                "Forward Pass": [
                                    [
                                        "Next we can port our entire forward pass to C++:",
                                        "markdown"
                                    ],
                                    [
                                        "#include &lt;vector&gt;\n\nstd::vector&lt;at::Tensor&gt; lltm_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell) {\n  auto X = torch::cat({old_h, input}, /*dim=*/1);\n\n  auto gate_weights = torch::addmm(bias, X, weights.transpose(0, 1));\n  auto gates = gate_weights.chunk(3, /*dim=*/1);\n\n  auto input_gate = torch::sigmoid(gates[0]);\n  auto output_gate = torch::sigmoid(gates[1]);\n  auto candidate_cell = torch::elu(gates[2], /*alpha=*/1.0);\n\n  auto new_cell = old_cell + candidate_cell * input_gate;\n  auto new_h = torch::tanh(new_cell) * output_gate;\n\n  return {new_h,\n          new_cell,\n          input_gate,\n          output_gate,\n          candidate_cell,\n          X,\n          gate_weights};\n}",
                                        "code"
                                    ]
                                ]
                            },
                            {
                                "Backward Pass": [
                                    [
                                        "The C++ extension API currently does not provide a way of automatically\ngenerating a backwards function for us. As such, we have to also implement the\nbackward pass of our LLTM, which computes the derivative of the loss with\nrespect to each input of the forward pass. Ultimately, we will plop both the\nforward and backward function into a  to create\na nice Python binding. The backward function is slightly more involved, so\nwe\u2019ll not dig deeper into the code (if you are interested,  is a good read for more\ninformation on this):",
                                        "markdown"
                                    ],
                                    [
                                        "// tanh'(z) = 1 - tanh^2(z)\ntorch::Tensor d_tanh(torch::Tensor z) {\n  return 1 - z.tanh().pow(2);\n}\n\n// elu'(z) = relu'(z) + { alpha * exp(z) if (alpha * (exp(z) - 1)) &lt; 0, else 0}\ntorch::Tensor d_elu(torch::Tensor z, torch::Scalar alpha = 1.0) {\n  auto e = z.exp();\n  auto mask = (alpha * (e - 1)) &lt; 0;\n  return (z &gt; 0).type_as(z) + mask.type_as(z) * (alpha * e);\n}\n\nstd::vector&lt;torch::Tensor&gt; lltm_backward(\n    torch::Tensor grad_h,\n    torch::Tensor grad_cell,\n    torch::Tensor new_cell,\n    torch::Tensor input_gate,\n    torch::Tensor output_gate,\n    torch::Tensor candidate_cell,\n    torch::Tensor X,\n    torch::Tensor gate_weights,\n    torch::Tensor weights) {\n  auto d_output_gate = torch::tanh(new_cell) * grad_h;\n  auto d_tanh_new_cell = output_gate * grad_h;\n  auto d_new_cell = d_tanh(new_cell) * d_tanh_new_cell + grad_cell;\n\n  auto d_old_cell = d_new_cell;\n  auto d_candidate_cell = input_gate * d_new_cell;\n  auto d_input_gate = candidate_cell * d_new_cell;\n\n  auto gates = gate_weights.chunk(3, /*dim=*/1);\n  d_input_gate *= d_sigmoid(gates[0]);\n  d_output_gate *= d_sigmoid(gates[1]);\n  d_candidate_cell *= d_elu(gates[2]);\n\n  auto d_gates =\n      torch::cat({d_input_gate, d_output_gate, d_candidate_cell}, /*dim=*/1);\n\n  auto d_weights = d_gates.t().mm(X);\n  auto d_bias = d_gates.sum(/*dim=*/0, /*keepdim=*/true);\n\n  auto d_X = d_gates.mm(weights);\n  const auto state_size = grad_h.size(1);\n  auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size);\n  auto d_input = d_X.slice(/*dim=*/1, state_size);\n\n  return {d_old_h, d_input, d_weights, d_bias, d_old_cell};\n}",
                                        "code"
                                    ]
                                ]
                            }
                        ]
                    },
                    {
                        "Binding to Python": [
                            [
                                "Once you have your operation written in C++ and ATen, you can use pybind11 to\nbind your C++ functions or classes into Python in a very simple manner.\nQuestions or issues you have about this part of PyTorch C++ extensions will\nlargely be addressed by .",
                                "markdown"
                            ],
                            [
                                "For our extensions, the necessary binding code spans only four lines:",
                                "markdown"
                            ],
                            [
                                "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &amp;lltm_forward, \"LLTM forward\");\n  m.def(\"backward\", &amp;lltm_backward, \"LLTM backward\");\n}",
                                "code"
                            ],
                            [
                                "One bit to note here is the macro TORCH_EXTENSION_NAME. The torch extension\nbuild will define it as the name you give your extension in the setup.py\nscript. In this case, the value of TORCH_EXTENSION_NAME would be \u201clltm_cpp\u201d.\nThis is to avoid having to maintain the name of the extension in two places\n(the build script and your C++ code), as a mismatch between the two can lead to\nnasty and hard to track issues.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Using Your Extension": [
                            [
                                "We are now set to import our extension in PyTorch. At this point, your directory\nstructure could look something like this:",
                                "markdown"
                            ],
                            [
                                "pytorch/\n  lltm-extension/\n    lltm.cpp\n    setup.py",
                                "code"
                            ],
                            [
                                "Now, run python setup.py install to build and install your extension. This\nshould look something like this:",
                                "markdown"
                            ],
                            [
                                "running install\nrunning bdist_egg\nrunning egg_info\ncreating lltm_cpp.egg-info\nwriting lltm_cpp.egg-info/PKG-INFO\nwriting dependency_links to lltm_cpp.egg-info/dependency_links.txt\nwriting top-level names to lltm_cpp.egg-info/top_level.txt\nwriting manifest file 'lltm_cpp.egg-info/SOURCES.txt'\nreading manifest file 'lltm_cpp.egg-info/SOURCES.txt'\nwriting manifest file 'lltm_cpp.egg-info/SOURCES.txt'\ninstalling library code to build/bdist.linux-x86_64/egg\nrunning install_lib\nrunning build_ext\nbuilding 'lltm_cpp' extension\ncreating build\ncreating build/temp.linux-x86_64-3.7\ngcc -pthread -B ~/local/miniconda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I~/local/miniconda/lib/python3.7/site-packages/torch/include -I~/local/miniconda/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I~/local/miniconda/lib/python3.7/site-packages/torch/include/TH -I~/local/miniconda/lib/python3.7/site-packages/torch/include/THC -I~/local/miniconda/include/python3.7m -c lltm.cpp -o build/temp.linux-x86_64-3.7/lltm.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=lltm_cpp -D_GLIBCXX_USE_CXX11_ABI=1 -std=c++11\ncc1plus: warning: command line option \u2018-Wstrict-prototypes\u2019 is valid for C/ObjC but not for C++\ncreating build/lib.linux-x86_64-3.7\ng++ -pthread -shared -B ~/local/miniconda/compiler_compat -L~/local/miniconda/lib -Wl,-rpath=~/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/lltm.o -o build/lib.linux-x86_64-3.7/lltm_cpp.cpython-37m-x86_64-linux-gnu.so\ncreating build/bdist.linux-x86_64\ncreating build/bdist.linux-x86_64/egg\ncopying build/lib.linux-x86_64-3.7/lltm_cpp.cpython-37m-x86_64-linux-gnu.so -&gt; build/bdist.linux-x86_64/egg\ncreating stub loader for lltm_cpp.cpython-37m-x86_64-linux-gnu.so\nbyte-compiling build/bdist.linux-x86_64/egg/lltm_cpp.py to lltm_cpp.cpython-37.pyc\ncreating build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm_cpp.egg-info/PKG-INFO -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm_cpp.egg-info/SOURCES.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm_cpp.egg-info/dependency_links.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\ncopying lltm_cpp.egg-info/top_level.txt -&gt; build/bdist.linux-x86_64/egg/EGG-INFO\nwriting build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\nzip_safe flag not set; analyzing archive contents...\n__pycache__.lltm_cpp.cpython-37: module references __file__\ncreating 'dist/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\nremoving 'build/bdist.linux-x86_64/egg' (and everything under it)\nProcessing lltm_cpp-0.0.0-py3.7-linux-x86_64.egg\nremoving '~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg' (and everything under it)\ncreating ~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg\nExtracting lltm_cpp-0.0.0-py3.7-linux-x86_64.egg to ~/local/miniconda/lib/python3.7/site-packages\nlltm-cpp 0.0.0 is already the active version in easy-install.pth\n\nInstalled ~/local/miniconda/lib/python3.7/site-packages/lltm_cpp-0.0.0-py3.7-linux-x86_64.egg\nProcessing dependencies for lltm-cpp==0.0.0\nFinished processing dependencies for lltm-cpp==0.0.0",
                                "code"
                            ],
                            [
                                "A small note on compilers: Due to ABI versioning issues, the compiler you use to\nbuild your C++ extension must be <em>ABI-compatible</em> with the compiler PyTorch was\nbuilt with. In practice, this means that you must use GCC version 4.9 and above on Linux.\nFor Ubuntu 16.04 and other more-recent Linux distributions, this should be the\ndefault compiler already. On MacOS, you must use clang (which does not have any ABI versioning issues). In the worst\ncase, you can build PyTorch from source with your compiler and then build the\nextension with that same compiler.",
                                "markdown"
                            ],
                            [
                                "Once your extension is built, you can simply import it in Python, using the\nname you specified in your setup.py script. Just be sure to import\ntorch first, as this will resolve some symbols that the dynamic linker must\nsee:",
                                "markdown"
                            ],
                            [
                                "In [1]: import torch\nIn [2]: import lltm_cpp\nIn [3]: lltm_cpp.forward\nOut[3]: &lt;function lltm.PyCapsule.forward&gt;",
                                "code"
                            ],
                            [
                                "If we call help() on the function or module, we can see that its signature\nmatches our C++ code:",
                                "markdown"
                            ],
                            [
                                "In[4] help(lltm_cpp.forward)\nforward(...) method of builtins.PyCapsule instance\n    forward(arg0: torch::Tensor, arg1: torch::Tensor, arg2: torch::Tensor, arg3: torch::Tensor, arg4: torch::Tensor) -&gt; List[torch::Tensor]\n\n    LLTM forward",
                                "code"
                            ],
                            [
                                "Since we are now able to call our C++ functions from Python, we can wrap them\nwith  and  to make them first\nclass citizens of PyTorch:",
                                "markdown"
                            ],
                            [
                                "import math\nimport torch\n\n# Our module!\nimport lltm_cpp\n\nclass LLTMFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, weights, bias, old_h, old_cell):\n        outputs = lltm_cpp.forward(input, weights, bias, old_h, old_cell)\n        new_h, new_cell = outputs[:2]\n        variables = outputs[1:] + [weights]\n        ctx.save_for_backward(*variables)\n\n        return new_h, new_cell\n\n    @staticmethod\n    def backward(ctx, grad_h, grad_cell):\n        outputs = lltm_cpp.backward(\n            grad_h.contiguous(), grad_cell.contiguous(), *ctx.saved_tensors)\n        d_old_h, d_input, d_weights, d_bias, d_old_cell = outputs\n        return d_input, d_weights, d_bias, d_old_h, d_old_cell\n\n\nclass LLTM(torch.nn.Module):\n    def __init__(self, input_features, state_size):\n        super(LLTM, self).__init__()\n        self.input_features = input_features\n        self.state_size = state_size\n        self.weights = torch.nn.Parameter(\n            torch.empty(3 * state_size, input_features + state_size))\n        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1.0 / math.sqrt(self.state_size)\n        for weight in self.parameters():\n            weight.data.uniform_(-stdv, +stdv)\n\n    def forward(self, input, state):\n        return LLTMFunction.apply(input, self.weights, self.bias, *state)",
                                "code"
                            ],
                            {
                                "Performance Comparison": [
                                    [
                                        "Now that we are able to use and call our C++ code from PyTorch, we can run a\nsmall benchmark to see how much performance we gained from rewriting our op in\nC++. We\u2019ll run the LLTM forwards and backwards a few times and measure the\nduration:",
                                        "markdown"
                                    ],
                                    [
                                        "import time\n\nimport torch\n\nbatch_size = 16\ninput_features = 32\nstate_size = 128\n\nX = torch.randn(batch_size, input_features)\nh = torch.randn(batch_size, state_size)\nC = torch.randn(batch_size, state_size)\n\nrnn = LLTM(input_features, state_size)\n\nforward = 0\nbackward = 0\nfor _ in range(100000):\n    start = time.time()\n    new_h, new_C = rnn(X, (h, C))\n    forward += time.time() - start\n\n    start = time.time()\n    (new_h.sum() + new_C.sum()).backward()\n    backward += time.time() - start\n\nprint('Forward: {:.3f} s | Backward {:.3f} s'.format(forward, backward))",
                                        "code"
                                    ],
                                    [
                                        "If we run this code with the original LLTM we wrote in pure Python at the start\nof this post, we get the following numbers (on my machine):",
                                        "markdown"
                                    ],
                                    [
                                        "Forward: 506.480 us | Backward 444.694 us",
                                        "code"
                                    ],
                                    [
                                        "and with our new C++ version:",
                                        "markdown"
                                    ],
                                    [
                                        "Forward: 349.335 us | Backward 443.523 us",
                                        "code"
                                    ],
                                    [
                                        "We can already see a significant speedup for the forward function (more than\n30%). For the backward function, a speedup is visible, albeit not a major one.\nThe backward pass I wrote above was not particularly optimized and could\ndefinitely be improved. Also, PyTorch\u2019s automatic differentiation engine can\nautomatically parallelize computation graphs, may use a more efficient flow of\noperations overall, and is also implemented in C++, so it\u2019s expected to be\nfast. Nevertheless, this is a good start.",
                                        "markdown"
                                    ]
                                ]
                            },
                            {
                                "Performance on GPU Devices": [
                                    [
                                        "A wonderful fact about PyTorch\u2019s <em>ATen</em> backend is that it abstracts the\ncomputing device you are running on. This means the same code we wrote for CPU\ncan <em>also</em> run on GPU, and individual operations will correspondingly dispatch\nto GPU-optimized implementations. For certain operations like matrix multiply\n(like mm or addmm), this is a big win. Let\u2019s take a look at how much\nperformance we gain from running our C++ code with CUDA tensors. No changes to\nour implementation are required, we simply need to put our tensors in GPU\nmemory from Python, with either adding device=cuda_device argument at\ncreation time or using .to(cuda_device) after creation:",
                                        "markdown"
                                    ],
                                    [
                                        "import torch\n\nassert torch.cuda.is_available()\ncuda_device = torch.device(\"cuda\")  # device object representing GPU\n\nbatch_size = 16\ninput_features = 32\nstate_size = 128\n\n# Note the device=cuda_device arguments here\nX = torch.randn(batch_size, input_features, device=cuda_device)\nh = torch.randn(batch_size, state_size, device=cuda_device)\nC = torch.randn(batch_size, state_size, device=cuda_device)\n\nrnn = LLTM(input_features, state_size).to(cuda_device)\n\nforward = 0\nbackward = 0\nfor _ in range(100000):\n    start = time.time()\n    new_h, new_C = rnn(X, (h, C))\n    torch.cuda.synchronize()\n    forward += time.time() - start\n\n    start = time.time()\n    (new_h.sum() + new_C.sum()).backward()\n    torch.cuda.synchronize()\n    backward += time.time() - start\n\nprint('Forward: {:.3f} us | Backward {:.3f} us'.format(forward * 1e6/1e5, backward * 1e6/1e5))",
                                        "code"
                                    ],
                                    [
                                        "Once more comparing our plain PyTorch code with our C++ version, now both\nrunning on CUDA devices, we again see performance gains. For Python/PyTorch:",
                                        "markdown"
                                    ],
                                    [
                                        "Forward: 187.719 us | Backward 410.815 us",
                                        "code"
                                    ],
                                    [
                                        "And C++/ATen:",
                                        "markdown"
                                    ],
                                    [
                                        "Forward: 149.802 us | Backward 393.458 us",
                                        "code"
                                    ],
                                    [
                                        "That\u2019s a great overall speedup compared to non-CUDA code. However, we can pull\neven more performance out of our C++ code by writing custom CUDA kernels, which\nwe\u2019ll dive into soon. Before that, let\u2019s discuss another way of building your C++\nextensions.",
                                        "markdown"
                                    ]
                                ]
                            }
                        ]
                    },
                    {
                        "JIT Compiling Extensions": [
                            [
                                "Previously, I mentioned there were two ways of building C++ extensions: using\nsetuptools or just in time (JIT). Having covered the former, let\u2019s\nelaborate on the latter. The JIT compilation mechanism provides you with a way\nof compiling and loading your extensions on the fly by calling a simple\nfunction in PyTorch\u2019s API called . For\nthe LLTM, this would look as simple as this:",
                                "markdown"
                            ],
                            [
                                "from torch.utils.cpp_extension import load\n\nlltm_cpp = load(name=\"lltm_cpp\", sources=[\"lltm.cpp\"])",
                                "code"
                            ],
                            [
                                "Here, we provide the function with the same information as for\nsetuptools. In the background, this will do the following:",
                                "markdown"
                            ],
                            [
                                "Create a temporary directory /tmp/torch_extensions/lltm,",
                                "markdown"
                            ],
                            [
                                "Emit a  build file into that temporary directory,",
                                "markdown"
                            ],
                            [
                                "Compile your source files into a shared library,",
                                "markdown"
                            ],
                            [
                                "Import this shared library as a Python module.",
                                "markdown"
                            ],
                            [
                                "In fact, if you pass verbose=True to cpp_extension.load(), you will\nbe informed about the process:",
                                "markdown"
                            ],
                            [
                                "Using /tmp/torch_extensions as PyTorch extensions root...\nEmitting ninja build file /tmp/torch_extensions/lltm_cpp/build.ninja...\nBuilding extension module lltm_cpp...\nLoading extension module lltm_cpp...",
                                "code"
                            ],
                            [
                                "The resulting Python module will be exactly the same as produced by setuptools,\nbut removes the requirement of having to maintain a separate setup.py build\nfile. If your setup is more complicated and you do need the full power of\nsetuptools, you <em>can</em> write your own setup.py \u2013 but in many cases\nthis JIT technique will do just fine. The first time you run through this line,\nit will take some time, as the extension is compiling in the background. Since\nwe use the Ninja build system to build your sources, re-compilation is\nincremental and thus re-loading the extension when you run your Python module a\nsecond time is fast and has low overhead if you didn\u2019t change the extension\u2019s\nsource files.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Writing a Mixed C++/CUDA extension": [
                    [
                        "To really take our implementation to the next level, we can hand-write parts of\nour forward and backward passes with custom CUDA kernels. For the LLTM, this has\nthe prospect of being particularly effective, as there are a large number of\npointwise operations in sequence, that can all be fused and parallelized in a\nsingle CUDA kernel. Let\u2019s see how we could write such a CUDA kernel and\nintegrate it with PyTorch using this extension mechanism.",
                        "markdown"
                    ],
                    [
                        "The general strategy for writing a CUDA extension is to first write a C++ file\nwhich defines the functions that will be called from Python, and binds those\nfunctions to Python with pybind11. Furthermore, this file will also <em>declare</em>\nfunctions that are defined in CUDA (.cu) files. The C++ functions will then\ndo some checks and ultimately forward its calls to the CUDA functions. In the\nCUDA files, we write our actual CUDA kernels. The cpp_extension package\nwill then take care of compiling the C++ sources with a C++ compiler like\ngcc and the CUDA sources with NVIDIA\u2019s nvcc compiler. This ensures that\neach compiler takes care of files it knows best to compile. Ultimately, they\nwill be linked into one shared library that is available to us from Python\ncode.",
                        "markdown"
                    ],
                    [
                        "We\u2019ll start with the C++ file, which we\u2019ll call lltm_cuda.cpp, for example:",
                        "markdown"
                    ],
                    [
                        "#include &lt;torch/extension.h&gt;\n\n#include &lt;vector&gt;\n\n// CUDA forward declarations\n\nstd::vector&lt;torch::Tensor&gt; lltm_cuda_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell);\n\nstd::vector&lt;torch::Tensor&gt; lltm_cuda_backward(\n    torch::Tensor grad_h,\n    torch::Tensor grad_cell,\n    torch::Tensor new_cell,\n    torch::Tensor input_gate,\n    torch::Tensor output_gate,\n    torch::Tensor candidate_cell,\n    torch::Tensor X,\n    torch::Tensor gate_weights,\n    torch::Tensor weights);\n\n// C++ interface\n\n#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n\nstd::vector&lt;torch::Tensor&gt; lltm_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell) {\n  CHECK_INPUT(input);\n  CHECK_INPUT(weights);\n  CHECK_INPUT(bias);\n  CHECK_INPUT(old_h);\n  CHECK_INPUT(old_cell);\n\n  return lltm_cuda_forward(input, weights, bias, old_h, old_cell);\n}\n\nstd::vector&lt;torch::Tensor&gt; lltm_backward(\n    torch::Tensor grad_h,\n    torch::Tensor grad_cell,\n    torch::Tensor new_cell,\n    torch::Tensor input_gate,\n    torch::Tensor output_gate,\n    torch::Tensor candidate_cell,\n    torch::Tensor X,\n    torch::Tensor gate_weights,\n    torch::Tensor weights) {\n  CHECK_INPUT(grad_h);\n  CHECK_INPUT(grad_cell);\n  CHECK_INPUT(input_gate);\n  CHECK_INPUT(output_gate);\n  CHECK_INPUT(candidate_cell);\n  CHECK_INPUT(X);\n  CHECK_INPUT(gate_weights);\n  CHECK_INPUT(weights);\n\n  return lltm_cuda_backward(\n      grad_h,\n      grad_cell,\n      new_cell,\n      input_gate,\n      output_gate,\n      candidate_cell,\n      X,\n      gate_weights,\n      weights);\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n  m.def(\"forward\", &amp;lltm_forward, \"LLTM forward (CUDA)\");\n  m.def(\"backward\", &amp;lltm_backward, \"LLTM backward (CUDA)\");\n}",
                        "code"
                    ],
                    [
                        "As you can see, it is largely boilerplate, checks and forwarding to functions\nthat we\u2019ll define in the CUDA file. We\u2019ll name this file\nlltm_cuda_kernel.cu (note the .cu extension!). NVCC can reasonably\ncompile C++11, thus we still have ATen and the C++ standard library available\nto us (but not torch.h). Note that setuptools cannot handle files\nwith the same name but different extensions, so if you use the setup.py\nmethod instead of the JIT method, you must give your CUDA file a different name\nthan your C++ file (for the JIT method, lltm.cpp and lltm.cu would work\nfine). Let\u2019s take a small peek at what this file will look like:",
                        "markdown"
                    ],
                    [
                        "#include &lt;torch/extension.h&gt;\n\n#include &lt;cuda.h&gt;\n#include &lt;cuda_runtime.h&gt;\n\n#include &lt;vector&gt;\n\ntemplate &lt;typename scalar_t&gt;\n__device__ __forceinline__ scalar_t sigmoid(scalar_t z) {\n  return 1.0 / (1.0 + exp(-z));\n}",
                        "code"
                    ],
                    [
                        "Here we see the headers I just described, as well as the fact that we are using\nCUDA-specific declarations like __device__ and __forceinline__ and\nfunctions like exp. Let\u2019s continue with a few more helper functions that\nwe\u2019ll need:",
                        "markdown"
                    ],
                    [
                        "template &lt;typename scalar_t&gt;\n__device__ __forceinline__ scalar_t d_sigmoid(scalar_t z) {\n  const auto s = sigmoid(z);\n  return (1.0 - s) * s;\n}\n\ntemplate &lt;typename scalar_t&gt;\n__device__ __forceinline__ scalar_t d_tanh(scalar_t z) {\n  const auto t = tanh(z);\n  return 1 - (t * t);\n}\n\ntemplate &lt;typename scalar_t&gt;\n__device__ __forceinline__ scalar_t elu(scalar_t z, scalar_t alpha = 1.0) {\n  return fmax(0.0, z) + fmin(0.0, alpha * (exp(z) - 1.0));\n}\n\ntemplate &lt;typename scalar_t&gt;\n__device__ __forceinline__ scalar_t d_elu(scalar_t z, scalar_t alpha = 1.0) {\n  const auto e = exp(z);\n  const auto d_relu = z &lt; 0.0 ? 0.0 : 1.0;\n  return d_relu + (((alpha * (e - 1.0)) &lt; 0.0) ? (alpha * e) : 0.0);\n}",
                        "code"
                    ],
                    [
                        "To now actually implement a function, we\u2019ll again need two things: one function\nthat performs operations we don\u2019t wish to explicitly write by hand and calls\ninto CUDA kernels, and then the actual CUDA kernel for the parts we want to\nspeed up. For the forward pass, the first function should look like this:",
                        "markdown"
                    ],
                    [
                        "std::vector&lt;torch::Tensor&gt; lltm_cuda_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell) {\n  auto X = torch::cat({old_h, input}, /*dim=*/1);\n  auto gates = torch::addmm(bias, X, weights.transpose(0, 1));\n\n  const auto batch_size = old_cell.size(0);\n  const auto state_size = old_cell.size(1);\n\n  auto new_h = torch::zeros_like(old_cell);\n  auto new_cell = torch::zeros_like(old_cell);\n  auto input_gate = torch::zeros_like(old_cell);\n  auto output_gate = torch::zeros_like(old_cell);\n  auto candidate_cell = torch::zeros_like(old_cell);\n\n  const int threads = 1024;\n  const dim3 blocks((state_size + threads - 1) / threads, batch_size);\n\n  AT_DISPATCH_FLOATING_TYPES(gates.type(), \"lltm_forward_cuda\", ([&amp;] {\n    lltm_cuda_forward_kernel&lt;scalar_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(\n        gates.data&lt;scalar_t&gt;(),\n        old_cell.data&lt;scalar_t&gt;(),\n        new_h.data&lt;scalar_t&gt;(),\n        new_cell.data&lt;scalar_t&gt;(),\n        input_gate.data&lt;scalar_t&gt;(),\n        output_gate.data&lt;scalar_t&gt;(),\n        candidate_cell.data&lt;scalar_t&gt;(),\n        state_size);\n  }));\n\n  return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gates};\n}",
                        "code"
                    ],
                    [
                        "The main point of interest here is the AT_DISPATCH_FLOATING_TYPES macro and\nthe kernel launch (indicated by the &lt;&lt;&lt;...&gt;&gt;&gt;). While ATen abstracts away\nthe device and datatype of the tensors we deal with, a tensor will, at runtime,\nstill be backed by memory of a concrete type on a concrete device. As such, we\nneed a way of determining at runtime what type a tensor is and then selectively\ncall functions with the corresponding correct type signature. Done manually,\nthis would (conceptually) look something like this:",
                        "markdown"
                    ],
                    [
                        "switch (tensor.type().scalarType()) {\n  case torch::ScalarType::Double:\n    return function&lt;double&gt;(tensor.data&lt;double&gt;());\n  case torch::ScalarType::Float:\n    return function&lt;float&gt;(tensor.data&lt;float&gt;());\n  ...\n}",
                        "code"
                    ],
                    [
                        "The purpose of AT_DISPATCH_FLOATING_TYPES is to take care of this dispatch\nfor us. It takes a type (gates.type() in our case), a name (for error\nmessages) and a lambda function. Inside this lambda function, the type alias\nscalar_t is available and is defined as the type that the tensor actually\nis at runtime in that context. As such, if we have a template function (which\nour CUDA kernel will be), we can instantiate it with this scalar_t alias,\nand the correct function will be called. In this case, we also want to retrieve\nthe data pointers of the tensors as pointers of that scalar_t type. If you\nwanted to dispatch over all types and not just floating point types (Float\nand Double), you can use AT_DISPATCH_ALL_TYPES.",
                        "markdown"
                    ],
                    [
                        "Note that we perform some operations with plain ATen. These operations will\nstill run on the GPU, but using ATen\u2019s default implementations. This makes\nsense because ATen will use highly optimized routines for things like matrix\nmultiplies (e.g. addmm) or convolutions which would be much harder to\nimplement and improve ourselves.",
                        "markdown"
                    ],
                    [
                        "As for the kernel launch itself, we are here specifying that each CUDA block\nwill have 1024 threads, and that the entire GPU grid is split into as many\nblocks of 1 x 1024 threads as are required to fill our matrices with one\nthread per component. For example, if our state size was 2048 and our batch\nsize 4, we\u2019d launch a total of 4 x 2 = 8 blocks with each 1024 threads. If\nyou\u2019ve never heard of CUDA \u201cblocks\u201d or \u201cgrids\u201d before, an  may\nhelp.",
                        "markdown"
                    ],
                    [
                        "The actual CUDA kernel is fairly simple (if you\u2019ve ever programmed GPUs before):",
                        "markdown"
                    ],
                    [
                        "template &lt;typename scalar_t&gt;\n__global__ void lltm_cuda_forward_kernel(\n    const scalar_t* __restrict__ gates,\n    const scalar_t* __restrict__ old_cell,\n    scalar_t* __restrict__ new_h,\n    scalar_t* __restrict__ new_cell,\n    scalar_t* __restrict__ input_gate,\n    scalar_t* __restrict__ output_gate,\n    scalar_t* __restrict__ candidate_cell,\n    size_t state_size) {\n  const int column = blockIdx.x * blockDim.x + threadIdx.x;\n  const int index = blockIdx.y * state_size + column;\n  const int gates_row = blockIdx.y * (state_size * 3);\n  if (column &lt; state_size) {\n    input_gate[index] = sigmoid(gates[gates_row + column]);\n    output_gate[index] = sigmoid(gates[gates_row + state_size + column]);\n    candidate_cell[index] = elu(gates[gates_row + 2 * state_size + column]);\n    new_cell[index] =\n        old_cell[index] + candidate_cell[index] * input_gate[index];\n    new_h[index] = tanh(new_cell[index]) * output_gate[index];\n  }\n}",
                        "code"
                    ],
                    [
                        "What\u2019s primarily interesting here is that we are able to compute all of these\npointwise operations entirely in parallel for each individual component in our\ngate matrices. If you imagine having to do this with a giant for loop over\na million elements in serial, you can see why this would be much faster.",
                        "markdown"
                    ],
                    {
                        "Using accessors": [
                            [
                                "You can see in the CUDA kernel that we work directly on pointers with the right\ntype. Indeed, working directly with high level type agnostic tensors inside cuda\nkernels would be very inefficient.",
                                "markdown"
                            ],
                            [
                                "However, this comes at a cost of ease of use and readability, especially for\nhighly dimensional data. In our example, we know for example that the contiguous\ngates tensor has 3 dimensions:",
                                "markdown"
                            ],
                            [
                                "batch, size of batch_size and stride of 3*state_size",
                                "markdown"
                            ],
                            [
                                "row, size of 3 and stride of state_size",
                                "markdown"
                            ],
                            [
                                "index, size  of state_size and stride of 1",
                                "markdown"
                            ],
                            [
                                "How can we access the element gates[n][row][column] inside the kernel then?\nIt turns out that you need the strides to access your element with some simple\narithmetic.",
                                "markdown"
                            ],
                            [
                                "gates.data&lt;scalar_t&gt;()[n*3*state_size + row*state_size + column]",
                                "code"
                            ],
                            [
                                "In addition to being verbose, this expression needs stride to be explicitly\nknown, and thus passed to the kernel function within its arguments. You can see\nthat in the case of kernel functions accepting multiple tensors with different\nsizes you will end up with a very long list of arguments.",
                                "markdown"
                            ],
                            [
                                "Fortunately for us, ATen provides accessors that are created with a single\ndynamic check that a Tensor is the type and number of dimensions.\nAccessors then expose an API for accessing the Tensor elements efficiently\nwithout having to convert to a single pointer:",
                                "markdown"
                            ],
                            [
                                "torch::Tensor foo = torch::rand({12, 12});\n\n// assert foo is 2-dimensional and holds floats.\nauto foo_a = foo.accessor&lt;float,2&gt;();\nfloat trace = 0;\n\nfor(int i = 0; i &lt; foo_a.size(0); i++) {\n  // use the accessor foo_a to get tensor data.\n  trace += foo_a[i][i];\n}",
                                "code"
                            ],
                            [
                                "Accessor objects have a relatively high level interface, with .size() and\n.stride() methods and multi-dimensional indexing. The .accessor&lt;&gt;\ninterface is designed to access data efficiently on cpu tensor. The equivalent\nfor cuda tensors are packed_accessor64&lt;&gt; and packed_accessor32&lt;&gt;, which\nproduce Packed Accessors with either 64-bit or 32-bit integer indexing.",
                                "markdown"
                            ],
                            [
                                "The fundamental difference with Accessor is that a Packed Accessor copies size\nand stride data inside of its structure instead of pointing to it. It allows us\nto pass it to a CUDA kernel function and use its interface inside it.",
                                "markdown"
                            ],
                            [
                                "We can design a function that takes Packed Accessors instead of pointers.",
                                "markdown"
                            ],
                            [
                                "__global__ void lltm_cuda_forward_kernel(\n    const torch::PackedTensorAccessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt; gates,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; old_cell,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; new_h,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; new_cell,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; input_gate,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; output_gate,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; candidate_cell)",
                                "code"
                            ],
                            [
                                "Let\u2019s decompose the template used here. the first two arguments scalar_t and\n2 are the same as regular Accessor. The argument\ntorch::RestrictPtrTraits indicates that the __restrict__ keyword must be\nused. Note also that we\u2019ve used the PackedAccessor32 variant which store the\nsizes and strides in an int32_t. This is important as using the 64-bit\nvariant (PackedAccessor64) can make the kernel slower.",
                                "markdown"
                            ],
                            [
                                "The function declaration becomes",
                                "markdown"
                            ],
                            [
                                "template &lt;typename scalar_t&gt;\n__global__ void lltm_cuda_forward_kernel(\n    const torch::PackedTensorAccessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt; gates,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; old_cell,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; new_h,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; new_cell,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; input_gate,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; output_gate,\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; candidate_cell) {\n  //batch index\n  const int n = blockIdx.y;\n  // column index\n  const int c = blockIdx.x * blockDim.x + threadIdx.x;\n  if (c &lt; gates.size(2)){\n    input_gate[n][c] = sigmoid(gates[n][0][c]);\n    output_gate[n][c] = sigmoid(gates[n][1][c]);\n    candidate_cell[n][c] = elu(gates[n][2][c]);\n    new_cell[n][c] =\n        old_cell[n][c] + candidate_cell[n][c] * input_gate[n][c];\n    new_h[n][c] = tanh(new_cell[n][c]) * output_gate[n][c];\n  }\n}",
                                "code"
                            ],
                            [
                                "The implementation is much more readable! This function is then called by\ncreating Packed Accessors with the .packed_accessor32&lt;&gt; method within the\nhost function.",
                                "markdown"
                            ],
                            [
                                "std::vector&lt;torch::Tensor&gt; lltm_cuda_forward(\n    torch::Tensor input,\n    torch::Tensor weights,\n    torch::Tensor bias,\n    torch::Tensor old_h,\n    torch::Tensor old_cell) {\n  auto X = torch::cat({old_h, input}, /*dim=*/1);\n  auto gate_weights = torch::addmm(bias, X, weights.transpose(0, 1));\n\n  const auto batch_size = old_cell.size(0);\n  const auto state_size = old_cell.size(1);\n\n  auto gates = gate_weights.reshape({batch_size, 3, state_size});\n  auto new_h = torch::zeros_like(old_cell);\n  auto new_cell = torch::zeros_like(old_cell);\n  auto input_gate = torch::zeros_like(old_cell);\n  auto output_gate = torch::zeros_like(old_cell);\n  auto candidate_cell = torch::zeros_like(old_cell);\n\n  const int threads = 1024;\n  const dim3 blocks((state_size + threads - 1) / threads, batch_size);\n\n  AT_DISPATCH_FLOATING_TYPES(gates.type(), \"lltm_forward_cuda\", ([&amp;] {\n    lltm_cuda_forward_kernel&lt;scalar_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(\n        gates.packed_accessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt;(),\n        old_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        new_h.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        new_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        input_gate.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        output_gate.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        candidate_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;());\n  }));\n\n  return {new_h, new_cell, input_gate, output_gate, candidate_cell, X, gates};\n}",
                                "code"
                            ],
                            [
                                "The backwards pass follows much the same pattern and I won\u2019t elaborate further\non it:",
                                "markdown"
                            ],
                            [
                                "template &lt;typename scalar_t&gt;\n__global__ void lltm_cuda_backward_kernel(\n    torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; d_old_cell,\n    torch::PackedTensorAccessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt; d_gates,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; grad_h,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; grad_cell,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; new_cell,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; input_gate,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; output_gate,\n    const torch::PackedTensorAccessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt; candidate_cell,\n    const torch::PackedTensorAccessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt; gate_weights) {\n  //batch index\n  const int n = blockIdx.y;\n  // column index\n  const int c = blockIdx.x * blockDim.x + threadIdx.x;\n  if (c &lt; d_gates.size(2)){\n    const auto d_output_gate = tanh(new_cell[n][c]) * grad_h[n][c];\n    const auto d_tanh_new_cell = output_gate[n][c] * grad_h[n][c];\n    const auto d_new_cell =\n        d_tanh(new_cell[n][c]) * d_tanh_new_cell + grad_cell[n][c];\n\n\n    d_old_cell[n][c] = d_new_cell;\n    const auto d_candidate_cell = input_gate[n][c] * d_new_cell;\n    const auto d_input_gate = candidate_cell[n][c] * d_new_cell;\n\n    d_gates[n][0][c] =\n        d_input_gate * d_sigmoid(gate_weights[n][0][c]);\n    d_gates[n][1][c] =\n        d_output_gate * d_sigmoid(gate_weights[n][1][c]);\n    d_gates[n][2][c] =\n        d_candidate_cell * d_elu(gate_weights[n][2][c]);\n  }\n}\n\nstd::vector&lt;torch::Tensor&gt; lltm_cuda_backward(\n    torch::Tensor grad_h,\n    torch::Tensor grad_cell,\n    torch::Tensor new_cell,\n    torch::Tensor input_gate,\n    torch::Tensor output_gate,\n    torch::Tensor candidate_cell,\n    torch::Tensor X,\n    torch::Tensor gates,\n    torch::Tensor weights) {\n  auto d_old_cell = torch::zeros_like(new_cell);\n  auto d_gates = torch::zeros_like(gates);\n\n  const auto batch_size = new_cell.size(0);\n  const auto state_size = new_cell.size(1);\n\n  const int threads = 1024;\n  const dim3 blocks((state_size + threads - 1) / threads, batch_size);\n\n  AT_DISPATCH_FLOATING_TYPES(X.type(), \"lltm_backward_cuda\", ([&amp;] {\n    lltm_cuda_backward_kernel&lt;scalar_t&gt;&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(\n        d_old_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        d_gates.packed_accessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt;(),\n        grad_h.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        grad_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        new_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        input_gate.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        output_gate.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        candidate_cell.packed_accessor32&lt;scalar_t,2,torch::RestrictPtrTraits&gt;(),\n        gates.packed_accessor32&lt;scalar_t,3,torch::RestrictPtrTraits&gt;());\n  }));\n\n  auto d_gate_weights = d_gates.reshape({batch_size, 3*state_size});\n  auto d_weights = d_gate_weights.t().mm(X);\n  auto d_bias = d_gate_weights.sum(/*dim=*/0, /*keepdim=*/true);\n\n  auto d_X = d_gate_weights.mm(weights);\n  auto d_old_h = d_X.slice(/*dim=*/1, 0, state_size);\n  auto d_input = d_X.slice(/*dim=*/1, state_size);\n\n  return {d_old_h, d_input, d_weights, d_bias, d_old_cell, d_gates};\n}",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Integrating a C++/CUDA Operation with PyTorch": [
                            [
                                "Integration of our CUDA-enabled op with PyTorch is again very straightforward.\nIf you want to write a setup.py script, it could look like this:",
                                "markdown"
                            ],
                            [
                                "from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\nsetup(\n    name='lltm',\n    ext_modules=[\n        CUDAExtension('lltm_cuda', [\n            'lltm_cuda.cpp',\n            'lltm_cuda_kernel.cu',\n        ])\n    ],\n    cmdclass={\n        'build_ext': BuildExtension\n    })",
                                "code"
                            ],
                            [
                                "Instead of CppExtension(), we now use CUDAExtension(). We can just\nspecify the .cu file along with the .cpp files \u2013 the library takes\ncare of all the hassle this entails for you. The JIT mechanism is even\nsimpler:",
                                "markdown"
                            ],
                            [
                                "from torch.utils.cpp_extension import load\n\nlltm = load(name='lltm', sources=['lltm_cuda.cpp', 'lltm_cuda_kernel.cu'])",
                                "code"
                            ],
                            {
                                "Performance Comparison": [
                                    [
                                        "Our hope was that parallelizing and fusing the pointwise operations of our code\nwith CUDA would improve the performance of our LLTM. Let\u2019s see if that holds\ntrue. We can run the code I listed earlier to run a benchmark. Our fastest\nversion earlier was the CUDA-based C++ code:",
                                        "markdown"
                                    ],
                                    [
                                        "Forward: 149.802 us | Backward 393.458 us",
                                        "code"
                                    ],
                                    [
                                        "And now with our custom CUDA kernel:",
                                        "markdown"
                                    ],
                                    [
                                        "Forward: 129.431 us | Backward 304.641 us",
                                        "code"
                                    ],
                                    [
                                        "More performance increases!",
                                        "markdown"
                                    ]
                                ]
                            }
                        ]
                    }
                ]
            },
            {
                "Conclusion": [
                    [
                        "You should now be equipped with a good overview of PyTorch\u2019s C++ extension\nmechanism as well as a motivation for using them. You can find the code\nexamples displayed in this note . If you have questions, please use\n. Also be sure to check our  in case you run into any issues.",
                        "markdown"
                    ]
                ]
            }
        ],
        "Extending TorchScript with Custom C++ Operators": [
            [
                "The PyTorch 1.0 release introduced a new programming model to PyTorch called\n. TorchScript is a\nsubset of the Python programming language which can be parsed, compiled and\noptimized by the TorchScript compiler. Further, compiled TorchScript models have\nthe option of being serialized into an on-disk file format, which you can\nsubsequently load and run from pure C++ (as well as Python) for inference.",
                "markdown"
            ],
            [
                "TorchScript supports a large subset of operations provided by the torch\npackage, allowing you to express many kinds of complex models purely as a series\nof tensor operations from PyTorch\u2019s \u201cstandard library\u201d. Nevertheless, there may\nbe times where you find yourself in need of extending TorchScript with a custom\nC++ or CUDA function. While we recommend that you only resort to this option if\nyour idea cannot be expressed (efficiently enough) as a simple Python function,\nwe do provide a very friendly and simple interface for defining custom C++ and\nCUDA kernels using , PyTorch\u2019s high\nperformance C++ tensor library. Once bound into TorchScript, you can embed these\ncustom kernels (or \u201cops\u201d) into your TorchScript model and execute them both in\nPython and in their serialized form directly in C++.",
                "markdown"
            ],
            [
                "The following paragraphs give an example of writing a TorchScript custom op to\ncall into , a computer vision library written\nin C++. We will discuss how to work with tensors in C++, how to efficiently\nconvert them to third party tensor formats (in this case, OpenCV Mat), how\nto register your operator with the TorchScript runtime and finally how to\ncompile the operator and use it in Python and C++.",
                "markdown"
            ],
            {
                "Implementing the Custom Operator in C++": [
                    [
                        "For this tutorial, we\u2019ll be exposing the \nfunction, which applies a perspective transformation to an image, from OpenCV to\nTorchScript as a custom operator. The first step is to write the implementation\nof our custom operator in C++. Let\u2019s call the file for this implementation\nop.cpp and make it look like this:",
                        "markdown"
                    ],
                    [
                        "torch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) {\n  // BEGIN image_mat\n  cv::Mat image_mat(/*rows=*/image.size(0),\n                    /*cols=*/image.size(1),\n                    /*type=*/CV_32FC1,\n                    /*data=*/image.data_ptr&lt;float&gt;());\n  // END image_mat\n\n  // BEGIN warp_mat\n  cv::Mat warp_mat(/*rows=*/warp.size(0),\n                   /*cols=*/warp.size(1),\n                   /*type=*/CV_32FC1,\n                   /*data=*/warp.data_ptr&lt;float&gt;());\n  // END warp_mat\n\n  // BEGIN output_mat\n  cv::Mat output_mat;\n  cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{8, 8});\n  // END output_mat\n\n  // BEGIN output_tensor\n  torch::Tensor output = torch::from_blob(output_mat.ptr&lt;float&gt;(), /*sizes=*/{8, 8});\n  return output.clone();\n  // END output_tensor\n}",
                        "code"
                    ],
                    [
                        "The code for this operator is quite short. At the top of the file, we include\nthe OpenCV header file, opencv2/opencv.hpp, alongside the torch/script.h\nheader which exposes all the necessary goodies from PyTorch\u2019s C++ API that we\nneed to write custom TorchScript operators. Our function warp_perspective\ntakes two arguments: an input image and the warp transformation matrix\nwe wish to apply to the image. The type of these inputs is torch::Tensor,\nPyTorch\u2019s tensor type in C++ (which is also the underlying type of all tensors\nin Python). The return type of our warp_perspective function will also be a\ntorch::Tensor.",
                        "markdown"
                    ],
                    [
                        "Tip",
                        "markdown"
                    ],
                    [
                        "See  for\nmore information about ATen, the library that provides the Tensor class to\nPyTorch. Further,  describes how to\nallocate and initialize new tensor objects in C++ (not required for this\noperator).",
                        "markdown"
                    ],
                    [
                        "Attention",
                        "markdown"
                    ],
                    [
                        "The TorchScript compiler understands a fixed number of types. Only these types\ncan be used as arguments to your custom operator. Currently these types are:\ntorch::Tensor, torch::Scalar, double, int64_t and\nstd::vector s of these types. Note that <em>only</em> double and <em>not</em>\nfloat, and <em>only</em> int64_t and <em>not</em> other integral types such as\nint, short or long are supported.",
                        "markdown"
                    ],
                    [
                        "Inside of our function, the first thing we need to do is convert our PyTorch\ntensors to OpenCV matrices, as OpenCV\u2019s warpPerspective expects cv::Mat\nobjects as inputs. Fortunately, there is a way to do this <strong>without copying\nany</strong> data. In the first few lines,",
                        "markdown"
                    ],
                    [
                        "  cv::Mat image_mat(/*rows=*/image.size(0),\n                    /*cols=*/image.size(1),\n                    /*type=*/CV_32FC1,\n                    /*data=*/image.data_ptr&lt;float&gt;());",
                        "code"
                    ],
                    [
                        "we are calling \nof the OpenCV Mat class to convert our tensor to a Mat object. We pass\nit the number of rows and columns of the original image tensor, the datatype\n(which we\u2019ll fix as float32 for this example), and finally a raw pointer to\nthe underlying data \u2013 a float*. What is special about this constructor of\nthe Mat class is that it does not copy the input data. Instead, it will\nsimply reference this memory for all operations performed on the Mat. If an\nin-place operation is performed on the image_mat, this will be reflected in\nthe original image tensor (and vice-versa). This allows us to call\nsubsequent OpenCV routines with the library\u2019s native matrix type, even though\nwe\u2019re actually storing the data in a PyTorch tensor. We repeat this procedure to\nconvert the warp PyTorch tensor to the warp_mat OpenCV matrix:",
                        "markdown"
                    ],
                    [
                        "  cv::Mat warp_mat(/*rows=*/warp.size(0),\n                   /*cols=*/warp.size(1),\n                   /*type=*/CV_32FC1,\n                   /*data=*/warp.data_ptr&lt;float&gt;());",
                        "code"
                    ],
                    [
                        "Next, we are ready to call the OpenCV function we were so eager to use in\nTorchScript: warpPerspective. For this, we pass the OpenCV function the\nimage_mat and warp_mat matrices, as well as an empty output matrix\ncalled output_mat. We also specify the size dsize we want the output\nmatrix (image) to be. It is hardcoded to 8 x 8 for this example:",
                        "markdown"
                    ],
                    [
                        "  cv::Mat output_mat;\n  cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{8, 8});",
                        "code"
                    ],
                    [
                        "The final step in our custom operator implementation is to convert the\noutput_mat back into a PyTorch tensor, so that we can further use it in\nPyTorch. This is strikingly similar to what we did earlier to convert in the\nother direction. In this case, PyTorch provides a torch::from_blob method. A\n<em>blob</em> in this case is intended to mean some opaque, flat pointer to memory that\nwe want to interpret as a PyTorch tensor. The call to torch::from_blob looks\nlike this:",
                        "markdown"
                    ],
                    [
                        "  torch::Tensor output = torch::from_blob(output_mat.ptr&lt;float&gt;(), /*sizes=*/{8, 8});\n  return output.clone();",
                        "code"
                    ],
                    [
                        "We use the .ptr&lt;float&gt;() method on the OpenCV Mat class to get a raw\npointer to the underlying data (just like .data_ptr&lt;float&gt;() for the PyTorch\ntensor earlier). We also specify the output shape of the tensor, which we\nhardcoded as 8 x 8. The output of torch::from_blob is then a\ntorch::Tensor, pointing to the memory owned by the OpenCV matrix.",
                        "markdown"
                    ],
                    [
                        "Before returning this tensor from our operator implementation, we must call\n.clone() on the tensor to perform a memory copy of the underlying data. The\nreason for this is that torch::from_blob returns a tensor that does not own\nits data. At that point, the data is still owned by the OpenCV matrix. However,\nthis OpenCV matrix will go out of scope and be deallocated at the end of the\nfunction. If we returned the output tensor as-is, it would point to invalid\nmemory by the time we use it outside the function. Calling .clone() returns\na new tensor with a copy of the original data that the new tensor owns itself.\nIt is thus safe to return to the outside world.",
                        "markdown"
                    ]
                ]
            },
            {
                "Registering the Custom Operator with TorchScript": [
                    [
                        "Now that have implemented our custom operator in C++, we need to <em>register</em> it\nwith the TorchScript runtime and compiler. This will allow the TorchScript\ncompiler to resolve references to our custom operator in TorchScript code.\nIf you have ever used the pybind11 library, our syntax for registration\nresembles the pybind11 syntax very closely.  To register a single function,\nwe write:",
                        "markdown"
                    ],
                    [
                        "TORCH_LIBRARY(my_ops, m) {\n  m.def(\"warp_perspective\", warp_perspective);\n}",
                        "code"
                    ],
                    [
                        "somewhere at the top level of our op.cpp file.  The TORCH_LIBRARY macro\ncreates a function that will be called when your program starts.  The name\nof your library (my_ops) is given as the first argument (it should not\nbe in quotes).  The second argument (m) defines a variable of type\ntorch::Library which is the main interface to register your operators.\nThe method Library::def actually creates an operator named warp_perspective,\nexposing it to both Python and TorchScript.  You can define as many operators\nas you like by making multiple calls to def.",
                        "markdown"
                    ],
                    [
                        "Behinds the scenes, the def function is actually doing quite a bit of work:\nit is using template metaprogramming to inspect the type signature of your\nfunction and translate it into an operator schema which specifies the operators\ntype within TorchScript\u2019s type system.",
                        "markdown"
                    ]
                ]
            },
            {
                "Building the Custom Operator": [
                    [
                        "Now that we have implemented our custom operator in C++ and written its\nregistration code, it is time to build the operator into a (shared) library that\nwe can load into Python for research and experimentation, or into C++ for\ninference in a no-Python environment. There exist multiple ways to build our\noperator, using either pure CMake, or Python alternatives like setuptools.\nFor brevity, the paragraphs below only discuss the CMake approach. The appendix\nof this tutorial dives into other alternatives.",
                        "markdown"
                    ],
                    {
                        "Environment setup": [
                            [
                                "We need an installation of PyTorch and OpenCV.  The easiest and most platform\nindependent way to get both is to via Conda:",
                                "markdown"
                            ],
                            [
                                "conda install -c pytorch pytorch\nconda install opencv",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Building with CMake": [
                            [
                                "To build our custom operator into a shared library using the  build system, we need to write a short CMakeLists.txt\nfile and place it with our previous op.cpp file. For this, let\u2019s agree on a\na directory structure that looks like this:",
                                "markdown"
                            ],
                            [
                                "warp-perspective/\n  op.cpp\n  CMakeLists.txt",
                                "code"
                            ],
                            [
                                "The contents of our CMakeLists.txt file should then be the following:",
                                "markdown"
                            ],
                            [
                                "cmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(warp_perspective)\n\nfind_package(Torch REQUIRED)\nfind_package(OpenCV REQUIRED)\n\n# Define our library target\nadd_library(warp_perspective SHARED op.cpp)\n# Enable C++14\ntarget_compile_features(warp_perspective PRIVATE cxx_std_14)\n# Link against LibTorch\ntarget_link_libraries(warp_perspective \"${TORCH_LIBRARIES}\")\n# Link against OpenCV\ntarget_link_libraries(warp_perspective opencv_core opencv_imgproc)",
                                "code"
                            ],
                            [
                                "To now build our operator, we can run the following commands from our\nwarp_perspective folder:",
                                "markdown"
                            ],
                            [
                                "$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /warp_perspective/build\n$ make -j\nScanning dependencies of target warp_perspective\n[ 50%] Building CXX object CMakeFiles/warp_perspective.dir/op.cpp.o\n[100%] Linking CXX shared library libwarp_perspective.so\n[100%] Built target warp_perspective",
                                "code"
                            ],
                            [
                                "which will place a libwarp_perspective.so shared library file in the\nbuild folder. In the cmake command above, we use the helper\nvariable torch.utils.cmake_prefix_path to conveniently tell us where\nthe cmake files for our PyTorch install are.",
                                "markdown"
                            ],
                            [
                                "We will explore how to use and call our operator in detail further below, but to\nget an early sensation of success, we can try running the following code in\nPython:",
                                "markdown"
                            ],
                            [
                                "import torch\ntorch.ops.load_library(\"build/libwarp_perspective.so\")\nprint(torch.ops.my_ops.warp_perspective)",
                                "code"
                            ],
                            [
                                "If all goes well, this should print something like:",
                                "markdown"
                            ],
                            [
                                "&lt;built-in method my_ops::warp_perspective of PyCapsule object at 0x7f618fc6fa50&gt;",
                                "code"
                            ],
                            [
                                "which is the Python function we will later use to invoke our custom operator.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Using the TorchScript Custom Operator in Python": [
                    [
                        "Once our custom operator is built into a shared library  we are ready to use\nthis operator in our TorchScript models in Python. There are two parts to this:\nfirst loading the operator into Python, and second using the operator in\nTorchScript code.",
                        "markdown"
                    ],
                    [
                        "You already saw how to import your operator into Python:\ntorch.ops.load_library(). This function takes the path to a shared library\ncontaining custom operators, and loads it into the current process. Loading the\nshared library will also execute the TORCH_LIBRARY block. This will register\nour custom operator with the TorchScript compiler and allow us to use that\noperator in TorchScript code.",
                        "markdown"
                    ],
                    [
                        "You can refer to your loaded operator as torch.ops.&lt;namespace&gt;.&lt;function&gt;,\nwhere &lt;namespace&gt; is the namespace part of your operator name, and\n&lt;function&gt; the function name of your operator. For the operator we wrote\nabove, the namespace was my_ops and the function name warp_perspective,\nwhich means our operator is available as torch.ops.my_ops.warp_perspective.\nWhile this function can be used in scripted or traced TorchScript modules, we\ncan also just use it in vanilla eager PyTorch and pass it regular PyTorch\ntensors:",
                        "markdown"
                    ],
                    [
                        "import torch\ntorch.ops.load_library(\"build/libwarp_perspective.so\")\nprint(torch.ops.my_ops.warp_perspective(torch.randn(32, 32), torch.rand(3, 3)))",
                        "code"
                    ],
                    [
                        "producing:",
                        "markdown"
                    ],
                    [
                        "tensor([[0.0000, 0.3218, 0.4611,  ..., 0.4636, 0.4636, 0.4636],\n      [0.3746, 0.0978, 0.5005,  ..., 0.4636, 0.4636, 0.4636],\n      [0.3245, 0.0169, 0.0000,  ..., 0.4458, 0.4458, 0.4458],\n      ...,\n      [0.1862, 0.1862, 0.1692,  ..., 0.0000, 0.0000, 0.0000],\n      [0.1862, 0.1862, 0.1692,  ..., 0.0000, 0.0000, 0.0000],\n      [0.1862, 0.1862, 0.1692,  ..., 0.0000, 0.0000, 0.0000]])",
                        "code"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "What happens behind the scenes is that the first time you access\ntorch.ops.namespace.function in Python, the TorchScript compiler (in C++\nland) will see if a function namespace::function has been registered, and\nif so, return a Python handle to this function that we can subsequently use to\ncall into our C++ operator implementation from Python. This is one noteworthy\ndifference between TorchScript custom operators and C++ extensions: C++\nextensions are bound manually using pybind11, while TorchScript custom ops are\nbound on the fly by PyTorch itself. Pybind11 gives you more flexibility with\nregards to what types and classes you can bind into Python and is thus\nrecommended for purely eager code, but it is not supported for TorchScript\nops.",
                        "markdown"
                    ],
                    [
                        "From here on, you can use your custom operator in scripted or traced code just\nas you would other functions from the torch package. In fact, \u201cstandard\nlibrary\u201d functions like torch.matmul go through largely the same\nregistration path as custom operators, which makes custom operators really\nfirst-class citizens when it comes to how and where they can be used in\nTorchScript.  (One difference, however, is that standard library functions\nhave custom written Python argument parsing logic that differs from\ntorch.ops argument parsing.)",
                        "markdown"
                    ],
                    {
                        "Using the Custom Operator with Tracing": [
                            [
                                "Let\u2019s start by embedding our operator in a traced function. Recall that for\ntracing, we start with some vanilla Pytorch code:",
                                "markdown"
                            ],
                            [
                                "def compute(x, y, z):\n    return x.matmul(y) + torch.relu(z)",
                                "code"
                            ],
                            [
                                "and then call torch.jit.trace on it. We further pass torch.jit.trace\nsome example inputs, which it will forward to our implementation to record the\nsequence of operations that occur as the inputs flow through it. The result of\nthis is effectively a \u201cfrozen\u201d version of the eager PyTorch program, which the\nTorchScript compiler can further analyze, optimize and serialize:",
                                "markdown"
                            ],
                            [
                                "inputs = [torch.randn(4, 8), torch.randn(8, 5), torch.randn(4, 5)]\ntrace = torch.jit.trace(compute, inputs)\nprint(trace.graph)",
                                "code"
                            ],
                            [
                                "Producing:",
                                "markdown"
                            ],
                            [
                                "graph(%x : Float(4:8, 8:1),\n      %y : Float(8:5, 5:1),\n      %z : Float(4:5, 5:1)):\n  %3 : Float(4:5, 5:1) = aten::matmul(%x, %y) # test.py:10:0\n  %4 : Float(4:5, 5:1) = aten::relu(%z) # test.py:10:0\n  %5 : int = prim::Constant[value=1]() # test.py:10:0\n  %6 : Float(4:5, 5:1) = aten::add(%3, %4, %5) # test.py:10:0\n  return (%6)",
                                "code"
                            ],
                            [
                                "Now, the exciting revelation is that we can simply drop our custom operator into\nour PyTorch trace as if it were torch.relu or any other torch function:",
                                "markdown"
                            ],
                            [
                                "def compute(x, y, z):\n    x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))\n    return x.matmul(y) + torch.relu(z)",
                                "code"
                            ],
                            [
                                "and then trace it as before:",
                                "markdown"
                            ],
                            [
                                "inputs = [torch.randn(4, 8), torch.randn(8, 5), torch.randn(8, 5)]\ntrace = torch.jit.trace(compute, inputs)\nprint(trace.graph)",
                                "code"
                            ],
                            [
                                "Producing:",
                                "markdown"
                            ],
                            [
                                "graph(%x.1 : Float(4:8, 8:1),\n      %y : Float(8:5, 5:1),\n      %z : Float(8:5, 5:1)):\n  %3 : int = prim::Constant[value=3]() # test.py:25:0\n  %4 : int = prim::Constant[value=6]() # test.py:25:0\n  %5 : int = prim::Constant[value=0]() # test.py:25:0\n  %6 : Device = prim::Constant[value=\"cpu\"]() # test.py:25:0\n  %7 : bool = prim::Constant[value=0]() # test.py:25:0\n  %8 : Float(3:3, 3:1) = aten::eye(%3, %4, %5, %6, %7) # test.py:25:0\n  %x : Float(8:8, 8:1) = my_ops::warp_perspective(%x.1, %8) # test.py:25:0\n  %10 : Float(8:5, 5:1) = aten::matmul(%x, %y) # test.py:26:0\n  %11 : Float(8:5, 5:1) = aten::relu(%z) # test.py:26:0\n  %12 : int = prim::Constant[value=1]() # test.py:26:0\n  %13 : Float(8:5, 5:1) = aten::add(%10, %11, %12) # test.py:26:0\n  return (%13)",
                                "code"
                            ],
                            [
                                "Integrating TorchScript custom ops into traced PyTorch code is as easy as this!",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Using the Custom Operator with Script": [
                            [
                                "Besides tracing, another way to arrive at a TorchScript representation of a\nPyTorch program is to directly write your code <em>in</em> TorchScript. TorchScript is\nlargely a subset of the Python language, with some restrictions that make it\neasier for the TorchScript compiler to reason about programs. You turn your\nregular PyTorch code into TorchScript by annotating it with\n@torch.jit.script for free functions and @torch.jit.script_method for\nmethods in a class (which must also derive from torch.jit.ScriptModule). See\n for more details on\nTorchScript annotations.",
                                "markdown"
                            ],
                            [
                                "One particular reason to use TorchScript instead of tracing is that tracing is\nunable to capture control flow in PyTorch code. As such, let us consider this\nfunction which does use control flow:",
                                "markdown"
                            ],
                            [
                                "def compute(x, y):\n  if bool(x[0][0] == 42):\n      z = 5\n  else:\n      z = 10\n  return x.matmul(y) + z",
                                "code"
                            ],
                            [
                                "To convert this function from vanilla PyTorch to TorchScript, we annotate it\nwith @torch.jit.script:",
                                "markdown"
                            ],
                            [
                                "@torch.jit.script\ndef compute(x, y):\n  if bool(x[0][0] == 42):\n      z = 5\n  else:\n      z = 10\n  return x.matmul(y) + z",
                                "code"
                            ],
                            [
                                "This will just-in-time compile the compute function into a graph\nrepresentation, which we can inspect in the compute.graph property:",
                                "markdown"
                            ],
                            [
                                "&gt;&gt;&gt; compute.graph\ngraph(%x : Dynamic\n    %y : Dynamic) {\n  %14 : int = prim::Constant[value=1]()\n  %2 : int = prim::Constant[value=0]()\n  %7 : int = prim::Constant[value=42]()\n  %z.1 : int = prim::Constant[value=5]()\n  %z.2 : int = prim::Constant[value=10]()\n  %4 : Dynamic = aten::select(%x, %2, %2)\n  %6 : Dynamic = aten::select(%4, %2, %2)\n  %8 : Dynamic = aten::eq(%6, %7)\n  %9 : bool = prim::TensorToBool(%8)\n  %z : int = prim::If(%9)\n    block0() {\n      -&gt; (%z.1)\n    }\n    block1() {\n      -&gt; (%z.2)\n    }\n  %13 : Dynamic = aten::matmul(%x, %y)\n  %15 : Dynamic = aten::add(%13, %z, %14)\n  return (%15);\n}",
                                "code"
                            ],
                            [
                                "And now, just like before, we can use our custom operator like any other\nfunction inside of our script code:",
                                "markdown"
                            ],
                            [
                                "torch.ops.load_library(\"libwarp_perspective.so\")\n\n@torch.jit.script\ndef compute(x, y):\n  if bool(x[0] == 42):\n      z = 5\n  else:\n      z = 10\n  x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))\n  return x.matmul(y) + z",
                                "code"
                            ],
                            [
                                "When the TorchScript compiler sees the reference to\ntorch.ops.my_ops.warp_perspective, it will find the implementation we\nregistered via the TORCH_LIBRARY function in C++, and compile it into its\ngraph representation:",
                                "markdown"
                            ],
                            [
                                "&gt;&gt;&gt; compute.graph\ngraph(%x.1 : Dynamic\n    %y : Dynamic) {\n    %20 : int = prim::Constant[value=1]()\n    %16 : int[] = prim::Constant[value=[0, -1]]()\n    %14 : int = prim::Constant[value=6]()\n    %2 : int = prim::Constant[value=0]()\n    %7 : int = prim::Constant[value=42]()\n    %z.1 : int = prim::Constant[value=5]()\n    %z.2 : int = prim::Constant[value=10]()\n    %13 : int = prim::Constant[value=3]()\n    %4 : Dynamic = aten::select(%x.1, %2, %2)\n    %6 : Dynamic = aten::select(%4, %2, %2)\n    %8 : Dynamic = aten::eq(%6, %7)\n    %9 : bool = prim::TensorToBool(%8)\n    %z : int = prim::If(%9)\n      block0() {\n        -&gt; (%z.1)\n      }\n      block1() {\n        -&gt; (%z.2)\n      }\n    %17 : Dynamic = aten::eye(%13, %14, %2, %16)\n    %x : Dynamic = my_ops::warp_perspective(%x.1, %17)\n    %19 : Dynamic = aten::matmul(%x, %y)\n    %21 : Dynamic = aten::add(%19, %z, %20)\n    return (%21);\n  }",
                                "code"
                            ],
                            [
                                "Notice in particular the reference to my_ops::warp_perspective at the end of\nthe graph.",
                                "markdown"
                            ],
                            [
                                "Attention",
                                "markdown"
                            ],
                            [
                                "The TorchScript graph representation is still subject to change. Do not rely\non it looking like this.",
                                "markdown"
                            ],
                            [
                                "And that\u2019s really it when it comes to using our custom operator in Python. In\nshort, you import the library containing your operator(s) using\ntorch.ops.load_library, and call your custom op like any other torch\noperator from your traced or scripted TorchScript code.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Using the TorchScript Custom Operator in C++": [
                    [
                        "One useful feature of TorchScript is the ability to serialize a model into an\non-disk file. This file can be sent over the wire, stored in a file system or,\nmore importantly, be dynamically deserialized and executed without needing to\nkeep the original source code around. This is possible in Python, but also in\nC++. For this, PyTorch provides \nfor deserializing as well as executing TorchScript models. If you haven\u2019t yet,\nplease read , on which the\nnext few paragraphs will build.",
                        "markdown"
                    ],
                    [
                        "In short, custom operators can be executed just like regular torch operators\neven when deserialized from a file and run in C++. The only requirement for this\nis to link the custom operator shared library we built earlier with the C++\napplication in which we execute the model. In Python, this worked simply calling\ntorch.ops.load_library. In C++, you need to link the shared library with\nyour main application in whatever build system you are using. The following\nexample will showcase this using CMake.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "Technically, you can also dynamically load the shared library into your C++\napplication at runtime in much the same way we did it in Python. On Linux,\n. There exist\nequivalents on other platforms.",
                        "markdown"
                    ],
                    [
                        "Building on the C++ execution tutorial linked above, let\u2019s start with a minimal\nC++ application in one file, main.cpp in a different folder from our\ncustom operator, that loads and executes a serialized TorchScript model:",
                        "markdown"
                    ],
                    [
                        "#include &lt;torch/script.h&gt; // One-stop header.\n\n#include &lt;iostream&gt;\n#include &lt;memory&gt;\n\n\nint main(int argc, const char* argv[]) {\n  if (argc != 2) {\n    std::cerr &lt;&lt; \"usage: example-app &lt;path-to-exported-script-module&gt;\\n\";\n    return -1;\n  }\n\n  // Deserialize the ScriptModule from a file using torch::jit::load().\n  torch::jit::script::Module module = torch::jit::load(argv[1]);\n\n  std::vector&lt;torch::jit::IValue&gt; inputs;\n  inputs.push_back(torch::randn({4, 8}));\n  inputs.push_back(torch::randn({8, 5}));\n\n  torch::Tensor output = module.forward(std::move(inputs)).toTensor();\n\n  std::cout &lt;&lt; output &lt;&lt; std::endl;\n}",
                        "code"
                    ],
                    [
                        "Along with a small CMakeLists.txt file:",
                        "markdown"
                    ],
                    [
                        "cmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(example_app)\n\nfind_package(Torch REQUIRED)\n\nadd_executable(example_app main.cpp)\ntarget_link_libraries(example_app \"${TORCH_LIBRARIES}\")\ntarget_compile_features(example_app PRIVATE cxx_range_for)",
                        "code"
                    ],
                    [
                        "At this point, we should be able to build the application:",
                        "markdown"
                    ],
                    [
                        "$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /example_app/build\n$ make -j\nScanning dependencies of target example_app\n[ 50%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o\n[100%] Linking CXX executable example_app\n[100%] Built target example_app",
                        "code"
                    ],
                    [
                        "And run it without passing a model just yet:",
                        "markdown"
                    ],
                    [
                        "$ ./example_app\nusage: example_app &lt;path-to-exported-script-module&gt;",
                        "code"
                    ],
                    [
                        "Next, let\u2019s serialize the script function we wrote earlier that uses our custom\noperator:",
                        "markdown"
                    ],
                    [
                        "torch.ops.load_library(\"libwarp_perspective.so\")\n\n@torch.jit.script\ndef compute(x, y):\n  if bool(x[0][0] == 42):\n      z = 5\n  else:\n      z = 10\n  x = torch.ops.my_ops.warp_perspective(x, torch.eye(3))\n  return x.matmul(y) + z\n\ncompute.save(\"example.pt\")",
                        "code"
                    ],
                    [
                        "The last line will serialize the script function into a file called\n\u201cexample.pt\u201d. If we then pass this serialized model to our C++ application, we\ncan run it straight away:",
                        "markdown"
                    ],
                    [
                        "$ ./example_app example.pt\nterminate called after throwing an instance of 'torch::jit::script::ErrorReport'\nwhat():\nSchema not found for node. File a bug report.\nNode: %16 : Dynamic = my_ops::warp_perspective(%0, %19)",
                        "code"
                    ],
                    [
                        "Or maybe not. Maybe not just yet. Of course! We haven\u2019t linked the custom\noperator library with our application yet. Let\u2019s do this right now, and to do it\nproperly let\u2019s update our file organization slightly, to look like this:",
                        "markdown"
                    ],
                    [
                        "example_app/\n  CMakeLists.txt\n  main.cpp\n  warp_perspective/\n    CMakeLists.txt\n    op.cpp",
                        "code"
                    ],
                    [
                        "This will allow us to add the warp_perspective library CMake target as a\nsubdirectory of our application target. The top level CMakeLists.txt in the\nexample_app folder should look like this:",
                        "markdown"
                    ],
                    [
                        "cmake_minimum_required(VERSION 3.1 FATAL_ERROR)\nproject(example_app)\n\nfind_package(Torch REQUIRED)\n\nadd_subdirectory(warp_perspective)\n\nadd_executable(example_app main.cpp)\ntarget_link_libraries(example_app \"${TORCH_LIBRARIES}\")\ntarget_link_libraries(example_app -Wl,--no-as-needed warp_perspective)\ntarget_compile_features(example_app PRIVATE cxx_range_for)",
                        "code"
                    ],
                    [
                        "This basic CMake configuration looks much like before, except that we add the\nwarp_perspective CMake build as a subdirectory. Once its CMake code runs, we\nlink our example_app application with the warp_perspective shared\nlibrary.",
                        "markdown"
                    ],
                    [
                        "Attention",
                        "markdown"
                    ],
                    [
                        "There is one crucial detail embedded in the above example: The\n-Wl,--no-as-needed prefix to the warp_perspective link line. This is\nrequired because we will not actually be calling any function from the\nwarp_perspective shared library in our application code. We only need the\nTORCH_LIBRARY function to run. Inconveniently, this\nconfuses the linker and makes it think it can just skip linking against the\nlibrary altogether. On Linux, the -Wl,--no-as-needed flag forces the link\nto happen (NB: this flag is specific to Linux!). There are other workarounds\nfor this. The simplest is to define <em>some function</em> in the operator library\nthat you need to call from the main application. This could be as simple as a\nfunction void init(); declared in some header, which is then defined as\nvoid init() { } in the operator library. Calling this init() function\nin the main application will give the linker the impression that this is a\nlibrary worth linking against. Unfortunately, this is outside of our control,\nand we would rather let you know the reason and the simple workaround for this\nthan handing you some opaque macro to plop in your code.",
                        "markdown"
                    ],
                    [
                        "Now, since we find the Torch package at the top level now, the\nCMakeLists.txt file in the  warp_perspective subdirectory can be\nshortened a bit. It should look like this:",
                        "markdown"
                    ],
                    [
                        "find_package(OpenCV REQUIRED)\nadd_library(warp_perspective SHARED op.cpp)\ntarget_compile_features(warp_perspective PRIVATE cxx_range_for)\ntarget_link_libraries(warp_perspective PRIVATE \"${TORCH_LIBRARIES}\")\ntarget_link_libraries(warp_perspective PRIVATE opencv_core opencv_photo)",
                        "code"
                    ],
                    [
                        "Let\u2019s re-build our example app, which will also link with the custom operator\nlibrary. In the top level example_app directory:",
                        "markdown"
                    ],
                    [
                        "$ mkdir build\n$ cd build\n$ cmake -DCMAKE_PREFIX_PATH=\"$(python -c 'import torch.utils; print(torch.utils.cmake_prefix_path)')\" ..\n-- The C compiler identification is GNU 5.4.0\n-- The CXX compiler identification is GNU 5.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found torch: /libtorch/lib/libtorch.so\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /warp_perspective/example_app/build\n$ make -j\nScanning dependencies of target warp_perspective\n[ 25%] Building CXX object warp_perspective/CMakeFiles/warp_perspective.dir/op.cpp.o\n[ 50%] Linking CXX shared library libwarp_perspective.so\n[ 50%] Built target warp_perspective\nScanning dependencies of target example_app\n[ 75%] Building CXX object CMakeFiles/example_app.dir/main.cpp.o\n[100%] Linking CXX executable example_app\n[100%] Built target example_app",
                        "code"
                    ],
                    [
                        "If we now run the example_app binary and hand it our serialized model, we\nshould arrive at a happy ending:",
                        "markdown"
                    ],
                    [
                        "$ ./example_app example.pt\n11.4125   5.8262   9.5345   8.6111  12.3997\n 7.4683  13.5969   9.0850  11.0698   9.4008\n 7.4597  15.0926  12.5727   8.9319   9.0666\n 9.4834  11.1747   9.0162  10.9521   8.6269\n10.0000  10.0000  10.0000  10.0000  10.0000\n10.0000  10.0000  10.0000  10.0000  10.0000\n10.0000  10.0000  10.0000  10.0000  10.0000\n10.0000  10.0000  10.0000  10.0000  10.0000\n[ Variable[CPUFloatType]{8,5} ]",
                        "code"
                    ],
                    [
                        "Success! You are now ready to inference away.",
                        "markdown"
                    ]
                ]
            },
            {
                "Conclusion": [
                    [
                        "This tutorial walked you throw how to implement a custom TorchScript operator in\nC++, how to build it into a shared library, how to use it in Python to define\nTorchScript models and lastly how to load it into a C++ application for\ninference workloads. You are now ready to extend your TorchScript models with\nC++ operators that interface with third party C++ libraries, write custom high\nperformance CUDA kernels, or implement any other use case that requires the\nlines between Python, TorchScript and C++ to blend smoothly.",
                        "markdown"
                    ],
                    [
                        "As always, if you run into any problems or have questions, you can use our\n or  to get in touch. Also, our\n may have helpful information.",
                        "markdown"
                    ]
                ]
            },
            {
                "Appendix A: More Ways of Building Custom Operators": [
                    [
                        "The section \u201cBuilding the Custom Operator\u201d explained how to build a custom\noperator into a shared library using CMake. This appendix outlines two further\napproaches for compilation. Both of them use Python as the \u201cdriver\u201d or\n\u201cinterface\u201d to the compilation process. Also, both re-use the  PyTorch\nprovides for , which are the\nvanilla (eager) PyTorch equivalent of TorchScript custom operators that rely on\n for \u201cexplicit\u201d binding of\nfunctions from C++ into Python.",
                        "markdown"
                    ],
                    [
                        "The first approach uses C++ extensions\u2019 \nto compile your code in the background of your PyTorch script the first time you\nrun it. The second approach relies on the venerable setuptools package and\ninvolves writing a separate setup.py file. This allows more advanced\nconfiguration as well as integration with other setuptools-based projects.\nWe will explore both approaches in detail below.",
                        "markdown"
                    ],
                    {
                        "Building with JIT compilation": [
                            [
                                "The JIT compilation feature provided by the PyTorch C++ extension toolkit allows\nembedding the compilation of your custom operator directly into your Python\ncode, e.g. at the top of your training script.",
                                "markdown"
                            ],
                            [
                                "Note",
                                "markdown"
                            ],
                            [
                                "\u201cJIT compilation\u201d here has nothing to do with the JIT compilation taking place\nin the TorchScript compiler to optimize your program. It simply means that\nyour custom operator C++ code will be compiled in a folder under your system\u2019s\n<cite>/tmp</cite> directory the first time you import it, as if you had compiled it\nyourself beforehand.",
                                "markdown"
                            ],
                            [
                                "This JIT compilation feature comes in two flavors. In the first, you still keep\nyour operator implementation in a separate file (op.cpp), and then use\ntorch.utils.cpp_extension.load() to compile your extension. Usually, this\nfunction will return the Python module exposing your C++ extension. However,\nsince we are not compiling our custom operator into its own Python module, we\nonly want to compile a plain shared library . Fortunately,\ntorch.utils.cpp_extension.load() has an argument is_python_module which\nwe can set to False to indicate that we are only interested in building a\nshared library and not a Python module. torch.utils.cpp_extension.load()\nwill then compile and also load the shared library into the current process,\njust like torch.ops.load_library did before:",
                                "markdown"
                            ],
                            [
                                "import torch.utils.cpp_extension\n\ntorch.utils.cpp_extension.load(\n    name=\"warp_perspective\",\n    sources=[\"op.cpp\"],\n    extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"],\n    is_python_module=False,\n    verbose=True\n)\n\nprint(torch.ops.my_ops.warp_perspective)",
                                "code"
                            ],
                            [
                                "This should approximately print:",
                                "markdown"
                            ],
                            [
                                "&lt;built-in method my_ops::warp_perspective of PyCapsule object at 0x7f3e0f840b10&gt;",
                                "code"
                            ],
                            [
                                "The second flavor of JIT compilation allows you to pass the source code for your\ncustom TorchScript operator as a string. For this, use\ntorch.utils.cpp_extension.load_inline:",
                                "markdown"
                            ],
                            [
                                "import torch\nimport torch.utils.cpp_extension\n\nop_source = \"\"\"\n#include &lt;opencv2/opencv.hpp&gt;\n#include &lt;torch/script.h&gt;\n\ntorch::Tensor warp_perspective(torch::Tensor image, torch::Tensor warp) {\n  cv::Mat image_mat(/*rows=*/image.size(0),\n                    /*cols=*/image.size(1),\n                    /*type=*/CV_32FC1,\n                    /*data=*/image.data&lt;float&gt;());\n  cv::Mat warp_mat(/*rows=*/warp.size(0),\n                   /*cols=*/warp.size(1),\n                   /*type=*/CV_32FC1,\n                   /*data=*/warp.data&lt;float&gt;());\n\n  cv::Mat output_mat;\n  cv::warpPerspective(image_mat, output_mat, warp_mat, /*dsize=*/{64, 64});\n\n  torch::Tensor output =\n    torch::from_blob(output_mat.ptr&lt;float&gt;(), /*sizes=*/{64, 64});\n  return output.clone();\n}\n\nTORCH_LIBRARY(my_ops, m) {\n  m.def(\"warp_perspective\", &amp;warp_perspective);\n}\n\"\"\"\n\ntorch.utils.cpp_extension.load_inline(\n    name=\"warp_perspective\",\n    cpp_sources=op_source,\n    extra_ldflags=[\"-lopencv_core\", \"-lopencv_imgproc\"],\n    is_python_module=False,\n    verbose=True,\n)\n\nprint(torch.ops.my_ops.warp_perspective)",
                                "code"
                            ],
                            [
                                "Naturally, it is best practice to only use\ntorch.utils.cpp_extension.load_inline if your source code is reasonably\nshort.",
                                "markdown"
                            ],
                            [
                                "Note that if you\u2019re using this in a Jupyter Notebook, you should not execute\nthe cell with the registration multiple times because each execution registers\na new library and re-registers the custom operator. If you need to re-execute it,\nplease restart the Python kernel of your notebook beforehand.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Building with Setuptools": [
                            [
                                "The second approach to building our custom operator exclusively from Python is\nto use setuptools. This has the advantage that setuptools has a quite\npowerful and extensive interface for building Python modules written in C++.\nHowever, since setuptools is really intended for building Python modules and\nnot plain shared libraries (which do not have the necessary entry points Python\nexpects from a module), this route can be slightly quirky. That said, all you\nneed is a setup.py file in place of the CMakeLists.txt which looks like\nthis:",
                                "markdown"
                            ],
                            [
                                "from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension\n\nsetup(\n    name=\"warp_perspective\",\n    ext_modules=[\n        CppExtension(\n            \"warp_perspective\",\n            [\"example_app/warp_perspective/op.cpp\"],\n            libraries=[\"opencv_core\", \"opencv_imgproc\"],\n        )\n    ],\n    cmdclass={\"build_ext\": BuildExtension.with_options(no_python_abi_suffix=True)},\n)",
                                "code"
                            ],
                            [
                                "Notice that we enabled the no_python_abi_suffix option in the\nBuildExtension at the bottom. This instructs setuptools to omit any\nPython-3 specific ABI suffixes in the name of the produced shared library.\nOtherwise, on Python 3.7 for example, the library may be called\nwarp_perspective.cpython-37m-x86_64-linux-gnu.so where\ncpython-37m-x86_64-linux-gnu is the ABI tag, but we really just want it to\nbe called warp_perspective.so",
                                "markdown"
                            ],
                            [
                                "If we now run python setup.py build develop in a terminal from within the\nfolder in which setup.py is situated, we should see something like:",
                                "markdown"
                            ],
                            [
                                "$ python setup.py build develop\nrunning build\nrunning build_ext\nbuilding 'warp_perspective' extension\ncreating build\ncreating build/temp.linux-x86_64-3.7\ngcc -pthread -B /root/local/miniconda/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/torch/csrc/api/include -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/TH -I/root/local/miniconda/lib/python3.7/site-packages/torch/lib/include/THC -I/root/local/miniconda/include/python3.7m -c op.cpp -o build/temp.linux-x86_64-3.7/op.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=warp_perspective -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\ncc1plus: warning: command line option \u2018-Wstrict-prototypes\u2019 is valid for C/ObjC but not for C++\ncreating build/lib.linux-x86_64-3.7\ng++ -pthread -shared -B /root/local/miniconda/compiler_compat -L/root/local/miniconda/lib -Wl,-rpath=/root/local/miniconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/op.o -lopencv_core -lopencv_imgproc -o build/lib.linux-x86_64-3.7/warp_perspective.so\nrunning develop\nrunning egg_info\ncreating warp_perspective.egg-info\nwriting warp_perspective.egg-info/PKG-INFO\nwriting dependency_links to warp_perspective.egg-info/dependency_links.txt\nwriting top-level names to warp_perspective.egg-info/top_level.txt\nwriting manifest file 'warp_perspective.egg-info/SOURCES.txt'\nreading manifest file 'warp_perspective.egg-info/SOURCES.txt'\nwriting manifest file 'warp_perspective.egg-info/SOURCES.txt'\nrunning build_ext\ncopying build/lib.linux-x86_64-3.7/warp_perspective.so -&gt;\nCreating /root/local/miniconda/lib/python3.7/site-packages/warp-perspective.egg-link (link to .)\nAdding warp-perspective 0.0.0 to easy-install.pth file\n\nInstalled /warp_perspective\nProcessing dependencies for warp-perspective==0.0.0\nFinished processing dependencies for warp-perspective==0.0.0",
                                "code"
                            ],
                            [
                                "This will produce a shared library called warp_perspective.so, which we can\npass to torch.ops.load_library as we did earlier to make our operator\nvisible to TorchScript:",
                                "markdown"
                            ],
                            [
                                "&gt;&gt;&gt; import torch\n&gt;&gt;&gt; torch.ops.load_library(\"warp_perspective.so\")\n&gt;&gt;&gt; print(torch.ops.my_ops.warp_perspective)\n&lt;built-in method custom::warp_perspective of PyCapsule object at 0x7ff51c5b7bd0&gt;",
                                "code"
                            ]
                        ]
                    }
                ]
            }
        ],
        "Registering a Dispatched Operator in C++": [
            [
                "The dispatcher is an internal component of PyTorch which is responsible for\nfiguring out what code should actually get run when you call a function like\ntorch::add.  This can be nontrivial, because PyTorch operations need\nto handle a lot of cross-cutting concerns that are \u201clayered\u201d on top of one\nof another.  Here is a sampling of some of the things it handles:",
                "markdown"
            ],
            [
                "Switching between the CPU and CUDA implementations of an operator, depending\non the devices of the input tensors.",
                "markdown"
            ],
            [
                "Switching between the autograd and backend implementations of an operator,\ndepending on whether or not autograd handling is necessary.",
                "markdown"
            ],
            [
                "Applying autocasting when necessary for automatic mixed precision.",
                "markdown"
            ],
            [
                "Applying batching rules when an operator is run under a vmap call.",
                "markdown"
            ],
            [
                "Tracing execution of operations, if you are tracing a model for export.",
                "markdown"
            ],
            [
                "If in your  you find yourself\nmanually writing if statements to handle these cases, the dispatcher APIs can\nhelp organize your code.  (Conversely, if your custom operator is very simple\nand is only for CPU inference, you probably don\u2019t need to use the dispatcher,\njust use the basic API.)",
                "markdown"
            ],
            [
                "In this tutorial, we will describe how to structure a custom operator\nregistration to use the dispatcher to organize various components.  We\u2019ll\nassume that you are familiar with how to\n and how to write\na .",
                "markdown"
            ],
            {
                "Defining schema and backend implementations": [
                    [
                        "The general principle behind the dispatcher is that it divides the\nimplementation of an operator into multiple kernels, each of which implements\nfunctionality for a specific <em>dispatch key</em>, e.g. CPU, CUDA.  The dispatcher\ndetermines what the highest priority dispatch key is at the time\nyou call an operator (this is done by looking at both the tensor arguments as\nwell as some thread local state), and transfers control to the kernel for that\ndispatch key.  The end effect is that when you call an operator, we first\nexecute the Autograd kernel, and then we redispatch to the backend kernel\ndepending on the device types of the passed in tensors.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s take a look at the various parts involved in making this\nhappen.  First, we must define the schema for the operator in question.\nUnlike simple pybind11-style operator registration, we don\u2019t actually\nprovide an implementation of our operator at this point; we just\nprovide a schema string specifying the type signature of the operator\nthat all of our other kernels will abide by:",
                        "markdown"
                    ],
                    [
                        "TORCH_LIBRARY(myops, m) {\n  m.def(\"myadd(Tensor self, Tensor other) -&gt; Tensor\");\n}",
                        "code"
                    ],
                    [
                        "Next, we need to actually provide some implementations of this operator.\nFor concreteness, here is a really simple implementation of addition on CPU:",
                        "markdown"
                    ],
                    [
                        "Tensor myadd_cpu(const Tensor&amp; self_, const Tensor&amp; other_) {\n  TORCH_CHECK(self_.sizes() == other_.sizes());\n  TORCH_INTERNAL_ASSERT(self_.device().type() == DeviceType::CPU);\n  TORCH_INTERNAL_ASSERT(other_.device().type() == DeviceType::CPU);\n  Tensor self = self_.contiguous();\n  Tensor other = other_.contiguous();\n  Tensor result = torch::empty(self.sizes(), self.options());\n  const float* self_ptr = self.data_ptr&lt;float&gt;();\n  const float* other_ptr = other.data_ptr&lt;float&gt;();\n  float* result_ptr = result.data_ptr&lt;float&gt;();\n  for (int64_t i = 0; i &lt; result.numel(); i++) {\n    result_ptr[i] = self_ptr[i] + other_ptr[i];\n  }\n  return result;\n}",
                        "code"
                    ],
                    [
                        "We\u2019d like to register this function as an implementation of myops::myadd.\nHowever, the simple way of registering it (def(\"myadd\", myadd_cpu)) would\nregister the kernel to run in all cases, even if the tensor is not a CPU\ntensor!  (Internally, we refer to these as \u201ccatch-all\u201d kernels, since they\ncatch all cases.)  To ensure that myadd_cpu is only run for\nCPU tensors, we can use the TORCH_LIBRARY_IMPL macro:",
                        "markdown"
                    ],
                    [
                        "TORCH_LIBRARY_IMPL(myops, CPU, m) {\n  m.impl(\"myadd\", myadd_cpu);\n}",
                        "code"
                    ],
                    [
                        "The TORCH_LIBRARY_IMPL lets us register implementations for operators on\na specific dispatch key (in this case, CPU).  Each call to impl\nassociates a CPU kernel with the corresponding operator (which we previously\ndefined in the TORCH_LIBRARY block).  If we also have a CUDA implementation myadd_cuda,\nwe can register it in a separate TORCH_LIBRARY_IMPL block:",
                        "markdown"
                    ],
                    [
                        "TORCH_LIBRARY_IMPL(myops, CUDA, m) {\n  m.impl(\"myadd\", myadd_cuda);\n}",
                        "code"
                    ],
                    [
                        "These registrations can be split across files or even across library boundaries; so\nfor example, you could have these two TORCH_LIBRARY_IMPL blocks compiled\ninto a separate myops_cpu and myops_cuda dynamic libraries.  Generally,\nspeaking, the structure of your registrations will look like this:",
                        "markdown"
                    ],
                    [
                        "A single TORCH_LIBRARY that lists every custom operator in your namespace\nin a centralized place.",
                        "markdown"
                    ],
                    [
                        "A TORCH_LIBRARY_IMPL per dispatch key that registers implementations for\nthat key (e.g., CPU or CUDA).  If you like, you can further subdivide\nTORCH_LIBRARY_IMPL blocks into a block per operator. This is convenient\nif you have a separate file per operator implementation, but don\u2019t want to\nexpose the operators in a header; you can just put the registration in the\ncpp file that defines your operator.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "Did you know that you can also write TORCH_LIBRARY_IMPL blocks for existing\ncore operators in PyTorch?  This is how XLA support for PyTorch is\nimplemented: the torch_xla library contains a TORCH_LIBRARY_IMPL\nthat provides implementations for all basic operators on the XLA dispatch\nkey.",
                        "markdown"
                    ]
                ]
            },
            {
                "For operators that do not need autograd": [
                    [
                        "Note: This section only applies to versions of PyTorch &gt;= 1.10.",
                        "markdown"
                    ],
                    [
                        "In the next section, we will discuss how to add autograd support to an operator.\nBut for the ops that do not need autograd support, the following kernel should be\nregistered improve useability and make your op behave like PyTorch\u2019s built-in\noperators.",
                        "markdown"
                    ],
                    [
                        "TORCH_LIBRARY_IMPL(myops, Autograd, m) {\n  m.impl(op, autogradNotImplementedFallback());\n}",
                        "code"
                    ],
                    [
                        "The above lines registers an Autograd kernel that appends a dummy\nNotImplemented node on forward (preserving the require_grad-ness of the inputs).\nOn backward, the NotImplemented node raises an error. This can be helpful\nfor debugging in larger models where previously it can be hard to pin-point\nexactly where the requires_grad-ness is lost during the forward pass.",
                        "markdown"
                    ],
                    {
                        "In-place or view ops": [
                            [
                                "To ensure correctness and best possible performance, if your op mutates an input\nin-place or returns a tensor that aliases with one of the inputs, two additional\nsteps should be taken:",
                                "markdown"
                            ],
                            [
                                "Register an ADInplaceOrView kernel in addition to the Autograd kernel\nabove. This kernel handles the necessary bookkeeping to ensure the correctness\nof in-place or view operations. It is important to note that this ADInplaceOrView\nkernel should only be used with autogradNotImplementedFallback.",
                                "markdown"
                            ],
                            [
                                "TORCH_LIBRARY_IMPL(myops, Autograd, m) {\n  m.impl(op, autogradNotImplementedFallback());\n}\nTORCH_LIBRARY_IMPL(myops, ADInplaceOrView, m) {\n  m.impl(op, autogradNotImplementedInplaceOrViewFallback());\n}",
                                "code"
                            ],
                            [
                                "The Autograd or ADInplaceOrView boxed kernels registered above\nrely on operator schema information in their logi. If your op mutates an input\nin-place or returns a tensor that aliases with one of the inputs it is important to\nensure that your schema properly reflects this. See\n\nfor more information on how to annotate the schema.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Adding autograd support": [
                    [
                        "At this point, we have an operator with both CPU and CUDA implementations.  How\ncan we add autograd support to it?  As you might guess, we will register an\nautograd kernel (similar to what\u2019s described in the  tutorial)!\nHowever, there is a twist: unlike the CPU and CUDA kernels, the autograd kernel\nneeds to <em>redispatch</em>: it needs to call back into the dispatcher to get to\nthe inference kernels, e.g. CPU or CUDA implementations.",
                        "markdown"
                    ],
                    [
                        "Thus, before we write the autograd kernel, let\u2019s write a <em>dispatching function</em>\nwhich calls into the dispatcher to find the right kernel for your operator.\nThis function constitutes the public C++ API for your operators\u2013in fact, all of\nthe tensor functions in PyTorch\u2019s C++ API all call the dispatcher in the same\nway under the hood.  Here\u2019s what the dispatching function looks like:",
                        "markdown"
                    ],
                    [
                        "Tensor myadd(const Tensor&amp; self, const Tensor&amp; other) {\n  static auto op = torch::Dispatcher::singleton()\n    .findSchemaOrThrow(\"myops::myadd\", \"\")\n    .typed&lt;decltype(myadd)&gt;();\n  return op.call(self, other);\n}",
                        "code"
                    ],
                    [
                        "Let\u2019s break it down:",
                        "markdown"
                    ],
                    [
                        "In the first line, we look up a typed operator handle from the dispatcher\ncorresponding to the operator that we are going to dispatch to.\nfindSchemaOrThrow takes two arguments: the (namespace qualified) name\nof the operator, and the overload name of the operator (typically just\nthe empty string).  typed casts the dynamically typed handle into\na statically typed handle (doing a runtime test to make sure you\u2019ve given\nthe correct C++ type), so that we can do a normal C++ call on it.  We\npass it decltype(myadd) since the type of the dispatching function is\nthe same as the type of the underlying kernels registered to the dispatcher.",
                        "markdown"
                    ],
                    [
                        "For performance, this computation is done in a static variable, so that\nwe only need to do the (slow) lookup once.  If you typoed the name of the\noperator you want to call, this lookup will error the first time you call this\nfunction.",
                        "markdown"
                    ],
                    [
                        "In the second line, we simply call the operator handle with all of the\narguments passed into the dispatching function.  This will actually invoke\nthe dispatcher and in the end control will be transferred to whatever kernel\nis appropriate for this call.",
                        "markdown"
                    ],
                    [
                        "With the dispatch function in hand, we can now write the autograd kernel:",
                        "markdown"
                    ],
                    [
                        "class MyAddFunction : public torch::autograd::Function&lt;MyAddFunction&gt; {\n public:\n  static Tensor forward(\n      AutogradContext *ctx, torch::Tensor self, torch::Tensor other) {\n    at::AutoNonVariableTypeMode g;\n    return myadd(self, other);\n  }\n\n  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n    auto grad_output = grad_outputs[0];\n    return {grad_output, grad_output};\n  }\n};\n\nTensor myadd_autograd(const Tensor&amp; self, const Tensor&amp; other) {\n  return MyAddFunction::apply(self, other)[0];\n}",
                        "code"
                    ],
                    [
                        "The autograd function is written as normal using torch::autograd::Function,\nexcept that instead of directly writing the implementation in forward(),\nwe:",
                        "markdown"
                    ],
                    [
                        "Turn off autograd handling with the at::AutoNonVariableTypeMode RAII\nguard, and then",
                        "markdown"
                    ],
                    [
                        "Call the dispatch function myadd to call back into the dispatcher.",
                        "markdown"
                    ],
                    [
                        "Without (1), your calls will infinite loop (and stack overflow), because\nmyadd will send you back to this function (as the highest priority dispatch\nkey would still be autograd.) With (1),\nautograd is excluded from the set of dispatch keys under consideration, and\nwe will go to the next handlers, which will either be CPU and CUDA.",
                        "markdown"
                    ],
                    [
                        "We can now register this function in the same way we registered the CPU/CUDA\nfunctions:",
                        "markdown"
                    ],
                    [
                        "TORCH_LIBRARY_IMPL(myops, Autograd, m) {\n  m.impl(\"myadd\", myadd_autograd);\n}",
                        "code"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "In this example we register the kernel to Autograd, which installs it as the\nautograd kernel for all backends. You can also register optimized kernels for specific\nbackends by using the corresponding backend-specific dispatch key - for example,\nAutogradCPU or AutogradCUDA. To explore these and other dispatch key\noptions in more detail, check out the PythonDispatcher tool provided in\n.",
                        "markdown"
                    ]
                ]
            },
            {
                "Going beyond autograd": [
                    [
                        "In some sense, the dispatcher isn\u2019t doing all that much: all it does is\nimplement a glorified if-statement, along the lines of this:",
                        "markdown"
                    ],
                    [
                        "class MyAddFunction : ... {\npublic:\n  static Tensor forward(\n    AutogradContext *ctx, torch::Tensor self, torch::Tensor other) {\n\n    if (self.device().type() == DeviceType::CPU) {\n      return add_cpu(self, other);\n    } else if (self.device().type() == DeviceType::CUDA) {\n      return add_cuda(self, other);\n    } else {\n      TORCH_CHECK(0, \"Unsupported device \", self.device().type());\n    }\n  }\n  ...\n}",
                        "code"
                    ],
                    [
                        "So why use the dispatcher?  There are a few reasons:",
                        "markdown"
                    ],
                    [
                        "It is decentralized.  You can assemble all of the pieces of an operator\n(CPU, CUDA, Autograd) without having to write a single, centralized\nif statement that refers to all of them.  Importantly, third parties can\nregister extra implementations for other aspects without having to patch the\noriginal definition of an operator.  We\u2019ll talk more about extending the\ndispatcher in .",
                        "markdown"
                    ],
                    [
                        "It supports more dispatch keys than CPU, CUDA and Autograd.  You can\nsee a full list of dispatch keys that are currently implemented\nin PyTorch in c10/core/DispatchKey.h.  These dispatch keys\nimplement a variety of optional functionality for operators, and if you\ndecide you want your custom operator to support this functionality,\nall you have to register a kernel for the appropriate key.",
                        "markdown"
                    ],
                    [
                        "The dispatcher implements support for boxed fallback functions, which\nare functions that can be implemented once and apply to all operators\nin the system.  Boxed fallbacks can be used to provide default behavior\nfor a dispatch key; if you use the dispatcher to implement your operator,\nyou also opt into the fallbacks for all of these operations.",
                        "markdown"
                    ],
                    [
                        "Here are some particular dispatch keys which you may need to define an operator\nfor.",
                        "markdown"
                    ],
                    {
                        "Autocast": [
                            [
                                "The Autocast dispatch key implements support for\n.\nAn autocast wrapper kernel typically casts incoming float16 or float32 CUDA tensors\nto some preferred precision before running the op.\nFor example, matmuls and convolutions on floating-point CUDA tensors usually run faster\nand use less memory in float16 without impairing convergence.\nAutocast wrappers only have an effect in\n.",
                                "markdown"
                            ],
                            [
                                "Here\u2019s an autocast wrapper for a hypothetical custom matmul, along with its registration:",
                                "markdown"
                            ],
                            [
                                "// Autocast-specific helper functions\n#include &lt;ATen/autocast_mode.h&gt;\n\nTensor mymatmul_autocast(const Tensor&amp; self, const Tensor&amp; other) {\n  c10::impl::ExcludeDispatchKeyGuard no_autocast(c10::DispatchKey::Autocast);\n  return mymatmul(at::autocast::cached_cast(at::kHalf, self),\n                  at::autocast::cached_cast(at::kHalf, other));\n}\n\nTORCH_LIBRARY_IMPL(myops, Autocast, m) {\n  m.impl(\"mymatmul\", mymatmul_autocast);\n}",
                                "code"
                            ],
                            [
                                "cached_cast(kHalf, tensor) casts tensor to float16 if tensor is CUDA and float32,\notherwise, it leaves tensor unchanged (c.f. the\n for natively autocasted ops).\nThis ensures if the network calls mymatmul on any mixture of float16 and float32 CUDA tensors,\nmymatmul runs in float16.  Meanwhile, calls to mymatmul with non-CUDA, integer-type, or float64\ninputs are unaffected.  Using cached_cast to follow the native eligibility policy in your own autocast wrapper\nis recommended, but not required.  For example, if you wanted to force float16 execution for all input types,\nyou could return mymatmul(self.half(), other.half()); instead of using cached_cast.",
                                "markdown"
                            ],
                            [
                                "Notice that, like our autograd kernels, we exclude the Autocast key from\ndispatch before redispatching.",
                                "markdown"
                            ],
                            [
                                "By default, if no autocast wrapper is provided,\nwe fallthrough directly to the regular operator implementation (no\nautocasting occurs).  (We didn\u2019t use myadd for this example, since pointwise\naddition doesn\u2019t need autocasting and should just fall through.)",
                                "markdown"
                            ],
                            [
                                "When should an autocast wrapper be registered? Unfortunately, there aren\u2019t\ncut-and-dried rules for an op\u2019s preferred precision.  You can\nget a sense for some native ops\u2019 preferred precisions by looking at the\n.\nGeneral guidance:",
                                "markdown"
                            ],
                            [
                                "Ops that do reductions should probably execute in float32,",
                                "markdown"
                            ],
                            [
                                "Any op that does a convolution or gemm under the hood should\nprobably execute in float16, and",
                                "markdown"
                            ],
                            [
                                "Other ops with multiple floating-point tensor inputs should standardize\nthem to a common precision (unless the implementation supports inputs with different precisions).",
                                "markdown"
                            ],
                            [
                                "If your custom op falls into the third category, the promote_type template\nhelps figure out the widest floating-point type present among input tensors, which is\nthe safest choice for the execution type:",
                                "markdown"
                            ],
                            [
                                "#include &lt;ATen/autocast_mode.h&gt;\n\nTensor my_multiple_input_op_autocast(const Tensor&amp; t0, const Tensor&amp; t1) {\n  c10::impl::ExcludeDispatchKeyGuard no_autocast(c10::DispatchKey::Autocast);\n  // The required at::kHalf argument is an optimistic initial guess.\n  auto exec_type = at::autocast::promote_type(at::kHalf, t0, t1);\n  return my_multiple_input_op(at::autocast::cached_cast(exec_type, t0),\n                              at::autocast::cached_cast(exec_type, t1));\n}",
                                "code"
                            ],
                            [
                                "If your custom op is , you only need to write and register\nan autocast wrapper for the same name onto which the autograd wrapper is registered.\nFor example, if you wanted an autocast wrapper for the myadd function shown\nin the autograd section, all you\u2019d need is",
                                "markdown"
                            ],
                            [
                                "Tensor myadd_autocast(const Tensor&amp; self, const Tensor&amp; other) {\n  c10::impl::ExcludeDispatchKeyGuard no_autocast(c10::DispatchKey::Autocast);\n  return myadd(at::autocast::cached_cast(&lt;desired dtype&gt;, self),\n               at::autocast::cached_cast(&lt;desired dtype&gt;, other));\n}\n\nTORCH_LIBRARY_IMPL(myops, Autocast, m) {\n  m.impl(\"myadd\", myadd_autocast);\n}",
                                "code"
                            ],
                            [
                                "There are no separate gymnastics to make the backward method autocast compatible.\nHowever, the backward method defined in your custom autograd function will run in the same\ndtype as autocast sets for the forward method, so you should choose a &lt;desired dtype&gt;\nsuitable for both your forward and backward methods.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Batched": [
                            [
                                "Batched tensors allow you to write your code in a per-example manner, and then\nhave them be automatically batched when run under a vmap invocation.  The\nAPI for writing batching rules is currently under development, but once it is\nstabilized, you can add support for vmap for your operators by registering\na kernel at the Batched dispatch key.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Tracer": [
                            [
                                "The Tracer dispatch key implements support for recording invocations of operators\ninto a trace when you run torch.jit.trace.  We intend to provide a\nboxed fallback that will implement tracing for arbitrary operations,\nsee  to track\nprogress.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            }
        ],
        "Extending dispatcher for a new backend in C++": [
            [
                "In this tutorial we will walk through all necessary steps to extend the dispatcher to\nadd a new device living outside pytorch/pytorch repo and maintain it to keep in\nsync with native PyTorch devices.  Here we\u2019ll assume that you\u2019re familiar with how\nto  and how to write a\n.",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                "This tutorial touches a lot of internal components inside PyTorch which are being actively improved,\nplease expect changes to APIs if you decide to follow this tutorial.  We\u2019ll keep this tutorial\nup to date with the latest APIs.",
                "markdown"
            ],
            {
                "What\u2019s a new backend?": [
                    [
                        "Adding a new backend to PyTorch requires a lot of developement and maintainence from backend extenders.\nBefore adding a new backend, let\u2019s first consider a few common use cases and recommended solutions for them:",
                        "markdown"
                    ],
                    [
                        "If you have new algorithms for an existing PyTorch operator, send a PR to PyTorch.",
                        "markdown"
                    ],
                    [
                        "If you want to propose a new operator, send a feature request/PR to PyTorch.",
                        "markdown"
                    ],
                    [
                        "If you want to add support for a new device/hardware like Google TPU and customized chips, which often requires using\nhardware-specific API to write kernels, follow this tutorial and add a out-of-tree backend to PyTorch.",
                        "markdown"
                    ],
                    [
                        "If you want to add support for existing operators but with a different Tensor layout/representation\nlike sparse and quantized, which enforces your kernels to be written in a way that\u2019s more efficient\ngiven the layout/representation limitation, follow this tutorial and add a out-of-tree backend to PyTorch.",
                        "markdown"
                    ],
                    [
                        "In this tutorial we\u2019ll mainly focus on adding a new out-of-tree device below.  Adding out-of-tree support\nfor a different tensor layout might share many common steps with devices, but we haven\u2019t seen an example of\nsuch integrations yet so it might require addtional work from PyTorch to support it.",
                        "markdown"
                    ]
                ]
            },
            {
                "Get a dispatch key for your backend": [
                    [
                        "PyTorch operators are implemented in C++ and made available in Python frontend through Python bindings.\nThe PyTorch dispatcher divides the implementation of an operator into multiple kernels, each of which is\nassociated with a specific dispatch key.  Supporting a new backend in PyTorch essentially means writing\na kernel for each PyTorch operator in C++ and then registering them to a dispatch key representing your\ncustomized backend in the dispatcher.",
                        "markdown"
                    ],
                    [
                        "Dispatch key is your identifier in the dispatcher system. The dispatcher looks at the dispatch keys carried on\ninput tensors and calls the right kernel accordingly.  PyTorch provides three reserved dispatch keys\n(and their corresponding Autograd keys) for prototyping out-of-tree backend extensions:",
                        "markdown"
                    ],
                    [
                        "PrivateUse1/AutogradPrivateUse1",
                        "markdown"
                    ],
                    [
                        "PrivateUse2/AutogradPrivateUse2",
                        "markdown"
                    ],
                    [
                        "PrivateUse3/AutogradPrivateUse3",
                        "markdown"
                    ],
                    [
                        "You can choose any of keys above to prototype your customized backend.\nTo create a Tensor on PrivateUse1 backend, you need to set dispatch key in TensorImpl constructor.",
                        "markdown"
                    ],
                    [
                        "/* Example TensorImpl constructor */\nTensorImpl(\n    Storage&amp;&amp; storage,\n    DispatchKeySet ks,\n    const caffe2::TypeMeta data_type);\n\n// To create a TensorImpl on PrivateUse1 backend, pass in the following ks to TensorImpl creation.\nDispatchKeySet ks = c10::DispatchKeySet{c10::DispatchKey::PrivateUse1, c10::DispatchKey::AutogradPrivateUse1};",
                        "code"
                    ],
                    [
                        "Note that TensorImpl class above assumes your Tensor is backed by a storage like CPU/CUDA. We also\nprovide OpaqueTensorImpl for backends without a storage. And you might need to tweak/override certain\nmethods to fit your customized hardware.\nOne example in pytorch repo is .",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "Once the prototype is done and you plan to do regular releases for your backend extension,  please feel free to\nsubmit a PR to pytorch/pytorch to reserve a dedicated dispath key for your backend.",
                        "markdown"
                    ]
                ]
            },
            {
                "Get the full list of PyTorch operators": [
                    [
                        "PyTorch provides a full list of extensible C++ operators in generated file\nbuild/aten/src/ATen/RegistrationDeclarations.h.\nThis file is only available after building PyTorch from source.\nHere\u2019s a snippet of the file:",
                        "markdown"
                    ],
                    [
                        "Tensor abs(const Tensor &amp; self); // {\"schema\": \"aten::abs(Tensor self) -&gt; Tensor\", \"dispatch\": \"True\", \"default\": \"True\"}\nTensor &amp; abs_(Tensor &amp; self); // {\"schema\": \"aten::abs_(Tensor(a!) self) -&gt; Tensor(a!)\", \"dispatch\": \"True\", \"default\": \"True\"}\nTensor &amp; abs_out(Tensor &amp; out, const Tensor &amp; self); // {\"schema\": \"aten::abs.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)\", \"dispatch\": \"True\", \"default\": \"False\"}\nTensor absolute(const Tensor &amp; self); // {\"schema\": \"aten::absolute(Tensor self) -&gt; Tensor\", \"dispatch\": \"False\", \"default\": \"False\"}\nTensor &amp; absolute_(Tensor &amp; self); // {\"schema\": \"aten::absolute_(Tensor(a!) self) -&gt; Tensor(a!)\", \"dispatch\": \"False\", \"default\": \"False\"}\nTensor &amp; absolute_out(Tensor &amp; out, const Tensor &amp; self); // {\"schema\": \"aten::absolute.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)\", \"dispatch\": \"False\", \"default\": \"False\"}\nTensor angle(const Tensor &amp; self); // {\"schema\": \"aten::angle(Tensor self) -&gt; Tensor\", \"dispatch\": \"True\", \"default\": \"True\"}\nTensor &amp; angle_out(Tensor &amp; out, const Tensor &amp; self); // {\"schema\": \"aten::angle.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!)\", \"dispatch\": \"True\", \"default\": \"False\"}\nTensor sgn(const Tensor &amp; self); // {\"schema\": \"aten::sgn(Tensor self) -&gt; Tensor\", \"dispatch\": \"True\", \"default\": \"True\"}",
                        "code"
                    ],
                    [
                        "There\u2019re multiple fields associated with a single operator. Let\u2019s break it down using abs_out as an example:",
                        "markdown"
                    ],
                    [
                        "Tensor &amp; abs_out(Tensor &amp; out, const Tensor &amp; self); is the C++ signature of the operator, your C++\nkernel should match this signature exactly.",
                        "markdown"
                    ],
                    [
                        "aten::abs.out(Tensor self, *, Tensor(a!) out) -&gt; Tensor(a!) is the unique schema representing the operator,\nwhich also contains aliasing and mutation annotations compared to the C++ signature.  This is the unique identifier\nthe dispatcher uses to find an operator.",
                        "markdown"
                    ],
                    [
                        "dispatch and default are boolean fields that provide information about what native PyTorch kernels\ncan do, thus implies whether it\u2019s required for backend extenders to implement the kernel.\nMore details can be found in .",
                        "markdown"
                    ]
                ]
            },
            {
                "Register kernels for the new backend": [
                    [
                        "To register your kernels to PyTorch dispatcher, you can use the\nTORCH_LIBRARY_IMPL API described in\n:",
                        "markdown"
                    ],
                    [
                        "TORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n  m.impl(&lt;schema_my_op1&gt;, &amp;my_op1);\n  m.impl(&lt;schema_my_op2&gt;, &amp;my_op2);\n  m.impl(&lt;schema_my_op2_backward&gt;, &amp;my_op2_backward);\n}",
                        "code"
                    ],
                    [
                        "Now let\u2019s zoom in and what operator requires a kernel from a customized backend and what\u2019s\ninside the kernels exactly.",
                        "markdown"
                    ],
                    [
                        "PyTorch currently has more than 1600 operators and it\u2019s still growing.  It\u2019s unrealistic\nfor backend extensions to keep up with this speed.  Even for native backends like CPU\nor CUDA, it often requires a lot of work to write dedicated kernels for every new op.",
                        "markdown"
                    ],
                    [
                        "Fortunately, some native PyTorch kernels are written in a way that they decompose to\ncombination of several known operators. In other words, you only need to implement\na set of known operators (ops that require registration below) instead of all PyTorch operators.",
                        "markdown"
                    ],
                    [
                        "PyTorch operators can be classified into two categories:",
                        "markdown"
                    ],
                    [
                        "Ops that require registration: PyTorch native implementation for these ops is backend specific\nand thus it\u2019s required to provide a kernel for customized backend.  Otherwise calling such op\non the customized backend will error out.\n<blockquote>",
                        "markdown"
                    ],
                    [
                        "In RegistrationDeclarations.h these operators have dispatch set to True <em>and</em> default set to False\nin the metadata found in their accompanying comments.\n\n</blockquote>",
                        "markdown"
                    ],
                    [
                        "Registration is optional: backend extenders can skip registering to these ops without sacrificing any support.\nHowever, if a backend extender wants to override the default kernel provided by PyTorch, they can still\nregister their customized kernel to their backend and the dispatcher will use it for your backend only.\nFor example, current implementation of PyTorch\u2019s max_pool2d returns indices as part of forward outputs which\ncreates overhead in torch_xla, so torch_xla registers its own kernel for max_pool2d instead.\n<blockquote>",
                        "markdown"
                    ],
                    [
                        "In RegistrationDeclarations.h these operators have dispatch set to False <em>or</em> default set to True\nin the metadata found in their accompanying comments.\n\n</blockquote>",
                        "markdown"
                    ]
                ]
            },
            {
                "Autograd support for the new backend": [
                    [
                        "Gradient formulas are mostly purely mathematical and thus are general for all backends.\nPyTorch often registers a kernel to alias dispatch key Autograd, which means it can be used by all backends.",
                        "markdown"
                    ],
                    [
                        "For these operators you don\u2019t have to worry about their derivative formulas,\nyou can just write forward definitions for operators in RegistrationDeclarations.h and PyTorch handles\nbackward for you automatically.",
                        "markdown"
                    ],
                    [
                        "Tensor my_op1(const Tensor&amp; self, const Tensor&amp; other) {\n  // call your backend-specific APIs to implement my_op so that\n  // it matches PyTorch's native behavior\n}\nTORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n  m.impl(&lt;schema_my_op1&gt;, &amp;my_op);\n}",
                        "code"
                    ],
                    [
                        "In some cases, PyTorch backward kernel implementations are also device specific so that they can squeeze out\nmax performance out of each backend. For those operators you\u2019ll see op_backward showing up in\nRegistrationDeclarations.h as <em>required registration</em> as well.",
                        "markdown"
                    ],
                    [
                        "Tensor my_op2_backward(const Tensor&amp; self, const Tensor&amp; other) {\n  // call your backend-specific APIs to implement my_op2_backward so that\n  // it matches PyTorch's native behavior\n}\n\n// Note backward kernel is still registered to PrivateUse1 instead of AutogradPrivateUse1.\n// PyTorch will wrap your backward kernel with proper autograd setup and then link to it in\n// my_op2's AutogradPrivateUse1 kernel.\nTORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n  m.impl(&lt;schema_my_op2&gt;, &amp;my_op2);\n  m.impl(&lt;schema_my_op2_backward&gt;, &amp;my_op2_backward);\n}",
                        "code"
                    ],
                    [
                        "In a few <em>rare</em> cases, PyTorch\u2019s gradient formula for certain operators may have assumptions that don\u2019t generalize\nfor all backends. In those cases backend extenders can optionally override PyTorch Autograd layer by registering\na kernel from torch::autograd::Function to the corresponding dispatch key (for example, AutogradPrivateUse1 if\nyou\u2019re using PrivateUse1 for your backend):",
                        "markdown"
                    ],
                    [
                        "class MyAddFunction : public torch::autograd::Function&lt;MyAddFunction&gt; {\n  public:\n  static Tensor forward(AutogradContext *ctx, torch::Tensor self, torch::Tensor other) {\n    at::AutoNonVariableTypeMode g;\n    return myadd(self, other);\n  }\n\n  static tensor_list backward(AutogradContext *ctx, tensor_list grad_outputs) {\n    auto grad_output = grad_outputs[0];\n    return {grad_output, grad_output};\n  }\n};\n\nTensor myadd_autograd(const Tensor&amp; self, const Tensor&amp; other) {\n  return MyAddFunction::apply(self, other)[0];\n}\n\n// Register the autograd kernel to AutogradPrivateUse1\nTORCH_LIBRARY_IMPL(aten, AutogradPrivateUse1, m) {\n  m.impl(&lt;myadd_schema&gt;, &amp;myadd_autograd);\n}\n\n// Register the inference kernel to PrivateUse1\nTORCH_LIBRARY_IMPL(aten, PrivateUse1, m) {\n  m.impl(&lt;myadd_schema&gt;, &amp;myadd);\n}",
                        "code"
                    ],
                    [
                        "With this trick you have full control over both training and inference behavior for my_add operator in your backend.\nHere\u2019s  in the pytorch/xla repository.",
                        "markdown"
                    ]
                ]
            },
            {
                "Build an extension": [
                    [
                        "Out-of-tree backend is supported by adding a C++ extension to PyTorch.\nOnce you have kernels and registrations ready, you can build a C++ extension by\nwriting a setup.py script that uses setuptools to compile C++ code.  Here\u2019s a simplified example from\n:",
                        "markdown"
                    ],
                    [
                        "from setuptools import setup\nfrom torch.utils.cpp_extension import BuildExtension, CppExtension\n\nsetup(\n    name='torch_xla',\n    ext_modules=[\n        CppExtension(\n            '_XLAC',\n            torch_xla_sources,\n            include_dirs=include_dirs,\n            extra_compile_args=extra_compile_args,\n            library_dirs=library_dirs,\n            extra_link_args=extra_link_args + \\\n                [make_relative_rpath('torch_xla/lib')],\n        ),\n    ],\n    cmdclass={\n        'build_ext': Build,  # Build is a derived class of BuildExtension\n    }\n    # more configs...\n)",
                        "code"
                    ],
                    [
                        "See \nfor more details.",
                        "markdown"
                    ]
                ]
            },
            {
                "Custom operator support": [
                    [
                        "Your new backend should work seamlessly with\n\nwithout writing any new kernels as long as the customized operator is composed of existing\nPyTorch operators (which are already supported by your backend).",
                        "markdown"
                    ],
                    [
                        "For  they often come with a\n\nas well as .\nTo support these operators, backend extenders will need to write a C++ kernel for your backend and properly\nregister it to the corresponding namespace in the dispatcher similar to supporting PyTorch native operators.\nAlternatively you could also add a customized API in your extension e.g torch_xla.core.functions.nms for\nthese adhoc requests.",
                        "markdown"
                    ]
                ]
            },
            {
                "JIT support": [
                    [
                        "As we mentioned in , kernels registered through <cite>m.impl()</cite> API\nsupport being called in both unboxed and boxed ways. In other words your customized backend can also work with our\nJIT tracing/scripting frontend just like the in-tree backends like CPU or CUDA do.  You could potentially also write specialized optimization\npasses for your backend on a JIT graph.  But we will not discuss it here since we haven\u2019t finalized the integration point\nin JIT, so the current backend support will focus on the eager frontend for now.",
                        "markdown"
                    ]
                ]
            },
            {
                "Testing your backend against native PyTorch backends": [
                    [
                        "PyTorch lets tests run on multiple device types using its .\nYou can find details about \nand information about .\nOnce added, PyTorch tests using the generic device type testing framework will be run using your device type, too.\nSee  for an example of how tests are instantiated.",
                        "markdown"
                    ],
                    [
                        "Running PyTorch\u2019s existing test suites with your device type is important to ensure correctness,\nbut not all PyTorch features are supported by every device type.  The generic device type testing\nframework allows for considerable customization so that device types can select which tests to run,\nwhich dtypes they support, and even which precisions to use when comparing tensors for equality.",
                        "markdown"
                    ],
                    [
                        "An example device type that uses the generic device type testing framework and doesn\u2019t ship with\nPyTorch is XLA.  See ,\nwhich contains examples of block listing tests, block listing dtypes, and overriding test precision.",
                        "markdown"
                    ],
                    [
                        "The generic device type testing framework is actively developed. To request a feature please file an\nissue on PyTorch\u2019s Github.",
                        "markdown"
                    ]
                ]
            },
            {
                "Backward Compatibility": [
                    [
                        "Currently PyTorch can\u2019t guarantee backward compatibility for registered operators.\nOperators, as well as their schemas, might be added/modified/deleted as needed.  Registered\nkernels must be <em>exactly</em> the same as PyTorch version.  If PyTorch adds more parameters (\neven with defaults) for an operator, your old registration won\u2019t work until it\u2019s updated\nto match PyTorch\u2019s new signature.",
                        "markdown"
                    ],
                    [
                        "As a result, we <em>highly recommend</em> out-of-tree backend extenders only sync with major PyTorch\nreleases to minimize interruptions in development.  PyTorch is on a quarterly release cadence.\nBackend extenders should join the <em>#announcement</em> channel at \nto get latest updates on releases.",
                        "markdown"
                    ]
                ]
            },
            {
                "Known issues &amp; additional notes": [
                    [
                        "Not all test suites are device generic yet. Extensible test classes can be found by searching\ninstantiate_device_type_tests in PyTorch codebase, e.g\nTestTorchDeviceType, TestViewOps, TestTensorDeviceOps, TestTypePromotion etc.",
                        "markdown"
                    ],
                    [
                        "There\u2019s no extension point in C++ for serializing a python Tensor object on customized backend. Currently\nyou can only extend it by modifying \nor monkey patching in out-of-tree repository.",
                        "markdown"
                    ],
                    [
                        "If your backend doesn\u2019t allow direct memory access, you should pay additional attention to supporting\nview ops since they\u2019re supposed to share storage. Changes to view tensor need to propagated to its\nbase tensor and vice versa.",
                        "markdown"
                    ],
                    [
                        "There\u2019s no extension point in C++ for Optimizer if your backend doesn\u2019t work with the native PyTorch\nOptimizers, e.g. need to carry the states to be updated in backward like torch-xla. Such use cases\ncurrently can only be done through adding customized API or monkey patching in out-of-tree repository.",
                        "markdown"
                    ]
                ]
            },
            {
                "Future Work": [
                    [
                        "Making every component in PyTorch extensible for an out-of-tree backend seamless\nrequires a lot of changes to PyTorch internals.  Here are a few items that we\u2019re\nactively working on might improve the experience in the future:",
                        "markdown"
                    ],
                    [
                        "Improve test coverage of generic testing framework.",
                        "markdown"
                    ],
                    [
                        "Improve Math kernel coverage and more comprehensive tests to make sure Math\nkernel bahavior matches other backends like CPU/CUDA.",
                        "markdown"
                    ],
                    [
                        "Refactor RegistrationDeclarations.h to carry the minimal information and reuse\nPyTorch\u2019s codegen as much as possible.",
                        "markdown"
                    ],
                    [
                        "Support a backend fallback kernel to automatic convert inputs to CPU and convert the\nresult back to the customized backend. This will allow \u201cfull\u201d operator coverage even\nthough you don\u2019t have kernels written for every operator.",
                        "markdown"
                    ]
                ]
            },
            {
                "Stay in touch": [
                    [
                        "Please use  for questions and discussions. If you have\nany feature requests or bug reports, please .",
                        "markdown"
                    ],
                    [
                        "If you\u2019re interested in helping in any of the future work items above (e.g adding more Math\nkernels for PyTorch operators in C++), please reach out to us through Github or Slack!",
                        "markdown"
                    ]
                ]
            }
        ]
    },
    "Model Optimization": {
        "Profiling your PyTorch Module": [
            [
                "<strong>Author:</strong> ",
                "markdown"
            ],
            [
                "PyTorch includes a profiler API that is useful to identify the time and\nmemory costs of various PyTorch operations in your code. Profiler can be\neasily integrated in your code, and the results can be printed as a table\nor retured in a JSON trace file.",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                "Profiler supports multithreaded models. Profiler runs in the\nsame thread as the operation but it will also profile child operators\nthat might run in another thread. Concurrently-running profilers will be\nscoped to their own thread to prevent mixing of results.",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                "PyTorch 1.8 introduces the new API that will replace the older profiler API\nin the future releases. Check the new API at .",
                "markdown"
            ],
            [
                "Head on over to \nfor a quicker walkthrough of Profiler API usage.",
                "markdown"
            ],
            [
                "import torch\nimport numpy as np\nfrom torch import nn\nimport torch.autograd.profiler as profiler",
                "code"
            ],
            {
                "Performance debugging using Profiler": [
                    [
                        "Profiler can be useful to identify performance bottlenecks in your\nmodels. In this example, we build a custom module that performs two\nsub-tasks:",
                        "markdown"
                    ],
                    [
                        "a linear transformation on the input, and",
                        "markdown"
                    ],
                    [
                        "use the transformation result to get indices on a mask tensor.",
                        "markdown"
                    ],
                    [
                        "We wrap the code for each sub-task in separate labelled context managers using\nprofiler.record_function(\"label\"). In the profiler output, the\naggregate performance metrics of all operations in the sub-task will\nshow up under its corresponding label.",
                        "markdown"
                    ],
                    [
                        "Note that using Profiler incurs some overhead, and is best used only for investigating\ncode. Remember to remove it if you are benchmarking runtimes.",
                        "markdown"
                    ],
                    [
                        "class MyModule():\n    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n        super(MyModule, self).__init__()\n        self.linear = (in_features, out_features, bias)\n\n    def forward(self, input, mask):\n        with profiler.record_function(\"LINEAR PASS\"):\n            out = self.linear(input)\n\n        with profiler.record_function(\"MASK INDICES\"):\n            threshold = out.sum(axis=1).mean().item()\n            hi_idx = np.argwhere(mask.cpu().numpy() &gt; threshold)\n            hi_idx = (hi_idx).cuda()\n\n        return out, hi_idx",
                        "code"
                    ]
                ]
            },
            {
                "Profile the forward pass": [
                    [
                        "We initialize random input and mask tensors, and the model.",
                        "markdown"
                    ],
                    [
                        "Before we run the profiler, we warm-up CUDA to ensure accurate\nperformance benchmarking. We wrap the forward pass of our module in the\nprofiler.profile context manager. The with_stack=True parameter appends the\nfile and line number of the operation in the trace.",
                        "markdown"
                    ],
                    [
                        "Warning",
                        "markdown"
                    ],
                    [
                        "with_stack=True incurs an additional overhead, and is better suited for investigating code.\nRemember to remove it if you are benchmarking performance.",
                        "markdown"
                    ],
                    [
                        "model = MyModule(500, 10).cuda()\ninput = (128, 500).cuda()\nmask = ((500, 500, 500), dtype=torch.double).cuda()\n\n# warm-up\nmodel(input, mask)\n\nwith (with_stack=True, profile_memory=True) as prof:\n    out, idx = model(input, mask)",
                        "code"
                    ]
                ]
            },
            {
                "Print profiler results": [
                    [
                        "Finally, we print the profiler results. profiler.key_averages\naggregates the results by operator name, and optionally by input\nshapes and/or stack trace events.\nGrouping by input shapes is useful to identify which tensor shapes\nare utilized by the model.",
                        "markdown"
                    ],
                    [
                        "Here, we use group_by_stack_n=5 which aggregates runtimes by the\noperation and its traceback (truncated to the most recent 5 events), and\ndisplay the events in the order they are registered. The table can also\nbe sorted by passing a sort_by argument (refer to the\n for\nvalid sorting keys).",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "When running profiler in a notebook, you might see entries like &lt;ipython-input-18-193a910735e8&gt;(13): forward\ninstead of filenames in the stacktrace. These correspond to &lt;notebook-cell&gt;(line number): calling-function.",
                        "markdown"
                    ],
                    [
                        "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n\n\"\"\"\n(Some columns are omitted)\n\n-------------  ------------  ------------  ------------  ---------------------------------\n         Name    Self CPU %      Self CPU  Self CPU Mem   Source Location\n-------------  ------------  ------------  ------------  ---------------------------------\n MASK INDICES        87.88%        5.212s    -953.67 Mb  /mnt/xarfuse/.../torch/au\n                                                         &lt;ipython-input-...&gt;(10): forward\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                         /mnt/xarfuse/.../IPython/\n\n  aten::copy_        12.07%     715.848ms           0 b  &lt;ipython-input-...&gt;(12): forward\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                         /mnt/xarfuse/.../IPython/\n                                                         /mnt/xarfuse/.../IPython/\n\n  LINEAR PASS         0.01%     350.151us         -20 b  /mnt/xarfuse/.../torch/au\n                                                         &lt;ipython-input-...&gt;(7): forward\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                         /mnt/xarfuse/.../IPython/\n\n  aten::addmm         0.00%     293.342us           0 b  /mnt/xarfuse/.../torch/nn\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         &lt;ipython-input-...&gt;(8): forward\n                                                         /mnt/xarfuse/.../torch/nn\n\n   aten::mean         0.00%     235.095us           0 b  &lt;ipython-input-...&gt;(11): forward\n                                                         /mnt/xarfuse/.../torch/nn\n                                                         &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                         /mnt/xarfuse/.../IPython/\n                                                         /mnt/xarfuse/.../IPython/\n\n-----------------------------  ------------  ---------- ----------------------------------\nSelf CPU time total: 5.931s\n\n\"\"\"",
                        "code"
                    ]
                ]
            },
            {
                "Improve memory performance": [
                    [
                        "Note that the most expensive operations - in terms of memory and time -\nare at forward (10) representing the operations within MASK INDICES. Let\u2019s try to\ntackle the memory consumption first. We can see that the .to()\noperation at line 12 consumes 953.67 Mb. This operation copies mask to the CPU.\nmask is initialized with a torch.double datatype. Can we reduce the memory footprint by casting\nit to torch.float instead?",
                        "markdown"
                    ],
                    [
                        "model = MyModule(500, 10).cuda()\ninput = (128, 500).cuda()\nmask = ((500, 500, 500), dtype=torch.float).cuda()\n\n# warm-up\nmodel(input, mask)\n\nwith (with_stack=True, profile_memory=True) as prof:\n    out, idx = model(input, mask)\n\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n\n\"\"\"\n(Some columns are omitted)\n\n-----------------  ------------  ------------  ------------  --------------------------------\n             Name    Self CPU %      Self CPU  Self CPU Mem   Source Location\n-----------------  ------------  ------------  ------------  --------------------------------\n     MASK INDICES        93.61%        5.006s    -476.84 Mb  /mnt/xarfuse/.../torch/au\n                                                             &lt;ipython-input-...&gt;(10): forward\n                                                             /mnt/xarfuse/  /torch/nn\n                                                             &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                             /mnt/xarfuse/.../IPython/\n\n      aten::copy_         6.34%     338.759ms           0 b  &lt;ipython-input-...&gt;(12): forward\n                                                             /mnt/xarfuse/.../torch/nn\n                                                             &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                             /mnt/xarfuse/.../IPython/\n                                                             /mnt/xarfuse/.../IPython/\n\n aten::as_strided         0.01%     281.808us           0 b  &lt;ipython-input-...&gt;(11): forward\n                                                             /mnt/xarfuse/.../torch/nn\n                                                             &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                             /mnt/xarfuse/.../IPython/\n                                                             /mnt/xarfuse/.../IPython/\n\n      aten::addmm         0.01%     275.721us           0 b  /mnt/xarfuse/.../torch/nn\n                                                             /mnt/xarfuse/.../torch/nn\n                                                             /mnt/xarfuse/.../torch/nn\n                                                             &lt;ipython-input-...&gt;(8): forward\n                                                             /mnt/xarfuse/.../torch/nn\n\n      aten::_local        0.01%     268.650us           0 b  &lt;ipython-input-...&gt;(11): forward\n      _scalar_dense                                          /mnt/xarfuse/.../torch/nn\n                                                             &lt;ipython-input-...&gt;(9): &lt;module&gt;\n                                                             /mnt/xarfuse/.../IPython/\n                                                             /mnt/xarfuse/.../IPython/\n\n-----------------  ------------  ------------  ------------  --------------------------------\nSelf CPU time total: 5.347s\n\n\"\"\"",
                        "code"
                    ],
                    [
                        "The CPU memory footprint for this operation has halved.",
                        "markdown"
                    ]
                ]
            },
            {
                "Improve time performance": [
                    [
                        "While the time consumed has also reduced a bit, it\u2019s still too high.\nTurns out copying a matrix from CUDA to CPU is pretty expensive!\nThe aten::copy_ operator in forward (12) copies mask to CPU\nso that it can use the NumPy argwhere function. aten::copy_ at forward(13)\ncopies the array back to CUDA as a tensor. We could eliminate both of these if we use a\ntorch function nonzero() here instead.",
                        "markdown"
                    ],
                    [
                        "class MyModule():\n    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n        super(MyModule, self).__init__()\n        self.linear = (in_features, out_features, bias)\n\n    def forward(self, input, mask):\n        with profiler.record_function(\"LINEAR PASS\"):\n            out = self.linear(input)\n\n        with profiler.record_function(\"MASK INDICES\"):\n            threshold = out.sum(axis=1).mean()\n            hi_idx = (mask &gt; threshold).nonzero(as_tuple=True)\n\n        return out, hi_idx\n\n\nmodel = MyModule(500, 10).cuda()\ninput = (128, 500).cuda()\nmask = ((500, 500, 500), dtype=torch.float).cuda()\n\n# warm-up\nmodel(input, mask)\n\nwith (with_stack=True, profile_memory=True) as prof:\n    out, idx = model(input, mask)\n\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n\n\"\"\"\n(Some columns are omitted)\n\n--------------  ------------  ------------  ------------  ---------------------------------\n          Name    Self CPU %      Self CPU  Self CPU Mem   Source Location\n--------------  ------------  ------------  ------------  ---------------------------------\n      aten::gt        57.17%     129.089ms           0 b  &lt;ipython-input-...&gt;(12): forward\n                                                          /mnt/xarfuse/.../torch/nn\n                                                          &lt;ipython-input-...&gt;(25): &lt;module&gt;\n                                                          /mnt/xarfuse/.../IPython/\n                                                          /mnt/xarfuse/.../IPython/\n\n aten::nonzero        37.38%      84.402ms           0 b  &lt;ipython-input-...&gt;(12): forward\n                                                          /mnt/xarfuse/.../torch/nn\n                                                          &lt;ipython-input-...&gt;(25): &lt;module&gt;\n                                                          /mnt/xarfuse/.../IPython/\n                                                          /mnt/xarfuse/.../IPython/\n\n   INDEX SCORE         3.32%       7.491ms    -119.21 Mb  /mnt/xarfuse/.../torch/au\n                                                          &lt;ipython-input-...&gt;(10): forward\n                                                          /mnt/xarfuse/.../torch/nn\n                                                          &lt;ipython-input-...&gt;(25): &lt;module&gt;\n                                                          /mnt/xarfuse/.../IPython/\n\naten::as_strided         0.20%    441.587us          0 b  &lt;ipython-input-...&gt;(12): forward\n                                                          /mnt/xarfuse/.../torch/nn\n                                                          &lt;ipython-input-...&gt;(25): &lt;module&gt;\n                                                          /mnt/xarfuse/.../IPython/\n                                                          /mnt/xarfuse/.../IPython/\n\n aten::nonzero\n     _numpy             0.18%     395.602us           0 b  &lt;ipython-input-...&gt;(12): forward\n                                                          /mnt/xarfuse/.../torch/nn\n                                                          &lt;ipython-input-...&gt;(25): &lt;module&gt;\n                                                          /mnt/xarfuse/.../IPython/\n                                                          /mnt/xarfuse/.../IPython/\n--------------  ------------  ------------  ------------  ---------------------------------\nSelf CPU time total: 225.801ms\n\n\"\"\"",
                        "code"
                    ]
                ]
            },
            {
                "Further Reading": [
                    [
                        "We have seen how Profiler can be used to investigate time and memory bottlenecks in PyTorch models.\nRead more about Profiler here:",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "PyTorch Profiler With TensorBoard": [
            [
                "This tutorial demonstrates how to use TensorBoard plugin with PyTorch Profiler\nto detect performance bottlenecks of the model.",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "PyTorch 1.8 includes an updated profiler API capable of\nrecording the CPU side operations as well as the CUDA kernel launches on the GPU side.\nThe profiler can visualize this information\nin TensorBoard Plugin and provide analysis of the performance bottlenecks.",
                        "markdown"
                    ],
                    [
                        "In this tutorial, we will use a simple Resnet model to demonstrate how to\nuse TensorBoard plugin to analyze model performance.",
                        "markdown"
                    ]
                ]
            },
            {
                "Setup": [
                    [
                        "To install torch and torchvision use the following command:",
                        "markdown"
                    ],
                    [
                        "pip install torch torchvision",
                        "code"
                    ]
                ]
            },
            {
                "Steps": [
                    [
                        "Prepare the data and model",
                        "markdown"
                    ],
                    [
                        "Use profiler to record execution events",
                        "markdown"
                    ],
                    [
                        "Run the profiler",
                        "markdown"
                    ],
                    [
                        "Use TensorBoard to view results and analyze model performance",
                        "markdown"
                    ],
                    [
                        "Improve performance with the help of profiler",
                        "markdown"
                    ],
                    [
                        "Analyze performance with other advanced features",
                        "markdown"
                    ],
                    {
                        "1. Prepare the data and model": [
                            [
                                "First, import all necessary libraries:",
                                "markdown"
                            ],
                            [
                                "import torch\nimport torch.nn\nimport torch.optim\nimport torch.profiler\nimport torch.utils.data\nimport torchvision.datasets\nimport torchvision.models\nimport torchvision.transforms as T",
                                "code"
                            ],
                            [
                                "Then prepare the input data. For this tutorial, we use the CIFAR10 dataset.\nTransform it to the desired format and use DataLoader to load each batch.",
                                "markdown"
                            ],
                            [
                                "transform = (\n    [(224),\n     (),\n     ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ntrain_set = (root='./data', train=True, download=True, transform=transform)\ntrain_loader = (train_set, batch_size=32, shuffle=True)",
                                "code"
                            ],
                            [
                                "Next, create Resnet model, loss function, and optimizer objects.\nTo run on GPU, move model and loss to GPU device.",
                                "markdown"
                            ],
                            [
                                "device = (\"cuda:0\")\nmodel = (pretrained=True).cuda(device)\ncriterion = ().cuda(device)\noptimizer = (model.parameters(), lr=0.001, momentum=0.9)\nmodel.train()",
                                "code"
                            ],
                            [
                                "Define the training step for each batch of input data.",
                                "markdown"
                            ],
                            [
                                "def train(data):\n    inputs, labels = data[0].to(device=device), data[1].to(device=device)\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2. Use profiler to record execution events": [
                            [
                                "The profiler is enabled through the context manager and accepts several parameters,\nsome of the most useful are:",
                                "markdown"
                            ],
                            [
                                "schedule - callable that takes step (int) as a single parameter\nand returns the profiler action to perform at each step.",
                                "markdown"
                            ],
                            [
                                "In this example with wait=1, warmup=1, active=3, repeat=2,\nprofiler will skip the first step/iteration,\nstart warming up on the second,\nrecord the following three iterations,\nafter which the trace will become available and on_trace_ready (when set) is called.\nIn total, the cycle repeats twice. Each cycle is called a \u201cspan\u201d in TensorBoard plugin.",
                                "markdown"
                            ],
                            [
                                "During wait steps, the profiler is disabled.\nDuring warmup steps, the profiler starts tracing but the results are discarded.\nThis is for reducing the profiling overhead.\nThe overhead at the beginning of profiling is high and easy to bring skew to the profiling result.\nDuring active steps, the profiler works and records events.",
                                "markdown"
                            ],
                            [
                                "on_trace_ready - callable that is called at the end of each cycle;\nIn this example we use torch.profiler.tensorboard_trace_handler to generate result files for TensorBoard.\nAfter profiling, result files will be saved into the ./log/resnet18 directory.\nSpecify this directory as a logdir parameter to analyze profile in TensorBoard.",
                                "markdown"
                            ],
                            [
                                "record_shapes - whether to record shapes of the operator inputs.",
                                "markdown"
                            ],
                            [
                                "profile_memory - Track tensor memory allocation/deallocation. Note, for old version of pytorch with version\nbefore 1.10, if you suffer long profiling time, please disable it or upgrade to new version.",
                                "markdown"
                            ],
                            [
                                "with_stack - Record source information (file and line number) for the ops.\nIf the TensorBoard is launched in VSCode (),\nclicking a stack frame will navigate to the specific code line.",
                                "markdown"
                            ],
                            [
                                "with (\n        schedule=(wait=1, warmup=1, active=3, repeat=2),\n        on_trace_ready=('./log/resnet18'),\n        record_shapes=True,\n        profile_memory=True,\n        with_stack=True\n) as prof:\n    for step, batch_data in enumerate(train_loader):\n        if step &gt;= (1 + 1 + 3) * 2:\n            break\n        train(batch_data)\n        prof.step()  # Need to call this at the end of each step to notify profiler of steps' boundary.",
                                "code"
                            ],
                            [
                                "Alternatively, the following non-context manager start/stop is supported as well.",
                                "markdown"
                            ],
                            [
                                "prof = (\n        schedule=(wait=1, warmup=1, active=3, repeat=2),\n        on_trace_ready=('./log/resnet18'),\n        record_shapes=True,\n        with_stack=True)\nprof.start()\nfor step, batch_data in enumerate(train_loader):\n    if step &gt;= (1 + 1 + 3) * 2:\n        break\n    train(batch_data)\n    prof.step()\nprof.stop()",
                                "code"
                            ]
                        ]
                    },
                    {
                        "3. Run the profiler": [
                            [
                                "Run the above code. The profiling result will be saved under ./log/resnet18 directory.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "4. Use TensorBoard to view results and analyze model performance": [
                            [
                                "Install PyTorch Profiler TensorBoard Plugin.",
                                "markdown"
                            ],
                            [
                                "pip install torch_tb_profiler",
                                "code"
                            ],
                            [
                                "Launch the TensorBoard.",
                                "markdown"
                            ],
                            [
                                "tensorboard --logdir=./log",
                                "code"
                            ],
                            [
                                "Open the TensorBoard profile URL in Google Chrome browser or Microsoft Edge browser.",
                                "markdown"
                            ],
                            [
                                "http://localhost:6006/#pytorch_profiler",
                                "code"
                            ],
                            [
                                "You could see Profiler plugin page as shown below.",
                                "markdown"
                            ],
                            [
                                "Overview",
                                "markdown"
                            ],
                            [
                                "The overview shows a high-level summary of model performance.",
                                "markdown"
                            ],
                            [
                                "The \u201cGPU Summary\u201d panel shows the GPU configuration, GPU usage and Tensor Cores usage.\nIn this example, the GPU Utilization is low.\nThe details of these metrics are .",
                                "markdown"
                            ],
                            [
                                "The \u201cStep Time Breakdown\u201d shows distribution of time spent in each step over different categories of execution.\nIn this example, you can see the DataLoader overhead is significant.",
                                "markdown"
                            ],
                            [
                                "The bottom \u201cPerformance Recommendation\u201d uses the profiling data\nto automatically highlight likely bottlenecks,\nand gives you actionable optimization suggestions.",
                                "markdown"
                            ],
                            [
                                "You can change the view page in left \u201cViews\u201d dropdown list.\n<img alt=\"\" src=\"../_static/img/profiler_views_list.png\"/>",
                                "markdown"
                            ],
                            [
                                "Operator view",
                                "markdown"
                            ],
                            [
                                "The operator view displays the performance of every PyTorch operator\nthat is executed either on the host or device.",
                                "markdown"
                            ],
                            [
                                "The \u201cSelf\u201d duration does not include its child operators\u2019 time.\nThe \u201cTotal\u201d duration includes its child operators\u2019 time.",
                                "markdown"
                            ],
                            [
                                "View call stack",
                                "markdown"
                            ],
                            [
                                "Click the \u201cView Callstack\u201d of an operator, the operators with same name but different call stacks will be shown.\nThen click a \u201cView Callstack\u201d in this sub-table, the call stack frames will be shown.",
                                "markdown"
                            ],
                            [
                                "If the TensorBoard is launched inside VSCode\n(),\nclicking a call stack frame will navigate to the specific code line.",
                                "markdown"
                            ],
                            [
                                "Kernel view",
                                "markdown"
                            ],
                            [
                                "The GPU kernel view shows all kernels\u2019 time spent on GPU.",
                                "markdown"
                            ],
                            [
                                "Tensor Cores Used:\nWhether this kernel uses Tensor Cores.",
                                "markdown"
                            ],
                            [
                                "Mean Blocks per SM:\nBlocks per SM = Blocks of this kernel / SM number of this GPU.\nIf this number is less than 1, it indicates the GPU multiprocessors are not fully utilized.\n\u201cMean Blocks per SM\u201d is weighted average of all runs of this kernel name, using each run\u2019s duration as weight.",
                                "markdown"
                            ],
                            [
                                "Mean Est. Achieved Occupancy:\nEst. Achieved Occupancy is defined in this column\u2019s tooltip.\nFor most cases such as memory bandwidth bounded kernels, the higher the better.\n\u201cMean Est. Achieved Occupancy\u201d is weighted average of all runs of this kernel name,\nusing each run\u2019s duration as weight.",
                                "markdown"
                            ],
                            [
                                "Trace view",
                                "markdown"
                            ],
                            [
                                "The trace view shows timeline of profiled operators and GPU kernels.\nYou can select it to see details as below.",
                                "markdown"
                            ],
                            [
                                "You can move the graph and zoom in/out with the help of right side toolbar.\nAnd keyboard can also be used to zoom and move around inside the timeline.\nThe \u2018w\u2019 and \u2018s\u2019 keys zoom in centered around the mouse,\nand the \u2018a\u2019 and \u2018d\u2019 keys move the timeline left and right.\nYou can hit these keys multiple times until you see a readable representation.",
                                "markdown"
                            ],
                            [
                                "If a backward operator\u2019s \u201cIncoming Flow\u201d field is with value \u201cforward correspond to backward\u201d,\nyou can click the text to get its launching forward operator.",
                                "markdown"
                            ],
                            [
                                "In this example, we can see the event prefixed with enumerate(DataLoader) costs a lot of time.\nAnd during most of this period, the GPU is idle.\nBecause this function is loading data and transforming data on host side,\nduring which the GPU resource is wasted.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "5. Improve performance with the help of profiler": [
                            [
                                "At the bottom of \u201cOverview\u201d page, the suggestion in \u201cPerformance Recommendation\u201d hints the bottleneck is DataLoader.\nThe PyTorch DataLoader uses single process by default.\nUser could enable multi-process data loading by setting the parameter num_workers.\n is more details.",
                                "markdown"
                            ],
                            [
                                "In this example, we follow the \u201cPerformance Recommendation\u201d and set num_workers as below,\npass a different name such as ./log/resnet18_4workers to tensorboard_trace_handler, and run it again.",
                                "markdown"
                            ],
                            [
                                "train_loader = (train_set, batch_size=32, shuffle=True, num_workers=4)",
                                "code"
                            ],
                            [
                                "Then let\u2019s choose the recently profiled run in left \u201cRuns\u201d dropdown list.",
                                "markdown"
                            ],
                            [
                                "From the above view, we can find the step time is reduced to about 76ms comparing with previous run\u2019s 132ms,\nand the time reduction of DataLoader mainly contributes.",
                                "markdown"
                            ],
                            [
                                "From the above view, we can see that the runtime of enumerate(DataLoader) is reduced,\nand the GPU utilization is increased.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "6. Analyze performance with other advanced features": [
                            [
                                "Memory view",
                                "markdown"
                            ],
                            [
                                "To profile memory, profile_memory must be set to True in arguments of torch.profiler.profile.",
                                "markdown"
                            ],
                            [
                                "You can try it by using existing example on Azure",
                                "markdown"
                            ],
                            [
                                "pip install azure-storage-blob\ntensorboard --logdir=https://torchtbprofiler.blob.core.windows.net/torchtbprofiler/demo/memory_demo_1_10",
                                "code"
                            ],
                            [
                                "The profiler records all memory allocation/release events and allocator\u2019s internal state during profiling.\nThe memory view consists of three components as shown in the following.",
                                "markdown"
                            ],
                            [
                                "The components are memory curve graph, memory events table and memory statistics table, from top to bottom, respectively.",
                                "markdown"
                            ],
                            [
                                "The memory type could be selected in \u201cDevice\u201d selection box.\nFor example, \u201cGPU0\u201d means the following table only shows each operator\u2019s memory usage on GPU 0, not including CPU or other GPUs.",
                                "markdown"
                            ],
                            [
                                "The memory curve shows the trends of memory consumption. The \u201cAllocated\u201d curve shows the total memory that is actually\nin use, e.g., tensors. In PyTorch, caching mechanism is employed in CUDA allocator and some other allocators. The\n\u201cReserved\u201d curve shows the total memory that is reserved by the allocator. You can left click and drag on the graph\nto select events in the desired range:",
                                "markdown"
                            ],
                            [
                                "After selection, the three components will be updated for the restricted time range, so that you can gain more\ninformation about it. By repeating this process, you can zoom into a very fine-grained detail. Right click on the graph\nwill reset the graph to the initial state.",
                                "markdown"
                            ],
                            [
                                "In the memory events table, the allocation and release events are paired into one entry. The \u201coperator\u201d column shows\nthe immediate ATen operator that is causing the allocation. Notice that in PyTorch, ATen operators commonly use\naten::empty to allocate memory. For example, aten::ones is implemented as aten::empty followed by an\naten::fill_. Solely display the opeartor name as aten::empty is of little help. It will be shown as\naten::ones (aten::empty) in this special case. The \u201cAllocation Time\u201d, \u201cRelease Time\u201d and \u201cDuration\u201d\ncolumns\u2019 data might be missing if the event occurs outside of the time range.",
                                "markdown"
                            ],
                            [
                                "In the memory statistics table, the \u201cSize Increase\u201d column sums up all allocation size and minus all the memory\nrelease size, that is, the net increase of memory usage after this operator. The \u201cSelf Size Increase\u201d column is\nsimilar to \u201cSize Increase\u201d, but it does not count children operators\u2019 allocation. With regards to ATen operators\u2019\nimplementation detail, some operators might call other operators, so memory allocations can happen at any level of the\ncall stack. That says, \u201cSelf Size Increase\u201d only count the memory usage increase at current level of call stack.\nFinally, the \u201cAllocation Size\u201d column sums up all allocation without considering the memory release.",
                                "markdown"
                            ],
                            [
                                "Distributed view",
                                "markdown"
                            ],
                            [
                                "The plugin now supports distributed view on profiling DDP with NCCL/GLOO as backend.",
                                "markdown"
                            ],
                            [
                                "You can try it by using existing example on Azure:",
                                "markdown"
                            ],
                            [
                                "pip install azure-storage-blob\ntensorboard --logdir=https://torchtbprofiler.blob.core.windows.net/torchtbprofiler/demo/distributed_bert",
                                "code"
                            ],
                            [
                                "The \u201cComputation/Communication Overview\u201d shows computation/communication ratio and their overlapping degree.\nFrom this view, User can figure out load balance issue among workers.\nFor example, if the computation + overlapping time of one worker is much larger than others,\nthere may be a problem of load balance or this worker may be a straggler.",
                                "markdown"
                            ],
                            [
                                "The \u201cSynchronizing/Communication Overview\u201d shows the efficiency of communication.\n\u201cData Transfer Time\u201d is the time for actual data exchanging.\n\u201cSynchronizing Time\u201d is the time for waiting and synchronizing with other workers.",
                                "markdown"
                            ],
                            [
                                "If one worker\u2019s \u201cSynchronizing Time\u201d is much shorter than that of other workers\u2019,\nthis worker may be a straggler which may have more computation workload than other workers\u2019.",
                                "markdown"
                            ],
                            [
                                "The \u201cCommunication Operations Stats\u201d summarizes the detailed statistics of all communication ops in each worker.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Learn More": [
                    [
                        "Take a look at the following documents to continue your learning,\nand feel free to open an issue .",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Hyperparameter tuning with Ray Tune": [
            [
                "Hyperparameter tuning can make the difference between an average model and a highly\naccurate one. Often simple things like choosing a different learning rate or changing\na network layer size can have a dramatic impact on your model performance.",
                "markdown"
            ],
            [
                "Fortunately, there are tools that help with finding the best combination of parameters.\n is an industry standard tool for\ndistributed hyperparameter tuning. Ray Tune includes the latest hyperparameter search\nalgorithms, integrates with TensorBoard and other analysis libraries, and natively\nsupports distributed training through .",
                "markdown"
            ],
            [
                "In this tutorial, we will show you how to integrate Ray Tune into your PyTorch\ntraining workflow. We will extend  for training\na CIFAR10 image classifier.",
                "markdown"
            ],
            [
                "As you will see, we only need to add some slight modifications. In particular, we\nneed to",
                "markdown"
            ],
            [
                "wrap data loading and training in functions,",
                "markdown"
            ],
            [
                "make some network parameters configurable,",
                "markdown"
            ],
            [
                "add checkpointing (optional),",
                "markdown"
            ],
            [
                "and define the search space for the model tuning\n\n\n<br/>",
                "markdown"
            ],
            [
                "To run this tutorial, please make sure the following packages are\ninstalled:",
                "markdown"
            ],
            [
                "ray[tune]: Distributed hyperparameter tuning library",
                "markdown"
            ],
            [
                "torchvision: For the data transformers",
                "markdown"
            ],
            {
                "Setup / Imports": [
                    [
                        "Let\u2019s start with the imports:",
                        "markdown"
                    ],
                    [
                        "from functools import partial\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import \nimport torchvision\nimport torchvision.transforms as transforms\nfrom ray import tune\nfrom ray.tune import CLIReporter\nfrom ray.tune.schedulers import ASHAScheduler",
                        "code"
                    ],
                    [
                        "Most of the imports are needed for building the PyTorch model. Only the last three\nimports are for Ray Tune.",
                        "markdown"
                    ]
                ]
            },
            {
                "Data loaders": [
                    [
                        "We wrap the data loaders in their own function and pass a global data directory.\nThis way we can share a data directory between different trials.",
                        "markdown"
                    ],
                    [
                        "def load_data(data_dir=\"./data\"):\n    transform = ([\n        (),\n        ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    trainset = (\n        root=data_dir, train=True, download=True, transform=transform)\n\n    testset = (\n        root=data_dir, train=False, download=True, transform=transform)\n\n    return trainset, testset",
                        "code"
                    ]
                ]
            },
            {
                "Configurable neural network": [
                    [
                        "We can only tune those parameters that are configurable. In this example, we can specify\nthe layer sizes of the fully connected layers:",
                        "markdown"
                    ],
                    [
                        "class Net():\n    def __init__(self, l1=120, l2=84):\n        super(, self).__init__()\n        self.conv1 = (3, 6, 5)\n        self.pool = (2, 2)\n        self.conv2 = (6, 16, 5)\n        self.fc1 = (16 * 5 * 5, l1)\n        self.fc2 = (l1, l2)\n        self.fc3 = (l2, 10)\n\n    def forward(self, x):\n        x = self.pool((self.conv1(x)))\n        x = self.pool((self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = (self.fc1(x))\n        x = (self.fc2(x))\n        x = self.fc3(x)\n        return x",
                        "code"
                    ]
                ]
            },
            {
                "The train function": [
                    [
                        "Now it gets interesting, because we introduce some changes to the example .",
                        "markdown"
                    ],
                    [
                        "We wrap the training script in a function train_cifar(config, checkpoint_dir=None, data_dir=None).\nAs you can guess, the config parameter will receive the hyperparameters we would like to\ntrain with. The checkpoint_dir parameter is used to restore checkpoints. The data_dir specifies\nthe directory where we load and store the data, so multiple runs can share the same data source.",
                        "markdown"
                    ],
                    [
                        "net = (config[\"l1\"], config[\"l2\"])\n\nif checkpoint_dir:\n    model_state, optimizer_state = (\n        os.path.join(checkpoint_dir, \"checkpoint\"))\n    net.load_state_dict(model_state)\n    optimizer.load_state_dict(optimizer_state)",
                        "code"
                    ],
                    [
                        "The learning rate of the optimizer is made configurable, too:",
                        "markdown"
                    ],
                    [
                        "optimizer = (net.parameters(), lr=config[\"lr\"], momentum=0.9)",
                        "code"
                    ],
                    [
                        "We also split the training data into a training and validation subset. We thus train on\n80% of the data and calculate the validation loss on the remaining 20%. The batch sizes\nwith which we iterate through the training and test sets are configurable as well.",
                        "markdown"
                    ],
                    {
                        "Adding (multi) GPU support with DataParallel": [
                            [
                                "Image classification benefits largely from GPUs. Luckily, we can continue to use\nPyTorch\u2019s abstractions in Ray Tune. Thus, we can wrap our model in nn.DataParallel\nto support data parallel training on multiple GPUs:",
                                "markdown"
                            ],
                            [
                                "device = \"cpu\"\nif ():\n    device = \"cuda:0\"\n    if () &gt; 1:\n        net = (net)\nnet.to(device)",
                                "code"
                            ],
                            [
                                "By using a device variable we make sure that training also works when we have\nno GPUs available. PyTorch requires us to send our data to the GPU memory explicitly,\nlike this:",
                                "markdown"
                            ],
                            [
                                "for i, data in enumerate(trainloader, 0):\n    inputs, labels = data\n    inputs, labels = inputs.to(device), labels.to(device)",
                                "code"
                            ],
                            [
                                "The code now supports training on CPUs, on a single GPU, and on multiple GPUs. Notably, Ray\nalso supports \nso we can share GPUs among trials, as long as the model still fits on the GPU memory. We\u2019ll come back\nto that later.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Communicating with Ray Tune": [
                            [
                                "The most interesting part is the communication with Ray Tune:",
                                "markdown"
                            ],
                            [
                                "with tune.checkpoint_dir(epoch) as checkpoint_dir:\n    path = os.path.join(checkpoint_dir, \"checkpoint\")\n    ((net.state_dict(), optimizer.state_dict()), path)\n\ntune.report(loss=(val_loss / val_steps), accuracy=correct / total)",
                                "code"
                            ],
                            [
                                "Here we first save a checkpoint and then report some metrics back to Ray Tune. Specifically,\nwe send the validation loss and accuracy back to Ray Tune. Ray Tune can then use these metrics\nto decide which hyperparameter configuration lead to the best results. These metrics\ncan also be used to stop bad performing trials early in order to avoid wasting\nresources on those trials.",
                                "markdown"
                            ],
                            [
                                "The checkpoint saving is optional, however, it is necessary if we wanted to use advanced\nschedulers like\n.\nAlso, by saving the checkpoint we can later load the trained models and validate them\non a test set.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Full training function": [
                            [
                                "The full code example looks like this:",
                                "markdown"
                            ],
                            [
                                "def train_cifar(config, checkpoint_dir=None, data_dir=None):\n    net = (config[\"l1\"], config[\"l2\"])\n\n    device = \"cpu\"\n    if ():\n        device = \"cuda:0\"\n        if () &gt; 1:\n            net = (net)\n    net.to(device)\n\n    criterion = ()\n    optimizer = (net.parameters(), lr=config[\"lr\"], momentum=0.9)\n\n    if checkpoint_dir:\n        model_state, optimizer_state = (\n            os.path.join(checkpoint_dir, \"checkpoint\"))\n        net.load_state_dict(model_state)\n        optimizer.load_state_dict(optimizer_state)\n\n    trainset, testset = load_data(data_dir)\n\n    test_abs = int(len(trainset) * 0.8)\n    train_subset, val_subset = (\n        trainset, [test_abs, len(trainset) - test_abs])\n\n    trainloader = (\n        train_subset,\n        batch_size=int(config[\"batch_size\"]),\n        shuffle=True,\n        num_workers=8)\n    valloader = (\n        val_subset,\n        batch_size=int(config[\"batch_size\"]),\n        shuffle=True,\n        num_workers=8)\n\n    for epoch in range(10):  # loop over the dataset multiple times\n        running_loss = 0.0\n        epoch_steps = 0\n        for i, data in enumerate(trainloader, 0):\n            # get the inputs; data is a list of [inputs, labels]\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            # print statistics\n            running_loss += loss.item()\n            epoch_steps += 1\n            if i % 2000 == 1999:  # print every 2000 mini-batches\n                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n                                                running_loss / epoch_steps))\n                running_loss = 0.0\n\n        # Validation loss\n        val_loss = 0.0\n        val_steps = 0\n        total = 0\n        correct = 0\n        for i, data in enumerate(valloader, 0):\n            with ():\n                inputs, labels = data\n                inputs, labels = inputs.to(device), labels.to(device)\n\n                outputs = net(inputs)\n                _, predicted = (outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n                loss = criterion(outputs, labels)\n                val_loss += loss.cpu().numpy()\n                val_steps += 1\n\n        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n            path = os.path.join(checkpoint_dir, \"checkpoint\")\n            ((net.state_dict(), optimizer.state_dict()), path)\n\n        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n    print(\"Finished Training\")",
                                "code"
                            ],
                            [
                                "As you can see, most of the code is adapted directly from the original example.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Test set accuracy": [
                    [
                        "Commonly the performance of a machine learning model is tested on a hold-out test\nset with data that has not been used for training the model. We also wrap this in a\nfunction:",
                        "markdown"
                    ],
                    [
                        "def test_accuracy(net, device=\"cpu\"):\n    trainset, testset = load_data()\n\n    testloader = (\n        testset, batch_size=4, shuffle=False, num_workers=2)\n\n    correct = 0\n    total = 0\n    with ():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = net(images)\n            _, predicted = (outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    return correct / total",
                        "code"
                    ],
                    [
                        "The function also expects a device parameter, so we can do the\ntest set validation on a GPU.",
                        "markdown"
                    ]
                ]
            },
            {
                "Configuring the search space": [
                    [
                        "Lastly, we need to define Ray Tune\u2019s search space. Here is an example:",
                        "markdown"
                    ],
                    [
                        "config = {\n    \"l1\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n    \"l2\": tune.sample_from(lambda _: 2**np.random.randint(2, 9)),\n    \"lr\": tune.loguniform(1e-4, 1e-1),\n    \"batch_size\": tune.choice([2, 4, 8, 16])\n}",
                        "code"
                    ],
                    [
                        "The tune.sample_from() function makes it possible to define your own sample\nmethods to obtain hyperparameters. In this example, the l1 and l2 parameters\nshould be powers of 2 between 4 and 256, so either 4, 8, 16, 32, 64, 128, or 256.\nThe lr (learning rate) should be uniformly sampled between 0.0001 and 0.1. Lastly,\nthe batch size is a choice between 2, 4, 8, and 16.",
                        "markdown"
                    ],
                    [
                        "At each trial, Ray Tune will now randomly sample a combination of parameters from these\nsearch spaces. It will then train a number of models in parallel and find the best\nperforming one among these. We also use the ASHAScheduler which will terminate bad\nperforming trials early.",
                        "markdown"
                    ],
                    [
                        "We wrap the train_cifar function with functools.partial to set the constant\ndata_dir parameter. We can also tell Ray Tune what resources should be\navailable for each trial:",
                        "markdown"
                    ],
                    [
                        "gpus_per_trial = 2\n# ...\nresult = tune.run(\n    partial(train_cifar, data_dir=data_dir),\n    resources_per_trial={\"cpu\": 8, \"gpu\": gpus_per_trial},\n    config=config,\n    num_samples=num_samples,\n    scheduler=scheduler,\n    progress_reporter=reporter,\n    checkpoint_at_end=True)",
                        "code"
                    ],
                    [
                        "You can specify the number of CPUs, which are then available e.g.\nto increase the num_workers of the PyTorch DataLoader instances. The selected\nnumber of GPUs are made visible to PyTorch in each trial. Trials do not have access to\nGPUs that haven\u2019t been requested for them - so you don\u2019t have to care about two trials\nusing the same set of resources.",
                        "markdown"
                    ],
                    [
                        "Here we can also specify fractional GPUs, so something like gpus_per_trial=0.5 is\ncompletely valid. The trials will then share GPUs among each other.\nYou just have to make sure that the models still fit in the GPU memory.",
                        "markdown"
                    ],
                    [
                        "After training the models, we will find the best performing one and load the trained\nnetwork from the checkpoint file. We then obtain the test set accuracy and report\neverything by printing.",
                        "markdown"
                    ],
                    [
                        "The full main function looks like this:",
                        "markdown"
                    ],
                    [
                        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=2):\n    data_dir = os.path.abspath(\"./data\")\n    load_data(data_dir)\n    config = {\n        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n        \"lr\": tune.loguniform(1e-4, 1e-1),\n        \"batch_size\": tune.choice([2, 4, 8, 16])\n    }\n    scheduler = ASHAScheduler(\n        metric=\"loss\",\n        mode=\"min\",\n        max_t=max_num_epochs,\n        grace_period=1,\n        reduction_factor=2)\n    reporter = CLIReporter(\n        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n    result = tune.run(\n        partial(train_cifar, data_dir=data_dir),\n        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n        config=config,\n        num_samples=num_samples,\n        scheduler=scheduler,\n        progress_reporter=reporter)\n\n    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n    print(\"Best trial config: {}\".format(best_trial.config))\n    print(\"Best trial final validation loss: {}\".format(\n        best_trial.last_result[\"loss\"]))\n    print(\"Best trial final validation accuracy: {}\".format(\n        best_trial.last_result[\"accuracy\"]))\n\n    best_trained_model = (best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n    device = \"cpu\"\n    if ():\n        device = \"cuda:0\"\n        if gpus_per_trial &gt; 1:\n            best_trained_model = (best_trained_model)\n    best_trained_model.to(device)\n\n    best_checkpoint_dir = best_trial.checkpoint.value\n    model_state, optimizer_state = (os.path.join(\n        best_checkpoint_dir, \"checkpoint\"))\n    best_trained_model.load_state_dict(model_state)\n\n    test_acc = test_accuracy(best_trained_model, device)\n    print(\"Best trial test set accuracy: {}\".format(test_acc))\n\n\nif __name__ == \"__main__\":\n    # You can change the number of GPUs per trial here:\n    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)",
                        "code"
                    ],
                    [
                        "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /var/lib/jenkins/workspace/beginner_source/data/cifar-10-python.tar.gz\n\n  0%|          | 0/170498071 [00:00&lt;?, ?it/s]\n  0%|          | 458752/170498071 [00:00&lt;00:40, 4186474.07it/s]\n  4%|4         | 7208960/170498071 [00:00&lt;00:04, 39950224.28it/s]\n 11%|#1        | 18907136/170498071 [00:00&lt;00:02, 74299871.50it/s]\n 18%|#7        | 30605312/170498071 [00:00&lt;00:01, 90840159.11it/s]\n 25%|##4       | 42303488/170498071 [00:00&lt;00:01, 100170270.54it/s]\n 32%|###1      | 54001664/170498071 [00:00&lt;00:01, 105728331.82it/s]\n 39%|###8      | 65699840/170498071 [00:00&lt;00:00, 109304415.25it/s]\n 45%|####5     | 77365248/170498071 [00:00&lt;00:00, 111624781.97it/s]\n 52%|#####2    | 89063424/170498071 [00:00&lt;00:00, 113270080.57it/s]\n 59%|#####9    | 100728832/170498071 [00:01&lt;00:00, 114262468.46it/s]\n 66%|######5   | 112459776/170498071 [00:01&lt;00:00, 115102688.65it/s]\n 73%|#######2  | 124190720/170498071 [00:01&lt;00:00, 115709271.02it/s]\n 80%|#######9  | 135921664/170498071 [00:01&lt;00:00, 116147541.76it/s]\n 87%|########6 | 147619840/170498071 [00:01&lt;00:00, 116321987.25it/s]\n 93%|#########3| 159318016/170498071 [00:01&lt;00:00, 116484832.18it/s]\n100%|##########| 170498071/170498071 [00:01&lt;00:00, 105946246.35it/s]\nExtracting /var/lib/jenkins/workspace/beginner_source/data/cifar-10-python.tar.gz to /var/lib/jenkins/workspace/beginner_source/data\nFiles already downloaded and verified\n2023-03-17 21:22:42,513 WARNING services.py:2002 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67104768 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=4.46gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n2023-03-17 21:22:45,624 WARNING tune.py:668 -- Tune detects GPUs, but no trials are using GPUs. To enable trials to use GPUs, set tune.run(resources_per_trial={'gpu': 1}...) which allows Tune to expose 1 GPU to each trial. You can also override `Trainable.default_resource_request` if using the Trainable API.\n== Status ==\nCurrent time: 2023-03-17 21:22:45 (running for 00:00:00.30)\nMemory usage on this node: 1.8/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (9 PENDING, 1 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |\n|-------------------------+----------+-----------------+--------------+------+------+-------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |\n| train_cifar_dbd80_00001 | PENDING  |                 |           16 |   64 |   64 | 0.00689867  |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n\n\n(func pid=1790) Files already downloaded and verified\n(func pid=1790) Files already downloaded and verified\n(func pid=1790) /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n(func pid=1790)   warnings.warn(_create_warning_msg(\n(func pid=1821) Files already downloaded and verified\n== Status ==\nCurrent time: 2023-03-17 21:22:54 (running for 00:00:09.28)\nMemory usage on this node: 2.6/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (8 PENDING, 2 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |\n|-------------------------+----------+-----------------+--------------+------+------+-------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |\n| train_cifar_dbd80_00001 | RUNNING  | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n\n\n(func pid=1821) Files already downloaded and verified\n(func pid=1821) /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n(func pid=1821)   warnings.warn(_create_warning_msg(\n== Status ==\nCurrent time: 2023-03-17 21:22:59 (running for 00:00:14.31)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (8 PENDING, 2 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |\n|-------------------------+----------+-----------------+--------------+------+------+-------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |\n| train_cifar_dbd80_00001 | RUNNING  | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n\n\n(func pid=1790) [1,  2000] loss: 2.151\n== Status ==\nCurrent time: 2023-03-17 21:23:04 (running for 00:00:19.34)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (8 PENDING, 2 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |\n|-------------------------+----------+-----------------+--------------+------+------+-------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |\n| train_cifar_dbd80_00001 | RUNNING  | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:23:09 (running for 00:00:24.36)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (8 PENDING, 2 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |\n|-------------------------+----------+-----------------+--------------+------+------+-------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |\n| train_cifar_dbd80_00001 | RUNNING  | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n\n\n(func pid=1821) [1,  2000] loss: 1.821\n== Status ==\nCurrent time: 2023-03-17 21:23:15 (running for 00:00:29.38)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (8 PENDING, 2 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |\n|-------------------------+----------+-----------------+--------------+------+------+-------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |\n| train_cifar_dbd80_00001 | RUNNING  | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n\n\n(func pid=1790) [1,  4000] loss: 1.037\n== Status ==\nCurrent time: 2023-03-17 21:23:20 (running for 00:00:34.39)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (8 PENDING, 2 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |\n|-------------------------+----------+-----------------+--------------+------+------+-------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |\n| train_cifar_dbd80_00001 | RUNNING  | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+\n\n\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.4741\n  date: 2023-03-17_21-23-21\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 1.4619517978668213\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 27.52146553993225\n  time_this_iter_s: 27.52146553993225\n  time_total_s: 27.52146553993225\n  timestamp: 1679088201\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:23:26 (running for 00:00:41.33)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=0\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.4619517978668213\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (8 PENDING, 2 RUNNING)\n+-------------------------+----------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status   | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+----------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00000 | RUNNING  | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   |         |            |                      |\n| train_cifar_dbd80_00001 | RUNNING  | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.46195 |     0.4741 |                    1 |\n| train_cifar_dbd80_00002 | PENDING  |                 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING  |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING  |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING  |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING  |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING  |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING  |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING  |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n+-------------------------+----------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00000:\n  accuracy: 0.1911\n  date: 2023-03-17_21-23-31\n  done: true\n  experiment_id: bf67837a32e9457489ba78a88a5e9de9\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 2.2055853404998778\n  node_ip: 172.17.0.2\n  pid: 1790\n  should_checkpoint: true\n  time_since_restore: 41.20965528488159\n  time_this_iter_s: 41.20965528488159\n  time_total_s: 41.20965528488159\n  timestamp: 1679088211\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00000\n  warmup_time: 0.003386974334716797\n\n(func pid=1790) Files already downloaded and verified\n(func pid=1790) Files already downloaded and verified\n== Status ==\nCurrent time: 2023-03-17 21:23:36 (running for 00:00:50.53)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.46195 |     0.4741 |                    1 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [2,  2000] loss: 1.404\n== Status ==\nCurrent time: 2023-03-17 21:23:41 (running for 00:00:55.55)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.46195 |     0.4741 |                    1 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  2000] loss: 2.323\n== Status ==\nCurrent time: 2023-03-17 21:23:46 (running for 00:01:00.58)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.46195 |     0.4741 |                    1 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.4988\n  date: 2023-03-17_21-23-46\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 2\n  loss: 1.4047928235054017\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 52.457958936691284\n  time_this_iter_s: 24.936493396759033\n  time_total_s: 52.457958936691284\n  timestamp: 1679088226\n  timesteps_since_restore: 0\n  training_iteration: 2\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:23:51 (running for 00:01:06.28)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.40479 |     0.4988 |                    2 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  4000] loss: 1.156\n== Status ==\nCurrent time: 2023-03-17 21:23:56 (running for 00:01:11.30)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.40479 |     0.4988 |                    2 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:24:01 (running for 00:01:16.32)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.40479 |     0.4988 |                    2 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [3,  2000] loss: 1.301\n== Status ==\nCurrent time: 2023-03-17 21:24:06 (running for 00:01:21.35)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.40479 |     0.4988 |                    2 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  6000] loss: 0.769\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5339\n  date: 2023-03-17_21-24-10\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 3\n  loss: 1.335026089000702\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 76.37172889709473\n  time_this_iter_s: 23.913769960403442\n  time_total_s: 76.37172889709473\n  timestamp: 1679088250\n  timesteps_since_restore: 0\n  training_iteration: 3\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:24:15 (running for 00:01:30.18)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33503 |     0.5339 |                    3 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  8000] loss: 0.575\n== Status ==\nCurrent time: 2023-03-17 21:24:20 (running for 00:01:35.20)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33503 |     0.5339 |                    3 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:24:25 (running for 00:01:40.24)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33503 |     0.5339 |                    3 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [4,  2000] loss: 1.226\n(func pid=1790) [1, 10000] loss: 0.459\n== Status ==\nCurrent time: 2023-03-17 21:24:30 (running for 00:01:45.27)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33503 |     0.5339 |                    3 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5321\n  date: 2023-03-17_21-24-34\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 4\n  loss: 1.337672945547104\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 100.5483820438385\n  time_this_iter_s: 24.176653146743774\n  time_total_s: 100.5483820438385\n  timestamp: 1679088274\n  timesteps_since_restore: 0\n  training_iteration: 4\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:24:39 (running for 00:01:54.36)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=1\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.8337685691833494\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (7 PENDING, 2 RUNNING, 1 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33767 |     0.5321 |                    4 |\n| train_cifar_dbd80_00002 | RUNNING    | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 |         |            |                      |\n| train_cifar_dbd80_00003 | PENDING    |                 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00002:\n  accuracy: 0.1028\n  date: 2023-03-17_21-24-40\n  done: true\n  experiment_id: bf67837a32e9457489ba78a88a5e9de9\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 2.2863188765525817\n  node_ip: 172.17.0.2\n  pid: 1790\n  should_checkpoint: true\n  time_since_restore: 69.34229445457458\n  time_this_iter_s: 69.34229445457458\n  time_total_s: 69.34229445457458\n  timestamp: 1679088280\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00002\n  warmup_time: 0.003386974334716797\n\n(func pid=1790) Files already downloaded and verified\n(func pid=1790) Files already downloaded and verified\n== Status ==\nCurrent time: 2023-03-17 21:24:45 (running for 00:01:59.91)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33767 |     0.5321 |                    4 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:24:50 (running for 00:02:04.96)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33767 |     0.5321 |                    4 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [5,  2000] loss: 1.177\n(func pid=1790) [1,  2000] loss: 2.188\n== Status ==\nCurrent time: 2023-03-17 21:24:55 (running for 00:02:09.99)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.33767 |     0.5321 |                    4 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) E0317 21:24:57.316660920    1817 chttp2_transport.cc:1103]   Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5775\n  date: 2023-03-17_21-24-58\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 5\n  loss: 1.2192501729011536\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 123.69131422042847\n  time_this_iter_s: 23.142932176589966\n  time_total_s: 123.69131422042847\n  timestamp: 1679088298\n  timesteps_since_restore: 0\n  training_iteration: 5\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:25:03 (running for 00:02:17.51)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.21925 |     0.5775 |                    5 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  4000] loss: 1.002\n== Status ==\nCurrent time: 2023-03-17 21:25:08 (running for 00:02:22.53)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.21925 |     0.5775 |                    5 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:25:13 (running for 00:02:27.55)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.21925 |     0.5775 |                    5 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  6000] loss: 0.642\n(func pid=1821) [6,  2000] loss: 1.131\n== Status ==\nCurrent time: 2023-03-17 21:25:18 (running for 00:02:32.56)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.21925 |     0.5775 |                    5 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5707\n  date: 2023-03-17_21-25-21\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 6\n  loss: 1.2494660420417785\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 146.70809984207153\n  time_this_iter_s: 23.016785621643066\n  time_total_s: 146.70809984207153\n  timestamp: 1679088321\n  timesteps_since_restore: 0\n  training_iteration: 6\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n(func pid=1790) [1,  8000] loss: 0.456\n== Status ==\nCurrent time: 2023-03-17 21:25:26 (running for 00:02:40.53)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.24947 |     0.5707 |                    6 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:25:31 (running for 00:02:45.55)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.24947 |     0.5707 |                    6 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 10000] loss: 0.352\n== Status ==\nCurrent time: 2023-03-17 21:25:36 (running for 00:02:50.58)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.24947 |     0.5707 |                    6 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [7,  2000] loss: 1.103\n== Status ==\nCurrent time: 2023-03-17 21:25:41 (running for 00:02:55.60)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.24947 |     0.5707 |                    6 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 12000] loss: 0.288\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5704\n  date: 2023-03-17_21-25-44\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 7\n  loss: 1.2636950318336486\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 169.82927751541138\n  time_this_iter_s: 23.121177673339844\n  time_total_s: 169.82927751541138\n  timestamp: 1679088344\n  timesteps_since_restore: 0\n  training_iteration: 7\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:25:49 (running for 00:03:03.64)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.2637  |     0.5704 |                    7 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 14000] loss: 0.242\n== Status ==\nCurrent time: 2023-03-17 21:25:54 (running for 00:03:08.66)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.2637  |     0.5704 |                    7 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:25:59 (running for 00:03:13.68)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.2637  |     0.5704 |                    7 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [8,  2000] loss: 1.088\n(func pid=1790) [1, 16000] loss: 0.210\n== Status ==\nCurrent time: 2023-03-17 21:26:04 (running for 00:03:18.70)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: None | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.2637  |     0.5704 |                    7 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5865\n  date: 2023-03-17_21-26-07\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 8\n  loss: 1.2344939463615416\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 192.97265577316284\n  time_this_iter_s: 23.143378257751465\n  time_total_s: 192.97265577316284\n  timestamp: 1679088367\n  timesteps_since_restore: 0\n  training_iteration: 8\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:26:12 (running for 00:03:26.81)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.23449 |     0.5865 |                    8 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 18000] loss: 0.186\n== Status ==\nCurrent time: 2023-03-17 21:26:17 (running for 00:03:31.84)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.23449 |     0.5865 |                    8 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:26:22 (running for 00:03:36.85)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.23449 |     0.5865 |                    8 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [9,  2000] loss: 1.071\n(func pid=1790) [1, 20000] loss: 0.164\n== Status ==\nCurrent time: 2023-03-17 21:26:27 (running for 00:03:41.87)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.23449 |     0.5865 |                    8 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5764\n  date: 2023-03-17_21-26-30\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 9\n  loss: 1.2786366604804993\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 216.41862058639526\n  time_this_iter_s: 23.445964813232422\n  time_total_s: 216.41862058639526\n  timestamp: 1679088390\n  timesteps_since_restore: 0\n  training_iteration: 9\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:26:35 (running for 00:03:50.23)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.27864 |     0.5764 |                    9 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:26:40 (running for 00:03:55.24)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -2.2055853404998778\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.27864 |     0.5764 |                    9 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  |         |            |                      |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00003:\n  accuracy: 0.3986\n  date: 2023-03-17_21-26-43\n  done: false\n  experiment_id: bf67837a32e9457489ba78a88a5e9de9\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 1.62057805493623\n  node_ip: 172.17.0.2\n  pid: 1790\n  should_checkpoint: true\n  time_since_restore: 122.60904693603516\n  time_this_iter_s: 122.60904693603516\n  time_total_s: 122.60904693603516\n  timestamp: 1679088403\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00003\n  warmup_time: 0.003386974334716797\n\n(func pid=1821) [10,  2000] loss: 1.057\n== Status ==\nCurrent time: 2023-03-17 21:26:48 (running for 00:04:02.53)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.27864 |     0.5764 |                    9 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:26:53 (running for 00:04:07.55)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=2\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (6 PENDING, 2 RUNNING, 2 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00001 | RUNNING    | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.27864 |     0.5764 |                    9 |\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | PENDING    |                 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2,  2000] loss: 1.614\nResult for train_cifar_dbd80_00001:\n  accuracy: 0.5834\n  date: 2023-03-17_21-26-54\n  done: true\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 10\n  loss: 1.2592490710258484\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 239.92289233207703\n  time_this_iter_s: 23.504271745681763\n  time_total_s: 239.92289233207703\n  timestamp: 1679088414\n  timesteps_since_restore: 0\n  training_iteration: 10\n  trial_id: dbd80_00001\n  warmup_time: 0.004238128662109375\n\n(func pid=1821) Files already downloaded and verified\n(func pid=1821) Files already downloaded and verified\n== Status ==\nCurrent time: 2023-03-17 21:26:59 (running for 00:04:13.76)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2,  4000] loss: 0.809\n== Status ==\nCurrent time: 2023-03-17 21:27:04 (running for 00:04:18.77)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1,  2000] loss: 2.404\n== Status ==\nCurrent time: 2023-03-17 21:27:09 (running for 00:04:23.79)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2,  6000] loss: 0.539\n== Status ==\nCurrent time: 2023-03-17 21:27:14 (running for 00:04:28.80)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1,  4000] loss: 1.209\n(func pid=1790) [2,  8000] loss: 0.405\n== Status ==\nCurrent time: 2023-03-17 21:27:19 (running for 00:04:33.82)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:27:24 (running for 00:04:38.83)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1,  6000] loss: 0.805\n(func pid=1790) [2, 10000] loss: 0.321\n== Status ==\nCurrent time: 2023-03-17 21:27:29 (running for 00:04:43.85)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:27:34 (running for 00:04:48.87)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2, 12000] loss: 0.265\n(func pid=1821) [1,  8000] loss: 0.601\n== Status ==\nCurrent time: 2023-03-17 21:27:39 (running for 00:04:53.89)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:27:44 (running for 00:04:58.90)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2, 14000] loss: 0.230\n(func pid=1821) [1, 10000] loss: 0.483\n== Status ==\nCurrent time: 2023-03-17 21:27:49 (running for 00:05:03.92)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2, 16000] loss: 0.202\n== Status ==\nCurrent time: 2023-03-17 21:27:54 (running for 00:05:08.94)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1, 12000] loss: 0.400\n== Status ==\nCurrent time: 2023-03-17 21:27:59 (running for 00:05:13.96)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2, 18000] loss: 0.175\n== Status ==\nCurrent time: 2023-03-17 21:28:04 (running for 00:05:18.97)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1, 14000] loss: 0.344\n== Status ==\nCurrent time: 2023-03-17 21:28:09 (running for 00:05:23.99)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [2, 20000] loss: 0.157\n== Status ==\nCurrent time: 2023-03-17 21:28:14 (running for 00:05:29.00)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1, 16000] loss: 0.301\n== Status ==\nCurrent time: 2023-03-17 21:28:19 (running for 00:05:34.03)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:28:24 (running for 00:05:39.04)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=3\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4047928235054017 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (5 PENDING, 2 RUNNING, 3 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00003 | RUNNING    | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.62058 |     0.3986 |                    1 |\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | PENDING    |                 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00003:\n  accuracy: 0.443\n  date: 2023-03-17_21-28-25\n  done: true\n  experiment_id: bf67837a32e9457489ba78a88a5e9de9\n  hostname: 6f9535515a81\n  iterations_since_restore: 2\n  loss: 1.5665831570994109\n  node_ip: 172.17.0.2\n  pid: 1790\n  should_checkpoint: true\n  time_since_restore: 224.4824321269989\n  time_this_iter_s: 101.87338519096375\n  time_total_s: 224.4824321269989\n  timestamp: 1679088505\n  timesteps_since_restore: 0\n  training_iteration: 2\n  trial_id: dbd80_00003\n  warmup_time: 0.003386974334716797\n\n(func pid=1790) Files already downloaded and verified\n(func pid=1790) Files already downloaded and verified\n== Status ==\nCurrent time: 2023-03-17 21:28:30 (running for 00:05:44.42)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (4 PENDING, 2 RUNNING, 4 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | RUNNING    | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1, 18000] loss: 0.267\n== Status ==\nCurrent time: 2023-03-17 21:28:35 (running for 00:05:49.44)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (4 PENDING, 2 RUNNING, 4 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | RUNNING    | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  2000] loss: 2.305\n== Status ==\nCurrent time: 2023-03-17 21:28:40 (running for 00:05:54.47)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (4 PENDING, 2 RUNNING, 4 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | RUNNING    | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1, 20000] loss: 0.242\n== Status ==\nCurrent time: 2023-03-17 21:28:45 (running for 00:05:59.49)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (4 PENDING, 2 RUNNING, 4 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | RUNNING    | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  4000] loss: 1.151\n== Status ==\nCurrent time: 2023-03-17 21:28:50 (running for 00:06:04.51)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (4 PENDING, 2 RUNNING, 4 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | RUNNING    | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:28:55 (running for 00:06:09.53)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=4\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -1.913081697718054\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (4 PENDING, 2 RUNNING, 4 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00004 | RUNNING    | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   |         |            |                      |\n| train_cifar_dbd80_00005 | RUNNING    | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  |         |            |                      |\n| train_cifar_dbd80_00006 | PENDING    |                 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00004:\n  accuracy: 0.0955\n  date: 2023-03-17_21-28-57\n  done: true\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 2.463977705168724\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 123.56385898590088\n  time_this_iter_s: 123.56385898590088\n  time_total_s: 123.56385898590088\n  timestamp: 1679088537\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00004\n  warmup_time: 0.004238128662109375\n\n(func pid=1821) Files already downloaded and verified\n(func pid=1821) Files already downloaded and verified\nResult for train_cifar_dbd80_00005:\n  accuracy: 0.1019\n  date: 2023-03-17_21-29-01\n  done: true\n  experiment_id: bf67837a32e9457489ba78a88a5e9de9\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 2.301889562225342\n  node_ip: 172.17.0.2\n  pid: 1790\n  should_checkpoint: true\n  time_since_restore: 36.047141790390015\n  time_this_iter_s: 36.047141790390015\n  time_total_s: 36.047141790390015\n  timestamp: 1679088541\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00005\n  warmup_time: 0.003386974334716797\n\n== Status ==\nCurrent time: 2023-03-17 21:29:01 (running for 00:06:15.48)\nMemory usage on this node: 2.9/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (3 PENDING, 2 RUNNING, 5 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00005 | RUNNING    | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | PENDING    |                 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) Files already downloaded and verified\n(func pid=1790) Files already downloaded and verified\n== Status ==\nCurrent time: 2023-03-17 21:29:06 (running for 00:06:20.51)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1,  2000] loss: 2.319\n== Status ==\nCurrent time: 2023-03-17 21:29:11 (running for 00:06:25.54)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  2000] loss: 2.305\n== Status ==\nCurrent time: 2023-03-17 21:29:16 (running for 00:06:30.55)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  4000] loss: 1.156\n(func pid=1821) [1,  4000] loss: 1.160\n== Status ==\nCurrent time: 2023-03-17 21:29:21 (running for 00:06:35.57)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:29:26 (running for 00:06:40.58)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  6000] loss: 0.768\n(func pid=1821) [1,  6000] loss: 0.774\n== Status ==\nCurrent time: 2023-03-17 21:29:31 (running for 00:06:45.60)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:29:36 (running for 00:06:50.62)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1,  8000] loss: 0.575\n== Status ==\nCurrent time: 2023-03-17 21:29:41 (running for 00:06:55.64)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1,  8000] loss: 0.580\n== Status ==\nCurrent time: 2023-03-17 21:29:46 (running for 00:07:00.66)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 10000] loss: 0.459\n== Status ==\nCurrent time: 2023-03-17 21:29:51 (running for 00:07:05.68)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1, 10000] loss: 0.465\n(func pid=1790) [1, 12000] loss: 0.393\n== Status ==\nCurrent time: 2023-03-17 21:29:56 (running for 00:07:10.70)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=6\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 PENDING, 2 RUNNING, 6 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00006 | RUNNING    | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   |         |            |                      |\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | PENDING    |                 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00006:\n  accuracy: 0.0967\n  date: 2023-03-17_21-30-00\n  done: true\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 2.32388559012413\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 62.30062174797058\n  time_this_iter_s: 62.30062174797058\n  time_total_s: 62.30062174797058\n  timestamp: 1679088600\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00006\n  warmup_time: 0.004238128662109375\n\n(func pid=1821) Files already downloaded and verified\n(func pid=1821) Files already downloaded and verified\n== Status ==\nCurrent time: 2023-03-17 21:30:05 (running for 00:07:19.68)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2863188765525817\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 14000] loss: 0.331\n== Status ==\nCurrent time: 2023-03-17 21:30:10 (running for 00:07:24.71)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2863188765525817\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:30:15 (running for 00:07:29.74)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2863188765525817\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 16000] loss: 0.289\n(func pid=1821) [1,  2000] loss: 2.211\n== Status ==\nCurrent time: 2023-03-17 21:30:20 (running for 00:07:34.77)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2863188765525817\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:30:25 (running for 00:07:39.79)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2863188765525817\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 |         |            |                      |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00008:\n  accuracy: 0.299\n  date: 2023-03-17_21-30-25\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 1.9318199354171752\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 25.344778776168823\n  time_this_iter_s: 25.344778776168823\n  time_total_s: 25.344778776168823\n  timestamp: 1679088625\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00008\n  warmup_time: 0.004238128662109375\n\n(func pid=1790) [1, 18000] loss: 0.257\n== Status ==\nCurrent time: 2023-03-17 21:30:30 (running for 00:07:45.02)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.93182 |     0.299  |                    1 |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:30:35 (running for 00:07:50.06)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.93182 |     0.299  |                    1 |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1790) [1, 20000] loss: 0.232\n== Status ==\nCurrent time: 2023-03-17 21:30:40 (running for 00:07:55.08)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.93182 |     0.299  |                    1 |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [2,  2000] loss: 1.797\n== Status ==\nCurrent time: 2023-03-17 21:30:45 (running for 00:08:00.10)\nMemory usage on this node: 3.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=7\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.4856879903024063 | Iter 1.000: -2.2459521085262297\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 PENDING, 2 RUNNING, 7 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    |         |            |                      |\n| train_cifar_dbd80_00008 | RUNNING    | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.93182 |     0.299  |                    1 |\n| train_cifar_dbd80_00009 | PENDING    |                 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00008:\n  accuracy: 0.4054\n  date: 2023-03-17_21-30-49\n  done: true\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 2\n  loss: 1.625212480354309\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 49.30907487869263\n  time_this_iter_s: 23.964296102523804\n  time_total_s: 49.30907487869263\n  timestamp: 1679088649\n  timesteps_since_restore: 0\n  training_iteration: 2\n  trial_id: dbd80_00008\n  warmup_time: 0.004238128662109375\n\n(func pid=1821) Files already downloaded and verified\n(func pid=1821) Files already downloaded and verified\nResult for train_cifar_dbd80_00007:\n  accuracy: 0.0976\n  date: 2023-03-17_21-30-53\n  done: true\n  experiment_id: bf67837a32e9457489ba78a88a5e9de9\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 2.329462006855011\n  node_ip: 172.17.0.2\n  pid: 1790\n  should_checkpoint: true\n  time_since_restore: 112.19222688674927\n  time_this_iter_s: 112.19222688674927\n  time_total_s: 112.19222688674927\n  timestamp: 1679088653\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00007\n  warmup_time: 0.003386974334716797\n\n== Status ==\nCurrent time: 2023-03-17 21:30:53 (running for 00:08:07.71)\nMemory usage on this node: 2.9/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2863188765525817\nResources requested: 4.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (2 RUNNING, 8 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00007 | RUNNING    | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:30:58 (running for 00:08:12.74)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2863188765525817\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1,  2000] loss: 2.274\n== Status ==\nCurrent time: 2023-03-17 21:31:03 (running for 00:08:17.75)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2863188765525817\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:31:08 (running for 00:08:22.78)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2863188765525817\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [1,  4000] loss: 1.043\n== Status ==\nCurrent time: 2023-03-17 21:31:13 (running for 00:08:27.79)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2863188765525817\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 |         |            |                      |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00009:\n  accuracy: 0.2988\n  date: 2023-03-17_21-31-18\n  done: false\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 1\n  loss: 1.888344468307495\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 28.592221975326538\n  time_this_iter_s: 28.592221975326538\n  time_total_s: 28.592221975326538\n  timestamp: 1679088678\n  timesteps_since_restore: 0\n  training_iteration: 1\n  trial_id: dbd80_00009\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:31:23 (running for 00:08:37.61)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2459521085262297\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 | 1.88834 |     0.2988 |                    1 |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [2,  2000] loss: 1.755\n== Status ==\nCurrent time: 2023-03-17 21:31:28 (running for 00:08:42.61)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2459521085262297\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 | 1.88834 |     0.2988 |                    1 |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:31:33 (running for 00:08:47.63)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2459521085262297\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 | 1.88834 |     0.2988 |                    1 |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n(func pid=1821) [2,  4000] loss: 0.801\n== Status ==\nCurrent time: 2023-03-17 21:31:38 (running for 00:08:52.64)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2459521085262297\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 | 1.88834 |     0.2988 |                    1 |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n== Status ==\nCurrent time: 2023-03-17 21:31:43 (running for 00:08:57.66)\nMemory usage on this node: 2.4/14.7 GiB\nUsing AsyncHyperBand: num_stopped=9\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5665831570994109 | Iter 1.000: -2.2459521085262297\nResources requested: 2.0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (1 RUNNING, 9 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00009 | RUNNING    | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 | 1.88834 |     0.2988 |                    1 |\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\nResult for train_cifar_dbd80_00009:\n  accuracy: 0.4218\n  date: 2023-03-17_21-31-44\n  done: true\n  experiment_id: 866233ab7ace4bd6aa755e4ffbf1fe72\n  hostname: 6f9535515a81\n  iterations_since_restore: 2\n  loss: 1.5962100546360016\n  node_ip: 172.17.0.2\n  pid: 1821\n  should_checkpoint: true\n  time_since_restore: 55.290165424346924\n  time_this_iter_s: 26.697943449020386\n  time_total_s: 55.290165424346924\n  timestamp: 1679088704\n  timesteps_since_restore: 0\n  training_iteration: 2\n  trial_id: dbd80_00009\n  warmup_time: 0.004238128662109375\n\n== Status ==\nCurrent time: 2023-03-17 21:31:44 (running for 00:08:59.30)\nMemory usage on this node: 2.0/14.7 GiB\nUsing AsyncHyperBand: num_stopped=10\nBracket: Iter 8.000: -1.2344939463615416 | Iter 4.000: -1.337672945547104 | Iter 2.000: -1.5813966058677063 | Iter 1.000: -2.2459521085262297\nResources requested: 0/4 CPUs, 0/2 GPUs, 0.0/8.11 GiB heap, 0.0/4.05 GiB objects (0.0/1.0 accelerator_type:T4)\nResult logdir: /var/lib/jenkins/ray_results/train_cifar_2023-03-17_21-22-45\nNumber of trials: 10/10 (10 TERMINATED)\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n| Trial name              | status     | loc             |   batch_size |   l1 |   l2 |          lr |    loss |   accuracy |   training_iteration |\n|-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------|\n| train_cifar_dbd80_00000 | TERMINATED | 172.17.0.2:1790 |            8 |    4 |  128 | 0.0144126   | 2.20559 |     0.1911 |                    1 |\n| train_cifar_dbd80_00001 | TERMINATED | 172.17.0.2:1821 |           16 |   64 |   64 | 0.00689867  | 1.25925 |     0.5834 |                   10 |\n| train_cifar_dbd80_00002 | TERMINATED | 172.17.0.2:1790 |            4 |    8 |    8 | 0.000123211 | 2.28632 |     0.1028 |                    1 |\n| train_cifar_dbd80_00003 | TERMINATED | 172.17.0.2:1790 |            2 |   16 |   16 | 0.00191446  | 1.56658 |     0.443  |                    2 |\n| train_cifar_dbd80_00004 | TERMINATED | 172.17.0.2:1821 |            2 |  128 |    8 | 0.0936523   | 2.46398 |     0.0955 |                    1 |\n| train_cifar_dbd80_00005 | TERMINATED | 172.17.0.2:1790 |            8 |   16 |  128 | 0.00010529  | 2.30189 |     0.1019 |                    1 |\n| train_cifar_dbd80_00006 | TERMINATED | 172.17.0.2:1821 |            4 |   64 |   16 | 0.0328736   | 2.32389 |     0.0967 |                    1 |\n| train_cifar_dbd80_00007 | TERMINATED | 172.17.0.2:1790 |            2 |    8 |   16 | 0.012856    | 2.32946 |     0.0976 |                    1 |\n| train_cifar_dbd80_00008 | TERMINATED | 172.17.0.2:1821 |           16 |  128 |  128 | 0.000729045 | 1.62521 |     0.4054 |                    2 |\n| train_cifar_dbd80_00009 | TERMINATED | 172.17.0.2:1821 |            8 |   16 |   64 | 0.000598045 | 1.59621 |     0.4218 |                    2 |\n+-------------------------+------------+-----------------+--------------+------+------+-------------+---------+------------+----------------------+\n\n\n2023-03-17 21:31:45,037 INFO tune.py:747 -- Total run time: 539.85 seconds (539.29 seconds for the tuning loop).\nBest trial config: {'l1': 64, 'l2': 64, 'lr': 0.0068986736602156895, 'batch_size': 16}\nBest trial final validation loss: 1.2592490710258484\nBest trial final validation accuracy: 0.5834\nFiles already downloaded and verified\nFiles already downloaded and verified\nBest trial test set accuracy: 0.5803",
                        "code"
                    ],
                    [
                        "If you run the code, an example output could look like this:",
                        "markdown"
                    ],
                    [
                        "Number of trials: 10 (10 TERMINATED)\n+-----+------+------+-------------+--------------+---------+------------+--------------------+\n| ... |   l1 |   l2 |          lr |   batch_size |    loss |   accuracy | training_iteration |\n|-----+------+------+-------------+--------------+---------+------------+--------------------|\n| ... |   64 |    4 | 0.00011629  |            2 | 1.87273 |     0.244  |                  2 |\n| ... |   32 |   64 | 0.000339763 |            8 | 1.23603 |     0.567  |                  8 |\n| ... |    8 |   16 | 0.00276249  |           16 | 1.1815  |     0.5836 |                 10 |\n| ... |    4 |   64 | 0.000648721 |            4 | 1.31131 |     0.5224 |                  8 |\n| ... |   32 |   16 | 0.000340753 |            8 | 1.26454 |     0.5444 |                  8 |\n| ... |    8 |    4 | 0.000699775 |            8 | 1.99594 |     0.1983 |                  2 |\n| ... |  256 |    8 | 0.0839654   |           16 | 2.3119  |     0.0993 |                  1 |\n| ... |   16 |  128 | 0.0758154   |           16 | 2.33575 |     0.1327 |                  1 |\n| ... |   16 |    8 | 0.0763312   |           16 | 2.31129 |     0.1042 |                  4 |\n| ... |  128 |   16 | 0.000124903 |            4 | 2.26917 |     0.1945 |                  1 |\n+-----+------+------+-------------+--------------+---------+------------+--------------------+\n\n\nBest trial config: {'l1': 8, 'l2': 16, 'lr': 0.00276249, 'batch_size': 16, 'data_dir': '...'}\nBest trial final validation loss: 1.181501\nBest trial final validation accuracy: 0.5836\nBest trial test set accuracy: 0.5806",
                        "code"
                    ],
                    [
                        "Most trials have been stopped early in order to avoid wasting resources.\nThe best performing trial achieved a validation accuracy of about 58%, which could\nbe confirmed on the test set.",
                        "markdown"
                    ],
                    [
                        "So that\u2019s it! You can now tune the parameters of your PyTorch models.",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 9 minutes  25.490 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Optimizing Vision Transformer Model for Deployment": [
            [
                ",",
                "markdown"
            ],
            [
                "Vision Transformer models apply the cutting-edge attention-based\ntransformer models, introduced in Natural Language Processing to achieve\nall kinds of the state of the art (SOTA) results, to Computer Vision\ntasks. Facebook Data-efficient Image Transformers \nis a Vision Transformer model trained on ImageNet for image\nclassification.",
                "markdown"
            ],
            [
                "In this tutorial, we will first cover what DeiT is and how to use it,\nthen go through the complete steps of scripting, quantizing, optimizing,\nand using the model in iOS and Android apps. We will also compare the\nperformance of quantized, optimized and non-quantized, non-optimized\nmodels, and show the benefits of applying quantization and optimization\nto the model along the steps.",
                "markdown"
            ],
            {
                "What is DeiT": [
                    [
                        "Convolutional Neural Networks (CNNs) have been the main models for image\nclassification since deep learning took off in 2012, but CNNs typically\nrequire hundreds of millions of images for training to achieve the\nSOTAresults. DeiT is a vision transformer model that requires a lot less\ndata and computing resources for training to compete with the leading\nCNNs in performing image classification, which is made possible by two\nkey components of of DeiT:",
                        "markdown"
                    ],
                    [
                        "Data augmentation that simulates training on a much larger dataset;",
                        "markdown"
                    ],
                    [
                        "Native distillation that allows the transformer network to learn from\na CNN\u2019s output.",
                        "markdown"
                    ],
                    [
                        "DeiT shows that Transformers can be successfully applied to computer\nvision tasks, with limited access to data and resources. For more\ndetails on DeiT, see the \nand .",
                        "markdown"
                    ]
                ]
            },
            {
                "Classifying Images with DeiT": [
                    [
                        "Follow the README at the DeiT repo for detailed information on how to\nclassify images using DeiT, or for a quick test, first install the\nrequired packages:",
                        "markdown"
                    ],
                    [
                        "# pip install torch torchvision timm pandas requests",
                        "code"
                    ],
                    [
                        "To run in Google Colab, uncomment the following line:",
                        "markdown"
                    ],
                    [
                        "# !pip install timm pandas requests",
                        "code"
                    ],
                    [
                        "then run the script below:",
                        "markdown"
                    ],
                    [
                        "from PIL import Image\nimport torch\nimport timm\nimport requests\nimport torchvision.transforms as transforms\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\nprint(torch.__version__)\n# should be 1.8.0\n\n\nmodel = ('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n()\n\n = ([\n    (256, interpolation=3),\n    (224),\n    (),\n    (IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n])\n\n = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\n = ()[None,]\n = model()\n = ()\nprint(.item())",
                        "code"
                    ],
                    [
                        "2.0.0+cu117\nDownloading: \"https://github.com/facebookresearch/deit/zipball/main\" to /var/lib/jenkins/.cache/torch/hub/main.zip\nDownloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /var/lib/jenkins/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n\n  0%|          | 0.00/330M [00:00&lt;?, ?B/s]\n  0%|          | 56.0k/330M [00:00&lt;10:58, 526kB/s]\n  0%|          | 256k/330M [00:00&lt;04:20, 1.33MB/s]\n  0%|          | 1.13M/330M [00:00&lt;01:15, 4.57MB/s]\n  1%|1         | 4.59M/330M [00:00&lt;00:21, 16.0MB/s]\n  4%|3         | 12.4M/330M [00:00&lt;00:09, 36.3MB/s]\n  6%|6         | 19.9M/330M [00:00&lt;00:06, 47.0MB/s]\n  8%|8         | 27.5M/330M [00:00&lt;00:05, 55.1MB/s]\n 11%|#         | 35.3M/330M [00:00&lt;00:05, 59.8MB/s]\n 13%|#2        | 42.9M/330M [00:01&lt;00:04, 62.9MB/s]\n 15%|#5        | 50.7M/330M [00:01&lt;00:04, 64.7MB/s]\n 18%|#7        | 58.4M/330M [00:01&lt;00:04, 67.6MB/s]\n 20%|#9        | 65.9M/330M [00:01&lt;00:04, 68.2MB/s]\n 22%|##2       | 73.6M/330M [00:01&lt;00:03, 68.7MB/s]\n 25%|##4       | 81.4M/330M [00:01&lt;00:03, 69.3MB/s]\n 27%|##6       | 89.0M/330M [00:01&lt;00:03, 68.8MB/s]\n 29%|##9       | 96.5M/330M [00:01&lt;00:03, 66.1MB/s]\n 32%|###1      | 104M/330M [00:01&lt;00:03, 66.9MB/s]\n 34%|###3      | 112M/330M [00:02&lt;00:03, 68.4MB/s]\n 36%|###5      | 118M/330M [00:02&lt;00:03, 65.9MB/s]\n 38%|###8      | 126M/330M [00:02&lt;00:03, 66.8MB/s]\n 41%|####      | 134M/330M [00:02&lt;00:03, 68.1MB/s]\n 43%|####2     | 142M/330M [00:02&lt;00:02, 67.2MB/s]\n 45%|####5     | 149M/330M [00:02&lt;00:02, 67.7MB/s]\n 47%|####7     | 156M/330M [00:02&lt;00:02, 65.5MB/s]\n 49%|####9     | 162M/330M [00:02&lt;00:03, 54.0MB/s]\n 51%|#####     | 168M/330M [00:03&lt;00:03, 54.9MB/s]\n 53%|#####3    | 176M/330M [00:03&lt;00:02, 59.3MB/s]\n 55%|#####5    | 183M/330M [00:03&lt;00:02, 62.6MB/s]\n 58%|#####7    | 191M/330M [00:03&lt;00:02, 63.2MB/s]\n 60%|#####9    | 198M/330M [00:03&lt;00:02, 65.3MB/s]\n 62%|######1   | 204M/330M [00:03&lt;00:02, 60.0MB/s]\n 65%|######4   | 213M/330M [00:03&lt;00:01, 66.8MB/s]\n 67%|######6   | 221M/330M [00:03&lt;00:01, 68.0MB/s]\n 69%|######9   | 229M/330M [00:03&lt;00:01, 69.0MB/s]\n 71%|#######1  | 236M/330M [00:04&lt;00:01, 69.2MB/s]\n 74%|#######3  | 244M/330M [00:04&lt;00:01, 67.8MB/s]\n 76%|#######5  | 251M/330M [00:04&lt;00:01, 67.4MB/s]\n 78%|#######8  | 258M/330M [00:04&lt;00:01, 67.6MB/s]\n 81%|########  | 266M/330M [00:04&lt;00:01, 67.4MB/s]\n 83%|########2 | 274M/330M [00:04&lt;00:00, 68.4MB/s]\n 85%|########5 | 281M/330M [00:04&lt;00:00, 68.4MB/s]\n 87%|########7 | 289M/330M [00:04&lt;00:00, 69.6MB/s]\n 90%|########9 | 297M/330M [00:05&lt;00:00, 69.9MB/s]\n 92%|#########2| 304M/330M [00:05&lt;00:00, 69.0MB/s]\n 94%|#########4| 312M/330M [00:05&lt;00:00, 69.6MB/s]\n 97%|#########6| 319M/330M [00:05&lt;00:00, 69.4MB/s]\n 99%|#########9| 327M/330M [00:05&lt;00:00, 70.3MB/s]\n100%|##########| 330M/330M [00:05&lt;00:00, 63.0MB/s]\n269",
                        "code"
                    ],
                    [
                        "The output should be 269, which, according to the ImageNet list of class\nindex to , maps to \u2018timber\nwolf, grey wolf, gray wolf, Canis lupus\u2019.",
                        "markdown"
                    ],
                    [
                        "Now that we have verified that we can use the DeiT model to classify\nimages, let\u2019s see how to modify the model so it can run on iOS and\nAndroid apps.",
                        "markdown"
                    ]
                ]
            },
            {
                "Scripting DeiT": [
                    [
                        "To use the model on mobile, we first need to script the\nmodel. See the  for a\nquick overview. Run the code below to convert the DeiT model used in the\nprevious step to the TorchScript format that can run on mobile.",
                        "markdown"
                    ],
                    [
                        "model = ('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n()\nscripted_model = (model)\n(\"fbdeit_scripted.pt\")",
                        "code"
                    ],
                    [
                        "Using cache found in /var/lib/jenkins/.cache/torch/hub/facebookresearch_deit_main",
                        "code"
                    ],
                    [
                        "The scripted model file fbdeit_scripted.pt of size about 346MB is\ngenerated.",
                        "markdown"
                    ]
                ]
            },
            {
                "Quantizing DeiT": [
                    [
                        "To reduce the trained model size significantly while\nkeeping the inference accuracy about the same, quantization can be\napplied to the model. Thanks to the transformer model used in DeiT, we\ncan easily apply dynamic-quantization to the model, because dynamic\nquantization works best for LSTM and transformer models (see \nfor more details).",
                        "markdown"
                    ],
                    [
                        "Now run the code below:",
                        "markdown"
                    ],
                    [
                        "# Use 'x86' for server inference (the old 'fbgemm' is still available but 'x86' is the recommended default) and 'qnnpack' for mobile inference.\nbackend = \"x86\" # replaced with qnnpack causing much worse inference speed for quantized model on this notebook\n = torch.quantization.get_default_qconfig(backend)\ntorch.backends.quantized.engine = backend\n\nquantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={}, dtype=)\nscripted_quantized_model = (quantized_model)\n(\"fbdeit_scripted_quantized.pt\")",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning:\n\nPlease use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.",
                        "code"
                    ],
                    [
                        "This generates the scripted and quantized version of the model\nfbdeit_quantized_scripted.pt, with size about 89MB, a 74% reduction of\nthe non-quantized model size of 346MB!",
                        "markdown"
                    ],
                    [
                        "You can use the scripted_quantized_model to generate the same\ninference result:",
                        "markdown"
                    ],
                    [
                        " = scripted_quantized_model()\n = ()\nprint(.item())\n# The same output 269 should be printed",
                        "code"
                    ],
                    [
                        "269",
                        "code"
                    ]
                ]
            },
            {
                "Optimizing DeiT": [
                    [
                        "The final step before using the quantized and scripted\nmodel on mobile is to optimize it:",
                        "markdown"
                    ],
                    [
                        "from torch.utils.mobile_optimizer import \noptimized_scripted_quantized_model = (scripted_quantized_model)\n(\"fbdeit_optimized_scripted_quantized.pt\")",
                        "code"
                    ],
                    [
                        "The generated fbdeit_optimized_scripted_quantized.pt file has about the\nsame size as the quantized, scripted, but non-optimized model. The\ninference result remains the same.",
                        "markdown"
                    ],
                    [
                        " = optimized_scripted_quantized_model()\n = ()\nprint(.item())\n# Again, the same output 269 should be printed",
                        "code"
                    ],
                    [
                        "269",
                        "code"
                    ]
                ]
            },
            {
                "Using Lite Interpreter": [
                    [
                        "To see how much model size reduction and inference speed up the Lite\nInterpreter can result in, let\u2019s create the lite version of the model.",
                        "markdown"
                    ],
                    [
                        "optimized_scripted_quantized_model._save_for_lite_interpreter(\"fbdeit_optimized_scripted_quantized_lite.ptl\")\nptl = (\"fbdeit_optimized_scripted_quantized_lite.ptl\")",
                        "code"
                    ],
                    [
                        "Although the lite model size is comparable to the non-lite version, when\nrunning the lite version on mobile, the inference speed up is expected.",
                        "markdown"
                    ]
                ]
            },
            {
                "Comparing Inference Speed": [
                    [
                        "To see how the inference speed differs for the four models - the\noriginal model, the scripted model, the quantized-and-scripted model,\nthe optimized-quantized-and-scripted model - run the code below:",
                        "markdown"
                    ],
                    [
                        "with (use_cuda=False) as :\n     = model()\nwith (use_cuda=False) as :\n     = scripted_model()\nwith (use_cuda=False) as :\n     = scripted_quantized_model()\nwith (use_cuda=False) as :\n     = optimized_scripted_quantized_model()\nwith (use_cuda=False) as :\n     = ptl()\n\nprint(\"original model: {:.2f}ms\".format(/1000))\nprint(\"scripted model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized model: {:.2f}ms\".format(/1000))\nprint(\"scripted &amp; quantized &amp; optimized model: {:.2f}ms\".format(/1000))\nprint(\"lite model: {:.2f}ms\".format(/1000))",
                        "code"
                    ],
                    [
                        "original model: 309.06ms\nscripted model: 335.36ms\nscripted &amp; quantized model: 240.60ms\nscripted &amp; quantized &amp; optimized model: 201.37ms\nlite model: 206.63ms",
                        "code"
                    ],
                    [
                        "The results running on a Google Colab are:",
                        "markdown"
                    ],
                    [
                        "original model: 1236.69ms\nscripted model: 1226.72ms\nscripted &amp; quantized model: 593.19ms\nscripted &amp; quantized &amp; optimized model: 598.01ms\nlite model: 600.72ms",
                        "code"
                    ],
                    [
                        "The following results summarize the inference time taken by each model\nand the percentage reduction of each model relative to the original\nmodel.",
                        "markdown"
                    ],
                    [
                        "import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Model': ['original model','scripted model', 'scripted &amp; quantized model', 'scripted &amp; quantized &amp; optimized model', 'lite model']})\ndf = pd.concat([df, pd.DataFrame([\n    [\"{:.2f}ms\".format(/1000), \"0%\"],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)],\n    [\"{:.2f}ms\".format(/1000),\n     \"{:.2f}%\".format((-)/*100)]],\n    columns=['Inference Time', 'Reduction'])], axis=1)\n\nprint(df)\n\n\"\"\"\n        Model                             Inference Time    Reduction\n0   original model                             1236.69ms           0%\n1   scripted model                             1226.72ms        0.81%\n2   scripted &amp; quantized model                  593.19ms       52.03%\n3   scripted &amp; quantized &amp; optimized model      598.01ms       51.64%\n4   lite model                                  600.72ms       51.43%\n\"\"\"",
                        "code"
                    ],
                    [
                        "                                    Model Inference Time Reduction\n0                          original model       309.06ms        0%\n1                          scripted model       335.36ms    -8.51%\n2              scripted &amp; quantized model       240.60ms    22.15%\n3  scripted &amp; quantized &amp; optimized model       201.37ms    34.84%\n4                              lite model       206.63ms    33.14%\n\n'\\n        Model                             Inference Time    Reduction\\n0\\toriginal model                             1236.69ms           0%\\n1\\tscripted model                             1226.72ms        0.81%\\n2\\tscripted &amp; quantized model                  593.19ms       52.03%\\n3\\tscripted &amp; quantized &amp; optimized model      598.01ms       51.64%\\n4\\tlite model                                  600.72ms       51.43%\\n'",
                        "code"
                    ],
                    {
                        "Learn More": [
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "<strong>Total running time of the script:</strong> ( 0 minutes  26.562 seconds)",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ],
                            [
                                "",
                                "markdown"
                            ]
                        ]
                    }
                ]
            }
        ],
        "Parametrizations Tutorial": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "Regularizing deep-learning models is a surprisingly challenging task.\nClassical techniques such as penalty methods often fall short when applied\non deep models due to the complexity of the function being optimized.\nThis is particularly problematic when working with ill-conditioned models.\nExamples of these are RNNs trained on long sequences and GANs. A number\nof techniques have been proposed in recent years to regularize these\nmodels and improve their convergence. On recurrent models, it has been\nproposed to control the singular values of the recurrent kernel for the\nRNN to be well-conditioned. This can be achieved, for example, by making\nthe recurrent kernel .\nAnother way to regularize recurrent models is via\n\u201c\u201d.\nThis approach proposes to decouple the learning of the parameters from the\nlearning of their norms.  To do so, the parameter is divided by its\n\nand a separate parameter encoding its norm is learnt.\nA similar regularization was proposed for GANs under the name of\n\u201c\u201d. This method\ncontrols the Lipschitz constant of the network by dividing its parameters by\ntheir ,\nrather than their Frobenius norm.",
                "markdown"
            ],
            [
                "All these methods have a common pattern: they all transform a parameter\nin an appropriate way before using it. In the first case, they make it orthogonal by\nusing a function that maps matrices to orthogonal matrices. In the case of weight\nand spectral normalization, they divide the original parameter by its norm.",
                "markdown"
            ],
            [
                "More generally, all these examples use a function to put extra structure on the parameters.\nIn other words, they use a function to constrain the parameters.",
                "markdown"
            ],
            [
                "In this tutorial, you will learn how to implement and use this pattern to put\nconstraints on your model. Doing so is as easy as writing your own nn.Module.",
                "markdown"
            ],
            [
                "Requirements: torch&gt;=1.9.0",
                "markdown"
            ],
            {
                "Implementing parametrizations by hand": [
                    [
                        "Assume that we want to have a square linear layer with symmetric weights, that is,\nwith weights X such that X = X\u1d40. One way to do so is\nto copy the upper-triangular part of the matrix into its lower-triangular part",
                        "markdown"
                    ],
                    [
                        "import torch\nimport torch.nn as nn\nimport torch.nn.utils.parametrize as parametrize\n\ndef symmetric(X):\n    return X.triu() + X.triu(1).transpose(-1, -2)\n\nX = (3, 3)\nA = symmetric(X)\nassert (A, A.T)  # A is symmetric\nprint(A)                       # Quick visual check",
                        "code"
                    ],
                    [
                        "We can then use this idea to implement a linear layer with symmetric weights",
                        "markdown"
                    ],
                    [
                        "class LinearSymmetric():\n    def __init__(self, n_features):\n        super().__init__()\n        self.weight = nn.Parameter((n_features, n_features))\n\n    def forward(self, x):\n        A = symmetric(self.weight)\n        return x @ A",
                        "code"
                    ],
                    [
                        "The layer can be then used as a regular linear layer",
                        "markdown"
                    ],
                    [
                        "layer = LinearSymmetric(3)\nout = layer((8, 3))",
                        "code"
                    ],
                    [
                        "This implementation, although correct and self-contained, presents a number of problems:",
                        "markdown"
                    ],
                    [
                        "It reimplements the layer. We had to implement the linear layer as x @ A. This is\nnot very problematic for a linear layer, but imagine having to reimplement a CNN or a\nTransformer\u2026",
                        "markdown"
                    ],
                    [
                        "It does not separate the layer and the parametrization.  If the parametrization were\nmore difficult, we would have to rewrite its code for each layer that we want to use it\nin.",
                        "markdown"
                    ],
                    [
                        "It recomputes the parametrization everytime we use the layer. If we use the layer\nseveral times during the forward pass, (imagine the recurrent kernel of an RNN), it\nwould compute the same A every time that the layer is called.",
                        "markdown"
                    ]
                ]
            },
            {
                "Introduction to parametrizations": [
                    [
                        "Parametrizations can solve all these problems as well as others.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s start by reimplementing the code above using torch.nn.utils.parametrize.\nThe only thing that we have to do is to write the parametrization as a regular nn.Module",
                        "markdown"
                    ],
                    [
                        "class Symmetric():\n    def forward(self, X):\n        return X.triu() + X.triu(1).transpose(-1, -2)",
                        "code"
                    ],
                    [
                        "This is all we need to do. Once we have this, we can transform any regular layer into a\nsymmetric layer by doing",
                        "markdown"
                    ],
                    [
                        "layer = (3, 3)\n(layer, \"weight\", Symmetric())",
                        "code"
                    ],
                    [
                        "Now, the matrix of the linear layer is symmetric",
                        "markdown"
                    ],
                    [
                        "A = layer.weight\nassert (A, A.T)  # A is symmetric\nprint(A)                       # Quick visual check",
                        "code"
                    ],
                    [
                        "We can do the same thing with any other layer. For example, we can create a CNN with\n kernels.\nWe use a similar parametrization, copying the upper-triangular part with signs\nreversed into the lower-triangular part",
                        "markdown"
                    ],
                    [
                        "class Skew():\n    def forward(self, X):\n        A = X.triu(1)\n        return A - A.transpose(-1, -2)\n\n\ncnn = (in_channels=5, out_channels=8, kernel_size=3)\n(cnn, \"weight\", Skew())\n# Print a few kernels\nprint(cnn.weight[0, 1])\nprint(cnn.weight[2, 2])",
                        "code"
                    ]
                ]
            },
            {
                "Inspecting a parametrized module": [
                    [
                        "When a module is parametrized, we find that the module has changed in three ways:",
                        "markdown"
                    ],
                    [
                        "model.weight is now a property",
                        "markdown"
                    ],
                    [
                        "It has a new module.parametrizations attribute",
                        "markdown"
                    ],
                    [
                        "The unparametrized weight has been moved to module.parametrizations.weight.original\n\n\n<br/>",
                        "markdown"
                    ],
                    [
                        "After parametrizing weight, layer.weight is turned into a\n.\nThis property computes parametrization(weight) every time we request layer.weight\njust as we did in our implementation of LinearSymmetric above.",
                        "markdown"
                    ],
                    [
                        "Registered parametrizations are stored under a parametrizations attribute within the module.",
                        "markdown"
                    ],
                    [
                        "layer = (3, 3)\nprint(f\"Unparametrized:\\n{layer}\")\n(layer, \"weight\", Symmetric())\nprint(f\"\\nParametrized:\\n{layer}\")",
                        "code"
                    ],
                    [
                        "This parametrizations attribute is an nn.ModuleDict, and it can be accessed as such",
                        "markdown"
                    ],
                    [
                        "print(layer.parametrizations)\nprint(layer.parametrizations.weight)",
                        "code"
                    ],
                    [
                        "Each element of this nn.ModuleDict is a ParametrizationList, which behaves like an\nnn.Sequential. This list will allow us to concatenate parametrizations on one weight.\nSince this is a list, we can access the parametrizations indexing it. Here\u2019s\nwhere our Symmetric parametrization sits",
                        "markdown"
                    ],
                    [
                        "print(layer.parametrizations.weight[0])",
                        "code"
                    ],
                    [
                        "The other thing that we notice is that, if we print the parameters, we see that the\nparameter weight has been moved",
                        "markdown"
                    ],
                    [
                        "print(dict(layer.named_parameters()))",
                        "code"
                    ],
                    [
                        "It now sits under layer.parametrizations.weight.original",
                        "markdown"
                    ],
                    [
                        "print(layer.parametrizations.weight.original)",
                        "code"
                    ],
                    [
                        "Besides these three small differences, the parametrization is doing exactly the same\nas our manual implementation",
                        "markdown"
                    ],
                    [
                        "symmetric = Symmetric()\nweight_orig = layer.parametrizations.weight.original\nprint((layer.weight, symmetric(weight_orig)))",
                        "code"
                    ]
                ]
            },
            {
                "Parametrizations are first-class citizens": [
                    [
                        "Since layer.parametrizations is an nn.ModuleList, it means that the parametrizations\nare properly registered as submodules of the original module. As such, the same rules\nfor registering parameters in a module apply to register a parametrization.\nFor example, if a parametrization has parameters, these will be moved from CPU\nto CUDA when calling model = model.cuda().",
                        "markdown"
                    ]
                ]
            },
            {
                "Caching the value of a parametrization": [
                    [
                        "Parametrizations come with an inbuilt caching system via the context manager\nparametrize.cached()",
                        "markdown"
                    ],
                    [
                        "class NoisyParametrization():\n    def forward(self, X):\n        print(\"Computing the Parametrization\")\n        return X\n\nlayer = (4, 4)\n(layer, \"weight\", NoisyParametrization())\nprint(\"Here, layer.weight is recomputed every time we call it\")\nfoo = layer.weight + layer.weight.T\nbar = layer.weight.sum()\nwith ():\n    print(\"Here, it is computed just the first time layer.weight is called\")\n    foo = layer.weight + layer.weight.T\n    bar = layer.weight.sum()",
                        "code"
                    ]
                ]
            },
            {
                "Concatenating parametrizations": [
                    [
                        "Concatenating two parametrizations is as easy as registering them on the same tensor.\nWe may use this to create more complex parametrizations from simpler ones. For example, the\n\nmaps the skew-symmetric matrices to the orthogonal matrices of positive determinant. We can\nconcatenate Skew and a parametrization that implements the Cayley map to get a layer with\northogonal weights",
                        "markdown"
                    ],
                    [
                        "class CayleyMap():\n    def __init__(self, n):\n        super().__init__()\n        self.register_buffer(\"Id\", (n))\n\n    def forward(self, X):\n        # (I + X)(I - X)^{-1}\n        return torch.solve(self.Id + X, self.Id - X).solution\n\nlayer = (3, 3)\n(layer, \"weight\", Skew())\n(layer, \"weight\", CayleyMap(3))\nX = layer.weight\nprint((X.T @ X, (3)))  # X is orthogonal",
                        "code"
                    ],
                    [
                        "This may also be used to prune a parametrized module, or to reuse parametrizations. For example,\nthe matrix exponential maps the symmetric matrices to the Symmetric Positive Definite (SPD) matrices\nBut the matrix exponential also maps the skew-symmetric matrices to the orthogonal matrices.\nUsing these two facts, we may reuse the parametrizations before to our advantage",
                        "markdown"
                    ],
                    [
                        "class MatrixExponential():\n    def forward(self, X):\n        return (X)\n\nlayer_orthogonal = (3, 3)\n(layer_orthogonal, \"weight\", Skew())\n(layer_orthogonal, \"weight\", MatrixExponential())\nX = layer_orthogonal.weight\nprint((X.T @ X, (3)))         # X is orthogonal\n\nlayer_spd = (3, 3)\n(layer_spd, \"weight\", Symmetric())\n(layer_spd, \"weight\", MatrixExponential())\nX = layer_spd.weight\nprint((X, X.T))                        # X is symmetric\nprint((torch.symeig(X).eigenvalues &gt; 0.).all())  # X is positive definite",
                        "code"
                    ]
                ]
            },
            {
                "Intializing parametrizations": [
                    [
                        "Parametrizations come with a mechanism to initialize them. If we implement a method\nright_inverse with signature",
                        "markdown"
                    ],
                    [
                        "def right_inverse(self, X: Tensor) -&gt; Tensor",
                        "code"
                    ],
                    [
                        "it will be used when assigning to the parametrized tensor.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s upgrade our implementation of the Skew class to support this",
                        "markdown"
                    ],
                    [
                        "class Skew():\n    def forward(self, X):\n        A = X.triu(1)\n        return A - A.transpose(-1, -2)\n\n    def right_inverse(self, A):\n        # We assume that A is skew-symmetric\n        # We take the upper-triangular elements, as these are those used in the forward\n        return A.triu(1)",
                        "code"
                    ],
                    [
                        "We may now initialize a layer that is parametrized with Skew",
                        "markdown"
                    ],
                    [
                        "layer = (3, 3)\n(layer, \"weight\", Skew())\nX = (3, 3)\nX = X - X.T                             # X is now skew-symmetric\nlayer.weight = X                        # Initialize layer.weight to be X\nprint((layer.weight, X))      # layer.weight == X",
                        "code"
                    ],
                    [
                        "This right_inverse works as expected when we concatenate parametrizations.\nTo see this, let\u2019s upgrade the Cayley parametrization to also support being initialized",
                        "markdown"
                    ],
                    [
                        "class CayleyMap():\n    def __init__(self, n):\n        super().__init__()\n        self.register_buffer(\"Id\", (n))\n\n    def forward(self, X):\n        # Assume X skew-symmetric\n        # (I + X)(I - X)^{-1}\n        return torch.solve(self.Id + X, self.Id - X).solution\n\n    def right_inverse(self, A):\n        # Assume A orthogonal\n        # See https://en.wikipedia.org/wiki/Cayley_transform#Matrix_map\n        # (X - I)(X + I)^{-1}\n        return torch.solve(X - self.Id, self.Id + X).solution\n\nlayer_orthogonal = (3, 3)\n(layer_orthogonal, \"weight\", Skew())\n(layer_orthogonal, \"weight\", CayleyMap(3))\n# Sample an orthogonal matrix with positive determinant\nX = (3, 3)\n(X)\nif X.det() &lt; 0.:\n    X[0].neg_()\nlayer_orthogonal.weight = X\nprint((layer_orthogonal.weight, X))  # layer_orthogonal.weight == X",
                        "code"
                    ],
                    [
                        "This initialization step can be written more succinctly as",
                        "markdown"
                    ],
                    [
                        "layer_orthogonal.weight = (layer_orthogonal.weight)",
                        "code"
                    ],
                    [
                        "The name of this method comes from the fact that we would often expect\nthat forward(right_inverse(X)) == X. This is a direct way of rewriting that\nthe forward afer the initalization with value X should return the value X.\nThis constraint is not strongly enforced in practice. In fact, at times, it might be of\ninterest to relax this relation. For example, consider the following implementation\nof a randomized pruning method:",
                        "markdown"
                    ],
                    [
                        "class PruningParametrization():\n    def __init__(self, X, p_drop=0.2):\n        super().__init__()\n        # sample zeros with probability p_drop\n        mask = (X, 1.0 - p_drop)\n        self.mask = (mask)\n\n    def forward(self, X):\n        return X * self.mask\n\n    def right_inverse(self, A):\n        return A",
                        "code"
                    ],
                    [
                        "In this case, it is not true that for every matrix A forward(right_inverse(A)) == A.\nThis is only true when the matrix A has zeros in the same positions as the mask.\nEven then, if we assign a tensor to a pruned parameter, it will comes as no surprise\nthat tensor will be, in fact, pruned",
                        "markdown"
                    ],
                    [
                        "layer = (3, 4)\nX = (layer.weight)\nprint(f\"Initialization matrix:\\n{X}\")\n(layer, \"weight\", PruningParametrization(layer.weight))\nlayer.weight = X\nprint(f\"\\nInitialized weight:\\n{layer.weight}\")",
                        "code"
                    ]
                ]
            },
            {
                "Removing parametrizations": [
                    [
                        "We may remove all the parametrizations from a parameter or a buffer in a module\nby using parametrize.remove_parametrizations()",
                        "markdown"
                    ],
                    [
                        "layer = (3, 3)\nprint(\"Before:\")\nprint(layer)\nprint(layer.weight)\n(layer, \"weight\", Skew())\nprint(\"\\nParametrized:\")\nprint(layer)\nprint(layer.weight)\n(layer, \"weight\")\nprint(\"\\nAfter. Weight has skew-symmetric values but it is unconstrained:\")\nprint(layer)\nprint(layer.weight)",
                        "code"
                    ],
                    [
                        "When removing a parametrization, we may choose to leave the original parameter (i.e. that in\nlayer.parametriations.weight.original) rather than its parametrized version by setting\nthe flag leave_parametrized=False",
                        "markdown"
                    ],
                    [
                        "layer = (3, 3)\nprint(\"Before:\")\nprint(layer)\nprint(layer.weight)\n(layer, \"weight\", Skew())\nprint(\"\\nParametrized:\")\nprint(layer)\nprint(layer.weight)\n(layer, \"weight\", leave_parametrized=False)\nprint(\"\\nAfter. Same as Before:\")\nprint(layer)\nprint(layer.weight)",
                        "code"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Pruning Tutorial": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "State-of-the-art deep learning techniques rely on over-parametrized models\nthat are hard to deploy. On the contrary, biological neural networks are\nknown to use efficient sparse connectivity. Identifying optimal\ntechniques to compress models by reducing the number of parameters in them is\nimportant in order to reduce memory, battery, and hardware consumption without\nsacrificing accuracy. This in turn allows you to deploy lightweight models on device, and guarantee\nprivacy with private on-device computation. On the research front, pruning is\nused to investigate the differences in learning dynamics between\nover-parametrized and under-parametrized networks, to study the role of lucky\nsparse subnetworks and initializations\n(\u201c\u201d) as a destructive\nneural architecture search technique, and more.",
                "markdown"
            ],
            [
                "In this tutorial, you will learn how to use torch.nn.utils.prune to\nsparsify your neural networks, and how to extend it to implement your\nown custom pruning technique.",
                "markdown"
            ],
            {
                "Requirements": [
                    [
                        "\"torch&gt;=1.4.0a0+8e8a5e0\"",
                        "markdown"
                    ],
                    [
                        "import torch\nfrom torch import nn\nimport torch.nn.utils.prune as prune\nimport torch.nn.functional as F",
                        "code"
                    ]
                ]
            },
            {
                "Create a model": [
                    [
                        "In this tutorial, we use the  architecture from\nLeCun et al., 1998.",
                        "markdown"
                    ],
                    [
                        " = (\"cuda\" if () else \"cpu\")\n\nclass LeNet():\n    def __init__(self):\n        super(, self).__init__()\n        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n        self.conv1 = (1, 6, 3)\n        self.conv2 = (6, 16, 3)\n        self.fc1 = (16 * 5 * 5, 120)  # 5x5 image dimension\n        self.fc2 = (120, 84)\n        self.fc3 = (84, 10)\n\n    def forward(self, x):\n        x = ((self.conv1(x)), (2, 2))\n        x = ((self.conv2(x)), 2)\n        x = x.view(-1, int(x.nelement() / x.shape[0]))\n        x = (self.fc1(x))\n        x = (self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nmodel = ().to(=)",
                        "code"
                    ]
                ]
            },
            {
                "Inspect a Module": [
                    [
                        "Let\u2019s inspect the (unpruned) conv1 layer in our LeNet model. It will contain two\nparameters weight and bias, and no buffers, for now.",
                        "markdown"
                    ],
                    [
                        " = \nprint(list(()))",
                        "code"
                    ],
                    [
                        "[('weight', Parameter containing:\ntensor([[[[ 0.1435, -0.0650, -0.0823],\n          [ 0.0881, -0.1054,  0.0788],\n          [-0.0560,  0.2107, -0.1601]]],\n\n\n        [[[-0.2217,  0.1166, -0.1881],\n          [-0.2317,  0.2586,  0.0918],\n          [ 0.0984, -0.1276, -0.1021]]],\n\n\n        [[[ 0.1736, -0.1684,  0.0646],\n          [-0.3216,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0352,  0.3101],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.2093,  0.2674, -0.0505],\n          [-0.2793,  0.1896,  0.1743],\n          [ 0.1594,  0.0905,  0.1410]]],\n\n\n        [[[-0.1667,  0.3219, -0.0257],\n          [ 0.0208, -0.0809,  0.1365],\n          [-0.1979,  0.1987, -0.1775]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\ntensor([-0.3213, -0.1174,  0.1717,  0.2642, -0.3046,  0.3249], device='cuda:0',\n       requires_grad=True))]",
                        "code"
                    ],
                    [
                        "print(list(()))",
                        "code"
                    ],
                    [
                        "[]",
                        "code"
                    ]
                ]
            },
            {
                "Pruning a Module": [
                    [
                        "To prune a module (in this example, the conv1 layer of our LeNet\narchitecture), first select a pruning technique among those available in\ntorch.nn.utils.prune (or\n\nyour own by subclassing\nBasePruningMethod). Then, specify the module and the name of the parameter to\nprune within that module. Finally, using the adequate keyword arguments\nrequired by the selected pruning technique, specify the pruning parameters.",
                        "markdown"
                    ],
                    [
                        "In this example, we will prune at random 30% of the connections in\nthe parameter named weight in the conv1 layer.\nThe module is passed as the first argument to the function; name\nidentifies the parameter within that module using its string identifier; and\namount indicates either the percentage of connections to prune (if it\nis a float between 0. and 1.), or the absolute number of connections to\nprune (if it is a non-negative integer).",
                        "markdown"
                    ],
                    [
                        "(, name=\"weight\", amount=0.3)",
                        "code"
                    ],
                    [
                        "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))",
                        "code"
                    ],
                    [
                        "Pruning acts by removing weight from the parameters and replacing it with\na new parameter called weight_orig (i.e. appending \"_orig\" to the\ninitial parameter name). weight_orig stores the unpruned version of\nthe tensor. The bias was not pruned, so it will remain intact.",
                        "markdown"
                    ],
                    [
                        "print(list(()))",
                        "code"
                    ],
                    [
                        "[('bias', Parameter containing:\ntensor([-0.3213, -0.1174,  0.1717,  0.2642, -0.3046,  0.3249], device='cuda:0',\n       requires_grad=True)), ('weight_orig', Parameter containing:\ntensor([[[[ 0.1435, -0.0650, -0.0823],\n          [ 0.0881, -0.1054,  0.0788],\n          [-0.0560,  0.2107, -0.1601]]],\n\n\n        [[[-0.2217,  0.1166, -0.1881],\n          [-0.2317,  0.2586,  0.0918],\n          [ 0.0984, -0.1276, -0.1021]]],\n\n\n        [[[ 0.1736, -0.1684,  0.0646],\n          [-0.3216,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0352,  0.3101],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.2093,  0.2674, -0.0505],\n          [-0.2793,  0.1896,  0.1743],\n          [ 0.1594,  0.0905,  0.1410]]],\n\n\n        [[[-0.1667,  0.3219, -0.0257],\n          [ 0.0208, -0.0809,  0.1365],\n          [-0.1979,  0.1987, -0.1775]]]], device='cuda:0', requires_grad=True))]",
                        "code"
                    ],
                    [
                        "The pruning mask generated by the pruning technique selected above is saved\nas a module buffer named weight_mask (i.e. appending \"_mask\" to the\ninitial parameter name).",
                        "markdown"
                    ],
                    [
                        "print(list(()))",
                        "code"
                    ],
                    [
                        "[('weight_mask', tensor([[[[0., 1., 0.],\n          [1., 0., 0.],\n          [1., 1., 1.]]],\n\n\n        [[[1., 0., 1.],\n          [1., 1., 0.],\n          [1., 0., 1.]]],\n\n\n        [[[1., 0., 0.],\n          [0., 1., 1.],\n          [1., 1., 1.]]],\n\n\n        [[[1., 0., 0.],\n          [1., 1., 1.],\n          [1., 1., 1.]]],\n\n\n        [[[1., 0., 1.],\n          [1., 1., 1.],\n          [0., 1., 1.]]],\n\n\n        [[[1., 1., 1.],\n          [1., 1., 0.],\n          [1., 1., 0.]]]], device='cuda:0'))]",
                        "code"
                    ],
                    [
                        "For the forward pass to work without modification, the weight attribute\nneeds to exist. The pruning techniques implemented in\ntorch.nn.utils.prune compute the pruned version of the weight (by\ncombining the mask with the original parameter) and store them in the\nattribute weight. Note, this is no longer a parameter of the module,\nit is now simply an attribute.",
                        "markdown"
                    ],
                    [
                        "print()",
                        "code"
                    ],
                    [
                        "tensor([[[[ 0.0000, -0.0650, -0.0000],\n          [ 0.0881, -0.0000,  0.0000],\n          [-0.0560,  0.2107, -0.1601]]],\n\n\n        [[[-0.2217,  0.0000, -0.1881],\n          [-0.2317,  0.2586,  0.0000],\n          [ 0.0984, -0.0000, -0.1021]]],\n\n\n        [[[ 0.1736, -0.0000,  0.0000],\n          [-0.0000,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0000,  0.0000],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.2093,  0.0000, -0.0505],\n          [-0.2793,  0.1896,  0.1743],\n          [ 0.0000,  0.0905,  0.1410]]],\n\n\n        [[[-0.1667,  0.3219, -0.0257],\n          [ 0.0208, -0.0809,  0.0000],\n          [-0.1979,  0.1987, -0.0000]]]], device='cuda:0',\n       grad_fn=&lt;MulBackward0&gt;)",
                        "code"
                    ],
                    [
                        "Finally, pruning is applied prior to each forward pass using PyTorch\u2019s\nforward_pre_hooks. Specifically, when the module is pruned, as we\nhave done here, it will acquire a forward_pre_hook for each parameter\nassociated with it that gets pruned. In this case, since we have so far\nonly pruned the original parameter named weight, only one hook will be\npresent.",
                        "markdown"
                    ],
                    [
                        "print(._forward_pre_hooks)",
                        "code"
                    ],
                    [
                        "OrderedDict([(0, &lt;torch.nn.utils.prune.RandomUnstructured object at 0x7ff5c140bf70&gt;)])",
                        "code"
                    ],
                    [
                        "For completeness, we can now prune the bias too, to see how the\nparameters, buffers, hooks, and attributes of the module change.\nJust for the sake of trying out another pruning technique, here we prune the\n3 smallest entries in the bias by L1 norm, as implemented in the\nl1_unstructured pruning function.",
                        "markdown"
                    ],
                    [
                        "(, name=\"bias\", amount=3)",
                        "code"
                    ],
                    [
                        "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))",
                        "code"
                    ],
                    [
                        "We now expect the named parameters to include both weight_orig (from\nbefore) and bias_orig. The buffers will include weight_mask and\nbias_mask. The pruned versions of the two tensors will exist as\nmodule attributes, and the module will now have two forward_pre_hooks.",
                        "markdown"
                    ],
                    [
                        "print(list(()))",
                        "code"
                    ],
                    [
                        "[('weight_orig', Parameter containing:\ntensor([[[[ 0.1435, -0.0650, -0.0823],\n          [ 0.0881, -0.1054,  0.0788],\n          [-0.0560,  0.2107, -0.1601]]],\n\n\n        [[[-0.2217,  0.1166, -0.1881],\n          [-0.2317,  0.2586,  0.0918],\n          [ 0.0984, -0.1276, -0.1021]]],\n\n\n        [[[ 0.1736, -0.1684,  0.0646],\n          [-0.3216,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0352,  0.3101],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.2093,  0.2674, -0.0505],\n          [-0.2793,  0.1896,  0.1743],\n          [ 0.1594,  0.0905,  0.1410]]],\n\n\n        [[[-0.1667,  0.3219, -0.0257],\n          [ 0.0208, -0.0809,  0.1365],\n          [-0.1979,  0.1987, -0.1775]]]], device='cuda:0', requires_grad=True)), ('bias_orig', Parameter containing:\ntensor([-0.3213, -0.1174,  0.1717,  0.2642, -0.3046,  0.3249], device='cuda:0',\n       requires_grad=True))]",
                        "code"
                    ],
                    [
                        "print(list(()))",
                        "code"
                    ],
                    [
                        "[('weight_mask', tensor([[[[0., 1., 0.],\n          [1., 0., 0.],\n          [1., 1., 1.]]],\n\n\n        [[[1., 0., 1.],\n          [1., 1., 0.],\n          [1., 0., 1.]]],\n\n\n        [[[1., 0., 0.],\n          [0., 1., 1.],\n          [1., 1., 1.]]],\n\n\n        [[[1., 0., 0.],\n          [1., 1., 1.],\n          [1., 1., 1.]]],\n\n\n        [[[1., 0., 1.],\n          [1., 1., 1.],\n          [0., 1., 1.]]],\n\n\n        [[[1., 1., 1.],\n          [1., 1., 0.],\n          [1., 1., 0.]]]], device='cuda:0')), ('bias_mask', tensor([1., 0., 0., 0., 1., 1.], device='cuda:0'))]",
                        "code"
                    ],
                    [
                        "print()",
                        "code"
                    ],
                    [
                        "tensor([-0.3213, -0.0000,  0.0000,  0.0000, -0.3046,  0.3249], device='cuda:0',\n       grad_fn=&lt;MulBackward0&gt;)",
                        "code"
                    ],
                    [
                        "print(._forward_pre_hooks)",
                        "code"
                    ],
                    [
                        "OrderedDict([(0, &lt;torch.nn.utils.prune.RandomUnstructured object at 0x7ff5c140bf70&gt;), (1, &lt;torch.nn.utils.prune.L1Unstructured object at 0x7ff5c140b760&gt;)])",
                        "code"
                    ]
                ]
            },
            {
                "Iterative Pruning": [
                    [
                        "The same parameter in a module can be pruned multiple times, with the\neffect of the various pruning calls being equal to the combination of the\nvarious masks applied in series.\nThe combination of a new mask with the old mask is handled by the\nPruningContainer\u2019s compute_mask method.",
                        "markdown"
                    ],
                    [
                        "Say, for example, that we now want to further prune module.weight, this\ntime using structured pruning along the 0th axis of the tensor (the 0th axis\ncorresponds to the output channels of the convolutional layer and has\ndimensionality 6 for conv1), based on the channels\u2019 L2 norm. This can be\nachieved using the ln_structured function, with n=2 and dim=0.",
                        "markdown"
                    ],
                    [
                        "(, name=\"weight\", amount=0.5, n=2, dim=0)\n\n# As we can verify, this will zero out all the connections corresponding to\n# 50% (3 out of 6) of the channels, while preserving the action of the\n# previous mask.\nprint()",
                        "code"
                    ],
                    [
                        "tensor([[[[ 0.0000, -0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000]]],\n\n\n        [[[-0.2217,  0.0000, -0.1881],\n          [-0.2317,  0.2586,  0.0000],\n          [ 0.0984, -0.0000, -0.1021]]],\n\n\n        [[[ 0.1736, -0.0000,  0.0000],\n          [-0.0000,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0000,  0.0000],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.0000,  0.0000, -0.0000],\n          [-0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000]]],\n\n\n        [[[-0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000]]]], device='cuda:0',\n       grad_fn=&lt;MulBackward0&gt;)",
                        "code"
                    ],
                    [
                        "The corresponding hook will now be of type\ntorch.nn.utils.prune.PruningContainer, and will store the history of\npruning applied to the weight parameter.",
                        "markdown"
                    ],
                    [
                        "for  in ._forward_pre_hooks.values():\n    if ._tensor_name == \"weight\":  # select out the correct hook\n        break\n\nprint(list())  # pruning history in the container",
                        "code"
                    ],
                    [
                        "[&lt;torch.nn.utils.prune.RandomUnstructured object at 0x7ff5c140bf70&gt;, &lt;torch.nn.utils.prune.LnStructured object at 0x7ff5c140bfd0&gt;]",
                        "code"
                    ]
                ]
            },
            {
                "Serializing a pruned model": [
                    [
                        "All relevant tensors, including the mask buffers and the original parameters\nused to compute the pruned tensors are stored in the model\u2019s state_dict\nand can therefore be easily serialized and saved, if needed.",
                        "markdown"
                    ],
                    [
                        "print(().keys())",
                        "code"
                    ],
                    [
                        "odict_keys(['conv1.weight_orig', 'conv1.bias_orig', 'conv1.weight_mask', 'conv1.bias_mask', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias'])",
                        "code"
                    ]
                ]
            },
            {
                "Remove pruning re-parametrization": [
                    [
                        "To make the pruning permanent, remove the re-parametrization in terms\nof weight_orig and weight_mask, and remove the forward_pre_hook,\nwe can use the remove functionality from torch.nn.utils.prune.\nNote that this doesn\u2019t undo the pruning, as if it never happened. It simply\nmakes it permanent, instead, by reassigning the parameter weight to the\nmodel parameters, in its pruned version.",
                        "markdown"
                    ],
                    [
                        "Prior to removing the re-parametrization:",
                        "markdown"
                    ],
                    [
                        "print(list(()))",
                        "code"
                    ],
                    [
                        "[('weight_orig', Parameter containing:\ntensor([[[[ 0.1435, -0.0650, -0.0823],\n          [ 0.0881, -0.1054,  0.0788],\n          [-0.0560,  0.2107, -0.1601]]],\n\n\n        [[[-0.2217,  0.1166, -0.1881],\n          [-0.2317,  0.2586,  0.0918],\n          [ 0.0984, -0.1276, -0.1021]]],\n\n\n        [[[ 0.1736, -0.1684,  0.0646],\n          [-0.3216,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0352,  0.3101],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.2093,  0.2674, -0.0505],\n          [-0.2793,  0.1896,  0.1743],\n          [ 0.1594,  0.0905,  0.1410]]],\n\n\n        [[[-0.1667,  0.3219, -0.0257],\n          [ 0.0208, -0.0809,  0.1365],\n          [-0.1979,  0.1987, -0.1775]]]], device='cuda:0', requires_grad=True)), ('bias_orig', Parameter containing:\ntensor([-0.3213, -0.1174,  0.1717,  0.2642, -0.3046,  0.3249], device='cuda:0',\n       requires_grad=True))]",
                        "code"
                    ],
                    [
                        "print(list(()))",
                        "code"
                    ],
                    [
                        "[('weight_mask', tensor([[[[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]]],\n\n\n        [[[1., 0., 1.],\n          [1., 1., 0.],\n          [1., 0., 1.]]],\n\n\n        [[[1., 0., 0.],\n          [0., 1., 1.],\n          [1., 1., 1.]]],\n\n\n        [[[1., 0., 0.],\n          [1., 1., 1.],\n          [1., 1., 1.]]],\n\n\n        [[[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]]],\n\n\n        [[[0., 0., 0.],\n          [0., 0., 0.],\n          [0., 0., 0.]]]], device='cuda:0')), ('bias_mask', tensor([1., 0., 0., 0., 1., 1.], device='cuda:0'))]",
                        "code"
                    ],
                    [
                        "print()",
                        "code"
                    ],
                    [
                        "tensor([[[[ 0.0000, -0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000]]],\n\n\n        [[[-0.2217,  0.0000, -0.1881],\n          [-0.2317,  0.2586,  0.0000],\n          [ 0.0984, -0.0000, -0.1021]]],\n\n\n        [[[ 0.1736, -0.0000,  0.0000],\n          [-0.0000,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0000,  0.0000],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.0000,  0.0000, -0.0000],\n          [-0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000]]],\n\n\n        [[[-0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000]]]], device='cuda:0',\n       grad_fn=&lt;MulBackward0&gt;)",
                        "code"
                    ],
                    [
                        "After removing the re-parametrization:",
                        "markdown"
                    ],
                    [
                        "(, 'weight')\nprint(list(()))",
                        "code"
                    ],
                    [
                        "[('bias_orig', Parameter containing:\ntensor([-0.3213, -0.1174,  0.1717,  0.2642, -0.3046,  0.3249], device='cuda:0',\n       requires_grad=True)), ('weight', Parameter containing:\ntensor([[[[ 0.0000, -0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000]]],\n\n\n        [[[-0.2217,  0.0000, -0.1881],\n          [-0.2317,  0.2586,  0.0000],\n          [ 0.0984, -0.0000, -0.1021]]],\n\n\n        [[[ 0.1736, -0.0000,  0.0000],\n          [-0.0000,  0.2425, -0.3189],\n          [-0.3193, -0.1481,  0.2698]]],\n\n\n        [[[-0.2372, -0.0000,  0.0000],\n          [ 0.2046, -0.1455, -0.1086],\n          [ 0.0302, -0.3148, -0.3269]]],\n\n\n        [[[ 0.0000,  0.0000, -0.0000],\n          [-0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000]]],\n\n\n        [[[-0.0000,  0.0000, -0.0000],\n          [ 0.0000, -0.0000,  0.0000],\n          [-0.0000,  0.0000, -0.0000]]]], device='cuda:0', requires_grad=True))]",
                        "code"
                    ],
                    [
                        "print(list(()))",
                        "code"
                    ],
                    [
                        "[('bias_mask', tensor([1., 0., 0., 0., 1., 1.], device='cuda:0'))]",
                        "code"
                    ]
                ]
            },
            {
                "Pruning multiple parameters in a model": [
                    [
                        "By specifying the desired pruning technique and parameters, we can easily\nprune multiple tensors in a network, perhaps according to their type, as we\nwill see in this example.",
                        "markdown"
                    ],
                    [
                        "new_model = ()\nfor name,  in ():\n    # prune 20% of connections in all 2D-conv layers\n    if isinstance(, ):\n        (, name='weight', amount=0.2)\n    # prune 40% of connections in all linear layers\n    elif isinstance(, ):\n        (, name='weight', amount=0.4)\n\nprint(dict(()).keys())  # to verify that all masks exist",
                        "code"
                    ],
                    [
                        "dict_keys(['conv1.weight_mask', 'conv2.weight_mask', 'fc1.weight_mask', 'fc2.weight_mask', 'fc3.weight_mask'])",
                        "code"
                    ]
                ]
            },
            {
                "Global pruning": [
                    [
                        "So far, we only looked at what is usually referred to as \u201clocal\u201d pruning,\ni.e. the practice of pruning tensors in a model one by one, by\ncomparing the statistics (weight magnitude, activation, gradient, etc.) of\neach entry exclusively to the other entries in that tensor. However, a\ncommon and perhaps more powerful technique is to prune the model all at\nonce, by removing (for example) the lowest 20% of connections across the\nwhole model, instead of removing the lowest 20% of connections in each\nlayer. This is likely to result in different pruning percentages per layer.\nLet\u2019s see how to do that using global_unstructured from\ntorch.nn.utils.prune.",
                        "markdown"
                    ],
                    [
                        "model = ()\n\nparameters_to_prune = (\n    (, 'weight'),\n    (, 'weight'),\n    (, 'weight'),\n    (, 'weight'),\n    (, 'weight'),\n)\n\n(\n    parameters_to_prune,\n    pruning_method=,\n    amount=0.2,\n)",
                        "code"
                    ],
                    [
                        "Now we can check the sparsity induced in every pruned parameter, which will\nnot be equal to 20% in each layer. However, the global sparsity will be\n(approximately) 20%.",
                        "markdown"
                    ],
                    [
                        "print(\n    \"Sparsity in conv1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in conv2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc1.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc2.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Sparsity in fc3.weight: {:.2f}%\".format(\n        100. * float(( == 0))\n        / float(.nelement())\n    )\n)\nprint(\n    \"Global sparsity: {:.2f}%\".format(\n        100. * float(\n            ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n            + ( == 0)\n        )\n        / float(\n            .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n            + .nelement()\n        )\n    )\n)",
                        "code"
                    ],
                    [
                        "Sparsity in conv1.weight: 0.00%\nSparsity in conv2.weight: 7.52%\nSparsity in fc1.weight: 22.14%\nSparsity in fc2.weight: 11.78%\nSparsity in fc3.weight: 10.71%\nGlobal sparsity: 20.00%",
                        "code"
                    ]
                ]
            },
            {
                "Extending torch.nn.utils.prune with custom pruning functions": [
                    [
                        "To implement your own pruning function, you can extend the\nnn.utils.prune module by subclassing the BasePruningMethod\nbase class, the same way all other pruning methods do. The base class\nimplements the following methods for you: __call__, apply_mask,\napply, prune, and remove. Beyond some special cases, you shouldn\u2019t\nhave to reimplement these methods for your new pruning technique.\nYou will, however, have to implement __init__ (the constructor),\nand compute_mask (the instructions on how to compute the mask\nfor the given tensor according to the logic of your pruning\ntechnique). In addition, you will have to specify which type of\npruning this technique implements (supported options are global,\nstructured, and unstructured). This is needed to determine\nhow to combine masks in the case in which pruning is applied\niteratively. In other words, when pruning a pre-pruned parameter,\nthe current prunining techique is expected to act on the unpruned\nportion of the parameter. Specifying the PRUNING_TYPE will\nenable the PruningContainer (which handles the iterative\napplication of pruning masks) to correctly identify the slice of the\nparameter to prune.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s assume, for example, that you want to implement a pruning\ntechnique that prunes every other entry in a tensor (or \u2013 if the\ntensor has previously been pruned \u2013 in the remaining unpruned\nportion of the tensor). This will be of PRUNING_TYPE='unstructured'\nbecause it acts on individual connections in a layer and not on entire\nunits/channels ('structured'), or across different parameters\n('global').",
                        "markdown"
                    ],
                    [
                        "class FooBarPruningMethod():\n    \"\"\"Prune every other entry in a tensor\n    \"\"\"\n    PRUNING_TYPE = 'unstructured'\n\n    def compute_mask(self, t, default_mask):\n        mask = default_mask.clone()\n        mask.view(-1)[::2] = 0\n        return mask",
                        "code"
                    ],
                    [
                        "Now, to apply this to a parameter in an nn.Module, you should\nalso provide a simple function that instantiates the method and\napplies it.",
                        "markdown"
                    ],
                    [
                        "def foobar_unstructured(, name):\n    \"\"\"Prunes tensor corresponding to parameter called `name` in `module`\n    by removing every other entry in the tensors.\n    Modifies module in place (and also return the modified module)\n    by:\n    1) adding a named buffer called `name+'_mask'` corresponding to the\n    binary mask applied to the parameter `name` by the pruning method.\n    The parameter `name` is replaced by its pruned version, while the\n    original (unpruned) parameter is stored in a new parameter named\n    `name+'_orig'`.\n\n    Args:\n        module (nn.Module): module containing the tensor to prune\n        name (string): parameter name within `module` on which pruning\n                will act.\n\n    Returns:\n        module (nn.Module): modified (i.e. pruned) version of the input\n            module\n\n    Examples:\n        &gt;&gt;&gt; m = nn.Linear(3, 4)\n        &gt;&gt;&gt; foobar_unstructured(m, name='bias')\n    \"\"\"\n    (, name)\n    return ",
                        "code"
                    ],
                    [
                        "Let\u2019s try it out!",
                        "markdown"
                    ],
                    [
                        "model = ()\nfoobar_unstructured(, name='bias')\n\nprint()",
                        "code"
                    ],
                    [
                        "tensor([0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])",
                        "code"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  2.603 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "(beta) Dynamic Quantization on an LSTM Word Language Model": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "<strong>Edited by</strong>: ",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "Quantization involves converting the weights and activations of your model from float\nto int, which can result in smaller model size and faster inference with only a small\nhit to accuracy.",
                        "markdown"
                    ],
                    [
                        "In this tutorial, we will apply the easiest form of quantization -\n -\nto an LSTM-based next word-prediction model, closely following the\n\nfrom the PyTorch examples.",
                        "markdown"
                    ],
                    [
                        "# imports\nimport os\nfrom io import open\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F",
                        "code"
                    ]
                ]
            },
            {
                "1. Define the model": [
                    [
                        "Here we define the LSTM model architecture, following the\n\nfrom the word language model example.",
                        "markdown"
                    ],
                    [
                        "class LSTMModel():\n    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n\n    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n        super(, self).__init__()\n        self.drop = (dropout)\n        self.encoder = (ntoken, ninp)\n        self.rnn = (ninp, nhid, nlayers, dropout=dropout)\n        self.decoder = (nhid, ntoken)\n\n        self.init_weights()\n\n        self.nhid = nhid\n        self.nlayers = nlayers\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, input, hidden):\n        emb = self.drop(self.encoder(input))\n        , hidden = self.rnn(emb, hidden)\n         = self.drop()\n        decoded = self.decoder()\n        return decoded, hidden\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters())\n        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n                weight.new_zeros(self.nlayers, bsz, self.nhid))",
                        "code"
                    ]
                ]
            },
            {
                "2. Load in the text data": [
                    [
                        "Next, we load the\n into a <cite>Corpus</cite>,\nagain following the\n\nfrom the word language model example.",
                        "markdown"
                    ],
                    [
                        "class Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n\nclass Corpus(object):\n    def __init__(self, path):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n\n    def tokenize(self, path):\n        \"\"\"Tokenizes a text file.\"\"\"\n        assert os.path.exists(path)\n        # Add words to the dictionary\n        with open(path, 'r', encoding=\"utf8\") as f:\n            for line in f:\n                words = line.split() + ['&lt;eos&gt;']\n                for word in words:\n                    self.dictionary.add_word(word)\n\n        # Tokenize file content\n        with open(path, 'r', encoding=\"utf8\") as f:\n            idss = []\n            for line in f:\n                words = line.split() + ['&lt;eos&gt;']\n                ids = []\n                for word in words:\n                    ids.append(self.dictionary.word2idx[word])\n                idss.append((ids).type())\n            ids = (idss)\n\n        return ids\n\nmodel_data_filepath = 'data/'\n\ncorpus = Corpus(model_data_filepath + 'wikitext-2')",
                        "code"
                    ]
                ]
            },
            {
                "3. Load the pre-trained model": [
                    [
                        "This is a tutorial on dynamic quantization, a quantization technique\nthat is applied after a model has been trained. Therefore, we\u2019ll simply load some\npre-trained weights into this model architecture; these weights were obtained\nby training for five epochs using the default settings in the word language model\nexample.",
                        "markdown"
                    ],
                    [
                        "ntokens = len(corpus.dictionary)\n\nmodel = (\n    ntoken = ntokens,\n    ninp = 512,\n    nhid = 256,\n    nlayers = 5,\n)\n\n(\n    (\n        model_data_filepath + 'word_language_model_quantize.pth',\n        map_location=('cpu')\n        )\n    )\n\n()\nprint(model)",
                        "code"
                    ],
                    [
                        "LSTMModel(\n  (drop): Dropout(p=0.5, inplace=False)\n  (encoder): Embedding(33278, 512)\n  (rnn): LSTM(512, 256, num_layers=5, dropout=0.5)\n  (decoder): Linear(in_features=256, out_features=33278, bias=True)\n)",
                        "code"
                    ],
                    [
                        "Now let\u2019s generate some text to ensure that the pre-trained model is working\nproperly - similarly to before, we follow",
                        "markdown"
                    ],
                    [
                        " = (ntokens, (1, 1), dtype=)\nhidden = model.init_hidden(1)\ntemperature = 1.0\nnum_words = 1000\n\nwith open(model_data_filepath + 'out.txt', 'w') as outf:\n    with ():  # no tracking history\n        for i in range(num_words):\n            , hidden = model(, hidden)\n             = .squeeze().div(temperature).exp().cpu()\n             = (, 1)[0]\n            .fill_()\n\n            word = corpus.dictionary.idx2word[]\n\n            outf.write(str(word.encode('utf-8')) + ('\\n' if i % 20 == 19 else ' '))\n\n            if i % 100 == 0:\n                print('| Generated {}/{} words'.format(i, 1000))\n\nwith open(model_data_filepath + 'out.txt', 'r') as outf:\n    all_output = outf.read()\n    print(all_output)",
                        "code"
                    ],
                    [
                        "| Generated 0/1000 words\n| Generated 100/1000 words\n| Generated 200/1000 words\n| Generated 300/1000 words\n| Generated 400/1000 words\n| Generated 500/1000 words\n| Generated 600/1000 words\n| Generated 700/1000 words\n| Generated 800/1000 words\n| Generated 900/1000 words\nb'independently' b'concentrates' b',' b'conducted' b'by' b'&lt;unk&gt;' b'Lancaster' b'as' b'one' b'of' b'his' b'own' b'Pacer' b'classmates' b'agent' b'from' b'&lt;unk&gt;' b'as' b'the' b'\"'\nb'principal' b'eukaryotic' b'but' b'D' b'\"' b'television' b'@-@' b'talking' b',' b'right' b'of' b'Cards' b'soon' b'bid' b'from' b'1953' b'from' b'1996' b'.' b'A'\nb'brief' b'year' b'in' b'Squadron' b'held' b'possibly' b'in' b'252' b'through' b'the' b'rubble' b'\"' b'which' b'vanished' b'longer' b',' b'I' b'think' b'known' b'on'\nb'the' b'North' b'or' b'a' b'opportunity' b'of' b'German' b'air' b'signal' b',' b'searching' b'.' b'Tanaka' b'that' b'headed' b'old' b'Mars' b'on' b'&lt;unk&gt;' b'and'\nb'even' b'upon' b'their' b'literary' b'work' b'.' b'\"' b'Along' b'after' b'goaltender' b'advances' b',' b'and' b'stated' b'it' b'underwent' b'the' b'description' b'played' b'after'\nb'there' b',' b'19th-' b'&lt;unk&gt;' b',' b'and' b'minor' b'story' b'@-@' b'&lt;unk&gt;' b'works' b'.' b'The' b'image' b'of' b'fifth' b'of' b'public' b'schools' b'from'\nb'roads' b'were' b'asked' b'by' b'loving' b'70' b'for' b'its' b'North' b'manuscripts' b'.' b'For' b'example' b'in' b'her' b'location' b',' b'he' b'Peru' b'addicted'\nb'with' b'five' b'years' b',' b'leaving' b'they' b'pitch' b'at' b'his' b'cartoon' b'Oxford' b'&lt;unk&gt;' b'&lt;unk&gt;' b'&lt;unk&gt;' b'(' b'&lt;unk&gt;' b'@-@' b'&lt;unk&gt;' b')' b','\nb'super' b'Werneth' b'(' b'1973' b')' b',' b'and' b'even' b'strained' b'as' b'more' b'approached' b'.' b'Two' b'saw' b'large' b'classes' b',' b'including' b'various'\nb'medical' b'schools' b',' b'or' b'surrounding' b',' b'and' b'Ravi' b'&lt;unk&gt;' b'(' b'died' b'Denham' b')' b'.' b'violacea' b'smoke' b'one' b'of' b'larger' b'notice'\nb',' b'only' b'in' b'Cloud' b',' b'but' b'Liam' b'preferring' b'his' b'home' b'scheme' b'under' b'the' b'&lt;unk&gt;' b'alone' b'was' b'given' b'to' b'means' b'when'\nb'feeling' b',' b'from' b'the' b'Flower' b'drum' b'study' b'of' b'lions' b'and' b'Gatrell' b'.' b'The' b'Gilmore' b\"'s\" b'&lt;unk&gt;' b'comprises' b'graphics' b'is' b'created'\nb'in' b'from' b'Lawson' b'.' b'&lt;eos&gt;' b'Diplocystis' b'and' b'Earth' b',' b'including' b'same' b'men' b',' b'concluded' b'his' b'neighbors' b',' b'performing' b'discourage' b'or'\nb'subsequent' b'licences' b'.' b'Wuzhu' b',' b'Piazzi' b'Thatgamecompany' b',' b'The' b'Cyrus' b'led' b'to' b'him' b'in' b'1217' b'for' b'storylines' b'for' b'the' b'HNC'\nb'or' b'could' b'be' b'put' b'to' b'games' b'by' b'that' b'time' b'so' b'the' b'EMI' b'blocks' b'marriages' b'on' b'their' b'website' b'.' b'Whether' b'the'\nb'crisis' b'played' b'by' b'du' b'Ma\\xc3\\xadl' b'Marc' b'Robinson' b'creates' b'David' b'teen' b',' b'observing' b'his' b'troops' b'and' b'all' b'two' b'novels' b',' b'calling'\nb'high' b'scenario' b'to' b'&lt;unk&gt;' b'.' b'In' b'Virginia' b'his' b'Personality' b',' b'elects' b'total' b'policemen' b'from' b'Finkelstein' b\"'s\" b'reluctance' b'has' b'long' b'finds'\nb'it' b'marked' b'attempts' b'to' b'continue' b'to' b'feed' b'from' b'the' b'Palmyrene' b'vote' b'from' b'reality' b',' b'who' b'\"' b'fuses' b'out' b'of' b'treason'\nb'for' b'the' b'support' b'of' b'convict' b'\"' b'.' b'He' b'then' b'says' b'that' b'&lt;unk&gt;' b'and' b'Technologies' b'were' b'buck' b'wanted' b'to' b'secure' b'short'\nb'support' b':' b'\"' b'I' b'think' b'the' b'large' b'Assmann' b'Bertin' b'has' b'bought' b'his' b'job' b'for' b'every' b'foreign' b',' b'and' b'good' b','\nb'such' b',' b'he' b'could' b'be' b'1804' b'in' b'Universities' b'in' b'a' b'whole' b'financial' b'manner' b',' b'more' b'brutal' b'admirals' b'run' b'and' b'painter'\nb'order' b'differently' b'customs' b'Amnesty' b'I' b'returned' b'to' b'a' b'hurricane' b'.' b'It' b'also' b'repeatedly' b'Church' b'\"' b'.' b'This' b'glancing' b'saw' b'to'\nb'towns' b\"'s\" b'friends' b'have' b'widely' b'been' b'spent' b'out' b'to' b'pass' b'version' b'of' b'elect' b'or' b'combined' b'that' b'people' b'concludes' b'.' b'&lt;eos&gt;'\nb'barring' b'that' b'producer' b\"'\" b'Jenova' b'is' b'questioned' b'.' b'East' b'supply' b'of' b'culinary' b'ports' b'estimated' b'its' b'training' b'calls' b',' b'with' b'competition'\nb'an' b'sudden' b'structure' b'by' b'The' b'other' b'\"' b'worst' b'school' b'be' b'the' b'&lt;unk&gt;' b'or' b'sensitive' b'colour' b'that' b'he' b'is' b'sort' b'of'\nb'affluent' b'&lt;unk&gt;' b'.' b'\"' b'He' b'commented' b'\"' b'It' b'is' b'one' b'of' b'his' b'&lt;unk&gt;' b',' b'at' b'the' b'Barcelona' b'entrance' b'and' b'that'\nb'the' b'people' b'implicated' b'away' b'into' b'\"' b',' b'it' b'considers' b'eclipse' b'a' b'greater' b'sort' b'of' b'Hairan' b'Carter' b'is' b'also' b'evident' b'.'\nb'In' b'western' b'and' b'early' b'1918' b'the' b'character' b'found' b'to' b'take' b'ammunition' b'when' b'security' b'or' b'earthworms' b'Transit' b'.' b'&lt;eos&gt;' b'Due' b'to'\nb'&lt;unk&gt;' b'&lt;unk&gt;' b'died' b'this' b'earlier' b'warrior' b'Mosley' b',' b'railways' b'this' b'Go' b'military' b'is' b'also' b'a' b'DRS' b',' b'but' b'Pelagius' b'chronicles'\nb'until' b'this' b'moment' b'.' b'Inside' b'there' b'they' b'Reala' b'sacrifices' b'of' b'five' b'minutes' b'a' b'lot' b'of' b'scale' b'suffers' b'the' b'papillae' b'.'\nb'Because' b'would' b'return' b'a' b'characteristic' b'in' b'their' b'explicit' b'business' b';' b'it' b'is' b'commonly' b'only' b'sheet' b'at' b'any' b'other' b'female' b'@-@'\nb'navigational' b'zoos' b'at' b'apparent' b'above' b'night' b'.' b'&lt;unk&gt;' b'changes' b',' b'etc' b',' b'necessitated' b'Ludacris' b'.' b'The' b'universal' b'form' b'is' b'usually'\nb'easy' b',' b'quite' b'after' b'a' b'able' b'for' b'keyboard' b'movement' b'.' b'In' b'the' b'same' b'year' b',' b'due' b'to' b'Raffles' b',' b'placing'\nb'&lt;unk&gt;' b',' b'fellow' b'or' b'fades' b'calling' b',' b'they' b'are' b'\"' b'OK' b'inclined' b',' b'to' b'contain' b'his' b'natural' b'trouble' b'\"' b'.'\nb'For' b'example' b'all' b'to' b'Love' b',' b'the' b'character' b'honorable' b'exactly' b'by' b'his' b'body' b'and' b'goes' b'down' b'and' b'climb' b'the' b'last'\nb'to' b'gravity' b'when' b'they' b'are' b'very' b'somewhat' b'at' b'a' b'large' b'end' b'.' b'A' b'anonymous' b'Athene' b'particle' b'supernatural' b'often' b'marked' b'the'\nb'&lt;unk&gt;' b'&lt;unk&gt;' b'possessing' b'&lt;unk&gt;' b'.' b'&lt;eos&gt;' b'Now' b'for' b'comedic' b'behaviour' b'into' b'Andrew' b'sexpunctatus' b'(' b'converted' b',' b'and' b'the' b'Lake' b'version'\nb')' b',' b'have' b'been' b'beside' b'that' b'on' b'weather' b'regions' b',' b'they' b'are' b'used' b'to' b'learn' b'in' b'their' b'size' b'due' b'to'\nb'a' b'behaviour' b'from' b'&lt;unk&gt;' b'for' b'525' b'\\xc2\\xb0' b'weeks' b'.' b'Later' b'there' b'is' b'reported' b'that' b'Authors' b'only' b'consider' b'very' b'one' b'of'\nb'its' b'reasons' b'from' b'a' b'tolerate' b'jumping' b'.' b'The' b'flanks' b'have' b'always' b'been' b'widely' b'compared' b'to' b'account' b\"'\" b'Perhaps' b',' b'handheld'\nb',' b'but' b'some' b'have' b'increase' b'alone' b'.' b'A' b'blast' b'on' b'either' b'ridge' b'is' b'probably' b'effective' b',' b'with' b'a' b'elaborate' b'eye'\nb'(' b'such' b'as' b'sulfide' b'or' b'external' b'wing' b'and' b'Bhakti' b'.' b')' b'flipped' b'Barrier' b'NW' b'expressed' b'it' b'as' b'\"' b'\"' b'I'\nb'adopted' b'you' b'Division' b'Mitchell' b'\"' b'.' b'In' b'his' b'lifetime' b'the' b'name' b'therapy' b'.' b'&lt;eos&gt;' b'&lt;eos&gt;' b'=' b'=' b'=' b'Selective' b'bodies'\nb'=' b'=' b'=' b'&lt;eos&gt;' b'&lt;eos&gt;' b'King' b'Meta' b'The' b'idea' b'that' b'year' b'\"' b'knew' b'it' b'a' b'tactical' b'one' b'root' b'wall' b'on'\nb'natural' b'or' b'&lt;unk&gt;' b',' b'wind' b',' b'and' b'grassy' b'cooking' b'words' b'that' b'were' b'&lt;unk&gt;' b'.' b'\"' b'&lt;eos&gt;' b'Because' b'they' b'go' b'a'\nb'same' b'situation' b'member' b'for' b'these' b'brood' b'depend' b'.' b'Its' b'pests' b'is' b'gradually' b'slowed' b'.' b'Tech' b'then' b'presents' b'it' b'back' b'as'\nb'it' b'appears' b'clicking' b'with' b'it' b',' b'breaking' b'him' b'to' b'make' b'pairs' b'of' b'community' b'corporal' b'patron' b'shape' b'of' b'clinical' b'mating' b'.'\nb'It' b'could' b'often' b'want' b'to' b'be' b'discovered' b'even' b'at' b',' b'though' b'there' b'are' b'harmonies' b'and' b'attaining' b'food' b',' b'most' b'lanthanides'\nb'be' b'won' b'that' b'bare' b'males' b'also' b'did' b'.' b'Although' b'their' b'diet' b'greatly' b'Various' b'sufficiently' b'on' b'another' b'hand' b'needs' b'alive' b','\nb'they' b'think' b'&lt;unk&gt;' b'and' b'sends' b'up' b'the' b'nominate' b'calls' b',' b'about' b'75' b'metres' b'(' b'22' b'to' b'9' b'@.@' b'4' b'mph'",
                        "code"
                    ],
                    [
                        "It\u2019s no GPT-2, but it looks like the model has started to learn the structure of\nlanguage!",
                        "markdown"
                    ],
                    [
                        "We\u2019re almost ready to demonstrate dynamic quantization. We just need to define a few more\nhelper functions:",
                        "markdown"
                    ],
                    [
                        "bptt = 25\n = ()\neval_batch_size = 1\n\n# create test data set\ndef batchify(data, bsz):\n    # Work out how cleanly we can divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    return data.view(bsz, -1).t().contiguous()\n\n = batchify(, eval_batch_size)\n\n# Evaluation functions\ndef get_batch(source, i):\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].reshape(-1)\n    return data, target\n\ndef repackage_hidden(h):\n  \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n\n  if isinstance(h, ):\n      return h.detach()\n  else:\n      return tuple(repackage_hidden(v) for v in h)\n\ndef evaluate(model_, data_source):\n    # Turn on evaluation mode which disables dropout.\n    model_.eval()\n    total_loss = 0.\n    hidden = model_.init_hidden(eval_batch_size)\n    with ():\n        for i in range(0, data_source.size(0) - 1, bptt):\n            data, targets = get_batch(data_source, i)\n            , hidden = model_(data, hidden)\n            hidden = repackage_hidden(hidden)\n            output_flat = .view(-1, ntokens)\n            total_loss += len(data) * (output_flat, targets).item()\n    return total_loss / (len(data_source) - 1)",
                        "code"
                    ]
                ]
            },
            {
                "4. Test dynamic quantization": [
                    [
                        "Finally, we can call torch.quantization.quantize_dynamic on the model!\nSpecifically,",
                        "markdown"
                    ],
                    [
                        "We specify that we want the nn.LSTM and nn.Linear modules in our\nmodel to be quantized",
                        "markdown"
                    ],
                    [
                        "We specify that we want weights to be converted to int8 values",
                        "markdown"
                    ],
                    [
                        "import torch.quantization\n\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {, }, dtype=\n)\nprint(quantized_model)",
                        "code"
                    ],
                    [
                        "LSTMModel(\n  (drop): Dropout(p=0.5, inplace=False)\n  (encoder): Embedding(33278, 512)\n  (rnn): DynamicQuantizedLSTM(512, 256, num_layers=5, dropout=0.5)\n  (decoder): DynamicQuantizedLinear(in_features=256, out_features=33278, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n)",
                        "code"
                    ],
                    [
                        "The model looks the same; how has this benefited us? First, we see a\nsignificant reduction in model size:",
                        "markdown"
                    ],
                    [
                        "def print_size_of_model(model):\n    ((), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\nprint_size_of_model(model)\nprint_size_of_model(quantized_model)",
                        "code"
                    ],
                    [
                        "Size (MB): 113.943637\nSize (MB): 79.738057",
                        "code"
                    ],
                    [
                        "Second, we see faster inference time, with no difference in evaluation loss:",
                        "markdown"
                    ],
                    [
                        "Note: we set the number of threads to one for single threaded comparison, since quantized\nmodels run single threaded.",
                        "markdown"
                    ],
                    [
                        "(1)\n\ndef time_model_evaluation(model, ):\n    s = time.time()\n    loss = evaluate(model, )\n    elapsed = time.time() - s\n    print('''loss: {0:.3f}\\nelapsed time (seconds): {1:.1f}'''.format(loss, elapsed))\n\ntime_model_evaluation(model, )\ntime_model_evaluation(quantized_model, )",
                        "code"
                    ],
                    [
                        "loss: 5.167\nelapsed time (seconds): 199.7\nloss: 5.168\nelapsed time (seconds): 115.0",
                        "code"
                    ],
                    [
                        "Running this locally on a MacBook Pro, without quantization, inference takes about 200 seconds,\nand with quantization it takes just about 100 seconds.",
                        "markdown"
                    ]
                ]
            },
            {
                "Conclusion": [
                    [
                        "Dynamic quantization can be an easy way to reduce model size while only\nhaving a limited effect on accuracy.",
                        "markdown"
                    ],
                    [
                        "Thanks for reading! As always, we welcome any feedback, so please create an issue\n if you have any.",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 5 minutes  23.575 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "(beta) Dynamic Quantization on BERT": [
            [
                "Tip",
                "markdown"
            ],
            [
                "To get the most of this tutorial, we suggest using this\n. This will allow you to experiment with the information presented below.",
                "markdown"
            ],
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "<strong>Reviewed by</strong>: ",
                "markdown"
            ],
            [
                "<strong>Edited by</strong>: ",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "In this tutorial, we will apply the dynamic quantization on a BERT\nmodel, closely following the BERT model from .\nWith this step-by-step journey, we would like to demonstrate how to\nconvert a well-known state-of-the-art model like BERT into dynamic\nquantized model.",
                        "markdown"
                    ],
                    [
                        "BERT, or Bidirectional Embedding Representations from Transformers,\nis a new method of pre-training language representations which\nachieves the state-of-the-art accuracy results on many popular\nNatural Language Processing (NLP) tasks, such as question answering,\ntext classification, and others. The original paper can be found\n.",
                        "markdown"
                    ],
                    [
                        "Dynamic quantization support in PyTorch converts a float model to a\nquantized model with static int8 or float16 data types for the\nweights and dynamic quantization for the activations. The activations\nare quantized dynamically (per batch) to int8 when the weights are\nquantized to int8. In PyTorch, we have ,\nwhich replaces specified modules with dynamic weight-only quantized\nversions and output the quantized model.",
                        "markdown"
                    ],
                    [
                        "We demonstrate the accuracy and inference performance results on the\n\nin the General Language Understanding Evaluation benchmark . The MRPC (Dolan and Brockett, 2005) is\na corpus of sentence pairs automatically extracted from online news\nsources, with human annotations of whether the sentences in the pair\nare semantically equivalent. As the classes are imbalanced (68%\npositive, 32% negative), we follow the common practice and report\n.\nMRPC is a common NLP task for language pair classification, as shown\nbelow.\n\n<img alt=\"../_images/bert.png\" src=\"../_images/bert.png\"/>",
                        "markdown"
                    ]
                ]
            },
            {
                "1. Setup": [
                    {
                        "1.1 Install PyTorch and HuggingFace Transformers": [
                            [
                                "To start this tutorial, let\u2019s first follow the installation instructions\nin PyTorch  and HuggingFace Github Repo .\nIn addition, we also install  package, as we will reuse its\nbuilt-in F1 score calculation helper function.",
                                "markdown"
                            ],
                            [
                                "pip install sklearn\npip install transformers",
                                "code"
                            ],
                            [
                                "Because we will be using the beta parts of the PyTorch, it is\nrecommended to install the latest version of torch and torchvision. You\ncan find the most recent instructions on local installation . For example, to install on\nMac:",
                                "markdown"
                            ],
                            [
                                "yes y | pip uninstall torch tochvision\nyes y | pip install --pre torch -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html",
                                "code"
                            ]
                        ]
                    },
                    {
                        "1.2 Import the necessary modules": [
                            [
                                "In this step we import the necessary Python modules for the tutorial.",
                                "markdown"
                            ],
                            [
                                "from __future__ import absolute_import, division, print_function\n\nimport logging\nimport numpy as np\nimport os\nimport random\nimport sys\nimport time\nimport torch\n\nfrom argparse import Namespace\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom tqdm import tqdm\nfrom transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)\nfrom transformers import glue_compute_metrics as compute_metrics\nfrom transformers import glue_output_modes as output_modes\nfrom transformers import glue_processors as processors\nfrom transformers import glue_convert_examples_to_features as convert_examples_to_features\n\n# Setup logging\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n                    datefmt = '%m/%d/%Y %H:%M:%S',\n                    level = logging.WARN)\n\nlogging.getLogger(\"transformers.modeling_utils\").setLevel(\n   logging.WARN)  # Reduce logging\n\nprint(torch.__version__)",
                                "code"
                            ],
                            [
                                "We set the number of threads to compare the single thread performance between FP32 and INT8 performance.\nIn the end of the tutorial, the user can set other number of threads by building PyTorch with right parallel backend.",
                                "markdown"
                            ],
                            [
                                "torch.set_num_threads(1)\nprint(torch.__config__.parallel_info())",
                                "code"
                            ]
                        ]
                    },
                    {
                        "1.3 Learn about helper functions": [
                            [
                                "The helper functions are built-in in transformers library. We mainly use\nthe following helper functions: one for converting the text examples\ninto the feature vectors; The other one for measuring the F1 score of\nthe predicted result.",
                                "markdown"
                            ],
                            [
                                "The  function converts the texts into input features:",
                                "markdown"
                            ],
                            [
                                "Tokenize the input sequences;",
                                "markdown"
                            ],
                            [
                                "Insert [CLS] in the beginning;",
                                "markdown"
                            ],
                            [
                                "Insert [SEP] between the first sentence and the second sentence, and\nin the end;",
                                "markdown"
                            ],
                            [
                                "Generate token type ids to indicate whether a token belongs to the\nfirst sequence or the second sequence.",
                                "markdown"
                            ],
                            [
                                "The   function has the compute metrics with\nthe , which\ncan be interpreted as a weighted average of the precision and recall,\nwhere an F1 score reaches its best value at 1 and worst score at 0. The\nrelative contribution of precision and recall to the F1 score are equal.",
                                "markdown"
                            ],
                            [
                                "The equation for the F1 score is:\n\n\n\\[F1 = 2 * (\\text{precision} * \\text{recall}) / (\\text{precision} + \\text{recall})\n\n\\]",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "1.4 Download the dataset": [
                            [
                                "Before running MRPC tasks we download the  by running \nand unpack it to a directory glue_data.",
                                "markdown"
                            ],
                            [
                                "python download_glue_data.py --data_dir='glue_data' --tasks='MRPC'",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "2. Fine-tune the BERT model": [
                    [
                        "The spirit of BERT is to pre-train the language representations and then\nto fine-tune the deep bi-directional representations on a wide range of\ntasks with minimal task-dependent parameters, and achieves\nstate-of-the-art results. In this tutorial, we will focus on fine-tuning\nwith the pre-trained BERT model to classify semantically equivalent\nsentence pairs on MRPC task.",
                        "markdown"
                    ],
                    [
                        "To fine-tune the pre-trained BERT model (bert-base-uncased model in\nHuggingFace transformers) for the MRPC task, you can follow the command\nin :",
                        "markdown"
                    ],
                    [
                        "export GLUE_DIR=./glue_data\nexport TASK_NAME=MRPC\nexport OUT_DIR=./$TASK_NAME/\npython ./run_glue.py \\\n    --model_type bert \\\n    --model_name_or_path bert-base-uncased \\\n    --task_name $TASK_NAME \\\n    --do_train \\\n    --do_eval \\\n    --do_lower_case \\\n    --data_dir $GLUE_DIR/$TASK_NAME \\\n    --max_seq_length 128 \\\n    --per_gpu_eval_batch_size=8   \\\n    --per_gpu_train_batch_size=8   \\\n    --learning_rate 2e-5 \\\n    --num_train_epochs 3.0 \\\n    --save_steps 100000 \\\n    --output_dir $OUT_DIR",
                        "code"
                    ],
                    [
                        "We provide the fined-tuned BERT model for MRPC task .\nTo save time, you can download the model file (~400 MB) directly into your local folder $OUT_DIR.",
                        "markdown"
                    ],
                    {
                        "2.1 Set global configurations": [
                            [
                                "Here we set the global configurations for evaluating the fine-tuned BERT\nmodel before and after the dynamic quantization.",
                                "markdown"
                            ],
                            [
                                "configs = Namespace()\n\n# The output directory for the fine-tuned model, $OUT_DIR.\nconfigs.output_dir = \"./MRPC/\"\n\n# The data directory for the MRPC task in the GLUE benchmark, $GLUE_DIR/$TASK_NAME.\nconfigs.data_dir = \"./glue_data/MRPC\"\n\n# The model name or path for the pre-trained model.\nconfigs.model_name_or_path = \"bert-base-uncased\"\n# The maximum length of an input sequence\nconfigs.max_seq_length = 128\n\n# Prepare GLUE task.\nconfigs.task_name = \"MRPC\".lower()\nconfigs.processor = processors[configs.task_name]()\nconfigs.output_mode = output_modes[configs.task_name]\nconfigs.label_list = configs.processor.get_labels()\nconfigs.model_type = \"bert\".lower()\nconfigs.do_lower_case = True\n\n# Set the device, batch size, topology, and caching flags.\nconfigs.device = \"cpu\"\nconfigs.per_gpu_eval_batch_size = 8\nconfigs.n_gpu = 0\nconfigs.local_rank = -1\nconfigs.overwrite_cache = False\n\n\n# Set random seed for reproducibility.\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\nset_seed(42)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2.2 Load the fine-tuned BERT model": [
                            [
                                "We load the tokenizer and fine-tuned BERT sequence classifier model\n(FP32) from the configs.output_dir.",
                                "markdown"
                            ],
                            [
                                "tokenizer = BertTokenizer.from_pretrained(\n    configs.output_dir, do_lower_case=configs.do_lower_case)\n\nmodel = BertForSequenceClassification.from_pretrained(configs.output_dir)\nmodel.to(configs.device)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "2.3 Define the tokenize and evaluation function": [
                            [
                                "We reuse the tokenize and evaluation function from .",
                                "markdown"
                            ],
                            [
                                "# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\ndef evaluate(args, model, tokenizer, prefix=\"\"):\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_task_names = (\"mnli\", \"mnli-mm\") if args.task_name == \"mnli\" else (args.task_name,)\n    eval_outputs_dirs = (args.output_dir, args.output_dir + '-MM') if args.task_name == \"mnli\" else (args.output_dir,)\n\n    results = {}\n    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n\n        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n        # Note that DistributedSampler samples randomly\n        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n\n        # multi-gpu eval\n        if args.n_gpu &gt; 1:\n            model = torch.nn.DataParallel(model)\n\n        # Eval!\n        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n        logger.info(\"  Num examples = %d\", len(eval_dataset))\n        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        preds = None\n        out_label_ids = None\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            model.eval()\n            batch = tuple(t.to(args.device) for t in batch)\n\n            with torch.no_grad():\n                inputs = {'input_ids':      batch[0],\n                          'attention_mask': batch[1],\n                          'labels':         batch[3]}\n                if args.model_type != 'distilbert':\n                    inputs['token_type_ids'] = batch[2] if args.model_type in ['bert', 'xlnet'] else None  # XLM, DistilBERT and RoBERTa don't use segment_ids\n                outputs = model(**inputs)\n                tmp_eval_loss, logits = outputs[:2]\n\n                eval_loss += tmp_eval_loss.mean().item()\n            nb_eval_steps += 1\n            if preds is None:\n                preds = logits.detach().cpu().numpy()\n                out_label_ids = inputs['labels'].detach().cpu().numpy()\n            else:\n                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n\n        eval_loss = eval_loss / nb_eval_steps\n        if args.output_mode == \"classification\":\n            preds = np.argmax(preds, axis=1)\n        elif args.output_mode == \"regression\":\n            preds = np.squeeze(preds)\n        result = compute_metrics(eval_task, preds, out_label_ids)\n        results.update(result)\n\n        output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n        with open(output_eval_file, \"w\") as writer:\n            logger.info(\"***** Eval results {} *****\".format(prefix))\n            for key in sorted(result.keys()):\n                logger.info(\"  %s = %s\", key, str(result[key]))\n                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return results\n\n\ndef load_and_cache_examples(args, task, tokenizer, evaluate=False):\n    if args.local_rank not in [-1, 0] and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    processor = processors[task]()\n    output_mode = output_modes[task]\n    # Load data features from cache or dataset file\n    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format(\n        'dev' if evaluate else 'train',\n        list(filter(None, args.model_name_or_path.split('/'))).pop(),\n        str(args.max_seq_length),\n        str(task)))\n    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n        logger.info(\"Loading features from cached file %s\", cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n        label_list = processor.get_labels()\n        if task in ['mnli', 'mnli-mm'] and args.model_type in ['roberta']:\n            # HACK(label indices are swapped in RoBERTa pretrained model)\n            label_list[1], label_list[2] = label_list[2], label_list[1]\n        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n        features = convert_examples_to_features(examples,\n                                                tokenizer,\n                                                label_list=label_list,\n                                                max_length=args.max_seq_length,\n                                                output_mode=output_mode,\n                                                pad_on_left=bool(args.model_type in ['xlnet']),                 # pad on the left for xlnet\n                                                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n                                                pad_token_segment_id=4 if args.model_type in ['xlnet'] else 0,\n        )\n        if args.local_rank in [-1, 0]:\n            logger.info(\"Saving features into cached file %s\", cached_features_file)\n            torch.save(features, cached_features_file)\n\n    if args.local_rank == 0 and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    if output_mode == \"classification\":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n    elif output_mode == \"regression\":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n\n    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n    return dataset",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "3. Apply the dynamic quantization": [
                    [
                        "We call torch.quantization.quantize_dynamic on the model to apply\nthe dynamic quantization on the HuggingFace BERT model. Specifically,",
                        "markdown"
                    ],
                    [
                        "We specify that we want the torch.nn.Linear modules in our model to\nbe quantized;",
                        "markdown"
                    ],
                    [
                        "We specify that we want weights to be converted to quantized int8\nvalues.",
                        "markdown"
                    ],
                    [
                        "quantized_model = torch.quantization.quantize_dynamic(\n    model, {torch.nn.Linear}, dtype=torch.qint8\n)\nprint(quantized_model)",
                        "code"
                    ],
                    {
                        "3.1 Check the model size": [
                            [
                                "Let\u2019s first check the model size. We can observe a significant reduction\nin model size (FP32 total size: 438 MB; INT8 total size: 181 MB):",
                                "markdown"
                            ],
                            [
                                "def print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\nprint_size_of_model(model)\nprint_size_of_model(quantized_model)",
                                "code"
                            ],
                            [
                                "The BERT model used in this tutorial (bert-base-uncased) has a\nvocabulary size V of 30522. With the embedding size of 768, the total\nsize of the word embedding table is ~ 4 (Bytes/FP32) * 30522 * 768 =\n90 MB. So with the help of quantization, the model size of the\nnon-embedding table part is reduced from 350 MB (FP32 model) to 90 MB\n(INT8 model).",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "3.2 Evaluate the inference accuracy and time": [
                            [
                                "Next, let\u2019s compare the inference time as well as the evaluation\naccuracy between the original FP32 model and the INT8 model after the\ndynamic quantization.",
                                "markdown"
                            ],
                            [
                                "def time_model_evaluation(model, configs, tokenizer):\n    eval_start_time = time.time()\n    result = evaluate(configs, model, tokenizer, prefix=\"\")\n    eval_end_time = time.time()\n    eval_duration_time = eval_end_time - eval_start_time\n    print(result)\n    print(\"Evaluate total time (seconds): {0:.1f}\".format(eval_duration_time))\n\n# Evaluate the original FP32 BERT model\ntime_model_evaluation(model, configs, tokenizer)\n\n# Evaluate the INT8 BERT model after the dynamic quantization\ntime_model_evaluation(quantized_model, configs, tokenizer)",
                                "code"
                            ],
                            [
                                "Running this locally on a MacBook Pro, without quantization, inference\n(for all 408 examples in MRPC dataset) takes about 160 seconds, and with\nquantization it takes just about 90 seconds. We summarize the results\nfor running the quantized BERT model inference on a Macbook Pro as the\nfollows:",
                                "markdown"
                            ],
                            [
                                "| Prec | F1 score | Model Size | 1 thread | 4 threads |\n| FP32 |  0.9019  |   438 MB   | 160 sec  | 85 sec    |\n| INT8 |  0.902   |   181 MB   |  90 sec  | 46 sec    |",
                                "code"
                            ],
                            [
                                "We have 0.6% lower F1 score accuracy after applying the post-training dynamic\nquantization on the fine-tuned BERT model on the MRPC task. As a\ncomparison, in a  (Table 1),\nit achieved 0.8788 by\napplying the post-training dynamic quantization and 0.8956 by applying\nthe quantization-aware training. The main difference is that we support the\nasymmetric quantization in PyTorch while that paper supports the\nsymmetric quantization only.",
                                "markdown"
                            ],
                            [
                                "Note that we set the number of threads to 1 for the single-thread\ncomparison in this tutorial. We also support the intra-op\nparallelization for these quantized INT8 operators. The users can now\nset multi-thread by torch.set_num_threads(N) (N is the number of\nintra-op parallelization threads). One preliminary requirement to enable\nthe intra-op parallelization support is to build PyTorch with the right\n\nsuch as OpenMP, Native or TBB.\nYou can use torch.__config__.parallel_info() to check the\nparallelization settings. On the same MacBook Pro using PyTorch with\nNative backend for parallelization, we can get about 46 seconds for\nprocessing the evaluation of MRPC dataset.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "3.3 Serialize the quantized model": [
                            [
                                "We can serialize and save the quantized model for the future use using\n<cite>torch.jit.save</cite> after tracing the model.",
                                "markdown"
                            ],
                            [
                                "input_ids = ids_tensor([8, 128], 2)\ntoken_type_ids = ids_tensor([8, 128], 2)\nattention_mask = ids_tensor([8, 128], vocab_size=2)\ndummy_input = (input_ids, attention_mask, token_type_ids)\ntraced_model = torch.jit.trace(quantized_model, dummy_input)\ntorch.jit.save(traced_model, \"bert_traced_eager_quant.pt\")",
                                "code"
                            ],
                            [
                                "To load the quantized model, we can use <cite>torch.jit.load</cite>",
                                "markdown"
                            ],
                            [
                                "loaded_quantized_model = torch.jit.load(\"bert_traced_eager_quant.pt\")",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Conclusion": [
                    [
                        "In this tutorial, we demonstrated how to convert a\nwell-known state-of-the-art NLP model like BERT into dynamic quantized\nmodel. Dynamic quantization can reduce the size of the model while only\nhaving a limited implication on accuracy.",
                        "markdown"
                    ],
                    [
                        "Thanks for reading! As always, we welcome any feedback, so please create\nan issue  if you have\nany.",
                        "markdown"
                    ]
                ]
            },
            {
                "References": [
                    [
                        "[1] J.Devlin, M. Chang, K. Lee and K. Toutanova, .",
                        "markdown"
                    ],
                    [
                        "[2] .",
                        "markdown"
                    ],
                    [
                        "[3] O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat (2019). .",
                        "markdown"
                    ]
                ]
            }
        ],
        "(beta) Quantized Transfer Learning for Computer Vision Tutorial": [
            [
                "Tip",
                "markdown"
            ],
            [
                "To get the most of this tutorial, we suggest using this\n.\nThis will allow you to experiment with the information presented below.",
                "markdown"
            ],
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "<strong>Reviewed by</strong>: ",
                "markdown"
            ],
            [
                "<strong>Edited by</strong>: ",
                "markdown"
            ],
            [
                "This tutorial builds on the original \ntutorial, written by .",
                "markdown"
            ],
            [
                "Transfer learning refers to techniques that make use of a pretrained model for\napplication on a different data-set.\nThere are two main ways the transfer learning is used:",
                "markdown"
            ],
            [
                "<strong>ConvNet as a fixed feature extractor</strong>: Here, you \nthe weights of all the parameters in the network except that of the final\nseveral layers (aka \u201cthe head\u201d, usually fully connected layers).\nThese last layers are replaced with new ones initialized with random\nweights and only these layers are trained.",
                "markdown"
            ],
            [
                "<strong>Finetuning the ConvNet</strong>: Instead of random initializaion, the model is\ninitialized using a pretrained network, after which the training proceeds as\nusual but with a different dataset.\nUsually the head (or part of it) is also replaced in the network in\ncase there is a different number of outputs.\nIt is common in this method to set the learning rate to a smaller number.\nThis is done because the network is already trained, and only minor changes\nare required to \u201cfinetune\u201d it to a new dataset.",
                "markdown"
            ],
            [
                "You can also combine the above two methods:\nFirst you can freeze the feature extractor, and train the head. After\nthat, you can unfreeze the feature extractor (or part of it), set the\nlearning rate to something smaller, and continue training.",
                "markdown"
            ],
            [
                "In this part you will use the first method \u2013 extracting the features\nusing a quantized model.",
                "markdown"
            ],
            {
                "Part 0. Prerequisites": [
                    [
                        "Before diving into the transfer learning, let us review the \u201cprerequisites\u201d,\nsuch as installations and data loading/visualizations.",
                        "markdown"
                    ],
                    [
                        "# Imports\nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport time\n\nplt.ion()",
                        "code"
                    ],
                    {
                        "Installing the Nightly Build": [
                            [
                                "Because you will be using the beta parts of the PyTorch, it is\nrecommended to install the latest version of torch and\ntorchvision. You can find the most recent instructions on local\ninstallation .\nFor example, to install without GPU support:",
                                "markdown"
                            ],
                            [
                                "pip install numpy\npip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n# For CUDA support use https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Load Data": [
                            [
                                "Note",
                                "markdown"
                            ],
                            [
                                "This section is identical to the original transfer learning tutorial.",
                                "markdown"
                            ],
                            [
                                "We will use torchvision and torch.utils.data packages to load\nthe data.",
                                "markdown"
                            ],
                            [
                                "The problem you are going to solve today is classifying <strong>ants</strong> and\n<strong>bees</strong> from images. The dataset contains about 120 training images\neach for ants and bees. There are 75 validation images for each class.\nThis is considered a very small dataset to generalize on. However, since\nwe are using transfer learning, we should be able to generalize\nreasonably well.",
                                "markdown"
                            ],
                            [
                                "<em>This dataset is a very small subset of imagenet.</em>",
                                "markdown"
                            ],
                            [
                                "Note",
                                "markdown"
                            ],
                            [
                                "Download the data from \nand extract it to the data directory.",
                                "markdown"
                            ],
                            [
                                "import torch\nfrom torchvision import transforms, datasets\n\n# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize(224),\n        transforms.RandomCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data/hymenoptera_data'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n                                              shuffle=True, num_workers=8)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Visualize a few images": [
                            [
                                "Let\u2019s visualize a few training images so as to understand the data\naugmentations.",
                                "markdown"
                            ],
                            [
                                "import torchvision\n\ndef imshow(inp, title=None, ax=None, figsize=(5, 5)):\n  \"\"\"Imshow for Tensor.\"\"\"\n  inp = inp.numpy().transpose((1, 2, 0))\n  mean = np.array([0.485, 0.456, 0.406])\n  std = np.array([0.229, 0.224, 0.225])\n  inp = std * inp + mean\n  inp = np.clip(inp, 0, 1)\n  if ax is None:\n    fig, ax = plt.subplots(1, figsize=figsize)\n  ax.imshow(inp)\n  ax.set_xticks([])\n  ax.set_yticks([])\n  if title is not None:\n    ax.set_title(title)\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs, nrow=4)\n\nfig, ax = plt.subplots(1, figsize=(10, 10))\nimshow(out, title=[class_names[x] for x in classes], ax=ax)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Support Function for Model Training": [
                            [
                                "Below is a generic function for model training.\nThis function also",
                                "markdown"
                            ],
                            [
                                "Schedules the learning rate",
                                "markdown"
                            ],
                            [
                                "Saves the best model",
                                "markdown"
                            ],
                            [
                                "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, device='cpu'):\n  \"\"\"\n  Support function for model training.\n\n  Args:\n    model: Model to be trained\n    criterion: Optimization criterion (loss)\n    optimizer: Optimizer to use for training\n    scheduler: Instance of ``torch.optim.lr_scheduler``\n    num_epochs: Number of epochs\n    device: Device to run the training on. Must be 'cpu' or 'cuda'\n  \"\"\"\n  since = time.time()\n\n  best_model_wts = copy.deepcopy(model.state_dict())\n  best_acc = 0.0\n\n  for epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n\n    # Each epoch has a training and validation phase\n    for phase in ['train', 'val']:\n      if phase == 'train':\n        model.train()  # Set model to training mode\n      else:\n        model.eval()   # Set model to evaluate mode\n\n      running_loss = 0.0\n      running_corrects = 0\n\n      # Iterate over data.\n      for inputs, labels in dataloaders[phase]:\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward\n        # track history if only in train\n        with torch.set_grad_enabled(phase == 'train'):\n          outputs = model(inputs)\n          _, preds = torch.max(outputs, 1)\n          loss = criterion(outputs, labels)\n\n          # backward + optimize only if in training phase\n          if phase == 'train':\n            loss.backward()\n            optimizer.step()\n\n        # statistics\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n      if phase == 'train':\n        scheduler.step()\n\n      epoch_loss = running_loss / dataset_sizes[phase]\n      epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n      print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n        phase, epoch_loss, epoch_acc))\n\n      # deep copy the model\n      if phase == 'val' and epoch_acc &gt; best_acc:\n        best_acc = epoch_acc\n        best_model_wts = copy.deepcopy(model.state_dict())\n\n    print()\n\n  time_elapsed = time.time() - since\n  print('Training complete in {:.0f}m {:.0f}s'.format(\n    time_elapsed // 60, time_elapsed % 60))\n  print('Best val Acc: {:4f}'.format(best_acc))\n\n  # load best model weights\n  model.load_state_dict(best_model_wts)\n  return model",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Support Function for Visualizing the Model Predictions": [
                            [
                                "Generic function to display predictions for a few images",
                                "markdown"
                            ],
                            [
                                "def visualize_model(model, rows=3, cols=3):\n  was_training = model.training\n  model.eval()\n  current_row = current_col = 0\n  fig, ax = plt.subplots(rows, cols, figsize=(cols*2, rows*2))\n\n  with torch.no_grad():\n    for idx, (imgs, lbls) in enumerate(dataloaders['val']):\n      imgs = imgs.cpu()\n      lbls = lbls.cpu()\n\n      outputs = model(imgs)\n      _, preds = torch.max(outputs, 1)\n\n      for jdx in range(imgs.size()[0]):\n        imshow(imgs.data[jdx], ax=ax[current_row, current_col])\n        ax[current_row, current_col].axis('off')\n        ax[current_row, current_col].set_title('predicted: {}'.format(class_names[preds[jdx]]))\n\n        current_col += 1\n        if current_col &gt;= cols:\n          current_row += 1\n          current_col = 0\n        if current_row &gt;= rows:\n          model.train(mode=was_training)\n          return\n    model.train(mode=was_training)",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Part 1. Training a Custom Classifier based on a Quantized Feature Extractor": [
                    [
                        "In this section you will use a \u201cfrozen\u201d quantized feature extractor, and\ntrain a custom classifier head on top of it. Unlike floating point\nmodels, you don\u2019t need to set requires_grad=False for the quantized\nmodel, as it has no trainable parameters. Please, refer to the\n for\nmore details.",
                        "markdown"
                    ],
                    [
                        "Load a pretrained model: for this exercise you will be using\n.",
                        "markdown"
                    ],
                    [
                        "import torchvision.models.quantization as models\n\n# You will need the number of filters in the `fc` for future use.\n# Here the size of each output sample is set to 2.\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\nmodel_fe = models.resnet18(pretrained=True, progress=True, quantize=True)\nnum_ftrs = model_fe.fc.in_features",
                        "code"
                    ],
                    [
                        "At this point you need to modify the pretrained model. The model\nhas the quantize/dequantize blocks in the beginning and the end. However,\nbecause you will only use the feature extractor, the dequantization layer has\nto move right before the linear layer (the head). The easiest way to do that\nis to wrap the model in the nn.Sequential module.",
                        "markdown"
                    ],
                    [
                        "The first step is to isolate the feature extractor in the ResNet\nmodel. Although in this example you are tasked to use all layers except\nfc as the feature extractor, in reality, you can take as many parts\nas you need. This would be useful in case you would like to replace some\nof the convolutional layers as well.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "When separating the feature extractor from the rest of a quantized\nmodel, you have to manually place the quantizer/dequantized in the\nbeginning and the end of the parts you want to keep quantized.",
                        "markdown"
                    ],
                    [
                        "The function below creates a model with a custom head.",
                        "markdown"
                    ],
                    [
                        "from torch import nn\n\ndef create_combined_model(model_fe):\n  # Step 1. Isolate the feature extractor.\n  model_fe_features = nn.Sequential(\n    model_fe.quant,  # Quantize the input\n    model_fe.conv1,\n    model_fe.bn1,\n    model_fe.relu,\n    model_fe.maxpool,\n    model_fe.layer1,\n    model_fe.layer2,\n    model_fe.layer3,\n    model_fe.layer4,\n    model_fe.avgpool,\n    model_fe.dequant,  # Dequantize the output\n  )\n\n  # Step 2. Create a new \"head\"\n  new_head = nn.Sequential(\n    nn.Dropout(p=0.5),\n    nn.Linear(num_ftrs, 2),\n  )\n\n  # Step 3. Combine, and don't forget the quant stubs.\n  new_model = nn.Sequential(\n    model_fe_features,\n    nn.Flatten(1),\n    new_head,\n  )\n  return new_model",
                        "code"
                    ],
                    [
                        "Warning",
                        "markdown"
                    ],
                    [
                        "Currently the quantized models can only be run on CPU.\nHowever, it is possible to send the non-quantized parts of the model to a GPU.",
                        "markdown"
                    ],
                    [
                        "import torch.optim as optim\nnew_model = create_combined_model(model_fe)\nnew_model = new_model.to('cpu')\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are only training the head.\noptimizer_ft = optim.SGD(new_model.parameters(), lr=0.01, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)",
                        "code"
                    ],
                    {
                        "Train and evaluate": [
                            [
                                "This step takes around 15-25 min on CPU. Because the quantized model can\nonly run on the CPU, you cannot run the training on GPU.",
                                "markdown"
                            ],
                            [
                                "new_model = train_model(new_model, criterion, optimizer_ft, exp_lr_scheduler,\n                        num_epochs=25, device='cpu')\n\nvisualize_model(new_model)\nplt.tight_layout()",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Part 2. Finetuning the Quantizable Model": [
                    [
                        "In this part, we fine tune the feature extractor used for transfer\nlearning, and quantize the feature extractor. Note that in both part 1\nand 2, the feature extractor is quantized. The difference is that in\npart 1, we use a pretrained quantized model. In this part, we create a\nquantized feature extractor after fine tuning on the data-set of\ninterest, so this is a way to get better accuracy with transfer learning\nwhile having the benefits of quantization. Note that in our specific\nexample, the training set is really small (120 images) so the benefits\nof fine tuning the entire model is not apparent. However, the procedure\nshown here will improve accuracy for transfer learning with larger\ndatasets.",
                        "markdown"
                    ],
                    [
                        "The pretrained feature extractor must be quantizable.\nTo make sure it is quantizable, perform the following steps:\n<blockquote>",
                        "markdown"
                    ],
                    [
                        "Fuse (Conv, BN, ReLU), (Conv, BN), and (Conv, ReLU) using\ntorch.quantization.fuse_modules.",
                        "markdown"
                    ],
                    [
                        "Connect the feature extractor with a custom head.\nThis requires dequantizing the output of the feature extractor.",
                        "markdown"
                    ],
                    [
                        "Insert fake-quantization modules at appropriate locations\nin the feature extractor to mimic quantization during training.\n\n</blockquote>",
                        "markdown"
                    ],
                    [
                        "For step (1), we use models from torchvision/models/quantization, which\nhave a member method fuse_model. This function fuses all the conv,\nbn, and relu modules. For custom models, this would require calling\nthe torch.quantization.fuse_modules API with the list of modules to fuse\nmanually.",
                        "markdown"
                    ],
                    [
                        "Step (2) is performed by the create_combined_model function\nused in the previous section.",
                        "markdown"
                    ],
                    [
                        "Step (3) is achieved by using torch.quantization.prepare_qat, which\ninserts fake-quantization modules.",
                        "markdown"
                    ],
                    [
                        "As step (4), you can start \u201cfinetuning\u201d the model, and after that convert\nit to a fully quantized version (Step 5).",
                        "markdown"
                    ],
                    [
                        "To convert the fine tuned model into a quantized model you can call the\ntorch.quantization.convert function (in our case only\nthe feature extractor is quantized).",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "Because of the random initialization your results might differ from\nthe results shown in this tutorial.",
                        "markdown"
                    ],
                    [
                        "# notice `quantize=False`\nmodel = models.resnet18(pretrained=True, progress=True, quantize=False)\nnum_ftrs = model.fc.in_features\n\n# Step 1\nmodel.train()\nmodel.fuse_model()\n# Step 2\nmodel_ft = create_combined_model(model)\nmodel_ft[0].qconfig = torch.quantization.default_qat_qconfig  # Use default QAT configuration\n# Step 3\nmodel_ft = torch.quantization.prepare_qat(model_ft, inplace=True)",
                        "code"
                    ],
                    {
                        "Finetuning the model": [
                            [
                                "In the current tutorial the whole model is fine tuned. In\ngeneral, this will lead to higher accuracy. However, due to the small\ntraining set used here, we end up overfitting to the training set.",
                                "markdown"
                            ],
                            [
                                "Step 4. Fine tune the model",
                                "markdown"
                            ],
                            [
                                "for param in model_ft.parameters():\n  param.requires_grad = True\n\nmodel_ft.to(device)  # We can fine-tune on GPU if available\n\ncriterion = nn.CrossEntropyLoss()\n\n# Note that we are training everything, so the learning rate is lower\n# Notice the smaller learning rate\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)\n\n# Decay LR by a factor of 0.3 every several epochs\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3)\n\nmodel_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                             num_epochs=25, device=device)",
                                "code"
                            ],
                            [
                                "Step 5. Convert to quantized model",
                                "markdown"
                            ],
                            [
                                "from torch.quantization import convert\nmodel_ft_tuned.cpu()\n\nmodel_quantized_and_trained = convert(model_ft_tuned, inplace=False)",
                                "code"
                            ],
                            [
                                "Lets see how the quantized model performs on a few images",
                                "markdown"
                            ],
                            [
                                "visualize_model(model_quantized_and_trained)\n\nplt.ioff()\nplt.tight_layout()\nplt.show()",
                                "code"
                            ]
                        ]
                    }
                ]
            }
        ],
        "(beta) Static Quantization with Eager Mode in PyTorch": [
            [
                "<strong>Author</strong>: \n<strong>Edited by</strong>: , ",
                "markdown"
            ],
            [
                "This tutorial shows how to do post-training static quantization, as well as illustrating\ntwo more advanced techniques - per-channel quantization and quantization-aware training -\nto further improve the model\u2019s accuracy. Note that quantization is currently only supported\nfor CPUs, so we will not be utilizing GPUs / CUDA in this tutorial.\nBy the end of this tutorial, you will see how quantization in PyTorch can result in\nsignificant decreases in model size while increasing speed. Furthermore, you\u2019ll see how\nto easily apply some advanced quantization techniques shown\n so that your quantized models take much less\nof an accuracy hit than they would otherwise.\nWarning: we use a lot of boilerplate code from other PyTorch repos to, for example,\ndefine the MobileNetV2 model architecture, define data loaders, and so on. We of course\nencourage you to read it; but if you want to get to the quantization features, feel free\nto skip to the \u201c4. Post-training static quantization\u201d section.\nWe\u2019ll start by doing the necessary imports:",
                "markdown"
            ],
            [
                "import os\nimport sys\nimport time\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\n\nimport torchvision\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\n# Set up warnings\nimport warnings\nwarnings.filterwarnings(\n    action='ignore',\n    category=DeprecationWarning,\n    module=r'.*'\n)\nwarnings.filterwarnings(\n    action='default',\n    module=r'torch.ao.quantization'\n)\n\n# Specify random seed for repeatable results\ntorch.manual_seed(191009)",
                "code"
            ],
            {
                "1. Model architecture": [
                    [
                        "We first define the MobileNetV2 model architecture, with several notable modifications\nto enable quantization:",
                        "markdown"
                    ],
                    [
                        "Replacing addition with nn.quantized.FloatFunctional",
                        "markdown"
                    ],
                    [
                        "Insert QuantStub and DeQuantStub at the beginning and end of the network.",
                        "markdown"
                    ],
                    [
                        "Replace ReLU6 with ReLU",
                        "markdown"
                    ],
                    [
                        "Note: this code is taken from\n.",
                        "markdown"
                    ],
                    [
                        "from torch.ao.quantization import QuantStub, DeQuantStub\n\ndef _make_divisible(v, divisor, min_value=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_value:\n    :return:\n    \"\"\"\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v &lt; 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass ConvBNReLU(nn.Sequential):\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n        padding = (kernel_size - 1) // 2\n        super(ConvBNReLU, self).__init__(\n            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n            nn.BatchNorm2d(out_planes, momentum=0.1),\n            # Replace with ReLU\n            nn.ReLU(inplace=False)\n        )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        if expand_ratio != 1:\n            # pw\n            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n        layers.extend([\n            # dw\n            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n            # pw-linear\n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup, momentum=0.1),\n        ])\n        self.conv = nn.Sequential(*layers)\n        # Replace torch.add with floatfunctional\n        self.skip_add = nn.quantized.FloatFunctional()\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return self.skip_add.add(x, self.conv(x))\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=1000, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n        \"\"\"\n        MobileNet V2 main class\n        Args:\n            num_classes (int): Number of classes\n            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n            inverted_residual_setting: Network structure\n            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n            Set to 1 to turn off rounding\n        \"\"\"\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n\n        if inverted_residual_setting is None:\n            inverted_residual_setting = [\n                # t, c, n, s\n                [1, 16, 1, 1],\n                [6, 24, 2, 2],\n                [6, 32, 3, 2],\n                [6, 64, 4, 2],\n                [6, 96, 3, 1],\n                [6, 160, 3, 2],\n                [6, 320, 1, 1],\n            ]\n\n        # only check the first element, assuming user knows t,c,n,s are required\n        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n            raise ValueError(\"inverted_residual_setting should be non-empty \"\n                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n\n        # building first layer\n        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n        features = [ConvBNReLU(3, input_channel, stride=2)]\n        # building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c * width_mult, round_nearest)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*features)\n        self.quant = QuantStub()\n        self.dequant = DeQuantStub()\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, num_classes),\n        )\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.quant(x)\n        x = self.features(x)\n        x = x.mean([2, 3])\n        x = self.classifier(x)\n        x = self.dequant(x)\n        return x\n\n    # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization\n    # This operation does not change the numerics\n    def fuse_model(self):\n        for m in self.modules():\n            if type(m) == ConvBNReLU:\n                torch.ao.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n            if type(m) == InvertedResidual:\n                for idx in range(len(m.conv)):\n                    if type(m.conv[idx]) == nn.Conv2d:\n                        torch.ao.quantization.fuse_modules(m.conv, [str(idx), str(idx + 1)], inplace=True)",
                        "code"
                    ]
                ]
            },
            {
                "2. Helper functions": [
                    [
                        "We next define several helper functions to help with model evaluation. These mostly come from\n.",
                        "markdown"
                    ],
                    [
                        "class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\n\n\ndef evaluate(model, criterion, data_loader, neval_batches):\n    model.eval()\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    cnt = 0\n    with torch.no_grad():\n        for image, target in data_loader:\n            output = model(image)\n            loss = criterion(output, target)\n            cnt += 1\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n            print('.', end = '')\n            top1.update(acc1[0], image.size(0))\n            top5.update(acc5[0], image.size(0))\n            if cnt &gt;= neval_batches:\n                 return top1, top5\n\n    return top1, top5\n\ndef load_model(model_file):\n    model = MobileNetV2()\n    state_dict = torch.load(model_file)\n    model.load_state_dict(state_dict)\n    model.to('cpu')\n    return model\n\ndef print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')",
                        "code"
                    ]
                ]
            },
            {
                "3. Define dataset and data loaders": [
                    [
                        "As our last major setup step, we define our dataloaders for our training and testing set.",
                        "markdown"
                    ],
                    {
                        "ImageNet Data": [
                            [
                                "To run the code in this tutorial using the entire ImageNet dataset, first download imagenet by following the instructions at here . Unzip the downloaded file into the \u2018data_path\u2019 folder.",
                                "markdown"
                            ],
                            [
                                "With the data downloaded, we show functions below that define dataloaders we\u2019ll use to read\nin this data. These functions mostly come from\n.",
                                "markdown"
                            ],
                            [
                                "def prepare_data_loaders(data_path):\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    dataset = torchvision.datasets.ImageNet(\n        data_path, split=\"train\", transform=transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n    dataset_test = torchvision.datasets.ImageNet(\n        data_path, split=\"val\", transform=transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    train_sampler = torch.utils.data.RandomSampler(dataset)\n    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n\n    data_loader = torch.utils.data.DataLoader(\n        dataset, batch_size=train_batch_size,\n        sampler=train_sampler)\n\n    data_loader_test = torch.utils.data.DataLoader(\n        dataset_test, batch_size=eval_batch_size,\n        sampler=test_sampler)\n\n    return data_loader, data_loader_test",
                                "code"
                            ],
                            [
                                "Next, we\u2019ll load in the pre-trained MobileNetV2 model. We provide the URL to download the model\n.",
                                "markdown"
                            ],
                            [
                                "data_path = '~/.data/imagenet'\nsaved_model_dir = 'data/'\nfloat_model_file = 'mobilenet_pretrained_float.pth'\nscripted_float_model_file = 'mobilenet_quantization_scripted.pth'\nscripted_quantized_model_file = 'mobilenet_quantization_scripted_quantized.pth'\n\ntrain_batch_size = 30\neval_batch_size = 50\n\ndata_loader, data_loader_test = prepare_data_loaders(data_path)\ncriterion = nn.CrossEntropyLoss()\nfloat_model = load_model(saved_model_dir + float_model_file).to('cpu')\n\n# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n# while also improving numerical accuracy. While this can be used with any model, this is\n# especially common with quantized models.\n\nprint('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1].conv)\nfloat_model.eval()\n\n# Fuses modules\nfloat_model.fuse_model()\n\n# Note fusion of Conv+BN+Relu and Conv+Relu\nprint('\\n Inverted Residual Block: After fusion\\n\\n',float_model.features[1].conv)",
                                "code"
                            ],
                            [
                                "Finally to get a \u201cbaseline\u201d accuracy, let\u2019s see the accuracy of our un-quantized model\nwith fused modules",
                                "markdown"
                            ],
                            [
                                "num_eval_batches = 1000\n\nprint(\"Size of baseline model\")\nprint_size_of_model(float_model)\n\ntop1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\ntorch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file)",
                                "code"
                            ],
                            [
                                "On the entire model, we get an accuracy of 71.9% on the eval dataset of 50,000 images.",
                                "markdown"
                            ],
                            [
                                "This will be our baseline to compare to. Next, let\u2019s try different quantization methods",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "4. Post-training static quantization": [
                    [
                        "Post-training static quantization involves not just converting the weights from float to int,\nas in dynamic quantization, but also performing the additional step of first feeding batches\nof data through the network and computing the resulting distributions of the different activations\n(specifically, this is done by inserting <cite>observer</cite> modules at different points that record this\ndata). These distributions are then used to determine how the specifically the different activations\nshould be quantized at inference time (a simple technique would be to simply divide the entire range\nof activations into 256 levels, but we support more sophisticated methods as well). Importantly,\nthis additional step allows us to pass quantized values between operations instead of converting these\nvalues to floats - and then back to ints - between every operation, resulting in a significant speed-up.",
                        "markdown"
                    ],
                    [
                        "num_calibration_batches = 32\n\nmyModel = load_model(saved_model_dir + float_model_file).to('cpu')\nmyModel.eval()\n\n# Fuse Conv, bn and relu\nmyModel.fuse_model()\n\n# Specify quantization configuration\n# Start with simple min/max range estimation and per-tensor quantization of weights\nmyModel.qconfig = torch.ao.quantization.default_qconfig\nprint(myModel.qconfig)\ntorch.ao.quantization.prepare(myModel, inplace=True)\n\n# Calibrate first\nprint('Post Training Quantization Prepare: Inserting Observers')\nprint('\\n Inverted Residual Block:After observer insertion \\n\\n', myModel.features[1].conv)\n\n# Calibrate with the training set\nevaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)\nprint('Post Training Quantization: Calibration done')\n\n# Convert to quantized model\ntorch.ao.quantization.convert(myModel, inplace=True)\nprint('Post Training Quantization: Convert done')\nprint('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n',myModel.features[1].conv)\n\nprint(\"Size of model after quantization\")\nprint_size_of_model(myModel)\n\ntop1, top5 = evaluate(myModel, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))",
                        "code"
                    ],
                    [
                        "For this quantized model, we see an accuracy of 56.7% on the eval dataset. This is because we used a simple min/max observer to determine quantization parameters. Nevertheless, we did reduce the size of our model down to just under 3.6 MB, almost a 4x decrease.",
                        "markdown"
                    ],
                    [
                        "In addition, we can significantly improve on the accuracy simply by using a different\nquantization configuration. We repeat the same exercise with the recommended configuration for\nquantizing for x86 architectures. This configuration does the following:",
                        "markdown"
                    ],
                    [
                        "Quantizes weights on a per-channel basis",
                        "markdown"
                    ],
                    [
                        "Uses a histogram observer that collects a histogram of activations and then picks\nquantization parameters in an optimal manner.",
                        "markdown"
                    ],
                    [
                        "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\nper_channel_quantized_model.eval()\nper_channel_quantized_model.fuse_model()\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nper_channel_quantized_model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\nprint(per_channel_quantized_model.qconfig)\n\ntorch.ao.quantization.prepare(per_channel_quantized_model, inplace=True)\nevaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches)\ntorch.ao.quantization.convert(per_channel_quantized_model, inplace=True)\ntop1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\nprint('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\ntorch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_quantized_model_file)",
                        "code"
                    ],
                    [
                        "Changing just this quantization configuration method resulted in an increase\nof the accuracy to over 67.3%! Still, this is 4% worse than the baseline of 71.9% achieved above.\nSo lets try quantization aware training.",
                        "markdown"
                    ]
                ]
            },
            {
                "5. Quantization-aware training": [
                    [
                        "Quantization-aware training (QAT) is the quantization method that typically results in the highest accuracy.\nWith QAT, all weights and activations are \u201cfake quantized\u201d during both the forward and backward passes of\ntraining: that is, float values are rounded to mimic int8 values, but all computations are still done with\nfloating point numbers. Thus, all the weight adjustments during training are made while \u201caware\u201d of the fact\nthat the model will ultimately be quantized; after quantizing, therefore, this method will usually yield\nhigher accuracy than either dynamic quantization or post-training static quantization.",
                        "markdown"
                    ],
                    [
                        "The overall workflow for actually performing QAT is very similar to before:",
                        "markdown"
                    ],
                    [
                        "We can use the same model as before: there is no additional preparation needed for quantization-aware\ntraining.",
                        "markdown"
                    ],
                    [
                        "We need to use a qconfig specifying what kind of fake-quantization is to be inserted after weights\nand activations, instead of specifying observers",
                        "markdown"
                    ],
                    [
                        "We first define a training function:",
                        "markdown"
                    ],
                    [
                        "def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n    model.train()\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n    avgloss = AverageMeter('Loss', '1.5f')\n\n    cnt = 0\n    for image, target in data_loader:\n        start_time = time.time()\n        print('.', end = '')\n        cnt += 1\n        image, target = image.to(device), target.to(device)\n        output = model(image)\n        loss = criterion(output, target)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n        top1.update(acc1[0], image.size(0))\n        top5.update(acc5[0], image.size(0))\n        avgloss.update(loss, image.size(0))\n        if cnt &gt;= ntrain_batches:\n            print('Loss', avgloss.avg)\n\n            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n                  .format(top1=top1, top5=top5))\n            return\n\n    print('Full imagenet train set:  * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}'\n          .format(top1=top1, top5=top5))\n    return",
                        "code"
                    ],
                    [
                        "We fuse modules as before",
                        "markdown"
                    ],
                    [
                        "qat_model = load_model(saved_model_dir + float_model_file)\nqat_model.fuse_model()\n\noptimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n# The old 'fbgemm' is still available but 'x86' is the recommended default.\nqat_model.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')",
                        "code"
                    ],
                    [
                        "Finally, prepare_qat performs the \u201cfake quantization\u201d, preparing the model for quantization-aware training",
                        "markdown"
                    ],
                    [
                        "torch.ao.quantization.prepare_qat(qat_model, inplace=True)\nprint('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n',qat_model.features[1].conv)",
                        "code"
                    ],
                    [
                        "Training a quantized model with high accuracy requires accurate modeling of numerics at\ninference. For quantization aware training, therefore, we modify the training loop by:",
                        "markdown"
                    ],
                    [
                        "Switch batch norm to use running mean and variance towards the end of training to better\nmatch inference numerics.",
                        "markdown"
                    ],
                    [
                        "We also freeze the quantizer parameters (scale and zero-point) and fine tune the weights.",
                        "markdown"
                    ],
                    [
                        "num_train_batches = 20\n\n# QAT takes time and one needs to train over a few epochs.\n# Train and check accuracy after each epoch\nfor nepoch in range(8):\n    train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device('cpu'), num_train_batches)\n    if nepoch &gt; 3:\n        # Freeze quantizer parameters\n        qat_model.apply(torch.ao.quantization.disable_observer)\n    if nepoch &gt; 2:\n        # Freeze batch norm mean and variance estimates\n        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n\n    # Check the accuracy after each epoch\n    quantized_model = torch.ao.quantization.convert(qat_model.eval(), inplace=False)\n    quantized_model.eval()\n    top1, top5 = evaluate(quantized_model,criterion, data_loader_test, neval_batches=num_eval_batches)\n    print('Epoch %d :Evaluation accuracy on %d images, %2.2f'%(nepoch, num_eval_batches * eval_batch_size, top1.avg))",
                        "code"
                    ],
                    [
                        "Quantization-aware training yields an accuracy of over 71.5% on the entire imagenet dataset, which is close to the floating point accuracy of 71.9%.",
                        "markdown"
                    ],
                    [
                        "More on quantization-aware training:",
                        "markdown"
                    ],
                    [
                        "QAT is a super-set of post training quant techniques that allows for more debugging.\nFor example, we can analyze if the accuracy of the model is limited by weight or activation\nquantization.",
                        "markdown"
                    ],
                    [
                        "We can also simulate the accuracy of a quantized model in floating point since\nwe are using fake-quantization to model the numerics of actual quantized arithmetic.",
                        "markdown"
                    ],
                    [
                        "We can mimic post training quantization easily too.",
                        "markdown"
                    ],
                    {
                        "Speedup from quantization": [
                            [
                                "Finally, let\u2019s confirm something we alluded to above: do our quantized models actually perform inference\nfaster? Let\u2019s test:",
                                "markdown"
                            ],
                            [
                                "def run_benchmark(model_file, img_loader):\n    elapsed = 0\n    model = torch.jit.load(model_file)\n    model.eval()\n    num_batches = 5\n    # Run the scripted model on a few batches of images\n    for i, (images, target) in enumerate(img_loader):\n        if i &lt; num_batches:\n            start = time.time()\n            output = model(images)\n            end = time.time()\n            elapsed = elapsed + (end-start)\n        else:\n            break\n    num_images = images.size()[0] * num_batches\n\n    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n    return elapsed\n\nrun_benchmark(saved_model_dir + scripted_float_model_file, data_loader_test)\n\nrun_benchmark(saved_model_dir + scripted_quantized_model_file, data_loader_test)",
                                "code"
                            ],
                            [
                                "Running this locally on a MacBook pro yielded 61 ms for the regular model, and\njust 20 ms for the quantized model, illustrating the typical 2-4x speedup\nwe see for quantized models compared to floating point ones.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Conclusion": [
                    [
                        "In this tutorial, we showed two quantization methods - post-training static quantization,\nand quantization-aware training - describing what they do \u201cunder the hood\u201d and how to use\nthem in PyTorch.",
                        "markdown"
                    ],
                    [
                        "Thanks for reading! As always, we welcome any feedback, so please create an issue\n if you have any.",
                        "markdown"
                    ]
                ]
            }
        ],
        "Grokking PyTorch Intel CPU performance from first principles": [
            [
                "A case study on the TorchServe inference framework optimized with .",
                "markdown"
            ],
            [
                "Authors: Min Jean Cho, Mark Saroufim",
                "markdown"
            ],
            [
                "Reviewers: Ashok Emani, Jiong Gong",
                "markdown"
            ],
            [
                "Getting a strong out-of-box performance for deep learning on CPUs can be tricky but it\u2019s much easier if you\u2019re aware of the main problems that affect performance, how to measure them and how to solve them.",
                "markdown"
            ],
            [
                "TL;DR",
                "markdown"
            ],
            [
                "<em>GEMM (General Matrix Multiply)</em> run on fused-multiply-add (FMA) or dot-product (DP) execution units which will be bottlenecked and cause delays in thread waiting/<em>spinning at synchronization</em> barrier when <em>hyperthreading</em> is enabled - because using logical cores causes insufficient concurrency for all working threads as each logical thread <em>contends for the same core resources</em>. Instead, if we use 1 thread per physical core, we avoid this contention. So we generally recommend <em>avoiding logical cores</em> by setting CPU <em>thread affinity</em> to physical cores via <em>core pinning</em>.",
                "markdown"
            ],
            [
                "Multi-socket systems have <em>Non-Uniform Memory Access (NUMA)</em> which is a shared memory architecture that describes the placement of main memory modules with respect to processors. But if a process is not NUMA-aware, slow <em>remote memory</em> is frequently accessed when <em>threads migrate</em> cross socket via <em>Intel Ultra Path Interconnect (UPI)</em> during run time. We address this problem by setting CPU <em>thread affinity</em> to a specific socket via <em>core pinning</em>.",
                "markdown"
            ],
            [
                "Knowing these principles in mind, proper CPU runtime configuration can significantly boost out-of-box performance.",
                "markdown"
            ],
            [
                "In this blog, we\u2019ll walk you through the important runtime configurations you should be aware of from , explain how they work, how to profile them and how to integrate them within a model serving framework like  via an easy to use  which we\u2019ve  <sup>1</sup> natively.",
                "markdown"
            ],
            [
                "We\u2019ll explain all of these ideas <strong>visually</strong> from <strong>first principles</strong> with lots of <strong>profiles</strong> and show you how we applied our learnings to make out of the box CPU performance on TorchServe better.",
                "markdown"
            ],
            [
                "The feature has to be explicitly enabled by setting <em>cpu_launcher_enable=true</em> in <em>config.properties</em>.",
                "markdown"
            ],
            {
                "Avoid logical cores for deep learning": [
                    [
                        "Avoiding logical cores for deep learning workloads generally improves performance. To understand this, let us take a step back to GEMM.",
                        "markdown"
                    ],
                    [
                        "<strong>Optimizing GEMM optimizes deep learning</strong>",
                        "markdown"
                    ],
                    [
                        "The majority of time in deep learning training or inference is spent on millions of repeated operations of GEMM which is at the core of fully connected layers. Fully connected layers have been used for decades since multi-layer perceptrons (MLP) . Any MLP can be entirely represented as GEMM. And even a convolution can be represented as a GEMM by using a .",
                        "markdown"
                    ],
                    [
                        "Returning to the original topic, most GEMM operators benefit from using non-hyperthreading, because the majority of time in deep learning training or inference is spent on millions of repeated operations of GEMM running on fused-multiply-add (FMA) or dot-product (DP) execution units shared by hyperthreading cores. With hyperthreading enabled, OpenMP threads will contend for the same GEMM execution units.",
                        "markdown"
                    ],
                    [
                        "And if 2 logical threads run GEMM at the same time, they will be sharing the same core resources causing front end bound, such that the overhead from this front end bound is greater than the gain from running both logical threads at the same time.",
                        "markdown"
                    ],
                    [
                        "Therefore we generally recommend avoiding using logical cores for deep learning workloads to achieve good performance. The launch script by default uses physical cores only; however, users can easily experiment with logical vs. physical cores by simply toggling the --use_logical_core launch script knob.",
                        "markdown"
                    ],
                    [
                        "<strong>Exercise</strong>",
                        "markdown"
                    ],
                    [
                        "We\u2019ll use the following example of feeding ResNet50 dummy tensor:",
                        "markdown"
                    ],
                    [
                        "import torch\nimport torchvision.models as models\nimport time\n\nmodel = models.resnet50(pretrained=False)\nmodel.eval()\ndata = torch.rand(1, 3, 224, 224)\n\n# warm up\nfor _ in range(100):\n    model(data)\n\nstart = time.time()\nfor _ in range(100):\n    model(data)\nend = time.time()\nprint('Inference took {:.2f} ms in average'.format((end-start)/100*1000))",
                        "code"
                    ],
                    [
                        "Throughout the blog, we\u2019ll use  to profile and verify optimizations. And we\u2019ll run all exercises on a machine with two Intel(R) Xeon(R) Platinum 8180M CPUs. The CPU information is shown in Figure 2.1.",
                        "markdown"
                    ],
                    [
                        "Environment variable OMP_NUM_THREADS is used to set the number of threads for parallel region. We\u2019ll compare OMP_NUM_THREADS=2 with (1) use of logical cores and (2) use of physical cores only.",
                        "markdown"
                    ],
                    [
                        "Both OpenMP threads trying to utilize the same GEMM execution units shared by hyperthreading cores (0, 56)",
                        "markdown"
                    ],
                    [
                        "We can visualize this by running htop command on Linux as shown below.",
                        "markdown"
                    ],
                    [
                        "We notice that the Spin Time is flagged, and Imbalance or Serial Spinning contributed to the majority of it - 4.980 seconds out of the 8.982 seconds total. The Imbalance or Serial Spinning when using logical cores is due to insufficient concurrency of working threads as each logical thread contends for the same core resources.",
                        "markdown"
                    ],
                    [
                        "The Top Hotspots section of the execution summary indicates that __kmp_fork_barrier took 4.589 seconds of CPU time - during 9.33% of the CPU execution time, threads were just spinning at this barrier due to thread synchronization.",
                        "markdown"
                    ],
                    [
                        "Each OpenMP thread utilizing GEMM execution units in respective physical cores (0,1)",
                        "markdown"
                    ],
                    [
                        "We first note that the execution time dropped from 32 seconds to 23 seconds by avoiding logical cores. While there\u2019s still some non-negligible Imbalance or Serial Spinning, we note relative improvement from 4.980 seconds to 3.887 seconds.",
                        "markdown"
                    ],
                    [
                        "By not using logical threads (instead, using 1 thread per physical core), we avoid logical threads contending for the same core resources. The Top Hotspots section also indicates relative improvement of __kmp_fork_barrier time from 4.589 seconds to 3.530 seconds.",
                        "markdown"
                    ]
                ]
            },
            {
                "Local memory access is always faster than remote memory access": [
                    [
                        "We generally recommend binding a process to a local socket such that the process does not migrate across sockets. Generally the goal of doing so is to utilize high speed cache on local memory and to avoid remote memory access which can be ~2x slower.",
                        "markdown"
                    ],
                    [
                        "Figure 1. Two-socket configuration",
                        "markdown"
                    ],
                    [
                        "Figure 1. shows a typical two-socket configuration. Notice that each socket has its own local memory. Sockets are connected to each other via Intel Ultra Path Interconnect (UPI) which allows each socket to access the local memory of another socket called remote memory. Local memory access is always faster than remote memory access.",
                        "markdown"
                    ],
                    [
                        "Figure 2.1. CPU information",
                        "markdown"
                    ],
                    [
                        "Users can get their CPU information by running lscpu command on their Linux machine. Figure 2.1. shows an example of lscpu  execution on a machine with two Intel(R) Xeon(R) Platinum 8180M CPUs. Notice that there are 28 cores per socket, and 2 threads per core (i.e., hyperthreading is enabled). In other words, there are 28 logical cores in addition to 28 physical cores, giving a total of 56 cores per socket. And there are 2 sockets, giving a total of 112 cores (Thread(s) per core x Core(s) per socket x Socket(s)).",
                        "markdown"
                    ],
                    [
                        "Figure 2.2. CPU information",
                        "markdown"
                    ],
                    [
                        "The 2 sockets are mapped to 2 NUMA nodes (NUMA node 0, NUMA node 1) respectively.  Physical cores are indexed prior to logical cores. As shown in Figure 2.2., the first 28 physical cores (0-27) and the first 28 logical cores (56-83) on the first socket are on NUMA node 0. And the second 28 physical cores (28-55) and the second 28 logical cores (84-111) on the second socket are on NUMA node 1. Cores on the same socket share local memory and last level cache (LLC) which is much faster than cross-socket communication via Intel UPI.",
                        "markdown"
                    ],
                    [
                        "Now that we understand NUMA, cross-socket (UPI) traffic, local vs. remote memory access in multi-processor systems, let\u2019s profile and verify our understanding.",
                        "markdown"
                    ],
                    [
                        "<strong>Exercise</strong>",
                        "markdown"
                    ],
                    [
                        "We\u2019ll reuse the ResNet50 example above.",
                        "markdown"
                    ],
                    [
                        "As we did not pin threads to processor cores of a specific socket, the operating system periodically schedules threads on processor cores located in different sockets.",
                        "markdown"
                    ],
                    [
                        "Figure 3. CPU usage of non NUMA-aware application. 1 main worker thread was launched, then it launched a physical core number (56) of threads on all cores, including logical cores.",
                        "markdown"
                    ],
                    [
                        "(Aside: If the number of threads is not set by , the default number of threads is the number of physical cores in a hyperthreading enabled system. This can be verified by . Hence we see above about half of the cores busy running the example script.)",
                        "markdown"
                    ],
                    [
                        "Figure 4. Non-Uniform Memory Access Analysis graph",
                        "markdown"
                    ],
                    [
                        "Figure 4. compares local vs. remote memory access over time. We verify usage of remote memory which could result in sub-optimal performance.",
                        "markdown"
                    ],
                    [
                        "<strong>Set thread affinity to reduce remote memory access and cross-socket (UPI) traffic</strong>",
                        "markdown"
                    ],
                    [
                        "Pinning threads to cores on the same socket helps maintain locality of memory access. In this example, we\u2019ll pin to the physical cores on the first NUMA node (0-27). With the launch script, users can easily experiment with NUMA nodes configuration by simply toggling the --node_id launch script knob.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s visualize the CPU usage now.",
                        "markdown"
                    ],
                    [
                        "Figure 5. CPU usage of NUMA-aware application",
                        "markdown"
                    ],
                    [
                        "1 main worker thread was launched, then it launched threads on all physical cores on the first numa node.",
                        "markdown"
                    ],
                    [
                        "Figure 6. Non-Uniform Memory Access Analysis graph",
                        "markdown"
                    ],
                    [
                        "As shown in Figure 6., now almost all memory accesses are local accesses.",
                        "markdown"
                    ]
                ]
            },
            {
                "Efficient CPU usage with core pinning for multi-worker inference": [
                    [
                        "When running multi-worker inference, cores are overlapped (or shared) between workers causing inefficient CPU usage. To address this problem, the launch script equally divides the number of available cores by the number of workers such that each worker is pinned to assigned cores during runtime.",
                        "markdown"
                    ],
                    [
                        "<strong>Exercise with TorchServe</strong>",
                        "markdown"
                    ],
                    [
                        "For this exercise, let\u2019s apply the CPU performance tuning principles and recommendations that we have discussed so far to .",
                        "markdown"
                    ],
                    [
                        "We\u2019ll use ResNet50 with 4 workers, concurrency 100, requests 10,000. All other parameters (e.g., batch_size, input, etc) are the same as the .",
                        "markdown"
                    ],
                    [
                        "We\u2019ll compare the following three configurations:",
                        "markdown"
                    ],
                    [
                        "default TorchServe setting (no core pinning)",
                        "markdown"
                    ],
                    [
                        " = number of physical cores / number of workers (no core pinning)",
                        "markdown"
                    ],
                    [
                        "core pinning via the launch script",
                        "markdown"
                    ],
                    [
                        "After this exercise, we\u2019ll have verified that we prefer avoiding logical cores and prefer local memory access via core pinning with a real TorchServe use case.",
                        "markdown"
                    ]
                ]
            },
            {
                "1. Default TorchServe setting (no core pinning)": [
                    [
                        "The  doesn\u2019t explicitly set . Hence the default number of threads is the number of physical CPU cores as described . Users can check the number of threads by  in the base_handler. Each of the 4 main worker threads launches a physical core number (56) of threads, launching a total of 56x4 = 224 threads, which is more than the total number of cores 112.  Therefore cores are guaranteed to be heavily overlapped with high logical core utilization- multiple workers using multiple cores at the same time. Furthermore, because threads are not affinitized to specific CPU cores, the operating system periodically schedules threads to cores located in different sockets.",
                        "markdown"
                    ],
                    [
                        "CPU usage",
                        "markdown"
                    ],
                    [
                        "4 main worker threads were launched, then each launched a physical core number (56) of threads on all cores, including logical cores.",
                        "markdown"
                    ],
                    [
                        "Core Bound stalls",
                        "markdown"
                    ],
                    [
                        "We observe a very high Core Bound stall of 88.4%, decreasing pipeline efficiency. Core Bound stalls indicate sub-optimal use of available execution units in the CPU. For example, several GEMM instructions in a row competing for fused-multiply-add (FMA) or dot-product (DP) execution units shared by hyperthreading cores could cause Core Bound stalls. And as described in the previous section, use of logical cores amplifies this problem.",
                        "markdown"
                    ],
                    [
                        "An empty pipeline slot not filled with micro-ops (uOps) is attributed to a stall. For example, without core pinning CPU usage may not effectively be on compute but on other operations like thread scheduling from Linux kernel. We see above that __sched_yield contributed to the majority of the Spin Time.",
                        "markdown"
                    ],
                    [
                        "Thread Migration",
                        "markdown"
                    ],
                    [
                        "Without core pinning, scheduler may migrate thread executing on a core to a different core. Thread migration can disassociate the thread from data that has already been fetched into the caches resulting in longer data access latencies. This problem is exacerbated in NUMA systems when thread migrates across sockets. Data that has been fetched to high speed cache on local memory now becomes remote memory, which is much slower.",
                        "markdown"
                    ],
                    [
                        "Generally the total number of threads should be less than or equal to the total number of threads supported by the core. In the above example, we notice a large number of threads executing on core_51 instead of the expected 2 threads (since hyperthreading is enabled in Intel(R) Xeon(R) Platinum 8180 CPUs) . This indicates thread migration.",
                        "markdown"
                    ],
                    [
                        "Additionally, notice that thread (TID:97097) was executing on a large number of CPU cores, indicating CPU migration. For example, this thread was executing on cpu_81, then migrated to cpu_14, then migrated to cpu_5, and so on. Furthermore, note that this thread migrated cross socket back and forth many times, resulting in very inefficient memory access. For example, this thread executed on cpu_70 (NUMA node 0), then migrated to cpu_100 (NUMA node 1), then migrated to cpu_24 (NUMA node 0).",
                        "markdown"
                    ],
                    [
                        "Non Uniform Memory Access Analysis",
                        "markdown"
                    ],
                    [
                        "Compare local vs. remote memory access over time. We observe that about half, 51.09%, of the memory accesses were remote accesses, indicating sub-optimal NUMA configuration.",
                        "markdown"
                    ]
                ]
            },
            {
                "2. torch.set_num_threads = number of physical cores / number of workers (no core pinning)": [
                    [
                        "For an apple-to-apple comparison with launcher\u2019s core pinning, we\u2019ll set the number of threads to the number of cores divided by the number of workers (launcher does this internally). Add the following code snippet in the :",
                        "markdown"
                    ],
                    [
                        "torch.set_num_threads(num_physical_cores/num_workers)",
                        "code"
                    ],
                    [
                        "As before without core pinning, these threads are not affinitized to specific CPU cores, causing the operating system to periodically schedule threads on cores located in different sockets.",
                        "markdown"
                    ],
                    [
                        "CPU usage",
                        "markdown"
                    ],
                    [
                        "4 main worker threads were launched, then each launched a num_physical_cores/num_workers number (14) of threads on all cores, including logical cores.",
                        "markdown"
                    ],
                    [
                        "Core Bound stalls",
                        "markdown"
                    ],
                    [
                        "Although the percentage of Core Bound stalls has decreased from 88.4% to 73.5%, the Core Bound is still very high.",
                        "markdown"
                    ],
                    [
                        "Thread Migration",
                        "markdown"
                    ],
                    [
                        "Similar as before, without core pinning thread (TID:94290) was executing on a large number of CPU cores, indicating CPU migration. We notice again cross-socket thread migration, resulting in very inefficient memory access. For example, this thread executed on cpu_78 (NUMA node 0), then migrated to cpu_108 (NUMA node 1).",
                        "markdown"
                    ],
                    [
                        "Non Uniform Memory Access Analysis",
                        "markdown"
                    ],
                    [
                        "Although an improvement from the original 51.09%, still 40.45% of memory access is remote, indicating sub-optimal NUMA configuration.",
                        "markdown"
                    ]
                ]
            },
            {
                "3. launcher core pinning": [
                    [
                        "Launcher will internally equally distribute physical cores to workers, and bind them to each worker. As a reminder, launcher by default uses physical cores only. In this example, launcher will bind worker 0 to cores 0-13 (NUMA node 0), worker 1 to cores 14-27 (NUMA node 0), worker 2 to cores 28-41 (NUMA node 1), and worker 3 to cores 42-55 (NUMA node 1). Doing so ensures that cores are not overlapped among workers and avoids logical core usage.",
                        "markdown"
                    ],
                    [
                        "CPU usage",
                        "markdown"
                    ],
                    [
                        "4 main worker threads were launched, then each launched a num_physical_cores/num_workers number (14) of threads affinitized to the assigned physical cores.",
                        "markdown"
                    ],
                    [
                        "Core Bound stalls",
                        "markdown"
                    ],
                    [
                        "Core Bound stalls has decreased significantly from the original 88.4% to 46.2% - almost a 2x improvement.",
                        "markdown"
                    ],
                    [
                        "We verify that with core binding, most CPU time is effectively used on compute - Spin Time of 0.256s.",
                        "markdown"
                    ],
                    [
                        "Thread Migration",
                        "markdown"
                    ],
                    [
                        "We verify that <cite>OMP Primary Thread #0</cite> was bound to assigned physical cores (42-55), and did not migrate cross-socket.",
                        "markdown"
                    ],
                    [
                        "Non Uniform Memory Access Analysis",
                        "markdown"
                    ],
                    [
                        "Now almost all, 89.52%, memory accesses are local accesses.",
                        "markdown"
                    ]
                ]
            },
            {
                "Conclusion": [
                    [
                        "In this blog, we\u2019ve showcased that properly setting your CPU runtime configuration can significantly boost out-of-box CPU performance.",
                        "markdown"
                    ],
                    [
                        "We have walked through some general CPU performance tuning principles and recommendations:",
                        "markdown"
                    ],
                    [
                        "In a hyperthreading enabled system, avoid logical cores by setting thread affinity to physical cores only via core pinning.",
                        "markdown"
                    ],
                    [
                        "In a multi-socket system with NUMA, avoid cross-socket remote memory access by setting thread affinity to a specific socket via core pinning.",
                        "markdown"
                    ],
                    [
                        "We have visually explained these ideas from first principles and have verified the performance boost with profiling. And finally, we have applied all of our learnings to TorchServe to boost out-of-box TorchServe CPU performance.",
                        "markdown"
                    ],
                    [
                        "These principles can be automatically configured via an easy to use launch script which has already been integrated into TorchServe.",
                        "markdown"
                    ],
                    [
                        "For interested readers, please check out the following documents:",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "And stay tuned for a follow-up posts on optimized kernels on CPU via  and advanced launcher configurations such as memory allocator.",
                        "markdown"
                    ]
                ]
            },
            {
                "Acknowledgement": [
                    [
                        "We would like to thank Ashok Emani (Intel) and Jiong Gong (Intel) for their immense guidance and support, and thorough feedback and reviews throughout many steps of this blog. We would also like to thank Hamid Shojanazeri (Meta), Li Ning (AWS) and Jing Xu (Intel) for helpful feedback in code review. And Suraj Subramanian (Meta) and Geeta Chauhan (Meta) for helpful feedback on the blog.",
                        "markdown"
                    ]
                ]
            }
        ],
        "Grokking PyTorch Intel CPU performance from first principles (Part 2)": [
            [
                "Authors: , , ",
                "markdown"
            ],
            [
                "In the  tutorial\n, we have introduced how to tune CPU runtime configurations, how to profile them, and how to integrate them into  for optimized CPU performance.",
                "markdown"
            ],
            [
                "In this tutorial, we will demonstrate boosting performance with memory allocator via the \n, and optimized kernels on CPU via \n, and apply them to TorchServe showcasing 7.71x throughput speedup for ResNet50 and 2.20x throughput speedup for BERT.",
                "markdown"
            ],
            {
                "Prerequisites": [
                    [
                        "Throughout this tutorial, we will use  to profile and show that the Back End Bound (Memory Bound, Core Bound) is often the primary bottleneck for under-optimized or under-tuned deep learning workloads, and demonstrate optimization techniques via Intel\u00ae Extension for PyTorch* for improving Back End Bound. We will use  , a tool part of  built on top of , for TMA.",
                        "markdown"
                    ],
                    [
                        "We will also use  to profile at finer granularity.",
                        "markdown"
                    ],
                    {
                        "Top-down Microarchitecture Analysis Method (TMA)": [
                            [
                                "When tuning CPU for optimal performance, it\u2019s useful to know where the bottleneck is. Most CPU cores have on-chip Performance Monitoring Units (PMUs). PMUs are dedicated pieces of logic within a CPU core that count specific hardware events as they occur on the system. Examples of these events may be Cache Misses or Branch Mispredictions. PMUs are used for Top-down Microarchitecture Analysis (TMA) to identify the bottlenecks. TMA consists of hierarchical levels as shown:",
                                "markdown"
                            ],
                            [
                                "The top level, level-1, metrics collect <em>Retiring</em>, <em>Bad Speculation</em>, <em>Front End Bound</em>, <em>Back End Bound</em>. The pipeline of CPU can conceptually be simplified and divided into two: the frontend and the backend. The <em>frontend</em> is responsible for fetching the program code and decoding them into low-level hardware operations called micro-ops (uOps). The uOps are then fed to the <em>backend</em> in a process called allocation. Once allocated, the backend is responsible for executing the uOp in an available execution unit. A completion of uOp\u2019s execution is called <em>retirement</em>. In contrast, a <em>bad speculation</em> is when speculatively fetched uOps are canceled before retiring such as in the case of mispredicted branches. Each of these metrics can further be broken down in the subsequent levels to pinpoint the bottleneck.",
                                "markdown"
                            ],
                            {
                                "Tune for the Back End Bound": [
                                    [
                                        "The majority of untuned deep learning workloads will be Back End Bound. Resolving Back End bound is often resolving sources of latency causing retirement to take longer than necessary. As shown above, Back End Bound has two sub-metrics \u2013 Core Bound and Memory Bound.",
                                        "markdown"
                                    ],
                                    [
                                        "Memory Bound stalls have causes related to the memory subsystem. For example, last-level cache (LLC or L3 cache) miss causing access to DRAM. Scaling deep learning models often requires significant compute. And high compute utilization requires that data is available when the execution units need it to execute the uOps. This requires prefetching the data and reusing the data in cache instead of fetching that same data multiple times from main memory which causes execution units to be starved while data is being returned. Throughout this tutorial, we wll show that a more efficient memory allocator, operator fusion, memory layout format optimization reduce overhead on Memory Bound with better cache locality.",
                                        "markdown"
                                    ],
                                    [
                                        "Core Bound stalls indicate sub-optimal use of available execution units while there are no uncompleted memory accesses. For example, several general matrix-matrix multiplication (GEMM) instructions in a row competing for fused-multiply-add (FMA) or dot-product (DP) execution units could cause Core Bound stalls. Key deep learning kernels, including the DP kernels, have been well optimized by  (oneAPI Deep Neural Network Library), reducing overhead on Core Bound.",
                                        "markdown"
                                    ],
                                    [
                                        "Operations like GEMM, convolution, deconvolution are compute-intensive. While operations like pooling, batch normalization, activation functions like ReLU are memory-bound.",
                                        "markdown"
                                    ]
                                ]
                            }
                        ]
                    },
                    {
                        "Intel\u00ae VTune\u2122 Profiler\u2019s Instrumentation and Tracing Technology (ITT)": [
                            [
                                "The ITT APIs of Intel\u00ae VTune Profiler is a useful tool to annotate a region of your workload for tracing to profile and visualize at a finer granularity of your annotation \u2013 OP/function/sub-function granularity. By annotating at the granularity of your PyTorch model\u2019s OPs, Intel\u00ae VTune Profiler\u2019s ITT enables op-level profiling. Intel\u00ae VTune Profiler\u2019s ITT has been integrated into . <sup>1</sup>",
                                "markdown"
                            ],
                            [
                                "The feature has to be explicitly enabled by <em>with torch.autograd.profiler.emit_itt()</em>.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "TorchServe with Intel\u00ae Extension for PyTorch*": [
                    [
                        " is a Python package to extend PyTorch with optimizations for extra performance boost on Intel hardware.",
                        "markdown"
                    ],
                    [
                        "Intel\u00ae Extension for PyTorch* has already been integrated into TorchServe to improve the performance out-of-box. <sup>2</sup> For custom handler scripts, we recommend adding the <em>intel_extension_for_pytorch</em> package in.",
                        "markdown"
                    ],
                    [
                        "The feature has to be explicitly enabled by setting <em>ipex_enable=true</em> in <em>config.properties</em>.",
                        "markdown"
                    ],
                    [
                        "Throughout this section, we will show that Back End Bound is often the primary bottleneck for under-optimized or under-tuned deep learning workloads, and demonstrate optimization techniques via Intel\u00ae Extension for PyTorch* for improving Back End Bound, which has two submetrics - Memory Bound, and Core Bound. A more efficient memory allocator, operator fusion, memory layout format optimization improve Memory Bound. Ideally, Memory Bound can be improved to Core Bound by optimized operators and better cache locality. And key deep learning primitives, such as convolution, matrix multiplication, dot-product, have been well optimized by Intel\u00ae Extension for PyTorch* and oneDNN library, improving Core Bound.",
                        "markdown"
                    ],
                    {
                        "Leveraging Advanced Launcher Configuration: Memory Allocator": [
                            [
                                "Memory allocator plays an important role from performance perspective. A more efficient memory usage reduces overhead on unnecessary memory allocations or destructions, and thus faster execution. For deep learning workloads in practice, especially those running on large multi-core systems or servers like TorchServe, TCMalloc, or JeMalloc can generally get better memory usage than the default PyTorch memory allocator, PTMalloc.",
                                "markdown"
                            ],
                            {
                                "TCMalloc, JeMalloc, PTMalloc": [
                                    [
                                        "Both TCMalloc and JeMalloc use thread-local caches to reduce overhead on thread synchronization, and lock contention by using spinlocks and per-thread arenas respectively. TCMalloc and JeMalloc reduce overhead on unnecessary memory allocation and deallocation. Both allocators categorize memory allocations by sizes to reduce overhead on memory fragmentation.",
                                        "markdown"
                                    ],
                                    [
                                        "With the launcher, users can easily experiment with different memory allocators by choosing one of the three launcher knobs <em>\u2013enable_tcmalloc</em> (TCMalloc), <em>\u2013enable_jemalloc</em> (JeMalloc), <em>\u2013use_default_allocator</em> (PTMalloc).",
                                        "markdown"
                                    ],
                                    {
                                        "Exercise": [
                                            [
                                                "Let\u2019s profile PTMalloc vs. JeMalloc.",
                                                "markdown"
                                            ],
                                            [
                                                "We will use the launcher to designate the memory allocator, and to bind the workload to physical cores of the first socket to avoid any NUMA complication \u2013 to profile the effect of memory allocator only.",
                                                "markdown"
                                            ],
                                            [
                                                "The following example measures the average inference time of ResNet50:",
                                                "markdown"
                                            ],
                                            [
                                                "import torch\nimport torchvision.models as models\nimport time\n\nmodel = models.resnet50(pretrained=False)\nmodel.eval()\nbatch_size = 32\ndata = torch.rand(batch_size, 3, 224, 224)\n\n# warm up\nfor _ in range(100):\n    model(data)\n\n# measure\n# Intel\u00ae VTune Profiler's ITT context manager\nwith torch.autograd.profiler.emit_itt():\n    start = time.time()\n    for i in range(100):\n   # Intel\u00ae VTune Profiler's ITT to annotate each step\n        torch.profiler.itt.range_push('step_{}'.format(i))\n        model(data)\n        torch.profiler.itt.range_pop()\n    end = time.time()\n\nprint('Inference took {:.2f} ms in average'.format((end-start)/100*1000))",
                                                "code"
                                            ],
                                            [
                                                "Let\u2019s collect level-1 TMA metrics.",
                                                "markdown"
                                            ],
                                            [
                                                "Level-1 TMA shows that both PTMalloc and JeMalloc are bounded by the backend. More than half of the execution time was stalled by the backend. Let\u2019s go one level deeper.",
                                                "markdown"
                                            ],
                                            [
                                                "Level-2 TMA shows that the Back End Bound was caused by Memory Bound. Let\u2019s go one level deeper.",
                                                "markdown"
                                            ],
                                            [
                                                "Most of the metrics under the Memory Bound identify which level of the memory hierarchy from the L1 cache to main memory is the bottleneck. A hotspot bounded at a given level indicates that most of the data was being retrieved from that cache or memory-level. Optimizations should focus on moving data closer to the core. Level-3 TMA shows that PTMalloc was bottlenecked by DRAM Bound. On the other hand, JeMalloc was bottlenecked by L1 Bound \u2013 JeMalloc moved data closer to the core, and thus faster execution.",
                                                "markdown"
                                            ],
                                            [
                                                "Let\u2019s look at Intel\u00ae VTune Profiler ITT trace. In the example script, we have annotated each <em>step_x</em> of the inference loop.",
                                                "markdown"
                                            ],
                                            [
                                                "Each step is traced in the timeline graph. The duration of model inference on the last step (step_99) decreased from 304.308 ms to 261.843 ms.",
                                                "markdown"
                                            ]
                                        ]
                                    },
                                    {
                                        "Exercise with TorchServe": [
                                            [
                                                "Let\u2019s profile PTMalloc vs. JeMalloc with TorchServe.",
                                                "markdown"
                                            ],
                                            [
                                                "We will use  with ResNet50 FP32, batch size 32, concurrency 32, requests 8960. All other parameters are the same as the .",
                                                "markdown"
                                            ],
                                            [
                                                "As in the previous exercise, we will use the launcher to designate the memory allocator, and to bind the workload to physical cores of the first socket. To do so, user simply needs to add a few lines in :",
                                                "markdown"
                                            ],
                                            [
                                                "PTMalloc",
                                                "markdown"
                                            ],
                                            [
                                                "cpu_launcher_enable=true\ncpu_launcher_args=--node_id 0 --use_default_allocator",
                                                "code"
                                            ],
                                            [
                                                "JeMalloc",
                                                "markdown"
                                            ],
                                            [
                                                "cpu_launcher_enable=true\ncpu_launcher_args=--node_id 0 --enable_jemalloc",
                                                "code"
                                            ],
                                            [
                                                "Let\u2019s collect level-1 TMA metrics.",
                                                "markdown"
                                            ],
                                            [
                                                "Let\u2019s go one level deeper.",
                                                "markdown"
                                            ],
                                            [
                                                "Let\u2019s use Intel\u00ae VTune Profiler ITT to annotate  to profile at inference-level granularity. As  consists of several sub-components, including the Java frontend for handling request/response, and the Python backend for running the actual inference on the models, it is helpful to use Intel\u00ae VTune Profiler ITT to limit the collection of trace data at inference-level.",
                                                "markdown"
                                            ],
                                            [
                                                "Each inference call is traced in the timeline graph. The duration of the last model inference decreased from 561.688 ms to 251.287 ms - 2.2x speedup.",
                                                "markdown"
                                            ],
                                            [
                                                "The timeline graph can be expanded to see op-level profiling results. The duration of <em>aten::conv2d</em> decreased from 16.401 ms to 6.392 ms - 2.6x speedup.",
                                                "markdown"
                                            ],
                                            [
                                                "In this section, we have demonstrated that JeMalloc can give better performance than the default PyTorch memory allocator, PTMalloc, with efficient thread-local caches improving Back-End-Bound.",
                                                "markdown"
                                            ]
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "Intel\u00ae Extension for PyTorch*": [
                            [
                                "The three major  optimization techniques, Operator, Graph, Runtime, are as shown:",
                                "markdown"
                            ],
                            {
                                "Operator Optimization": [
                                    [
                                        "Optimized operators and kernels are registered through PyTorch dispatching mechanism. These operators and kernels are accelerated from native vectorization feature and matrix calculation feature of Intel hardware. During execution, Intel\u00ae Extension for PyTorch* intercepts invocation of ATen operators, and replaces the original ones with these optimized ones. Popular operators like Convolution, Linear have been optimized in Intel\u00ae Extension for PyTorch*.",
                                        "markdown"
                                    ],
                                    {
                                        "Exercise": [
                                            [
                                                "Let\u2019s profile optimized operator with Intel\u00ae Extension for PyTorch*. We will compare with and without the lines in code changes.",
                                                "markdown"
                                            ],
                                            [
                                                "As in the previous exercises, we will bind the workload to physical cores of the first socket.",
                                                "markdown"
                                            ],
                                            [
                                                "import torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(16, 33, 3, stride=2)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        return x\n\nmodel = Model()\nmodel.eval()\ndata = torch.rand(20, 16, 50, 100)\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model)\n######################################################\n\nprint(model)",
                                                "code"
                                            ],
                                            [
                                                "The model consists of two operations\u2014Conv2d and ReLU. By printing the model object, we get the following output.",
                                                "markdown"
                                            ],
                                            [
                                                "Let\u2019s collect level-1 TMA metrics.",
                                                "markdown"
                                            ],
                                            [
                                                "Notice the Back End Bound reduced from 68.9 to 38.5 \u2013 1.8x speedup.",
                                                "markdown"
                                            ],
                                            [
                                                "Additionally, let\u2019s profile with PyTorch Profiler.",
                                                "markdown"
                                            ],
                                            [
                                                "Notice the CPU time reduced from 851 us to 310 us \u2013 2.7X speedup.",
                                                "markdown"
                                            ]
                                        ]
                                    }
                                ]
                            },
                            {
                                "Graph Optimization": [
                                    [
                                        "It is highly recommended for users to take advantage of Intel\u00ae Extension for PyTorch* with  for further graph optimizations. To optimize performance further with TorchScript, Intel\u00ae Extension for PyTorch* supports oneDNN fusion of frequently used FP32/BF16 operator patterns, like Conv2D+ReLU, Linear+ReLU, and more to reduce operator/kernel invocation overheads, and for better cache locality. Some operator fusions allow to maintain temporary calculations, data type conversions, data layouts for better cache locality. As well as for INT8, Intel\u00ae Extension for PyTorch* has built-in quantization recipes to deliver good statistical accuracy for popular DL workloads including CNN, NLP and recommendation models. The quantized model is then optimized with oneDNN fusion support.",
                                        "markdown"
                                    ],
                                    {
                                        "Exercise": [
                                            [
                                                "Let\u2019s profile FP32 graph optimization with TorchScript.",
                                                "markdown"
                                            ],
                                            [
                                                "As in the previous exercises, we will bind the workload to physical cores of the first socket.",
                                                "markdown"
                                            ],
                                            [
                                                "import torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(16, 33, 3, stride=2)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        return x\n\nmodel = Model()\nmodel.eval()\ndata = torch.rand(20, 16, 50, 100)\n\n#################### code changes ####################\nimport intel_extension_for_pytorch as ipex\nmodel = ipex.optimize(model)\n######################################################\n\n# torchscript\nwith torch.no_grad():\n    model = torch.jit.trace(model, data)\n    model = torch.jit.freeze(model)",
                                                "code"
                                            ],
                                            [
                                                "Let\u2019s collect level-1 TMA metrics.",
                                                "markdown"
                                            ],
                                            [
                                                "Notice the Back End Bound reduced from 67.1 to 37.5 \u2013 1.8x speedup.",
                                                "markdown"
                                            ],
                                            [
                                                "Additionally, let\u2019s profile with PyTorch Profiler.",
                                                "markdown"
                                            ],
                                            [
                                                "Notice that with Intel\u00ae Extension for PyTorch*  Conv + ReLU operators are fused, and the CPU time reduced from 803 us to 248 us \u2013 3.2X speedup. The oneDNN eltwise post-op enables fusing a primitive with an elementwise primitive. This is one of the most popular kinds of fusion: an eltwise (typically an activation function such as ReLU) with preceding convolution or inner product. Have a look at the oneDNN verbose log shown in the next section.",
                                                "markdown"
                                            ]
                                        ]
                                    }
                                ]
                            },
                            {
                                "Channels Last Memory Format": [
                                    [
                                        "When invoking <em>ipex.optimize</em> on model, Intel\u00ae Extension for PyTorch* automatically converts the model to optimized memory format, channels last. Channels last is a memory format that is more friendly to Intel Architecture. Compared to PyTorch default channels first NCHW (batch, channels, height, width) memory format, channels last NHWC (batch, height, width, channels) memory format generally accelerates convolutional neural networks with better cache locality.",
                                        "markdown"
                                    ],
                                    [
                                        "One thing to note is that it is expensive to convert memory format. So it\u2019s better to convert the memory format prior to deployment once, and keep the memory format conversion minimum during deployment. As the data propagates through model\u2019s layers the channels last memory format is preserved through consecutive channels last supported layers (for example, Conv2d -&gt; ReLU -&gt; Conv2d) and conversions are only made in between channels last unsupported layers. See  for more details.",
                                        "markdown"
                                    ],
                                    {
                                        "Exercise": [
                                            [
                                                "Let\u2019s demonstrate channels last optimization.",
                                                "markdown"
                                            ],
                                            [
                                                "import torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = torch.nn.Conv2d(16, 33, 3, stride=2)\n        self.relu = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.relu(x)\n        return x\n\nmodel = Model()\nmodel.eval()\ndata = torch.rand(20, 16, 50, 100)\n\nimport intel_extension_for_pytorch as ipex\n############################### code changes ###############################\nipex.disable_auto_channels_last() # omit this line for channels_last (default)\n############################################################################\nmodel = ipex.optimize(model)\n\nwith torch.no_grad():\n    model = torch.jit.trace(model, data)\n    model = torch.jit.freeze(model)",
                                                "code"
                                            ],
                                            [
                                                "We will use , a tool to help collect information at oneDNN graph level such as operator fusions, kernel execution time spent on executing oneDNN primitives. For more information, refer to the .",
                                                "markdown"
                                            ],
                                            [
                                                "Above is oneDNN verbose from channels first. We can verify that there are reorders from weight and data, then do computation, and finally reorder output back.",
                                                "markdown"
                                            ],
                                            [
                                                "Above is oneDNN verbose from channels last. We can verify that channels last memory format avoids unnecessary reorders.",
                                                "markdown"
                                            ]
                                        ]
                                    }
                                ]
                            }
                        ]
                    },
                    {
                        "Performance Boost with Intel\u00ae Extension for PyTorch*": [
                            [
                                "Below summarizes performance boost of TorchServe with Intel\u00ae Extension for PyTorch* for ResNet50 and BERT-base-uncased.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Exercise with TorchServe": [
                            [
                                "Let\u2019s profile Intel\u00ae Extension for PyTorch* optimizations with TorchServe.",
                                "markdown"
                            ],
                            [
                                "We will use  with ResNet50 FP32 TorchScript, batch size 32, concurrency 32, requests 8960. All other parameters are the same as the .",
                                "markdown"
                            ],
                            [
                                "As in the previous exercise, we will use the launcher to bind the workload to physical cores of the first socket. To do so, user simply needs to add a few lines in :",
                                "markdown"
                            ],
                            [
                                "cpu_launcher_enable=true\ncpu_launcher_args=--node_id 0",
                                "code"
                            ],
                            [
                                "Let\u2019s collect level-1 TMA metrics.",
                                "markdown"
                            ],
                            [
                                "Level-1 TMA shows that both are bounded by the backend. As discussed earlier, the majority of untuned deep learning workloads will be Back End Bound. Notice the Back End Bound reduced from 70.0 to 54.1. Let\u2019s go one level deeper.",
                                "markdown"
                            ],
                            [
                                "As discussed earlier, Back End Bound has two submetrics \u2013 Memory Bound and Core Bound. Memory Bound indicates the workload is under-optimized or under-utilized, and ideally memory-bound operations can be improved to core-bound by optimizing the OPs and improving cache locality. Level-2 TMA shows that the Back End Bound improved from Memory Bound to Core Bound. Let\u2019s go one level deeper.",
                                "markdown"
                            ],
                            [
                                "Scaling deep learning models for production on a model serving framework like TorchServe requires high compute utilization. This requires that data is available through prefetching and reusing the data in cache when the execution units need it to execute the uOps. Level-3 TMA shows that the Back End Memory Bound improved from DRAM Bound to Core Bound.",
                                "markdown"
                            ],
                            [
                                "As in the previous exercise with TorchServe, let\u2019s use Intel\u00ae VTune Profiler ITT to annotate  to profile at inference-level granularity.",
                                "markdown"
                            ],
                            [
                                "Each inference call is traced in the timeline graph. The duration of the last inference call decreased from 215.731 ms to 95.634 ms - 2.3x speedup.",
                                "markdown"
                            ],
                            [
                                "The timeline graph can be expanded to see op-level profiling results. Notice that Conv + ReLU has been fused, and the duration decreased from 6.393 ms + 1.731 ms to 3.408 ms - 2.4x speedup.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Conclusion": [
                    [
                        "In this tutorial, we have used Top-down Microarchitecture Analysis (TMA) and Intel\u00ae VTune\u2122 Profiler\u2019s Instrumentation and Tracing Technology (ITT) to demonstrate that",
                        "markdown"
                    ],
                    [
                        "Often the primary bottleneck of under-optimized or under-tuned deep learning workloads are Back End Bound, which has two submetrics, Memory Bound and Core Bound.",
                        "markdown"
                    ],
                    [
                        "A more efficient memory allocator, operator fusion, memory layout format optimization by Intel\u00ae Extension for PyTorch* improve Memory Bound.",
                        "markdown"
                    ],
                    [
                        "Key deep learning primitives, such as convolution, matrix multiplication, dot-product, etc have been well optimized by Intel\u00ae Extension for PyTorch* and oneDNN library, improving Core Bound.",
                        "markdown"
                    ],
                    [
                        "Intel\u00ae Extension for PyTorch* has been integrated into TorchServe with an ease-of-use API.",
                        "markdown"
                    ],
                    [
                        "TorchServe with Intel\u00ae Extension for PyTorch* shows 7.71x throughput speedup for ResNet50, and 2.20x throughput speedup for BERT.",
                        "markdown"
                    ]
                ]
            },
            {
                "Related Readings": [
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            },
            {
                "Acknowledgement": [
                    [
                        "We would like to thank Ashok Emani (Intel) and Jiong Gong (Intel) for their immense guidance and support, and thorough feedback and reviews throughout many steps of this tutorial. We would also like to thank Hamid Shojanazeri (Meta) and Li Ning (AWS) for their helpful feedback in code review and the tutorial.",
                        "markdown"
                    ]
                ]
            }
        ],
        "Getting Started - Accelerate Your Scripts with nvFuser": [
            [
                "<strong>Authors</strong>: \n\n\n\n\n<cite>Neal Vaidya</cite>",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "This tutorial will demonstrate how you can accelerate your networks\nwith nvFuser. nvFuser is a Deep Learning Compiler that just-in-time\ncompiles fast and flexible GPU specific code to reliably accelerate\nusers\u2019 networks automatically, providing speedups for deep learning\nnetworks running on Volta and later CUDA accelerators by generating\nfast custom \u201cfusion\u201d kernels at runtime. nvFuser is specifically\ndesigned to meet the unique requirements of the PyTorch community,\nand it supports diverse network architectures and programs with\ndynamic inputs of varying shapes and strides.",
                        "markdown"
                    ]
                ]
            },
            {
                "Importing Packages and Selecting a Device": [
                    [
                        "In order to run this tutorial and see the benefits of using nvFuser,\nyou would need to install the <cite>1.12.0</cite> PyTorch release as well as\n<cite>functorch</cite> <cite>0.2</cite> or newer version of them. <cite>functorch</cite> also needs\n<cite>networkx</cite> for its smart recomputation heuristics which you can\ninstall via <cite>pip install networkx</cite>.\nAdditionally, a GPU is required.",
                        "markdown"
                    ],
                    [
                        "import torch\nimport torch.nn.functional as F\nimport functorch\nfrom functorch.compile import memory_efficient_fusion\nfrom copy import deepcopy\nfrom typing import List\nimport time\nimport functools\nimport random\n\nrandom.seed(42)\n\nif torch.__version__ &lt; (1, 12, 0):\n    raise RuntimeError(\n        \"PyTorch &gt;= 1.12.0 required, but your environment uses torch=={}\".format(\n            torch.__version__\n        )\n    )\n\nmajor, minor, _ = functorch.__version__.split(\".\")\nif int(major) == 0 and int(minor) &lt; 2:\n    raise RuntimeError(\n        \"FuncTorch &gt;= 0.2.0 required, but your environment uses functorch=={}\".format(\n            functorch.__version__\n        )\n    )",
                        "code"
                    ]
                ]
            },
            {
                "The Transformer Block": [
                    [
                        "The network topology we\u2019re going to focus on is the Transformer\nBlock for networks like BERT. As of writing this tutorial, nvFuser\nprovides acceleration of pointwise, reduction, and normalization\noperations. These simple operations are the backbone of large\nnetworks, so improving the speed of these operations can improve\noverall network training speed. Future releases of nvFuser will\nimprove the performance of Linear Layers, but for now we will\nspecifically look at the Bias-Dropout-Add-LayerNorm section of this\nTransformer Block.\n\n<img alt=\"../_images/nvfuser_transformer_block.png\" src=\"../_images/nvfuser_transformer_block.png\"/>",
                        "markdown"
                    ],
                    [
                        "First, let\u2019s define the forward pass for this section of our network.\nFor when we\u2019ll use TorchScript on this function, we decorate the\nfunction with type information of the function parameters. This isn\u2019t\nalways required, but it can often help to provide this information to\nTorchScript because it is a strictly typed system. Since we have\nPyTorch\u2019s autograd system, we don\u2019t need to explicitly define the\nbackwards pass.",
                        "markdown"
                    ],
                    [
                        "def composite_definition(\n    : ,\n    : ,\n    : ,\n    : ,\n    : ,\n    normalization_axis: int,\n    dropout_prob: float,\n) -&gt; :\n    bias1_out =  + \n    dropout_out = (bias1_out, dropout_prob, training=True)\n    norm_input = dropout_out + \n    norm_output = (\n        norm_input, (.size(normalization_axis),), , \n    )\n    return norm_output",
                        "code"
                    ]
                ]
            },
            {
                "Setup and Performance Metrics": [
                    [
                        "Next, we initialize some inputs, parameters, and a simulated gradient\noutput tensor for the backwards pass since we aren\u2019t including a\nloss function.",
                        "markdown"
                    ],
                    [
                        "# Setup initial tensors and parameters\ninput_size = [64, 128, 1024]\ndevice = \"cuda\"\n = \n\n# Create sample inputs\n = (*input_size, device=device, =, requires_grad=True)\n = ().requires_grad_()\n\n# Precompute a grad output tensor, for this example it's the same size\n# as the inputs\n = ()\n\n# Randomly initialize the model parameters\n = ((input_size[2], =, device=device))\n = ((input_size[2], =, device=device))\n = ((input_size[2], =, device=device))\n\nparameters = [, , , , ]",
                        "code"
                    ],
                    [
                        "To produce a baseline performance we will measure the speed of our\nforward and backward passes in PyTorch\u2019s default eager mode. To get\naccurate and comparable measurements, we perform a few warm up\niterations. Then, we time many iterations of the forward and backward\npass using performance counters combined with proper GPU\nsynchronization, then compute the average iterations per second.\nIt\u2019s important to be very careful when measuring performance on the\nGPU, as we want to remove any initialization costs and need\nsynchronization since it\u2019s an asynchronous device. Since we will\nmeasure many variations of this problem with and without nvFuser we\ndefine a helper method called <cite>profile_workload</cite> and will use\n<cite>functool.partial</cite> to concisely profile the workload.",
                        "markdown"
                    ],
                    [
                        "# Utility to profile the workload\ndef profile_workload(forward_func, , iteration_count=100, label=\"\"):\n    # Perform warm-up iterations\n    for _ in range(3):\n        # Run model, forward and backward\n         = forward_func()\n        ()\n        # delete gradiens to avoid profiling the gradient accumulation\n        for  in parameters:\n            .grad = None\n\n    # Synchronize the GPU before starting the timer\n    ()\n    start = time.perf_counter()\n    for _ in range(iteration_count):\n        # Run model, forward and backward\n         = forward_func()\n        ()\n        # delete gradiens to avoid profiling the gradient accumulation\n        for  in parameters:\n            .grad = None\n\n    # Synchronize the GPU before stopping the timer\n    ()\n    stop = time.perf_counter()\n    iters_per_second = iteration_count / (stop - start)\n    if label:\n        print(label)\n    print(\"Average iterations per second: {:.2f}\".format(iters_per_second))",
                        "code"
                    ],
                    [
                        "We can now measure a baseline performance of PyTorch\u2019s eager mode\n(without nvFuser).",
                        "markdown"
                    ],
                    [
                        "# Run and profile eager mode execution on the composite definition of our\n# operations.\nfunc = functools.partial(\n    composite_definition,\n    ,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n)\nprofile_workload(\n    func, , iteration_count=100, label=\"Eager Mode - Composite definition\"\n)",
                        "code"
                    ],
                    [
                        "Eager Mode - Composite definition\nAverage iterations per second: 193.33",
                        "code"
                    ],
                    [
                        "It\u2019s important for PyTorch and nvFuser to work well across diverse\nGPU architectures. For our measurements we\u2019ve run this tutorial on\nfive GPUs ranging from consumer to enterprise grade. Our baseline\ngeometric mean (geomean) performance across these GPUs is 850\niterations per second, plotted in the figure below.\n\n<img alt=\"../_images/nvfuser_tutorial_0.png\" src=\"../_images/nvfuser_tutorial_0.png\"/>",
                        "markdown"
                    ],
                    [
                        "As we run different variations of this script with nvFuser, we will\ncontinue to add the results to this figure for the same GPUs.",
                        "markdown"
                    ]
                ]
            },
            {
                "TorchScript &amp; nvFuser": [
                    [
                        "nvFuser is the default fusion system in TorchScript since PyTorch\nversion 1.12, so to turn on nvFuser we need to enable TorchScript.\nThis will allow nvFuser to automatically generate fast kernels and\ntake over execution of these operations. TorchScript can be a\nchallenging system to get working, but with our current definition\nof our operators, all we need to do is wrap our function in the\n<cite>torch.jit.script</cite> compile function. We can then simply run our\nworkload as before.",
                        "markdown"
                    ],
                    [
                        " = (composite_definition)\nfunc = functools.partial(\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n)\nprofile_workload(\n    func, , iteration_count=100, label=\"TorchScript - Composite definition\"\n)",
                        "code"
                    ],
                    [
                        "TorchScript - Composite definition\nAverage iterations per second: 248.66",
                        "code"
                    ],
                    [
                        "Before we get to the results, it is important to mention here that\nnvFuser does not generate the exact same sequence of random numbers,\nas random number generation in PyTorch is dependent on the precise\nparallelization scheme used for the GPU function. Therefore, if you\nwant to validate the output of nvFuser with the output without\nnvFuser, it would require disabling the random number generation\nfunctions. In this example, we would simply need to change\n<cite>dropout_out = F.dropout(bias1_out, dropout_prob, training=True)</cite>\nto\n<cite>dropout_out = F.dropout(bias1_out, dropout_prob, training=False)</cite>\nas the dropout function is the only function in this example that\ndepends on random number generation.\n\n<img alt=\"../_images/nvfuser_tutorial_1.png\" src=\"../_images/nvfuser_tutorial_1.png\"/>",
                        "markdown"
                    ],
                    [
                        "Our geomean performance with nvFuser is 1,394 images per second\nwhich is a geomean of 1.64x faster than eager mode. We did not\ninclude the time that TorchScript and nvFuser take to compile the\nprogram and GPU functions. For real end-to-end training the\ncompile time of TorchScript and nvFuser are negligible. For\nexample, in this tutorial the combination of TorchScript and\nnvFuser took around 2.4s in total to compile these high speed\nGPU functions.",
                        "markdown"
                    ],
                    [
                        "nvFuser\u2019s capabilities extend well beyond this initial performance gain.",
                        "markdown"
                    ]
                ]
            },
            {
                "nvFuser &amp; Dynamic Shapes": [
                    [
                        "It is challenging for Deep Learning Compilers to provide performance\ngains when the user changes the input sizes of the tensors. However,\nsupporting changing shapes has always been a fundamental design\ncriteria for nvFuser, as processing different-sized input tensors is\ncritical to many applications like Natural Language Processing and\nGraph Neural Networks.",
                        "markdown"
                    ],
                    [
                        "To use nvFuser on inputs that change shape from iteration, we\ngenerate new input and output gradient tensors and make a few\ndifferent sizes. Since the last dimension is shared with the\nparameters and cannot be changed dynamically in LayerNorm, we\nperturb the first two dimensions of the input and gradient tensors.",
                        "markdown"
                    ],
                    [
                        "SHAPE_COUNT = 20\ndynamic_sizes = deepcopy(input_size)\n\ninputs1: List[] = []\ninputs2: List[] = []\ngrad_outputs: List[] = []\n\n\n# Create some random shapes\nfor _ in range(SHAPE_COUNT):\n    dynamic_sizes[0] = input_size[0] + random.randrange(-2, 3)\n    dynamic_sizes[1] = input_size[1] + random.randrange(-2, 3)\n    input = (*dynamic_sizes, device=device, =, requires_grad=True)\n    inputs1.append(input)\n    inputs2.append((input))\n    grad_outputs.append((input))",
                        "code"
                    ],
                    [
                        "No changes from before are required for running with TorchScript, we\nsimply reuse the previous definition that we wrapped in\n<cite>torch.jit.script</cite>.",
                        "markdown"
                    ],
                    [
                        "We\u2019ll start as usual by performing some warm-up iterations, however\nwe won\u2019t show nvFuser all of the input sizes, we\u2019ll only show one\nsize for the warm-up.",
                        "markdown"
                    ],
                    [
                        "# Perform warm-up iterations\nfor _ in range(3):\n     = inputs1[0]\n     = inputs2[0]\n     = grad_outputs[0]\n    # Run model, forward and backward\n     = (\n        ,\n        ,\n        ,\n        ,\n        ,\n        normalization_axis=2,\n        dropout_prob=0.1,\n    )\n    ()",
                        "code"
                    ],
                    [
                        "Now, we can measure the performance metrics of nvFuser as we have\npreviously.",
                        "markdown"
                    ],
                    [
                        "# Profile manually as our helper function expects static inputs\niteration_count = 100\n# Synchronize the GPU before starting the timer\n()\nstart = time.perf_counter()\nfor i in range(iteration_count):\n     = inputs1[i % SHAPE_COUNT]\n     = inputs2[i % SHAPE_COUNT]\n     = grad_outputs[i % SHAPE_COUNT]\n    dynamic_parameters = [, , , , ]\n\n    # Run model, forward and backward\n     = (\n        ,\n        ,\n        ,\n        ,\n        ,\n        normalization_axis=2,\n        dropout_prob=0.1,\n    )\n    ()\n    # Delete the gradients to avoid profiling the gradient accumulation\n    for  in dynamic_parameters:\n        .grad = None\n\n# Synchronize the GPU before stopping the timer\n()\nstop = time.perf_counter()\niters_per_second = iteration_count / (stop - start)\nprint(\"TorchScript - Random Sizes\")\nprint(\"Average iterations per second: {:.2f}\".format(iters_per_second))",
                        "code"
                    ],
                    [
                        "TorchScript - Random Sizes\nAverage iterations per second: 244.98",
                        "code"
                    ],
                    [
                        "Performance across our GPUs is very similar to the previous\nperformance seen. Only the performance of the A100 degraded\nslightly, but is still much higher than without nvFuser. The small\nchange in performance of the A100 is actually related to the\nadditional CPU overhead that dynamic shapes cause in nvFuser.\nnvFuser at runtime has to infer how to run the different sized\nkernels, so additional CPU time is consumed. This CPU time is\npresent with all GPUs, but since the A100 runs its functions so fast\nthis CPU overhead cannot be fully hidden by the asynchronous nature\nof GPU execution.\n\n<img alt=\"../_images/nvfuser_tutorial_2.png\" src=\"../_images/nvfuser_tutorial_2.png\"/>",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "Today, nvFuser in TorchScript is the only exposure of\nnvFuser that allows for dynamic shape changes, although we will\nexpand this capability to other systems in the future. For more\ninsight into how dynamic shapes are implemented in nvFuser, you can\nview this presentation from GTC 2021:",
                        "markdown"
                    ]
                ]
            },
            {
                "Defining novel operations with nvFuser and FuncTorch": [
                    [
                        "One of the primary benefits of nvFuser is the ability to define\nnovel operations composed of PyTorch \u201cprimitives\u201d which are then\njust-in-time compiled into efficient kernels.",
                        "markdown"
                    ],
                    [
                        "PyTorch has strong performance for any individual operation,\nespecially composite operations like LayerNorm. However, if\nLayerNorm wasn\u2019t already implemented in PyTorch as a composite\noperation, then you\u2019d have to define it as a series of simpler\n(primitive) operations. Let\u2019s make such a definition and run it\nwithout nvFuser.",
                        "markdown"
                    ],
                    [
                        "def primitive_definition(\n    : ,\n    : ,\n    : ,\n    : ,\n    : ,\n    normalization_axis: int,\n    dropout_prob: float,\n    keepdim: bool,\n) -&gt; :\n    bias1_out =  + \n    dropout_out = (bias1_out, dropout_prob, training=True)\n    norm_input = dropout_out + \n    mean = norm_input.mean(normalization_axis, keepdim=keepdim)\n    diff = norm_input - mean\n    diff_sq = diff * diff\n    var = diff_sq.mean(normalization_axis, keepdim=keepdim)\n    pre_shift_scale_norm_output = (norm_input - mean) / (var + 1e-12)\n    norm_output =  * pre_shift_scale_norm_output + \n    return norm_output\n\n\n# Profile primitive definition\nfunc = functools.partial(\n    primitive_definition,\n    ,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(\n    func, , iteration_count=100, label=\"Eager Mode - Primitive Definition\"\n)",
                        "code"
                    ],
                    [
                        "Eager Mode - Primitive Definition\nAverage iterations per second: 61.56",
                        "code"
                    ],
                    [
                        "While the above is mathematically equivalent to our previous\ndefinition, benchmarking our new function with the original static\nshape using TorchScript and nvFuser shows the iterations per second\ndecreases \u2013 mostly due to the cost of accessing memory to save\nintermediate results.\n\n<img alt=\"../_images/nvfuser_tutorial_3.png\" src=\"../_images/nvfuser_tutorial_3.png\"/>",
                        "markdown"
                    ],
                    [
                        "The geomean iterations per second is 260 iterations per second,\n3.26x slower than the composite definition in eager mode and 5.35x\nslower than the nvFuser composite operation! For more information on\nwhy there\u2019s such a drastic decrease in compute speed please see this\npresentation from GTC 2022:",
                        "markdown"
                    ],
                    [
                        "nvFuser with TorchScript can improve the performance of this\noperation even though it\u2019s defined with primitive PyTorch\noperations. Simply by enabling TorchScript on the new function\n(just like before), we can see much of the performance returns.",
                        "markdown"
                    ],
                    [
                        "# Profile scripted primitive definition\n = (primitive_definition)\nfunc = functools.partial(\n    ,\n    ,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(\n    func, , iteration_count=100, label=\"TorchScript - Primitive definition\"\n)",
                        "code"
                    ],
                    [
                        "TorchScript - Primitive definition\nAverage iterations per second: 120.92\n\n\n\n<img alt=\"../_images/nvfuser_tutorial_4.png\" src=\"../_images/nvfuser_tutorial_4.png\"/>",
                        "code"
                    ],
                    [
                        "However, the performance is still slower than the original eager\nmode performance of the composite definition. TorchScript works well\nwhen predefined composite operations are used, however TorchScript\u2019s\napplication of Autograd saves all of the activations for each\noperator in the fusion for re-use in the backwards pass. However,\nthis is not typically the optimal choice. Especially when chaining\ntogether multiple simple operations, it is often much faster to\nrecompute some intermediate tensors rather than spend the time\nstoring and retrieving several saved results from memory.",
                        "markdown"
                    ],
                    [
                        "It\u2019s possible to optimize away many of these unnecessary memory\naccesses, but it requires building a connected forward and backward\ngraph which isn\u2019t possible with TorchScript. The\n<cite>memory_efficient_fusion</cite> pass in FuncTorch, however, is such an\noptimization pass. To use this pass, we have to redefine our\nfunction to pull the constants inside (for now it\u2019s easiest to make\nnon-tensor constants literals in the function definition):",
                        "markdown"
                    ],
                    [
                        "def primitive_definition_for_memory_efficient_fusion(\n    : ,\n    : ,\n    : ,\n    : ,\n    : ,\n) -&gt; :\n    bias1_out =  + \n    dropout_out = (bias1_out, 0.1, training=True)\n    norm_input = dropout_out + \n    mean = norm_input.mean(2, keepdim=True)\n    diff = norm_input - mean\n    diff_sq = diff * diff\n    var = diff_sq.mean(2, keepdim=True)\n    pre_shift_scale_norm_output = (norm_input - mean) / (var + 1e-12)\n    norm_output =  * pre_shift_scale_norm_output + \n    return norm_output",
                        "code"
                    ],
                    [
                        "Now, instead of passing our function to TorchScript, we will pass it\nto FuncTorch\u2019s optimization pass.",
                        "markdown"
                    ],
                    [
                        "# Optimize the model with FuncTorch tracing and the memory efficiency\n# optimization pass\nmemory_efficient_primitive_definition = memory_efficient_fusion(\n    primitive_definition_for_memory_efficient_fusion\n)\n\n# Profile memory efficient primitive definition\nfunc = functools.partial(\n    memory_efficient_primitive_definition, , , , , \n)\nprofile_workload(\n    func,\n    ,\n    iteration_count=100,\n    label=\"FuncTorch - Primitive definition\",\n)",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torch/jit/_check.py:172: UserWarning:\n\nThe TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n\nFuncTorch - Primitive definition\nAverage iterations per second: 125.71",
                        "code"
                    ],
                    [
                        "This recovers even more speed, but it\u2019s still not as fast as\nTorchScripts original performance with the composite definition.\nHowever, this is still faster than running this new definition\nwithout nvFuser, and is still faster than the composite definition\nwithout nvFuser.\n\n<img alt=\"../_images/nvfuser_tutorial_5.png\" src=\"../_images/nvfuser_tutorial_5.png\"/>",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "FuncTorch\u2019s memory efficient pass is experimental and still\nactively in development.\nFuture versions of the API are expected to achieve performance\ncloser to that of TorchScript with the composite definition.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "FuncTorch\u2019s memory efficient pass specializes on the shapes of\nthe inputs to the function. If new inputs are provided with\ndifferent shapes, then you need to construct a new function\nusing <cite>memory_efficient_fusion</cite> and apply it to the new inputs.",
                        "markdown"
                    ]
                ]
            },
            {
                "Transformer Block With a Novel Normalization": [
                    [
                        "The ability to quickly execute chains of simple operations is\nimportant as not every operation has a composite operation defined\nin PyTorch. Previously, this meant researchers either had to define\nan entirely new operation in PyTorch \u2013 which takes a lot of time and\nknowledge of the lower-level PyTorch code as well as parallel\nprogramming \u2013 or writing the operation in simpler PyTorch ops and\nsettling for poor performance. For example, let\u2019s replace LayerNorm\nin our example with RMSNorm. Even though RMSNorm is a bit simpler\nthan LayerNorm, it doesn\u2019t have an existing compound operation in\nPyTorch. See the  paper for more information about RMSNorm.\nAs before, we\u2019ll define our new transformer block with\nprimitive PyTorch operations.",
                        "markdown"
                    ],
                    [
                        "def with_rms_norm(\n    : ,\n    : ,\n    : ,\n    bias: ,\n    normalization_axis: int,\n    dropout_prob: float,\n    keepdim: bool,\n) -&gt; :\n    bias_out =  + bias\n    dropout_out = (bias_out, dropout_prob, training=True)\n    norm_input = dropout_out + \n    var = norm_input.mul(norm_input).mean(normalization_axis, keepdim)\n    pre_shift_scale_norm_output = norm_input / (var + 1e-12)\n    norm_output =  * pre_shift_scale_norm_output\n    return norm_output",
                        "code"
                    ],
                    [
                        "As before, we\u2019ll get a baseline by running PyTorch without nvFuser.",
                        "markdown"
                    ],
                    [
                        "# Profile rms_norm\nfunc = functools.partial(\n    with_rms_norm,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"Eager Mode - RMS Norm\")",
                        "code"
                    ],
                    [
                        "Eager Mode - RMS Norm\nAverage iterations per second: 83.70",
                        "code"
                    ],
                    [
                        "With nvFuser through TorchScript.",
                        "markdown"
                    ],
                    [
                        "# Profile scripted rms_norm\n = (with_rms_norm)\nfunc = functools.partial(\n    ,\n    ,\n    ,\n    ,\n    ,\n    normalization_axis=2,\n    dropout_prob=0.1,\n    keepdim=True,\n)\nprofile_workload(func, , iteration_count=100, label=\"TorchScript - RMS Norm\")",
                        "code"
                    ],
                    [
                        "TorchScript - RMS Norm\nAverage iterations per second: 172.33",
                        "code"
                    ],
                    [
                        "With nvFuser through Functorch.",
                        "markdown"
                    ],
                    [
                        "def with_rms_norm_for_memory_efficient_fusion(\n    : , : , : , bias: \n) -&gt; :\n    bias_out =  + bias\n    dropout_out = (bias_out, 0.1)\n    norm_input = dropout_out + \n    var = norm_input.mul(norm_input).mean(2, keepdim=True)\n    pre_shift_scale_norm_output = norm_input / (var + 1e-12)\n    norm_output =  * pre_shift_scale_norm_output\n    return norm_output\n\n\n# Profile memory efficient rms_norm\nmemory_efficient_rms_norm = memory_efficient_fusion(\n    with_rms_norm_for_memory_efficient_fusion\n)\nfunc = functools.partial(memory_efficient_rms_norm, , , , )\nprofile_workload(func, , iteration_count=100, label=\"FuncTorch - RMS Norm\")",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torch/jit/_check.py:172: UserWarning:\n\nThe TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n\nFuncTorch - RMS Norm\nAverage iterations per second: 201.74\n\n\n\n<img alt=\"../_images/nvfuser_tutorial_6.png\" src=\"../_images/nvfuser_tutorial_6.png\"/>",
                        "code"
                    ],
                    [
                        "Since RMSNorm is simpler than LayerNorm the performance of our new\ntransformer block is a little higher than the primitive definition\nwithout nvFuser (354 iterations per second compared with 260\niterations per second). With TorchScript, the iterations per second\nincreases by 2.68x and 3.36x to 952 iterations per second and 1,191\niterations per second with TorchScript and FuncTorch\u2019s memory\nefficient optimization pass, respectively. The performance of this\nnew operation nearly matches the performance of the composite Layer\nNorm definition with TorchScript.",
                        "markdown"
                    ],
                    [
                        "nvFuser is here to provide the ability to define novel operations in\nsimple PyTorch and get performance that\u2019s close to a highly optimized\ncomposite operation in PyTorch. We believe this will enable research\ninto novel network topologies without paying for sometimes devastating\neffects on speed of training. nvFuser provides this unique ability as\nit\u2019s able to analyze users\u2019 programs to provide performance as fast as a\nhighly hand tuned implementation, regardless of how the operations are\ndefined. nvFuser still cannot support every operation in PyTorch,\nhowever its capabilities will continue to grow over time.",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  19.393 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Multi-Objective NAS with Ax": [
            [
                "<strong>Authors:</strong> ,\n,\nand the Adaptive Experimentation team at Meta.",
                "markdown"
            ],
            [
                "In this tutorial, we show how to use  to run\nmulti-objective neural architecture search (NAS) for a simple neural\nnetwork model on the popular MNIST dataset. While the underlying\nmethodology would typically be used for more complicated models and\nlarger datasets, we opt for a tutorial that is easily runnable\nend-to-end on a laptop in less than 20 minutes.",
                "markdown"
            ],
            [
                "In many NAS applications, there is a natural tradeoff between multiple\nobjectives of interest. For instance, when deploying models on-device\nwe may want to maximize model performance (for example, accuracy), while\nsimultaneously minimizing competing metrics like power consumption,\ninference latency, or model size in order to satisfy deployment\nconstraints. Often, we may be able to reduce computational requirements\nor latency of predictions substantially by accepting minimally lower\nmodel performance. Principled methods for exploring such tradeoffs\nefficiently are key enablers of scalable and sustainable AI, and have\nmany successful applications at Meta - see for instance our\n\non a Natural Language Understanding model.",
                "markdown"
            ],
            [
                "In our example here, we will tune the widths of two hidden layers,\nthe learning rate, the dropout probability, the batch size, and the\nnumber of training epochs. The goal is to trade off performance\n(accuracy on the validation set) and model size (the number of\nmodel parameters).",
                "markdown"
            ],
            [
                "This tutorial makes use of the following PyTorch libraries:",
                "markdown"
            ],
            [
                " (specifying the model and training loop)",
                "markdown"
            ],
            [
                " (for running training jobs remotely / asynchronously)",
                "markdown"
            ],
            [
                " (the Bayesian Optimization library powering Ax\u2019s algorithms)",
                "markdown"
            ],
            {
                "Defining the TorchX App": [
                    [
                        "Our goal is to optimize the PyTorch Lightning training job defined in\n.\nTo do this using TorchX, we write a helper function that takes in\nthe values of the architcture and hyperparameters of the training\njob and creates a \nwith the appropriate settings.",
                        "markdown"
                    ],
                    [
                        "from pathlib import Path\n\nimport torchx\n\nfrom torchx import specs\nfrom torchx.components import utils\n\n\ndef trainer(\n    log_path: str,\n    hidden_size_1: int,\n    hidden_size_2: int,\n    learning_rate: float,\n    epochs: int,\n    dropout: float,\n    batch_size: int,\n    trial_idx: int = -1,\n) -&gt; specs.AppDef:\n\n    # define the log path so we can pass it to the TorchX AppDef\n    if trial_idx &gt;= 0:\n        log_path = Path(log_path).joinpath(str(trial_idx)).absolute().as_posix()\n\n    return utils.python(\n        # command line args to the training script\n        \"--log_path\",\n        log_path,\n        \"--hidden_size_1\",\n        str(hidden_size_1),\n        \"--hidden_size_2\",\n        str(hidden_size_2),\n        \"--learning_rate\",\n        str(learning_rate),\n        \"--epochs\",\n        str(epochs),\n        \"--dropout\",\n        str(dropout),\n        \"--batch_size\",\n        str(batch_size),\n        # other config options\n        name=\"trainer\",\n        script=\"mnist_train_nas.py\",\n        image=torchx.version.TORCHX_IMAGE,\n    )",
                        "code"
                    ]
                ]
            },
            {
                "Setting up the Runner": [
                    [
                        "Ax\u2019s \nabstraction allows writing interfaces to various backends.\nAx already comes with Runner for TorchX, and so we just need to\nconfigure it. For the purpose of this tutorial we run jobs locally\nin a fully asynchronous fashion.",
                        "markdown"
                    ],
                    [
                        "In order to launch them on a cluster, you can instead specify a\ndifferent TorchX scheduler and adjust the configuration appropriately.\nFor example, if you have a Kubernetes cluster, you just need to change the\nscheduler from local_cwd to kubernetes).",
                        "markdown"
                    ],
                    [
                        "import tempfile\nfrom ax.runners.torchx import TorchXRunner\n\n# Make a temporary dir to log our results into\nlog_dir = tempfile.mkdtemp()\n\nax_runner = TorchXRunner(\n    tracker_base=\"/tmp/\",\n    component=trainer,\n    # NOTE: To launch this job on a cluster instead of locally you can\n    # specify a different scheduler and adjust args appropriately.\n    scheduler=\"local_cwd\",\n    component_const_params={\"log_path\": log_dir},\n    cfg={},\n)",
                        "code"
                    ]
                ]
            },
            {
                "Setting up the SearchSpace": [
                    [
                        "First, we define our search space. Ax supports both range parameters\nof type integer and float as well as choice parameters which can have\nnon-numerical types such as strings.\nWe will tune the hidden sizes, learning rate, dropout, and number of\nepochs as range parameters and tune the batch size as an ordered choice\nparameter to enforce it to be a power of 2.",
                        "markdown"
                    ],
                    [
                        "from ax.core import (\n    ChoiceParameter,\n    ParameterType,\n    RangeParameter,\n    SearchSpace,\n)\n\nparameters = [\n    # NOTE: In a real-world setting, hidden_size_1 and hidden_size_2\n    # should probably be powers of 2, but in our simple example this\n    # would mean that num_params can't take on that many values, which\n    # in turn makes the Pareto frontier look pretty weird.\n    RangeParameter(\n        name=\"hidden_size_1\",\n        lower=16,\n        upper=128,\n        parameter_type=ParameterType.INT,\n        log_scale=True,\n    ),\n    RangeParameter(\n        name=\"hidden_size_2\",\n        lower=16,\n        upper=128,\n        parameter_type=ParameterType.INT,\n        log_scale=True,\n    ),\n    RangeParameter(\n        name=\"learning_rate\",\n        lower=1e-4,\n        upper=1e-2,\n        parameter_type=ParameterType.FLOAT,\n        log_scale=True,\n    ),\n    RangeParameter(\n        name=\"epochs\",\n        lower=1,\n        upper=4,\n        parameter_type=ParameterType.INT,\n    ),\n    RangeParameter(\n        name=\"dropout\",\n        lower=0.0,\n        upper=0.5,\n        parameter_type=ParameterType.FLOAT,\n    ),\n    ChoiceParameter(  # NOTE: ChoiceParameters don't require log-scale\n        name=\"batch_size\",\n        values=[32, 64, 128, 256],\n        parameter_type=ParameterType.INT,\n        is_ordered=True,\n        sort_values=True,\n    ),\n]\n\nsearch_space = SearchSpace(\n    parameters=parameters,\n    # NOTE: In practice, it may make sense to add a constraint\n    # hidden_size_2 &lt;= hidden_size_1\n    parameter_constraints=[],\n)",
                        "code"
                    ]
                ]
            },
            {
                "Setting up Metrics": [
                    [
                        "Ax has the concept of a \nthat defines properties of outcomes and how observations are obtained\nfor these outcomes. This allows e.g. encodig how data is fetched from\nsome distributed execution backend and post-processed before being\npassed as input to Ax.",
                        "markdown"
                    ],
                    [
                        "In this tutorial we will use\n\nwith the goal of maximizing the validation accuracy and minimizing\nthe number of model parameters. The latter represents a simple proxy\nof model latency, which is hard to estimate accurately for small ML\nmodels (in an actual application we would benchmark the latency while\nrunning the model on-device).",
                        "markdown"
                    ],
                    [
                        "In our example TorchX will run the training jobs in a fully asynchronous\nfashion locally and write the results to the log_dir based on the trial\nindex (see the trainer() function above). We will define a metric\nclass that is aware of that logging directory. By subclassing\n\nwe get the logic to read and parse the Tensorboard logs for free.",
                        "markdown"
                    ],
                    [
                        "from ax.metrics.tensorboard import TensorboardCurveMetric\n\n\nclass MyTensorboardMetric(TensorboardCurveMetric):\n\n    # NOTE: We need to tell the new Tensorboard metric how to get the id /\n    # file handle for the tensorboard logs from a trial. In this case\n    # our convention is to just save a separate file per trial in\n    # the pre-specified log dir.\n    @classmethod\n    def get_ids_from_trials(cls, trials):\n        return {\n            trial.index: Path(log_dir).joinpath(str(trial.index)).as_posix()\n            for trial in trials\n        }\n\n    # This indicates whether the metric is queryable while the trial is\n    # still running. We don't use this in the current tutorial, but Ax\n    # utilizes this to implement trial-level early-stopping functionality.\n    @classmethod\n    def is_available_while_running(cls):\n        return False",
                        "code"
                    ],
                    [
                        "Now we can instatiate the metrics for accuracy and the number of\nmodel parameters. Here <cite>curve_name</cite> is the name of the metric in the\nTensorboard logs, while <cite>name</cite> is the metric name used internally\nby Ax. We also specify <cite>lower_is_better</cite> to indicate the favorable\ndirection of the two metrics.",
                        "markdown"
                    ],
                    [
                        "val_acc = MyTensorboardMetric(\n    name=\"val_acc\",\n    curve_name=\"val_acc\",\n    lower_is_better=False,\n)\nmodel_num_params = MyTensorboardMetric(\n    name=\"num_params\",\n    curve_name=\"num_params\",\n    lower_is_better=True,\n)",
                        "code"
                    ]
                ]
            },
            {
                "Setting up the OptimizationConfig": [
                    [
                        "The way to tell Ax what it should optimize is by means of an\n.\nHere we use a MultiObjectiveOptimizationConfig as we will\nbe performing multi-objective optimization.",
                        "markdown"
                    ],
                    [
                        "Additionally, Ax supports placing constraints on the different\nmetrics by specifying objective thresholds, which bound the region\nof interest in the outcome space that we want to explore. For this\nexample, we will constrain the validation accuracy to be at least\n0.94 (94%) and the number of model parameters to be at most 80,000.",
                        "markdown"
                    ],
                    [
                        "from ax.core import MultiObjective, Objective, ObjectiveThreshold\nfrom ax.core.optimization_config import MultiObjectiveOptimizationConfig\n\n\nopt_config = MultiObjectiveOptimizationConfig(\n    objective=MultiObjective(\n        objectives=[\n            Objective(metric=val_acc, minimize=False),\n            Objective(metric=model_num_params, minimize=True),\n        ],\n    ),\n    objective_thresholds=[\n        ObjectiveThreshold(metric=val_acc, bound=0.94, relative=False),\n        ObjectiveThreshold(metric=model_num_params, bound=80_000, relative=False),\n    ],\n)",
                        "code"
                    ]
                ]
            },
            {
                "Creating the Ax Experiment": [
                    [
                        "In Ax, the \nobject is the object that stores all the information about the problem\nsetup.",
                        "markdown"
                    ],
                    [
                        "from ax.core import Experiment\n\nexperiment = Experiment(\n    name=\"torchx_mnist\",\n    search_space=search_space,\n    optimization_config=opt_config,\n    runner=ax_runner,\n)",
                        "code"
                    ]
                ]
            },
            {
                "Choosing the GenerationStrategy": [
                    [
                        "A \nis the abstract representation of how we would like to perform the\noptimization. While this can be customized (if you\u2019d like to do so, see\n),\nin most cases Ax can automatically determine an appropriate strategy\nbased on the search space, optimization config, and the total number\nof trials we want to run.",
                        "markdown"
                    ],
                    [
                        "Typically, Ax chooses to evaluate a number of random configurations\nbefore starting a model-based Bayesian Optimization strategy.",
                        "markdown"
                    ],
                    [
                        "total_trials = 48  # total evaluation budget\n\nfrom ax.modelbridge.dispatch_utils import choose_generation_strategy\n\ngs = choose_generation_strategy(\n    search_space=experiment.search_space,\n    optimization_config=experiment.optimization_config,\n    num_trials=total_trials,\n  )",
                        "code"
                    ]
                ]
            },
            {
                "Configuring the Scheduler": [
                    [
                        "The <cite>Scheduler</cite> (TODO: link) acts as the loop control for the optimization.\nIt communicates with the backend to launch trials, check their status,\nand retrieve results. In the case of this tutorial, it is simply reading\nand parsing the locally saved logs. In a remote execution setting,\nit would call APIs. The following illustration from the Ax\n\nsummarizes how the Scheduler interacts with external systems used to run\ntrial evaluations:\n<img alt=\"../_static/img/ax_scheduler_illustration.png\" src=\"../_static/img/ax_scheduler_illustration.png\"/>",
                        "markdown"
                    ],
                    [
                        "The Scheduler requires the Experiment and the GenerationStrategy.\nA set of options can be passed in via SchedulerOptions. Here, we\nconfigure the number of total evaluations as well as max_pending_trials,\nthe maximum number of trials that should run concurrently. In our\nlocal setting, this is the number of training jobs running as individual\nprocesses, while in a remote execution setting, this would be the number\nof machines you want to use in parallel.",
                        "markdown"
                    ],
                    [
                        "from ax.service.scheduler import Scheduler, SchedulerOptions\n\nscheduler = Scheduler(\n    experiment=experiment,\n    generation_strategy=gs,\n    options=SchedulerOptions(\n        total_trials=total_trials, max_pending_trials=4\n    ),\n)",
                        "code"
                    ]
                ]
            },
            {
                "Running the optimization": [
                    [
                        "Now that everything is configured, we can let Ax run the optimization\nin a fully automated fashion. The Scheduler will periodially check\nthe logs for the status of all currently running trials, and if a\ntrial completes the scheduler will update its status on the\nexperiment and fetch the observations needed for the Bayesian\noptimization algorithm.",
                        "markdown"
                    ],
                    [
                        "scheduler.run_all_trials()",
                        "code"
                    ]
                ]
            },
            {
                "Evaluating the results": [
                    [
                        "We can now inspect the result of the optimization using helper\nfunctions and visualizations included with Ax.",
                        "markdown"
                    ],
                    [
                        "First, we generate a dataframe with a summary of the results\nof the experiment. Each row in this dataframe corresponds to a\ntrial (that is, a training job that was run), and contains information\non the status of the trial, the parameter configuration that was\nevaluated, and the metric values that were observed. This provides\nan easy way to sanity check the optimization.",
                        "markdown"
                    ],
                    [
                        "from ax.service.utils.report_utils import exp_to_df\n\ndf = exp_to_df(experiment)\ndf.head(10)",
                        "code"
                    ],
                    [
                        "We can also visualize the Pareto frontier of tradeoffs between the\nvalidation accuracy and the number of model parameters.",
                        "markdown"
                    ],
                    [
                        "Tip",
                        "markdown"
                    ],
                    [
                        "Ax uses Plotly to produce interactive plots, which allow you to\ndo things like zoom, crop, or hover in order to view details\nof components of the plot. Try it out, and take a look at the\n\nif you\u2019d like to learn more).",
                        "markdown"
                    ],
                    [
                        "The final optimization results are shown in the figure below where\nthe color corresponds to the iteration number for each trial.\nWe see that our method was able to successfully explore the\ntrade-offs and found both large models with high validation\naccuracy as well as small models with comparatively lower\nvalidation accuracy.",
                        "markdown"
                    ],
                    [
                        "from ax.service.utils.report_utils import _pareto_frontier_scatter_2d_plotly\n\n_pareto_frontier_scatter_2d_plotly(experiment)",
                        "code"
                    ],
                    [
                        "To better understand what our surrogate models have learned about\nthe black box objectives, we can take a look at the leave-one-out\ncross validation results. Since our models are Gaussian Processes,\nthey not only provide point predictions but also uncertainty estimates\nabout these predictions. A good model means that the predicted means\n(the points in the figure) are close to the 45 degree line and that the\nconfidence intervals cover the 45 degree line with the expected frequency\n(here we use 95% confidence intervals, so we would expect them to contain\nthe true observation 95% of the time).",
                        "markdown"
                    ],
                    [
                        "As the figures below show, the model size (num_params) metric is\nmuch easier to model than the validation accuracy (val_acc) metric.",
                        "markdown"
                    ],
                    [
                        "from ax.modelbridge.cross_validation import compute_diagnostics, cross_validate\nfrom ax.plot.diagnostic import interact_cross_validation_plotly\nfrom ax.utils.notebook.plotting import init_notebook_plotting, render\n\ncv = cross_validate(model=gs.model)  # The surrogate model is stored on the GenerationStrategy\ncompute_diagnostics(cv)\n\ninteract_cross_validation_plotly(cv)",
                        "code"
                    ],
                    [
                        "We can also make contour plots to better understand how the different\nobjectives depend on two of the input parameters. In the figure below,\nwe show the validation accuracy predicted by the model as a function\nof the two hidden sizes. The validation accuracy clearly increases\nas the hidden sizes increase.",
                        "markdown"
                    ],
                    [
                        "from ax.plot.contour import interact_contour_plotly\n\ninteract_contour_plotly(model=gs.model, metric_name=\"val_acc\")",
                        "code"
                    ],
                    [
                        "Similarly, we show the number of model parameters as a function of\nthe hidden sizes in the figure below and see that it also increases\nas a function of the hidden sizes (the dependency on hidden_size_1\nis much larger).",
                        "markdown"
                    ],
                    [
                        "interact_contour_plotly(model=gs.model, metric_name=\"num_params\")",
                        "code"
                    ]
                ]
            },
            {
                "Acknowledgements": [
                    [
                        "We thank the TorchX team (in particular Kiuk Chung and Tristan Rice)\nfor their help with integrating TorchX with Ax.",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "torch.compile Tutorial": [
            [
                "<strong>Author:</strong> William Wen",
                "markdown"
            ],
            [
                "torch.compile is the latest method to speed up your PyTorch code!\ntorch.compile makes PyTorch code run faster by\nJIT-compiling PyTorch code into optimized kernels,\nall while requiring minimal code changes.",
                "markdown"
            ],
            [
                "In this tutorial, we cover basic torch.compile usage,\nand demonstrate the advantages of torch.compile over\nprevious PyTorch compiler solutions, such as\n and\n.",
                "markdown"
            ],
            [
                "<strong>Contents</strong>",
                "markdown"
            ],
            [
                "Basic Usage",
                "markdown"
            ],
            [
                "Demonstrating Speedups",
                "markdown"
            ],
            [
                "Comparison to TorchScript and FX Tracing",
                "markdown"
            ],
            [
                "TorchDynamo and FX Graphs",
                "markdown"
            ],
            [
                "Conclusion",
                "markdown"
            ],
            [
                "<strong>Required pip Dependencies</strong>",
                "markdown"
            ],
            [
                "torch &gt;= 2.0",
                "markdown"
            ],
            [
                "torchvision",
                "markdown"
            ],
            [
                "numpy",
                "markdown"
            ],
            [
                "scipy",
                "markdown"
            ],
            [
                "tabulate",
                "markdown"
            ],
            [
                "Note: a modern NVIDIA GPU (Volta or Ampere) is recommended for this tutorial.",
                "markdown"
            ],
            {
                "Basic Usage": [
                    [
                        "torch.compile is included in the latest PyTorch nightlies.\nRunning TorchInductor on GPU requires Triton, which is included with the PyTorch 2.0 nightly\nbinary. If Triton is still missing, try installing torchtriton via pip\n(pip install torchtriton --extra-index-url \"https://download.pytorch.org/whl/nightly/cu117\"\nfor CUDA 11.7).",
                        "markdown"
                    ],
                    [
                        "Arbitrary Python functions can be optimized by passing the callable to\ntorch.compile. We can then call the returned optimized\nfunction in place of the original function.",
                        "markdown"
                    ],
                    [
                        "import torch\n\ndef foo(x, y):\n    a = (x)\n    b = (x)\n    return a + b\nopt_foo1 = (foo)\nprint(opt_foo1((10, 10), (10, 10)))",
                        "code"
                    ],
                    [
                        "tensor([[ 0.1798,  1.2844,  1.1928, -0.6577,  1.2001,  1.2458,  1.3295,  1.0679,\n         -0.5651,  1.2174],\n        [ 0.4180,  0.8621,  0.4904,  1.2344,  1.3135,  1.0205,  1.1573,  1.4137,\n          0.6963,  0.1998],\n        [ 0.8186,  1.3638,  1.1769,  1.1858,  0.1157,  0.5435,  1.2660,  0.0452,\n          1.4071,  1.2790],\n        [ 0.1720,  0.7816,  1.1149,  0.2946,  1.0323,  1.3251,  1.2137,  0.7413,\n          1.3243,  1.3410],\n        [-0.5763, -0.8852,  0.0997,  0.5206,  1.2721, -0.8215,  1.1307,  0.6280,\n          0.9548, -0.4519],\n        [ 0.5277,  1.1179,  0.8299,  1.1068,  1.1056, -0.9118,  1.1369,  0.1787,\n          1.4070,  1.2695],\n        [ 1.1402,  1.3995,  1.3867,  1.4065, -0.1616,  1.3379,  0.5723,  1.4052,\n          0.9187,  0.7396],\n        [ 1.4136,  0.9392,  1.1151, -0.1921,  0.6561,  1.3462, -0.8778,  0.0633,\n         -0.8164,  1.3842],\n        [-0.2066,  1.3659,  1.4083,  1.1795,  0.8535, -1.2620,  0.5423, -0.1378,\n          0.9112,  1.4097],\n        [-0.7677,  0.8684, -0.4427,  1.2253,  1.3321,  1.2597,  1.1444,  0.5330,\n          1.2131,  0.6155]])",
                        "code"
                    ],
                    [
                        "Alternatively, we can decorate the function.",
                        "markdown"
                    ],
                    [
                        "@torch.compile\ndef opt_foo2(x, y):\n    a = (x)\n    b = (x)\n    return a + b\nprint(opt_foo2((10, 10), (10, 10)))",
                        "code"
                    ],
                    [
                        "tensor([[ 1.3819, -0.0023,  0.3889, -0.3917,  0.3476,  0.8949, -0.1183,  1.2301,\n         -0.7860,  0.5604],\n        [ 1.2832,  0.9980, -1.3994,  0.3217,  1.4017,  1.1034,  1.3659,  1.2822,\n          0.8117, -1.4010],\n        [ 0.0598,  1.3736,  1.1738,  1.4119,  0.9839,  1.2038,  1.4142, -0.9450,\n         -0.6137,  1.3810],\n        [ 0.1338,  1.4103,  0.9247,  0.5043,  1.4142, -0.2322,  1.0996,  0.8513,\n          0.9817, -0.7160],\n        [ 0.8685,  1.1138, -0.5582,  0.7401,  1.0455,  0.6791,  1.3404,  0.9539,\n          0.6790,  0.5284],\n        [ 0.8021,  1.3120, -1.0966,  1.2934, -0.0197, -1.1680,  0.5190,  1.3328,\n         -0.6529, -0.8327],\n        [ 0.2134, -0.8958,  1.2135, -0.7837,  0.8079,  0.9630,  1.3625,  0.7420,\n          1.1731,  1.3631],\n        [ 1.3967, -0.9851,  0.8375,  1.0817, -0.0596, -0.4823,  1.0790,  0.1602,\n         -0.3130,  1.2681],\n        [ 1.3737,  1.0730,  1.3989,  0.8458, -1.1168,  0.7682,  1.2389,  0.8003,\n          1.2728,  1.3231],\n        [-0.3265,  1.1508, -0.0631,  1.3986,  1.0830,  0.8830,  1.1943, -0.0505,\n         -0.8125,  0.9741]])",
                        "code"
                    ],
                    [
                        "We can also optimize torch.nn.Module instances.",
                        "markdown"
                    ],
                    [
                        "class MyModule():\n    def __init__(self):\n        super().__init__()\n        self.lin = (100, 10)\n\n    def forward(self, x):\n        return (self.lin(x))\n\nmod = ()\n = (mod)\nprint(((10, 100)))",
                        "code"
                    ],
                    [
                        "tensor([[0.6872, -0.0000, 0.6799, 0.8060, -0.0000, -0.0000, 0.4238, 0.1070, 0.1104,\n         -0.0000],\n        [-0.0000, 0.0631, -0.0000, 1.3911, 0.0685, 0.0367, -0.0000, 0.4435, -0.0000,\n         -0.0000],\n        [-0.0000, 0.8475, -0.0000, -0.0000, 1.2670, 0.0052, -0.0000, -0.0000, 1.4696,\n         0.7722],\n        [-0.0000, 0.3604, 0.7189, 0.1741, -0.0000, 0.0089, 0.4830, -0.0000, -0.0000,\n         -0.0000],\n        [0.2388, -0.0000, 0.3756, 0.1995, -0.0000, 0.3563, 1.1686, -0.0000, -0.0000,\n         -0.0000],\n        [-0.0000, 0.3833, 0.2159, -0.0000, 0.0479, -0.0000, -0.0000, -0.0000, 0.3953,\n         0.7013],\n        [-0.0000, 0.6361, -0.0000, -0.0000, 1.6845, 0.3640, -0.0000, 0.0496, 0.7504,\n         0.4540],\n        [-0.0000, -0.0000, 0.8502, -0.0000, 0.1043, -0.0000, -0.0000, 0.1842, 0.0391,\n         0.3200],\n        [-0.0000, -0.0000, -0.0000, 0.7361, -0.0000, -0.0000, -0.0000, 0.4034, -0.0000,\n         1.0752],\n        [0.7645, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, 1.0489,\n         -0.0000]], grad_fn=&lt;CompiledFunctionBackward&gt;)",
                        "code"
                    ]
                ]
            },
            {
                "Demonstrating Speedups": [
                    [
                        "Let\u2019s now demonstrate that using torch.compile can speed\nup real models. We will compare standard eager mode and\ntorch.compile by evaluating and training ResNet-18 on random data.",
                        "markdown"
                    ],
                    [
                        "Before we start, we need to define some utility functions.",
                        "markdown"
                    ],
                    [
                        "# Returns the result of running `fn()` and the time it took for `fn()` to run,\n# in seconds. We use CUDA events and synchronization for the most accurate\n# measurements.\ndef timed(fn):\n    start = (enable_timing=True)\n    end = (enable_timing=True)\n    start.record()\n    result = fn()\n    end.record()\n    ()\n    return result, start.elapsed_time(end) / 1000\n\n# Generates random input and targets data for the model, where `b` is\n# batch size.\ndef generate_data(b):\n    return (\n        (b, 3, 128, 128).to().cuda(),\n        (1000, (b,)).cuda(),\n    )\n\nN_ITERS = 10\n\nfrom torchvision.models import \ndef init_model():\n    return ().to().cuda()",
                        "code"
                    ],
                    [
                        "First, let\u2019s compare inference.",
                        "markdown"
                    ],
                    [
                        "Note that in the call to torch.compile, we have have the additional\nmode kwarg, which we will discuss below.",
                        "markdown"
                    ],
                    [
                        "def evaluate(mod, inp):\n    return mod(inp)\n\nmodel = init_model()\n\n# Reset since we are using a different mode.\nimport torch._dynamo\n()\n\nevaluate_opt = (evaluate, mode=\"reduce-overhead\")\n\ninp = generate_data(16)[0]\nprint(\"eager:\", timed(lambda: evaluate(model, inp))[1])\nprint(\"compile:\", timed(lambda: evaluate_opt(model, inp))[1])",
                        "code"
                    ],
                    [
                        "eager: 2.106290283203125\ncompile: 10.0058740234375",
                        "code"
                    ],
                    [
                        "Notice that torch.compile takes a lot longer to complete\ncompared to eager. This is because torch.compile compiles\nthe model into optimized kernels as it executes. In our example, the\nstructure of the model doesn\u2019t change, and so recompilation is not\nneeded. So if we run our optimized model several more times, we should\nsee a significant improvement compared to eager.",
                        "markdown"
                    ],
                    [
                        "eager_times = []\ncompile_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)[0]\n    _, eager_time = timed(lambda: evaluate(model, inp))\n    eager_times.append(eager_time)\n    print(f\"eager eval time {i}: {eager_time}\")\n\nprint(\"~\" * 10)\n\ncompile_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)[0]\n    _, compile_time = timed(lambda: evaluate_opt(model, inp))\n    compile_times.append(compile_time)\n    print(f\"compile eval time {i}: {compile_time}\")\nprint(\"~\" * 10)\n\nimport numpy as np\neager_med = np.median(eager_times)\ncompile_med = np.median(compile_times)\nspeedup = eager_med / compile_med\nprint(f\"(eval) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\")\nprint(\"~\" * 10)",
                        "code"
                    ],
                    [
                        "eager eval time 0: 0.009543840408325194\neager eval time 1: 0.008958080291748046\neager eval time 2: 0.0089717435836792\neager eval time 3: 0.008945695877075195\neager eval time 4: 0.008947936058044434\neager eval time 5: 0.008927264213562013\neager eval time 6: 0.008937503814697266\neager eval time 7: 0.008948351860046387\neager eval time 8: 0.008941663742065429\neager eval time 9: 0.00894159984588623\n~~~~~~~~~~\ncompile eval time 0: 0.009375519752502441\ncompile eval time 1: 0.009378111839294434\ncompile eval time 2: 0.009247039794921875\ncompile eval time 3: 0.009245856285095215\ncompile eval time 4: 0.008823455810546875\ncompile eval time 5: 0.00714246416091919\ncompile eval time 6: 0.007155712127685547\ncompile eval time 7: 0.007184415817260742\ncompile eval time 8: 0.007147552013397217\ncompile eval time 9: 0.007140128135681152\n~~~~~~~~~~\n(eval) eager median: 0.008946815967559814, compile median: 0.00800393581390381, speedup: 1.1178020633321555x\n~~~~~~~~~~",
                        "code"
                    ],
                    [
                        "And indeed, we can see that running our model with torch.compile\nresults in a significant speedup. Speedup mainly comes from reducing Python overhead and\nGPU read/writes, and so the observed speedup may vary on factors such as model\narchitecture and batch size. For example, if a model\u2019s architecture is simple\nand the amount of data is large, then the bottleneck would be\nGPU compute and the observed speedup may be less significant.",
                        "markdown"
                    ],
                    [
                        "You may also see different speedup results depending on the chosen mode\nkwarg. Since our model and data are small, we want to reduce overhead as\nmuch as possible, and so we chose \"reduce-overhead\". For your own models,\nyou may need to experiment with different modes to maximize speedup. You can\nread more about modes .",
                        "markdown"
                    ],
                    [
                        "For general PyTorch benchmarking, you can try using torch.utils.benchmark instead of the timed\nfunction we defined above. We wrote our own timing function in this tutorial to show\ntorch.compile\u2019s compilation latency.",
                        "markdown"
                    ],
                    [
                        "Now, let\u2019s consider comparing training.",
                        "markdown"
                    ],
                    [
                        "model = init_model()\n = (())\n\ndef train(mod, data):\n    (True)\n    pred = mod(data[0])\n    loss = ()(pred, data[1])\n    loss.backward()\n    ()\n\neager_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)\n    _, eager_time = timed(lambda: train(model, inp))\n    eager_times.append(eager_time)\n    print(f\"eager train time {i}: {eager_time}\")\nprint(\"~\" * 10)\n\nmodel = init_model()\n = (())\ntrain_opt = (train, mode=\"reduce-overhead\")\n\ncompile_times = []\nfor i in range(N_ITERS):\n    inp = generate_data(16)\n    _, compile_time = timed(lambda: train_opt(model, inp))\n    compile_times.append(compile_time)\n    print(f\"compile train time {i}: {compile_time}\")\nprint(\"~\" * 10)\n\neager_med = np.median(eager_times)\ncompile_med = np.median(compile_times)\nspeedup = eager_med / compile_med\nprint(f\"(train) eager median: {eager_med}, compile median: {compile_med}, speedup: {speedup}x\")\nprint(\"~\" * 10)",
                        "code"
                    ],
                    [
                        "eager train time 0: 0.4737531127929687\neager train time 1: 0.02575574493408203\neager train time 2: 0.025766271591186524\neager train time 3: 0.02569219207763672\neager train time 4: 0.02566761589050293\neager train time 5: 0.026013599395751954\neager train time 6: 0.025726463317871092\neager train time 7: 0.025708160400390624\neager train time 8: 0.025737247467041015\neager train time 9: 0.02180371284484863\n~~~~~~~~~~\ncompile train time 0: 22.97644140625\ncompile train time 1: 0.02164531135559082\ncompile train time 2: 0.02067158317565918\ncompile train time 3: 0.02125984001159668\ncompile train time 4: 0.021174720764160156\ncompile train time 5: 0.020945056915283203\ncompile train time 6: 0.021309440612792968\ncompile train time 7: 0.021041824340820314\ncompile train time 8: 0.020995264053344728\ncompile train time 9: 0.021283327102661134\n~~~~~~~~~~\n(train) eager median: 0.02573185539245605, compile median: 0.02121728038787842, speedup: 1.2127782129493299x\n~~~~~~~~~~",
                        "code"
                    ],
                    [
                        "Again, we can see that torch.compile takes longer in the first\niteration, as it must compile the model, but in subsequent iterations, we see\nsignificant speedups compared to eager.",
                        "markdown"
                    ]
                ]
            },
            {
                "Comparison to TorchScript and FX Tracing": [
                    [
                        "We have seen that torch.compile can speed up PyTorch code.\nWhy else should we use torch.compile over existing PyTorch\ncompiler solutions, such as TorchScript or FX Tracing? Primarily, the\nadvantage of torch.compile lies in its ability to handle\narbitrary Python code with minimal changes to existing code.",
                        "markdown"
                    ],
                    [
                        "One case that torch.compile can handle that other compiler\nsolutions struggle with is data-dependent control flow (the\nif x.sum() &lt; 0: line below).",
                        "markdown"
                    ],
                    [
                        "def f1(x, y):\n    if x.sum() &lt; 0:\n        return -y\n    return y\n\n# Test that `fn1` and `fn2` return the same result, given\n# the same arguments `args`. Typically, `fn1` will be an eager function\n# while `fn2` will be a compiled function (torch.compile, TorchScript, or FX graph).\ndef test_fns(fn1, fn2, args):\n    out1 = fn1(*args)\n    out2 = fn2(*args)\n    return (out1, out2)\n\n = (5, 5)\n = (5, 5)",
                        "code"
                    ],
                    [
                        "TorchScript tracing f1 results in\nsilently incorrect results, since only the actual control flow path\nis traced.",
                        "markdown"
                    ],
                    [
                        " = (f1, (, ))\nprint(\"traced 1, 1:\", test_fns(f1, , (, )))\nprint(\"traced 1, 2:\", test_fns(f1, , (-, )))",
                        "code"
                    ],
                    [
                        "/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py:254: TracerWarning:\n\nConverting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n\ntraced 1, 1: True\ntraced 1, 2: False",
                        "code"
                    ],
                    [
                        "FX tracing f1 results in an error due to the presence of\ndata-dependent control flow.",
                        "markdown"
                    ],
                    [
                        "import traceback as tb\ntry:\n    (f1)\nexcept:\n    tb.print_exc()",
                        "code"
                    ],
                    [
                        "Traceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 284, in &lt;module&gt;\n    torch.fx.symbolic_trace(f1)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 1109, in symbolic_trace\n    graph = tracer.trace(root, concrete_args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 209, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 778, in trace\n    (self.create_arg(fn(*args)),),\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 254, in f1\n    if x.sum() &lt; 0:\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/proxy.py\", line 413, in __bool__\n    return self.tracer.to_bool(self)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/proxy.py\", line 276, in to_bool\n    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow",
                        "code"
                    ],
                    [
                        "If we provide a value for x as we try to FX trace f1, then\nwe run into the same problem as TorchScript tracing, as the data-dependent\ncontrol flow is removed in the traced function.",
                        "markdown"
                    ],
                    [
                        "fx_f1 = (f1, concrete_args={\"x\": })\nprint(\"fx 1, 1:\", test_fns(f1, fx_f1, (, )))\nprint(\"fx 1, 2:\", test_fns(f1, fx_f1, (-, )))",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py:602: UserWarning:\n\nWas not able to add assertion to guarantee correct input x to specialized function. It is up to the user to make sure that your inputs match the inputs you specialized the function with.\n\nfx 1, 1: True\nfx 1, 2: False",
                        "code"
                    ],
                    [
                        "Now we can see that torch.compile correctly handles\ndata-dependent control flow.",
                        "markdown"
                    ],
                    [
                        "# Reset since we are using a different mode.\n()\n\ncompile_f1 = (f1)\nprint(\"compile 1, 1:\", test_fns(f1, compile_f1, (, )))\nprint(\"compile 1, 2:\", test_fns(f1, compile_f1, (-, )))\nprint(\"~\" * 10)",
                        "code"
                    ],
                    [
                        "compile 1, 1: True\ncompile 1, 2: True\n~~~~~~~~~~",
                        "code"
                    ],
                    [
                        "TorchScript scripting can handle data-dependent control flow, but this\nsolution comes with its own set of problems. Namely, TorchScript scripting\ncan require major code changes and will raise errors when unsupported Python\nis used.",
                        "markdown"
                    ],
                    [
                        "In the example below, we forget TorchScript type annotations and we receive\na TorchScript error because the input type for argument y, an int,\ndoes not match with the default argument type, torch.Tensor.",
                        "markdown"
                    ],
                    [
                        "def f2(x, y):\n    return x + y\n\n = (5, 5)\n = 3\n\n = (f2)\ntry:\n    (, )\nexcept:\n    tb.print_exc()",
                        "code"
                    ],
                    [
                        "Traceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 327, in &lt;module&gt;\n    script_f2(inp1, inp2)\nRuntimeError: f2() Expected a value of type 'Tensor (inferred)' for argument 'y' but instead found type 'int'.\nInferred 'y' to be of type 'Tensor' because it was not annotated with an explicit type.\nPosition: 1\nValue: 3\nDeclaration: f2(Tensor x, Tensor y) -&gt; Tensor\nCast error details: Unable to cast 3 to Tensor",
                        "code"
                    ],
                    [
                        "However, torch.compile is easily able to handle f2.",
                        "markdown"
                    ],
                    [
                        "compile_f2 = (f2)\nprint(\"compile 2:\", test_fns(f2, compile_f2, (, )))\nprint(\"~\" * 10)",
                        "code"
                    ],
                    [
                        "compile 2: True\n~~~~~~~~~~",
                        "code"
                    ],
                    [
                        "Another case that torch.compile handles well compared to\nprevious compilers solutions is the usage of non-PyTorch functions.",
                        "markdown"
                    ],
                    [
                        "import scipy\ndef f3(x):\n    x = x * 2\n    x = scipy.fft.dct(x.numpy())\n    x = (x)\n    x = x * 2\n    return x",
                        "code"
                    ],
                    [
                        "TorchScript tracing treats results from non-PyTorch function calls\nas constants, and so our results can be silently wrong.",
                        "markdown"
                    ],
                    [
                        " = (5, 5)\n = (5, 5)\n = (f3, (,))\nprint(\"traced 3:\", test_fns(f3, , (,)))",
                        "code"
                    ],
                    [
                        "/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py:345: TracerWarning:\n\nConverting a tensor to a NumPy array might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n\n/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py:346: TracerWarning:\n\ntorch.from_numpy results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n\ntraced 3: False",
                        "code"
                    ],
                    [
                        "TorchScript scripting and FX tracing disallow non-PyTorch function calls.",
                        "markdown"
                    ],
                    [
                        "try:\n    (f3)\nexcept:\n    tb.print_exc()\n\ntry:\n    (f3)\nexcept:\n    tb.print_exc()",
                        "code"
                    ],
                    [
                        "Traceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 363, in &lt;module&gt;\n    torch.jit.script(f3)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/jit/_script.py\", line 1341, in script\n    fn = torch._C._jit_script_compile(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_jit_internal.py\", line 1198, in _try_get_dispatched_fn\n    return boolean_dispatched.get(fn)\n  File \"/opt/conda/lib/python3.10/weakref.py\", line 453, in get\n    return self.data.get(ref(key),default)\nTypeError: cannot create weak reference to 'uarray._Function' object\nTraceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 368, in &lt;module&gt;\n    torch.fx.symbolic_trace(f3)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 1109, in symbolic_trace\n    graph = tracer.trace(root, concrete_args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 209, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 778, in trace\n    (self.create_arg(fn(*args)),),\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 345, in f3\n    x = scipy.fft.dct(x.numpy())\n  File \"/opt/conda/lib/python3.10/site-packages/scipy/fft/_backend.py\", line 25, in __ua_function__\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/scipy/fft/_pocketfft/realtransforms.py\", line 19, in _r2r\n    tmp = _asfarray(x)\n  File \"/opt/conda/lib/python3.10/site-packages/scipy/fft/_pocketfft/helper.py\", line 89, in _asfarray\n    if x.dtype == np.float16:\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/proxy.py\", line 518, in impl\n    return tracer.create_proxy('call_function', target, args, kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/proxy.py\", line 151, in create_proxy\n    args_ = self.create_arg(args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 373, in create_arg\n    return super().create_arg(a)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/proxy.py\", line 239, in create_arg\n    return type(a)(self.create_arg(elem) for elem in a)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/proxy.py\", line 239, in &lt;genexpr&gt;\n    return type(a)(self.create_arg(elem) for elem in a)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/_symbolic_trace.py\", line 373, in create_arg\n    return super().create_arg(a)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/fx/proxy.py\", line 267, in create_arg\n    raise NotImplementedError(f\"argument of type: {type(a)}\")\nNotImplementedError: argument of type: &lt;class 'type'&gt;",
                        "code"
                    ],
                    [
                        "In comparison, torch.compile is easily able to handle\nthe non-PyTorch function call.",
                        "markdown"
                    ],
                    [
                        "compile_f3 = (f3)\nprint(\"compile 3:\", test_fns(f3, compile_f3, (,)))",
                        "code"
                    ],
                    [
                        "compile 3: True",
                        "code"
                    ]
                ]
            },
            {
                "TorchDynamo and FX Graphs": [
                    [
                        "One important component of torch.compile is TorchDynamo.\nTorchDynamo is responsible for JIT compiling arbitrary Python code into\n, which can\nthen be further optimized. TorchDynamo extracts FX graphs by analyzing Python bytecode\nduring runtime and detecting calls to PyTorch operations.",
                        "markdown"
                    ],
                    [
                        "Normally, TorchInductor, another component of torch.compile,\nfurther compiles the FX graphs into optimized kernels,\nbut TorchDynamo allows for different backends to be used. In order to inspect\nthe FX graphs that TorchDynamo outputs, let us create a custom backend that\noutputs the FX graph and simply returns the graph\u2019s unoptimized forward method.",
                        "markdown"
                    ],
                    [
                        "from typing import List\ndef custom_backend(gm: , example_inputs: List[]):\n    print(\"custom backend called with FX graph:\")\n    gm.graph.print_tabular()\n    return gm.forward\n\n# Reset since we are using a different backend.\n()\n\n = (init_model(), backend=custom_backend)\n(generate_data(16)[0])",
                        "code"
                    ],
                    [
                        "custom backend called with FX graph:\nopcode         name                        target                                                      args                                             kwargs\n-------------  --------------------------  ----------------------------------------------------------  -----------------------------------------------  --------\nplaceholder    x                           x                                                           ()                                               {}\ncall_module    self_conv1                  self_conv1                                                  (x,)                                             {}\ncall_module    self_bn1                    self_bn1                                                    (self_conv1,)                                    {}\ncall_module    self_relu                   self_relu                                                   (self_bn1,)                                      {}\ncall_module    self_maxpool                self_maxpool                                                (self_relu,)                                     {}\ncall_module    self_layer1_0_conv1         self_layer1_0_conv1                                         (self_maxpool,)                                  {}\ncall_module    self_layer1_0_bn1           self_layer1_0_bn1                                           (self_layer1_0_conv1,)                           {}\ncall_module    self_layer1_0_relu          self_layer1_0_relu                                          (self_layer1_0_bn1,)                             {}\ncall_module    self_layer1_0_conv2         self_layer1_0_conv2                                         (self_layer1_0_relu,)                            {}\ncall_module    self_layer1_0_bn2           self_layer1_0_bn2                                           (self_layer1_0_conv2,)                           {}\ncall_function  iadd                        &lt;built-in function iadd&gt;                                    (self_layer1_0_bn2, self_maxpool)                {}\ncall_module    self_layer1_0_relu_1        self_layer1_0_relu                                          (iadd,)                                          {}\ncall_module    self_layer1_1_conv1         self_layer1_1_conv1                                         (self_layer1_0_relu_1,)                          {}\ncall_module    self_layer1_1_bn1           self_layer1_1_bn1                                           (self_layer1_1_conv1,)                           {}\ncall_module    self_layer1_1_relu          self_layer1_1_relu                                          (self_layer1_1_bn1,)                             {}\ncall_module    self_layer1_1_conv2         self_layer1_1_conv2                                         (self_layer1_1_relu,)                            {}\ncall_module    self_layer1_1_bn2           self_layer1_1_bn2                                           (self_layer1_1_conv2,)                           {}\ncall_function  iadd_1                      &lt;built-in function iadd&gt;                                    (self_layer1_1_bn2, self_layer1_0_relu_1)        {}\ncall_module    self_layer1_1_relu_1        self_layer1_1_relu                                          (iadd_1,)                                        {}\ncall_module    self_layer2_0_conv1         self_layer2_0_conv1                                         (self_layer1_1_relu_1,)                          {}\ncall_module    self_layer2_0_bn1           self_layer2_0_bn1                                           (self_layer2_0_conv1,)                           {}\ncall_module    self_layer2_0_relu          self_layer2_0_relu                                          (self_layer2_0_bn1,)                             {}\ncall_module    self_layer2_0_conv2         self_layer2_0_conv2                                         (self_layer2_0_relu,)                            {}\ncall_module    self_layer2_0_bn2           self_layer2_0_bn2                                           (self_layer2_0_conv2,)                           {}\ncall_module    self_layer2_0_downsample_0  self_layer2_0_downsample_0                                  (self_layer1_1_relu_1,)                          {}\ncall_module    self_layer2_0_downsample_1  self_layer2_0_downsample_1                                  (self_layer2_0_downsample_0,)                    {}\ncall_function  iadd_2                      &lt;built-in function iadd&gt;                                    (self_layer2_0_bn2, self_layer2_0_downsample_1)  {}\ncall_module    self_layer2_0_relu_1        self_layer2_0_relu                                          (iadd_2,)                                        {}\ncall_module    self_layer2_1_conv1         self_layer2_1_conv1                                         (self_layer2_0_relu_1,)                          {}\ncall_module    self_layer2_1_bn1           self_layer2_1_bn1                                           (self_layer2_1_conv1,)                           {}\ncall_module    self_layer2_1_relu          self_layer2_1_relu                                          (self_layer2_1_bn1,)                             {}\ncall_module    self_layer2_1_conv2         self_layer2_1_conv2                                         (self_layer2_1_relu,)                            {}\ncall_module    self_layer2_1_bn2           self_layer2_1_bn2                                           (self_layer2_1_conv2,)                           {}\ncall_function  iadd_3                      &lt;built-in function iadd&gt;                                    (self_layer2_1_bn2, self_layer2_0_relu_1)        {}\ncall_module    self_layer2_1_relu_1        self_layer2_1_relu                                          (iadd_3,)                                        {}\ncall_module    self_layer3_0_conv1         self_layer3_0_conv1                                         (self_layer2_1_relu_1,)                          {}\ncall_module    self_layer3_0_bn1           self_layer3_0_bn1                                           (self_layer3_0_conv1,)                           {}\ncall_module    self_layer3_0_relu          self_layer3_0_relu                                          (self_layer3_0_bn1,)                             {}\ncall_module    self_layer3_0_conv2         self_layer3_0_conv2                                         (self_layer3_0_relu,)                            {}\ncall_module    self_layer3_0_bn2           self_layer3_0_bn2                                           (self_layer3_0_conv2,)                           {}\ncall_module    self_layer3_0_downsample_0  self_layer3_0_downsample_0                                  (self_layer2_1_relu_1,)                          {}\ncall_module    self_layer3_0_downsample_1  self_layer3_0_downsample_1                                  (self_layer3_0_downsample_0,)                    {}\ncall_function  iadd_4                      &lt;built-in function iadd&gt;                                    (self_layer3_0_bn2, self_layer3_0_downsample_1)  {}\ncall_module    self_layer3_0_relu_1        self_layer3_0_relu                                          (iadd_4,)                                        {}\ncall_module    self_layer3_1_conv1         self_layer3_1_conv1                                         (self_layer3_0_relu_1,)                          {}\ncall_module    self_layer3_1_bn1           self_layer3_1_bn1                                           (self_layer3_1_conv1,)                           {}\ncall_module    self_layer3_1_relu          self_layer3_1_relu                                          (self_layer3_1_bn1,)                             {}\ncall_module    self_layer3_1_conv2         self_layer3_1_conv2                                         (self_layer3_1_relu,)                            {}\ncall_module    self_layer3_1_bn2           self_layer3_1_bn2                                           (self_layer3_1_conv2,)                           {}\ncall_function  iadd_5                      &lt;built-in function iadd&gt;                                    (self_layer3_1_bn2, self_layer3_0_relu_1)        {}\ncall_module    self_layer3_1_relu_1        self_layer3_1_relu                                          (iadd_5,)                                        {}\ncall_module    self_layer4_0_conv1         self_layer4_0_conv1                                         (self_layer3_1_relu_1,)                          {}\ncall_module    self_layer4_0_bn1           self_layer4_0_bn1                                           (self_layer4_0_conv1,)                           {}\ncall_module    self_layer4_0_relu          self_layer4_0_relu                                          (self_layer4_0_bn1,)                             {}\ncall_module    self_layer4_0_conv2         self_layer4_0_conv2                                         (self_layer4_0_relu,)                            {}\ncall_module    self_layer4_0_bn2           self_layer4_0_bn2                                           (self_layer4_0_conv2,)                           {}\ncall_module    self_layer4_0_downsample_0  self_layer4_0_downsample_0                                  (self_layer3_1_relu_1,)                          {}\ncall_module    self_layer4_0_downsample_1  self_layer4_0_downsample_1                                  (self_layer4_0_downsample_0,)                    {}\ncall_function  iadd_6                      &lt;built-in function iadd&gt;                                    (self_layer4_0_bn2, self_layer4_0_downsample_1)  {}\ncall_module    self_layer4_0_relu_1        self_layer4_0_relu                                          (iadd_6,)                                        {}\ncall_module    self_layer4_1_conv1         self_layer4_1_conv1                                         (self_layer4_0_relu_1,)                          {}\ncall_module    self_layer4_1_bn1           self_layer4_1_bn1                                           (self_layer4_1_conv1,)                           {}\ncall_module    self_layer4_1_relu          self_layer4_1_relu                                          (self_layer4_1_bn1,)                             {}\ncall_module    self_layer4_1_conv2         self_layer4_1_conv2                                         (self_layer4_1_relu,)                            {}\ncall_module    self_layer4_1_bn2           self_layer4_1_bn2                                           (self_layer4_1_conv2,)                           {}\ncall_function  iadd_7                      &lt;built-in function iadd&gt;                                    (self_layer4_1_bn2, self_layer4_0_relu_1)        {}\ncall_module    self_layer4_1_relu_1        self_layer4_1_relu                                          (iadd_7,)                                        {}\ncall_module    self_avgpool                self_avgpool                                                (self_layer4_1_relu_1,)                          {}\ncall_function  flatten                     &lt;built-in method flatten of type object at 0x7f051b9b3540&gt;  (self_avgpool, 1)                                {}\ncall_module    self_fc                     self_fc                                                     (flatten,)                                       {}\noutput         output                      output                                                      ((self_fc,),)                                    {}\n\ntensor([[-1.3284, -0.5548, -0.2535,  ...,  0.0955, -0.5363, -0.3258],\n        [-1.3473, -0.4153, -0.1871,  ...,  0.0867, -0.4691, -0.0939],\n        [-1.0568, -0.3973, -0.1293,  ...,  0.1281, -0.6533, -0.5252],\n        ...,\n        [-0.9553, -0.1680, -0.1153,  ...,  0.0278, -0.7660, -0.4412],\n        [-1.1365, -0.3513, -0.3417,  ..., -0.1511, -0.8385, -0.4293],\n        [-1.2228, -0.3285, -0.4461,  ...,  0.0856, -0.3251, -0.3988]],\n       device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)",
                        "code"
                    ],
                    [
                        "Using our custom backend, we can now see how TorchDynamo is able to handle\ndata-dependent control flow. Consider the function below, where the line\nif b.sum() &lt; 0 is the source of data-dependent control flow.",
                        "markdown"
                    ],
                    [
                        "def bar(a, b):\n    x = a / ((a) + 1)\n    if b.sum() &lt; 0:\n        b = b * -1\n    return x * b\n\nopt_bar = (bar, backend=custom_backend)\n = (10)\n = (10)\nopt_bar(, )\nopt_bar(, -)",
                        "code"
                    ],
                    [
                        "custom backend called with FX graph:\nopcode         name     target                                                  args              kwargs\n-------------  -------  ------------------------------------------------------  ----------------  --------\nplaceholder    a        a                                                       ()                {}\nplaceholder    b        b                                                       ()                {}\ncall_function  abs_1    &lt;built-in method abs of type object at 0x7f051b9b3540&gt;  (a,)              {}\ncall_function  add      &lt;built-in function add&gt;                                 (abs_1, 1)        {}\ncall_function  truediv  &lt;built-in function truediv&gt;                             (a, add)          {}\ncall_method    sum_1    sum                                                     (b,)              {}\ncall_function  lt       &lt;built-in function lt&gt;                                  (sum_1, 0)        {}\noutput         output   output                                                  ((truediv, lt),)  {}\ncustom backend called with FX graph:\nopcode         name    target                   args         kwargs\n-------------  ------  -----------------------  -----------  --------\nplaceholder    b       b                        ()           {}\nplaceholder    x       x                        ()           {}\ncall_function  mul     &lt;built-in function mul&gt;  (b, -1)      {}\ncall_function  mul_1   &lt;built-in function mul&gt;  (x, mul)     {}\noutput         output  output                   ((mul_1,),)  {}\ncustom backend called with FX graph:\nopcode         name    target                   args       kwargs\n-------------  ------  -----------------------  ---------  --------\nplaceholder    b       b                        ()         {}\nplaceholder    x       x                        ()         {}\ncall_function  mul     &lt;built-in function mul&gt;  (x, b)     {}\noutput         output  output                   ((mul,),)  {}\n\ntensor([-0.1855, -0.0932,  0.0438, -0.1475, -0.0601,  0.3393, -1.4583,  0.0015,\n         0.7143,  0.0911])",
                        "code"
                    ],
                    [
                        "The output reveals that TorchDynamo extracted 3 different FX graphs\ncorresponding the following code (order may differ from the output above):",
                        "markdown"
                    ],
                    [
                        "x = a / (torch.abs(a) + 1)",
                        "markdown"
                    ],
                    [
                        "b = b * -1; return x * b",
                        "markdown"
                    ],
                    [
                        "return x * b",
                        "markdown"
                    ],
                    [
                        "When TorchDynamo encounters unsupported Python features, such as data-dependent\ncontrol flow, it breaks the computation graph, lets the default Python\ninterpreter handle the unsupported code, then resumes capturing the graph.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s investigate by example how TorchDynamo would step through bar.\nIf b.sum() &lt; 0, then TorchDynamo would run graph 1, let\nPython determine the result of the conditional, then run\ngraph 2. On the other hand, if not b.sum() &lt; 0, then TorchDynamo\nwould run graph 1, let Python determine the result of the conditional, then\nrun graph 3.",
                        "markdown"
                    ],
                    [
                        "This highlights a major difference between TorchDynamo and previous PyTorch\ncompiler solutions. When encountering unsupported Python features,\nprevious solutions either raise an error or silently fail.\nTorchDynamo, on the other hand, will break the computation graph.",
                        "markdown"
                    ],
                    [
                        "We can see where TorchDynamo breaks the graph by using torch._dynamo.explain:",
                        "markdown"
                    ],
                    [
                        "# Reset since we are using a different backend.\n()\nexplanation, out_guards, graphs, ops_per_graph, break_reasons, explanation_verbose = torch._dynamo.explain(\n    bar, (10), (10)\n)\nprint(explanation_verbose)",
                        "code"
                    ],
                    [
                        "Dynamo produced 2 graphs with 1 graph break and 6 ops\n Break reasons:\n\n1. generic_jump TensorVariable()\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 414, in bar\n    if b.sum() &lt; 0:\n\n2. return_value\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 416, in &lt;graph break in bar&gt;\n    return x * b\n\nTorchDynamo compilation metrics:\nFunction                        Runtimes (s)\n------------------------------  --------------\n_compile                        0.0122, 0.0055\nOutputGraph.call_user_compiler  0.0000, 0.0000",
                        "code"
                    ],
                    [
                        "In order to maximize speedup, graph breaks should be limited.\nWe can force TorchDynamo to raise an error upon the first graph\nbreak encountered by using fullgraph=True:",
                        "markdown"
                    ],
                    [
                        "opt_bar = (bar, fullgraph=True)\ntry:\n    opt_bar((10), (10))\nexcept:\n    tb.print_exc()",
                        "code"
                    ],
                    [
                        "Traceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 464, in &lt;module&gt;\n    opt_bar(torch.randn(10), torch.randn(10))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 209, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 337, in catch_errors\n    return callback(frame, cache_size, hooks)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 104, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 262, in _convert_frame_assert\n    return _compile(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 163, in time_wrapper\n    r = func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 324, in _compile\n    out_code = transform_code_object(code, transform)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 445, in transform_code_object\n    transformations(instructions, code_options)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 311, in transform\n    tracer.run()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1726, in run\n    super().run()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 576, in run\n    and self.step()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 540, in step\n    getattr(self, inst.opname)(inst)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 327, in inner\n    unimplemented(f\"generic_jump {typestr(value)}\")\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 71, in unimplemented\n    raise Unsupported(msg)\ntorch._dynamo.exc.Unsupported: generic_jump TensorVariable()\n\nfrom user code:\n   File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 414, in bar\n    if b.sum() &lt; 0:\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True",
                        "code"
                    ],
                    [
                        "And below, we demonstrate that TorchDynamo does not break the graph on\nthe model we used above for demonstrating speedups.",
                        "markdown"
                    ],
                    [
                        " = (init_model(), fullgraph=True)\nprint((generate_data(16)[0]))",
                        "code"
                    ],
                    [
                        "tensor([[-0.0031, -0.6601,  0.7708,  ...,  0.5529, -0.0965,  0.2389],\n        [ 0.5157, -0.6799,  0.6711,  ...,  0.4190, -0.0511,  0.3566],\n        [ 0.4879, -0.5442,  0.6752,  ...,  0.2638, -0.2817,  0.6400],\n        ...,\n        [ 0.0579, -0.2516,  0.5776,  ...,  0.2413, -0.0513,  0.4131],\n        [ 0.2299, -0.3535,  0.3686,  ...,  0.4281, -0.1155,  0.4612],\n        [ 0.1491, -0.5038,  0.5811,  ...,  0.4087, -0.0058,  0.4349]],\n       device='cuda:0', grad_fn=&lt;CompiledFunctionBackward&gt;)",
                        "code"
                    ],
                    [
                        "Finally, if we simply want TorchDynamo to output the FX graph for export,\nwe can use torch._dynamo.export. Note that torch._dynamo.export, like\nfullgraph=True, raises an error if TorchDynamo breaks the graph.",
                        "markdown"
                    ],
                    [
                        "try:\n    torch._dynamo.export(bar, (10), (10))\nexcept:\n    tb.print_exc()\n\nmodel_exp = torch._dynamo.export(init_model(), generate_data(16)[0])\nprint(model_exp[0](generate_data(16)[0]))",
                        "code"
                    ],
                    [
                        "Traceback (most recent call last):\n  File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 481, in &lt;module&gt;\n    torch._dynamo.export(bar, torch.randn(10), torch.randn(10))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 601, in export\n    result_traced = opt_f(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 209, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 337, in catch_errors\n    return callback(frame, cache_size, hooks)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 104, in _fn\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 262, in _convert_frame_assert\n    return _compile(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 163, in time_wrapper\n    r = func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 324, in _compile\n    out_code = transform_code_object(code, transform)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py\", line 445, in transform_code_object\n    transformations(instructions, code_options)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 311, in transform\n    tracer.run()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 1726, in run\n    super().run()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 576, in run\n    and self.step()\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 540, in step\n    getattr(self, inst.opname)(inst)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 327, in inner\n    unimplemented(f\"generic_jump {typestr(value)}\")\n  File \"/opt/conda/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 71, in unimplemented\n    raise Unsupported(msg)\ntorch._dynamo.exc.Unsupported: generic_jump TensorVariable()\n\nfrom user code:\n   File \"/var/lib/jenkins/workspace/intermediate_source/torch_compile_tutorial.py\", line 414, in bar\n    if b.sum() &lt; 0:\n\nSet torch._dynamo.config.verbose=True for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    torch._dynamo.config.suppress_errors = True\n\ntensor([[ 0.1920, -0.1323,  0.9817,  ...,  0.5318, -0.5725,  0.6707],\n        [ 0.1287, -0.2137,  0.7767,  ...,  0.4573, -0.3710,  0.4077],\n        [ 0.4535, -0.0487,  0.8373,  ...,  0.5423, -0.3619,  0.3945],\n        ...,\n        [ 0.1012, -0.2960,  0.6673,  ...,  0.4942, -0.4441,  0.5463],\n        [ 0.0787, -0.0200,  0.7137,  ...,  0.5139, -0.4842,  0.4904],\n        [ 0.2168, -0.1528,  0.7756,  ...,  0.7245, -0.3392,  0.6314]],\n       device='cuda:0', grad_fn=&lt;AddmmBackward0&gt;)",
                        "code"
                    ]
                ]
            },
            {
                "Conclusion": [
                    [
                        "In this tutorial, we introduced torch.compile by covering\nbasic usage, demonstrating speedups over eager mode, comparing to previous\nPyTorch compiler solutions, and briefly investigating TorchDynamo and its interactions\nwith FX graphs. We hope that you will give torch.compile a try!",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 1 minutes  2.322 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "(Beta) Implementing High-Performance Transformers with Scaled Dot Product Attention (SDPA)": [
            [
                "<strong>Author:</strong> ",
                "markdown"
            ],
            {
                "Summary": [
                    [
                        "In this tutorial, we want to highlight a new torch.nn.functional function\nthat can be helpful for implementing transformer architectures. The\nfunction is named torch.nn.functional.scaled_dot_product_attention.\nFor detailed description of the function, see the .\nThis function has already been incorporated into torch.nn.MultiheadAttention and torch.nn.TransformerEncoderLayer.",
                        "markdown"
                    ]
                ]
            },
            {
                "Overview": [
                    [
                        "At a high level, this PyTorch function calculates the\nscaled dot product attention (SDPA) between query, key, and value according to\nthe definition found in the paper . While this function can\nbe written in PyTorch using existing functions, a fused implementation can provide\nlarge performance benefits over a naive implementation.",
                        "markdown"
                    ]
                ]
            },
            {
                "Fused implementations": [
                    [
                        "For CUDA tensor inputs, the function will dispatch into one of the following\nimplementations:",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "A PyTorch implementation defined in C++",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "This tutorial requires PyTorch 2.0.0 or later.",
                        "markdown"
                    ],
                    [
                        "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ndevice = \"cuda\" if () else \"cpu\"\n\n# Example Usage:\n, ,  = (2, 3, 8, device=device), (2, 3, 8, device=device), (2, 3, 8, device=device)\n(, , )",
                        "code"
                    ],
                    [
                        "tensor([[[ 1.1316, -0.3213, -0.0318, -0.3642, -0.0631, -0.3050, -0.2020,\n          -0.8197],\n         [ 1.1733, -0.3907, -0.1463,  0.0058, -0.2564, -0.3103,  0.1328,\n          -0.9343],\n         [ 1.5885, -0.6964, -1.4466,  0.0382, -0.9734,  0.4265,  0.2052,\n          -2.3355]],\n\n        [[-1.1003, -0.6316,  0.0504,  0.3032, -0.1389,  0.6405,  0.6810,\n          -0.5061],\n         [-1.3532, -0.1394,  0.1714,  0.2172,  0.4992,  0.5198,  1.0496,\n          -0.5356],\n         [-0.7486, -1.4682, -0.2393,  0.4146, -1.0318,  0.9081, -0.0883,\n          -0.3074]]], device='cuda:0')",
                        "code"
                    ]
                ]
            },
            {
                "Explicit Dispatcher Control": [
                    [
                        "While the function will implicitly dispatch to one of the three\nimplementations, the user can also explicitly control the dispatch via\nthe use of a context manager. This context manager allows users to\nexplicitly disable certain implementations. If a user wants to ensure\nthe function is indeed using the fastest implementation for their\nspecific inputs, the context manager can be used to sweep through\nmeasuring performance.",
                        "markdown"
                    ],
                    [
                        "# Lets define a helpful benchmarking function:\nimport torch.utils.benchmark as benchmark\ndef benchmark_torch_function_in_microseconds(f, *args, **kwargs):\n    t0 = (\n        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n    )\n    return t0.blocked_autorange().mean * 1e6\n\n# Lets define the hyper-parameters of our input\nbatch_size = 32\nmax_sequence_len = 1024\nnum_heads = 32\nembed_dimension = 32\n\n = \n\n = (batch_size, num_heads, max_sequence_len, embed_dimension, device=device, =)\n = (batch_size, num_heads, max_sequence_len, embed_dimension, device=device, =)\n = (batch_size, num_heads, max_sequence_len, embed_dimension, device=device, =)\n\nprint(f\"The default implementation runs in {benchmark_torch_function_in_microseconds(, , , ):.3f} microseconds\")\n\n# Lets explore the speed of each of the 3 implementations\nfrom torch.backends.cuda import , SDPBackend\n\n# Helpful arg mapper\nbackend_map = {\n    : {\"enable_math\": True, \"enable_flash\": False, \"enable_mem_efficient\": False},\n    : {\"enable_math\": False, \"enable_flash\": True, \"enable_mem_efficient\": False},\n    : {\n        \"enable_math\": False, \"enable_flash\": False, \"enable_mem_efficient\": True}\n}\n\nwith (**backend_map[]):\n    print(f\"The math implementation runs in {benchmark_torch_function_in_microseconds(, , , ):.3f} microseconds\")\n\n\nwith (**backend_map[]):\n    try:\n        print(f\"The flash attention implementation runs in {benchmark_torch_function_in_microseconds(, , , ):.3f} microseconds\")\n    except RuntimeError:\n        print(\"FlashAttention is not supported. See warnings for reasons.\")\n\nwith (**backend_map[]):\n    try:\n        print(f\"The memory efficient implementation runs in {benchmark_torch_function_in_microseconds(, , , ):.3f} microseconds\")\n    except RuntimeError:\n        print(\"EfficientAttention is not supported. See warnings for reasons.\")",
                        "code"
                    ],
                    [
                        "The default implementation runs in 1784774.944 microseconds\nThe math implementation runs in 143012.950 microseconds\n&lt;timeit-src&gt;:6: UserWarning:\n\nMemory efficient kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:527.)\n\n&lt;timeit-src&gt;:6: UserWarning:\n\nMemory Efficient attention has been runtime disabled. (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:338.)\n\n&lt;timeit-src&gt;:6: UserWarning:\n\nFlash attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:529.)\n\n&lt;timeit-src&gt;:6: UserWarning:\n\nFlash attention only supports sm75 and sm8x gpu architectures. Attempting to run on a sm 6.1 gpu. (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:352.)\n\nFlashAttention is not supported. See warnings for reasons.\nThe memory efficient implementation runs in 1786708.727 microseconds",
                        "code"
                    ]
                ]
            },
            {
                "Hardware dependence": [
                    [
                        "Depending on what machine you ran the above cell on and what hardware is\navailable, your results might be different.\n- If you don\u2019t have a GPU and are running on CPU then the context manager\nwill have no effect and all three runs should return similar timings.\n- Depending on what compute capability your graphics card supports\nflash attention or memory efficient might have failed.",
                        "markdown"
                    ]
                ]
            },
            {
                "Causal Self Attention": [
                    [
                        "Below is an example implementation of a multi-headed causal self\nattention block inspired by Andrej Karpathy\u2019s\n repository.",
                        "markdown"
                    ],
                    [
                        "class CausalSelfAttention():\n\n    def __init__(self, num_heads: int, embed_dimension: int, bias: bool=False, is_causal: bool=False, dropout:float=0.0):\n        super().__init__()\n        assert embed_dimension % num_heads == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = (embed_dimension, 3 * embed_dimension, bias=bias)\n        # output projection\n        self.c_proj = (embed_dimension, embed_dimension, bias=bias)\n        # regularization\n        self.dropout = dropout\n        self.resid_dropout = (dropout)\n        self.num_heads = num_heads\n        self.embed_dimension = embed_dimension\n        # Perform causal masking\n        self.is_causal = is_causal\n\n    def forward(self, ):\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        query_projected = self.c_attn()\n\n        batch_size = query_projected.size(0)\n        embed_dim = query_projected.size(2)\n        head_dim = embed_dim // (self.num_heads * 3)\n\n        , ,  = query_projected.chunk(3, -1)\n         = .view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n         = .view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n         = .view(batch_size, -1, self.num_heads, head_dim).transpose(1, 2)\n\n        if self.training:\n            dropout = self.dropout\n            is_causal = self.is_causal\n        else:\n            dropout = 0.0\n            is_causal = False\n\n        y = (, , , attn_mask=None, dropout_p=dropout, is_causal=is_causal)\n        y = y.transpose(1, 2).view(batch_size, -1, self.num_heads * head_dim)\n\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n\n\nnum_heads = 8\nheads_per_dim = 64\nembed_dimension = num_heads * heads_per_dim\n = \nmodel = (num_heads=num_heads, embed_dimension=embed_dimension, bias=False, is_causal=True, dropout=0.1).to(\"cuda\").to().eval()\nprint(model)",
                        "code"
                    ],
                    [
                        "CausalSelfAttention(\n  (c_attn): Linear(in_features=512, out_features=1536, bias=False)\n  (c_proj): Linear(in_features=512, out_features=512, bias=False)\n  (resid_dropout): Dropout(p=0.1, inplace=False)\n)",
                        "code"
                    ],
                    {
                        "NestedTensor and Dense tensor support": [
                            [
                                "SDPA supports both NestedTensor and Dense tensor inputs. NestedTensors handle the case where the input is a batch of variable length sequences\nwithout needing to pad each sequence to the maximum length in the batch. For more information about NestedTensors see\n and .",
                                "markdown"
                            ],
                            [
                                "import random\ndef generate_rand_batch(\n    batch_size,\n    max_sequence_len,\n    embed_dimension,\n    pad_percentage=None,\n    =,\n    device=\"cuda\",\n):\n    if not pad_percentage:\n        return (\n            (\n                batch_size,\n                max_sequence_len,\n                embed_dimension,\n                =,\n                device=device,\n            ),\n            None,\n        )\n    # Random sequence lengths\n    seq_len_list = [\n        int(max_sequence_len * (1 - random.gauss(pad_percentage, 0.01)))\n        for _ in range(batch_size)\n    ]\n    # Make random entry in the batch have max sequence length\n    seq_len_list[random.randint(0, batch_size - 1)] = max_sequence_len\n    return (\n        (\n            [\n                (seq_len, embed_dimension,\n                            =, device=device)\n                for seq_len in seq_len_list\n            ]\n        ),\n        seq_len_list,\n    )\n\n, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=0.5, =, device=device)\n, _ = generate_rand_batch(32, 512, embed_dimension, pad_percentage=None, =, device=device)\n\n# Currently the fused implementations don't support NestedTensor for training\n()\n\nwith (**backend_map[]):\n    try:\n        print(f\"Random NT runs in {benchmark_torch_function_in_microseconds(model, ):.3f} microseconds\")\n        print(f\"Random Dense runs in {benchmark_torch_function_in_microseconds(model, ):.3f} microseconds\")\n    except RuntimeError:\n        print(\"FlashAttention is not supported. See warnings for reasons.\")",
                                "code"
                            ],
                            [
                                "/var/lib/jenkins/workspace/intermediate_source/scaled_dot_product_attention_tutorial.py:226: UserWarning:\n\nThe PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:177.)\n\n/var/lib/jenkins/workspace/intermediate_source/scaled_dot_product_attention_tutorial.py:174: UserWarning:\n\nMemory efficient kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:527.)\n\n/var/lib/jenkins/workspace/intermediate_source/scaled_dot_product_attention_tutorial.py:174: UserWarning:\n\nMemory Efficient attention has been runtime disabled. (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:338.)\n\n/var/lib/jenkins/workspace/intermediate_source/scaled_dot_product_attention_tutorial.py:174: UserWarning:\n\nFlash attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:529.)\n\n/var/lib/jenkins/workspace/intermediate_source/scaled_dot_product_attention_tutorial.py:174: UserWarning:\n\nFlash attention only supports sm75 and sm8x gpu architectures. Attempting to run on a sm 6.1 gpu. (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.h:352.)\n\nFlashAttention is not supported. See warnings for reasons.",
                                "code"
                            ]
                        ]
                    }
                ]
            }
        ],
        "Using SDPA with torch.compile": [
            [
                "With the release of PyTorch 2.0, a new feature called\ntorch.compile() has been introduced, which can provide\nsignificant performance improvements over eager mode.\nScaled dot product attention is fully composable with torch.compile().\nTo demonstrate this, let\u2019s compile the CausalSelfAttention module using\ntorch.compile() and observe the resulting performance improvements.",
                "markdown"
            ],
            [
                "batch_size = 32\nmax_sequence_len = 256\n = (batch_size, max_sequence_len,\n               embed_dimension, device=device, =)\nprint(\n    f\"The non compiled module runs in  {benchmark_torch_function_in_microseconds(model, ):.3f} microseconds\")\n\n\n = (model)\n# Let's compile it\n()\nprint(\n    f\"The compiled module runs in  {benchmark_torch_function_in_microseconds(, ):.3f} microseconds\")",
                "code"
            ],
            [
                "The non compiled module runs in  42948.685 microseconds\nThe compiled module runs in  43165.903 microseconds",
                "code"
            ],
            [
                "The exact execution time is dependent on machine, however the results for mine:\nThe non compiled module runs in  166.616 microseconds\nThe compiled module runs in  166.726 microseconds\nThat is not what we were expecting. Let\u2019s dig a little deeper.\nPyTorch comes with an amazing built-in profiler that you can use to\ninspect the performance characteristics of your code.",
                "markdown"
            ],
            [
                "from torch.profiler import , record_function, ProfilerActivity\nactivities = []\nif device == 'cuda':\n    activities.append()\n\nwith (activities=activities, record_shapes=False) as :\n    with record_function(\" Non-Compilied Causal Attention\"):\n        for _ in range(25):\n            model()\nprint(().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n\nwith (activities=activities, record_shapes=False) as :\n    with record_function(\"Compiled Causal Attention\"):\n        for _ in range(25):\n            ()\nprint(().table(sort_by=\"cuda_time_total\", row_limit=10))\n\n# For even more insights, you can export the trace and use ``chrome://tracing`` to view the results\n# prof.export_chrome_trace(\"compiled_causal_attention_trace.json\").",
                "code"
            ],
            [
                "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\n                         Non-Compilied Causal Attention         0.42%       4.576ms         1.61%      17.413ms      17.413ms       0.000us         0.00%        1.213s        1.213s             1\n                     aten::scaled_dot_product_attention         0.08%     902.000us         0.21%       2.240ms      89.600us       0.000us         0.00%     972.027ms      38.881ms            25\n          aten::_scaled_dot_product_efficient_attention         0.04%     454.000us         0.12%       1.338ms      53.520us       0.000us         0.00%     972.027ms      38.881ms            25\n                     aten::_efficient_attention_forward         0.03%     277.000us         0.08%     818.000us      32.720us     972.027ms        90.31%     972.027ms      38.881ms            25\nvoid attention_kernel_batched&lt;AttentionKernel&lt;cutlas...         0.00%       0.000us         0.00%       0.000us       0.000us     972.027ms        90.31%     972.027ms      38.881ms            25\n                                           aten::matmul         0.10%       1.042ms         0.75%       8.089ms     161.780us       0.000us         0.00%     241.153ms       4.823ms            50\n                                               aten::mm         0.41%       4.423ms         0.53%       5.733ms     114.660us     104.261ms         9.69%     241.153ms       4.823ms            50\n                                           aten::linear         0.04%     411.000us         0.79%       8.570ms     171.400us       0.000us         0.00%     234.146ms       4.683ms            50\ncudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.07%     812.000us         0.07%     812.000us       4.640us      96.656ms         8.98%      96.656ms     552.320us           175\n                      maxwell_fp16_sgemm_fp16_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us      80.984ms         7.52%      80.984ms       3.239ms            25\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 1.083s\nSelf CUDA time total: 1.076s\n\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\n                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\n                              Compiled Causal Attention         0.32%       3.414ms         1.40%      15.124ms      15.124ms       0.000us         0.00%        1.361s        1.361s             1\n                                       CompiledFunction         0.68%       7.383ms         1.08%      11.605ms     464.200us       0.000us         0.00%        1.361s      54.437ms            25\n          aten::_scaled_dot_product_efficient_attention         0.03%     289.000us         0.11%       1.182ms      47.280us       0.000us         0.00%        1.049s      41.950ms            25\n                     aten::_efficient_attention_forward         0.03%     297.000us         0.07%     803.000us      32.120us     970.600ms        90.33%        1.049s      41.950ms            25\nvoid attention_kernel_batched&lt;AttentionKernel&lt;cutlas...         0.00%       0.000us         0.00%       0.000us       0.000us     970.600ms        90.33%     970.600ms      38.824ms            25\n                                               aten::mm         0.12%       1.305ms         0.18%       1.977ms      39.540us     103.919ms         9.67%     312.165ms       6.243ms            50\n                                       cudaLaunchKernel         0.08%     812.000us         0.08%     812.000us      10.827us     157.432ms        14.65%     157.432ms       2.099ms            75\ncudaOccupancyMaxActiveBlocksPerMultiprocessorWithFla...         0.01%      71.000us         0.01%      71.000us       0.406us     128.971ms        12.00%     128.971ms     736.977us           175\n                      maxwell_fp16_sgemm_fp16_32x128_tn         0.00%       0.000us         0.00%       0.000us       0.000us      80.978ms         7.54%      80.978ms       3.239ms            25\n                                 hgemm_128x128x8_NT_vec         0.00%       0.000us         0.00%       0.000us       0.000us      22.941ms         2.14%      22.941ms     917.640us            25\n-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------\nSelf CPU time total: 1.078s\nSelf CUDA time total: 1.075s",
                "code"
            ],
            [
                "The previous code snippet generates a report of the top 10 PyTorch functions\nthat consumed the most GPU execution time, for both the compiled and non-compiled module.\nThe analysis reveals that the majority of time spent on the GPU is concentrated\non the same set of functions for both modules.\nThe reason for this here is that torch.compile is very good at removing the\nframework overhead associated with PyTorch. If your model is launching\nlarge, efficient CUDA kernels, which in this case CausaulSelfAttention\nis, then the overhead of PyTorch can be hidden.",
                "markdown"
            ],
            [
                "In reality, your module does not normally consist of a singular\nCausalSelfAttention block. When experimenting with Andrej Karpathy\u2019s\n repository, compiling\nthe module took the time per train step from: 6090.49ms to\n3273.17ms! This was done on commit: ae3a8d5 of NanoGPT training on\nthe shakespeare dataset.",
                "markdown"
            ]
        ],
        "Conclusion": [
            [
                "In this tutorial, we have demonstrated the basic usage of\ntorch.nn.functional.scaled_dot_product_attention. We have shown how\nthe sdp_kernel context manager can be used to assert a certain\nimplementation is used on GPU. As well, we built a simple\nCausalSelfAttention module that works with NestedTensor and is torch\ncompilable. In the process we have shown how to the profiling tools can\nbe used to explore the performance characteristics of a user defined\nmodule.",
                "markdown"
            ],
            [
                "<strong>Total running time of the script:</strong> ( 0 minutes  18.529 seconds)",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ]
        ]
    },
    "Parallel and Distributed Training": {
        "Distributed and Parallel Training Tutorials": [
            [
                "Distributed training is a model training paradigm that involves\nspreading training workload across multiple worker nodes, therefore\nsignificantly improving the speed of training and model accuracy. While\ndistributed training can be used for any type of ML model training, it\nis most beneficial to use it for large models and compute demanding\ntasks as deep learning.",
                "markdown"
            ],
            [
                "There are a few ways you can perform distributed training in\nPyTorch with each method having their advantages in certain use cases:",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "Read more about these options in .",
                "markdown"
            ],
            {
                "Learn DDP\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nDDP Intro Video Tutorials": [
                    [
                        "A step-by-step video series on how to get started with\n<cite>DistributedDataParallel</cite> and advance to more complex topics",
                        "markdown"
                    ],
                    [
                        "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code <svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-square-fill\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M5.75 4A1.75 1.75 0 004 5.75v4.5c0 .966.784 1.75 1.75 1.75h4.5A1.75 1.75 0 0012 10.25v-4.5A1.75 1.75 0 0010.25 4h-4.5z\" fill-rule=\"evenodd\"></path></svg> <svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-video\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M1.75 3.5a.25.25 0 00-.25.25v8.5c0 .138.112.25.25.25h12.5a.25.25 0 00.25-.25v-8.5a.25.25 0 00-.25-.25H1.75zM0 3.75C0 2.784.784 2 1.75 2h12.5c.966 0 1.75.784 1.75 1.75v8.5A1.75 1.75 0 0114.25 14H1.75A1.75 1.75 0 010 12.25v-8.5z\" fill-rule=\"evenodd\"></path><path d=\"M6 10.559V5.442a.25.25 0 01.379-.215l4.264 2.559a.25.25 0 010 .428l-4.264 2.559A.25.25 0 016 10.559z\"></path></svg> Video\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nGetting Started with Distributed Data Parallel",
                        "markdown"
                    ],
                    [
                        "This tutorial provides a short and gentle intro to the PyTorch\nDistributedData Parallel.",
                        "markdown"
                    ],
                    [
                        "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nDistributed Training with Uneven Inputs Using\nthe Join Context Manager",
                        "markdown"
                    ],
                    [
                        "This tutorial describes the Join context manager and\ndemonstrates it\u2019s use with DistributedData Parallel.",
                        "markdown"
                    ],
                    [
                        "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code",
                        "markdown"
                    ]
                ]
            },
            {
                "Learn FSDP\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nGetting Started with FSDP": [
                    [
                        "This tutorial demonstrates how you can perform distributed training\nwith FSDP on a MNIST dataset.",
                        "markdown"
                    ],
                    [
                        "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nFSDP Advanced",
                        "markdown"
                    ],
                    [
                        "In this tutorial, you will learn how to fine-tune a HuggingFace (HF) T5\nmodel with FSDP for text summarization.",
                        "markdown"
                    ],
                    [
                        "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code",
                        "markdown"
                    ]
                ]
            },
            {
                "Learn RPC\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nGetting Started with Distributed RPC Framework": [
                    [
                        "This tutorial demonstrates how to get started with RPC-based distributed\ntraining.",
                        "markdown"
                    ],
                    [
                        "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nImplementing a Parameter Server Using Distributed RPC Framework",
                        "markdown"
                    ],
                    [
                        "This tutorial walks you through a simple example of implementing a\nparameter server using PyTorch\u2019s Distributed RPC framework.",
                        "markdown"
                    ],
                    [
                        "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nImplementing Batch RPC Processing Using Asynchronous Executions",
                        "markdown"
                    ],
                    [
                        "In this tutorial you will build batch-processing RPC applications\nwith the @rpc.functions.async_execution decorator.",
                        "markdown"
                    ],
                    [
                        "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code\n\n\n\n\n\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nCombining Distributed DataParallel with Distributed RPC Framework",
                        "markdown"
                    ],
                    [
                        "In this tutorial you will learn how to combine distributed data\nparallelism with distributed model parallelism.",
                        "markdown"
                    ],
                    [
                        "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code",
                        "markdown"
                    ]
                ]
            },
            {
                "Custom Extensions\n\n\n\n\n\n\n<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-file-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4 1.75C4 .784 4.784 0 5.75 0h5.586c.464 0 .909.184 1.237.513l2.914 2.914c.329.328.513.773.513 1.237v8.586A1.75 1.75 0 0114.25 15h-9a.75.75 0 010-1.5h9a.25.25 0 00.25-.25V6h-2.75A1.75 1.75 0 0110 4.25V1.5H5.75a.25.25 0 00-.25.25v2.5a.75.75 0 01-1.5 0v-2.5zm7.5-.188V4.25c0 .138.112.25.25.25h2.688a.252.252 0 00-.011-.013l-2.914-2.914a.272.272 0 00-.013-.011zM5.72 6.72a.75.75 0 000 1.06l1.47 1.47-1.47 1.47a.75.75 0 101.06 1.06l2-2a.75.75 0 000-1.06l-2-2a.75.75 0 00-1.06 0zM3.28 7.78a.75.75 0 00-1.06-1.06l-2 2a.75.75 0 000 1.06l2 2a.75.75 0 001.06-1.06L1.81 9.25l1.47-1.47z\" fill-rule=\"evenodd\"></path></svg>\nCustomize Process Group Backends Using Cpp Extensions": [
                    [
                        "In this tutorial you will learn to implement a custom <cite>ProcessGroup</cite>\nbackend and plug that into PyTorch distributed package using\ncpp extensions.",
                        "markdown"
                    ],
                    [
                        "<svg aria-hidden=\"true\" class=\"sd-octicon sd-octicon-code\" height=\"1.0em\" version=\"1.1\" viewbox=\"0 0 16 16\" width=\"1.0em\"><path d=\"M4.72 3.22a.75.75 0 011.06 1.06L2.06 8l3.72 3.72a.75.75 0 11-1.06 1.06L.47 8.53a.75.75 0 010-1.06l4.25-4.25zm6.56 0a.75.75 0 10-1.06 1.06L13.94 8l-3.72 3.72a.75.75 0 101.06 1.06l4.25-4.25a.75.75 0 000-1.06l-4.25-4.25z\" fill-rule=\"evenodd\"></path></svg> Code",
                        "markdown"
                    ]
                ]
            }
        ],
        "PyTorch Distributed Overview": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                " View and edit this tutorial in .",
                "markdown"
            ],
            [
                "This is the overview page for the torch.distributed package. The goal of\nthis page is to categorize documents into different topics and briefly\ndescribe each of them. If this is your first time building distributed training\napplications using PyTorch, it is recommended to use this document to navigate\nto the technology that can best serve your use case.",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "As of PyTorch v1.6.0, features in torch.distributed can be categorized into\nthree main components:",
                        "markdown"
                    ],
                    [
                        "(DDP) is a widely adopted single-program multiple-data training paradigm. With\nDDP, the model is replicated on every process, and every model replica will be\nfed with a different set of input data samples. DDP takes care of gradient\ncommunication to keep model replicas synchronized and overlaps it with the\ngradient computations to speed up training.",
                        "markdown"
                    ],
                    [
                        "(RPC) supports general training structures that cannot fit into\ndata-parallel training such as distributed pipeline parallelism, parameter\nserver paradigm, and combinations of DDP with other training paradigms. It\nhelps manage remote object lifetime and extends the\n beyond\nmachine boundaries.",
                        "markdown"
                    ],
                    [
                        "(c10d) library supports sending tensors across processes within a group. It\noffers both collective communication APIs (e.g.,\n\nand )\nand P2P communication APIs (e.g.,\n\nand ).\nDDP and RPC ()\nare built on c10d, where the former uses collective communications\nand the latter uses P2P communications. Usually, developers do not need to\ndirectly use this raw communication API, as the DDP and RPC APIs can serve\nmany distributed training scenarios. However, there are use cases where this API\nis still helpful. One example would be distributed parameter averaging, where\napplications would like to compute the average values of all model parameters\nafter the backward pass instead of using DDP to communicate gradients. This can\ndecouple communications from computations and allow finer-grain control over\nwhat to communicate, but on the other hand, it also gives up the performance\noptimizations offered by DDP.\n\nshows examples of using c10d communication APIs.",
                        "markdown"
                    ]
                ]
            },
            {
                "Data Parallel Training": [
                    [
                        "PyTorch provides several options for data-parallel training. For applications\nthat gradually grow from simple to complex and from prototype to production, the\ncommon development trajectory would be:",
                        "markdown"
                    ],
                    [
                        "Use single-device training if the data and model can fit in one GPU, and\ntraining speed is not a concern.",
                        "markdown"
                    ],
                    [
                        "Use single-machine multi-GPU\n\nto make use of multiple GPUs on a single machine to speed up training with\nminimal code changes.",
                        "markdown"
                    ],
                    [
                        "Use single-machine multi-GPU\n,\nif you would like to further speed up training and are willing to write a\nlittle more code to set it up.",
                        "markdown"
                    ],
                    [
                        "Use multi-machine \nand the ,\nif the application needs to scale across machine boundaries.",
                        "markdown"
                    ],
                    [
                        "Use \nto launch distributed training if errors (e.g., out-of-memory) are expected or if\nresources can join and leave dynamically during training.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "Data-parallel training also works with .",
                        "markdown"
                    ],
                    {
                        "torch.nn.DataParallel": [
                            [
                                "The \npackage enables single-machine multi-GPU parallelism with the lowest coding\nhurdle. It only requires a one-line change to the application code. The tutorial\n\nshows an example. Although DataParallel is very easy to\nuse, it usually does not offer the best performance because it replicates the\nmodel in every forward pass, and its single-process multi-thread parallelism\nnaturally suffers from\n contention. To get\nbetter performance, consider using\n.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "torch.nn.parallel.DistributedDataParallel": [
                            [
                                "Compared to ,\n\nrequires one more step to set up, i.e., calling\n.\nDDP uses multi-process parallelism, and hence there is no GIL contention across\nmodel replicas. Moreover, the model is broadcast at DDP construction time instead\nof in every forward pass, which also helps to speed up training. DDP is shipped\nwith several performance optimization technologies. For a more in-depth\nexplanation, refer to this\n (VLDB\u201920).",
                                "markdown"
                            ],
                            [
                                "DDP materials are listed below:",
                                "markdown"
                            ],
                            [
                                "offer a starter example and some brief descriptions of its design and\nimplementation. If this is your first time using DDP, start from this\ndocument.",
                                "markdown"
                            ],
                            [
                                "explains some common problems with DDP training, including unbalanced\nworkload, checkpointing, and multi-device models. Note that, DDP can be\neasily combined with single-machine multi-device model parallelism which is\ndescribed in the\n\ntutorial.",
                                "markdown"
                            ],
                            [
                                "The \ndocument shows how to use the DDP launching script.",
                                "markdown"
                            ],
                            [
                                "The \nrecipe demonstrates how \nhelps to reduce optimizer memory footprint.",
                                "markdown"
                            ],
                            [
                                "The \ntutorial walks through using the generic join context for distributed training with uneven inputs.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "torch.distributed.elastic": [
                            [
                                "With the growth of the application complexity and scale, failure recovery\nbecomes a requirement. Sometimes it is inevitable to hit errors\nlike out-of-memory (OOM) when using DDP, but DDP itself cannot recover from those errors,\nand it is not possible to handle them using a standard try-except construct.\nThis is because DDP requires all processes to operate in a closely synchronized manner\nand all AllReduce communications launched in different processes must match.\nIf one of the processes in the group\nthrows an exception, it is likely to lead to desynchronization (mismatched\nAllReduce operations) which would then cause a crash or hang.\n\nadds fault tolerance and the ability to make use of a dynamic pool of machines (elasticity).",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "RPC-Based Distributed Training": [
                    [
                        "Many training paradigms do not fit into data parallelism, e.g.,\nparameter server paradigm, distributed pipeline parallelism, reinforcement\nlearning applications with multiple observers or agents, etc.\n aims at\nsupporting general distributed training scenarios.",
                        "markdown"
                    ],
                    [
                        "has four main pillars:",
                        "markdown"
                    ],
                    [
                        " supports running\na given function on a remote worker.",
                        "markdown"
                    ],
                    [
                        " helps to manage the\nlifetime of a remote object. The reference counting protocol is presented in the\n.",
                        "markdown"
                    ],
                    [
                        "extends the autograd engine beyond machine boundaries. Please refer to\n\nfor more details.",
                        "markdown"
                    ],
                    [
                        "automatically reaches out to all participating workers to update\nparameters using gradients computed by the distributed autograd engine.",
                        "markdown"
                    ],
                    [
                        "RPC Tutorials are listed below:",
                        "markdown"
                    ],
                    [
                        "The \ntutorial first uses a simple Reinforcement Learning (RL) example to\ndemonstrate RPC and RRef. Then, it applies a basic distributed model\nparallelism to an RNN example to show how to use distributed autograd and\ndistributed optimizer.",
                        "markdown"
                    ],
                    [
                        "The \ntutorial borrows the spirit of\n\nand applies it to an asynchronous parameter server (PS) training application.",
                        "markdown"
                    ],
                    [
                        "The \ntutorial extends the single-machine pipeline parallel example (presented in\n)\nto a distributed environment and shows how to implement it using RPC.",
                        "markdown"
                    ],
                    [
                        "The \ntutorial demonstrates how to implement RPC batch processing using the\n\ndecorator, which can help speed up inference and training. It uses\nRL and PS examples similar to those in the above tutorials 1 and 2.",
                        "markdown"
                    ],
                    [
                        "The \ntutorial demonstrates how to combine DDP with RPC to train a model using\ndistributed data parallelism combined with distributed model parallelism.",
                        "markdown"
                    ]
                ]
            },
            {
                "PyTorch Distributed Developers": [
                    [
                        "If you\u2019d like to contribute to PyTorch Distributed, please refer to our\n.",
                        "markdown"
                    ]
                ]
            }
        ],
        "Single-Machine Model Parallel Best Practices": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "Model parallel is widely-used in distributed training\ntechniques. Previous posts have explained how to use\n\nto train a neural network on multiple GPUs; this feature replicates the\nsame model to all GPUs, where each GPU consumes a different partition of the\ninput data. Although it can significantly accelerate the training process, it\ndoes not work for some use cases where the model is too large to fit into a\nsingle GPU. This post shows how to solve that problem by using <strong>model parallel</strong>,\nwhich, in contrast to DataParallel, splits a single model onto different GPUs,\nrather than replicating the entire model on each GPU (to be concrete, say a model\nm contains 10 layers: when using DataParallel, each GPU will have a\nreplica of each of these 10 layers, whereas when using model parallel on two GPUs,\neach GPU could host 5 layers).",
                "markdown"
            ],
            [
                "The high-level idea of model parallel is to place different sub-networks of a\nmodel onto different devices, and implement the forward method accordingly\nto move intermediate outputs across devices. As only part of a model operates\non any individual device, a set of devices can collectively serve a larger\nmodel. In this post, we will not try to construct huge models and squeeze them\ninto a limited number of GPUs. Instead, this post focuses on showing the idea\nof model parallel. It is up to the readers to apply the ideas to real-world\napplications.",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                "For distributed model parallel training where a model spans multiple\nservers, please refer to\n\nfor examples and details.",
                "markdown"
            ],
            {
                "Basic Usage": [
                    [
                        "Let us start with a toy model that contains two linear layers. To run this\nmodel on two GPUs, simply put each linear layer on a different GPU, and move\ninputs and intermediate outputs to match the layer devices accordingly.",
                        "markdown"
                    ],
                    [
                        "import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\nclass ToyModel():\n    def __init__(self):\n        super(, self).__init__()\n        self.net1 = (10, 10).to('cuda:0')\n        self.relu = ()\n        self.net2 = (10, 5).to('cuda:1')\n\n    def forward(self, x):\n        x = self.relu(self.net1(x.to('cuda:0')))\n        return self.net2(x.to('cuda:1'))",
                        "code"
                    ],
                    [
                        "Note that, the above ToyModel looks very similar to how one would\nimplement it on a single GPU, except the four to(device) calls which\nplace linear layers and tensors on proper devices. That is the only place in\nthe model that requires changes. The backward() and torch.optim will\nautomatically take care of gradients as if the model is on one GPU. You only\nneed to make sure that the labels are on the same device as the outputs when\ncalling the loss function.",
                        "markdown"
                    ],
                    [
                        "model = ()\n = ()\n = ((), lr=0.001)\n\n()\n = model((20, 10))\n = (20, 5).to('cuda:1')\n(, ).backward()\n()",
                        "code"
                    ]
                ]
            },
            {
                "Apply Model Parallel to Existing Modules": [
                    [
                        "It is also possible to run an existing single-GPU module on multiple GPUs\nwith just a few lines of changes. The code below shows how to decompose\ntorchvision.models.resnet50() to two GPUs. The idea is to inherit from\nthe existing ResNet module, and split the layers to two GPUs during\nconstruction. Then, override the forward method to stitch two\nsub-networks by moving the intermediate outputs accordingly.",
                        "markdown"
                    ],
                    [
                        "from torchvision.models.resnet import , \n\nnum_classes = 1000\n\n\nclass ModelParallelResNet50():\n    def __init__(self, *args, **kwargs):\n        super(, self).__init__(\n            , [3, 4, 6, 3], num_classes=num_classes, *args, **kwargs)\n\n        self.seq1 = (\n            self.conv1,\n            self.bn1,\n            self.relu,\n            self.maxpool,\n\n            self.layer1,\n            self.layer2\n        ).to('cuda:0')\n\n        self.seq2 = (\n            self.layer3,\n            self.layer4,\n            self.avgpool,\n        ).to('cuda:1')\n\n        self.fc.to('cuda:1')\n\n    def forward(self, x):\n        x = self.seq2(self.seq1(x).to('cuda:1'))\n        return self.fc(x.view(x.size(0), -1))",
                        "code"
                    ],
                    [
                        "The above implementation solves the problem for cases where the model is too\nlarge to fit into a single GPU. However, you might have already noticed that\nit will be slower than running it on a single GPU if your model fits. It is\nbecause, at any point in time, only one of the two GPUs are working, while\nthe other one is sitting there doing nothing. The performance further\ndeteriorates as the intermediate outputs need to be copied from cuda:0 to\ncuda:1 between layer2 and layer3.",
                        "markdown"
                    ],
                    [
                        "Let us run an experiment to get a more quantitative view of the execution\ntime. In this experiment, we train ModelParallelResNet50 and the existing\ntorchvision.models.resnet50() by running random inputs and labels through\nthem. After the training, the models will not produce any useful predictions,\nbut we can get a reasonable understanding of the execution times.",
                        "markdown"
                    ],
                    [
                        "import torchvision.models as models\n\nnum_batches = 3\nbatch_size = 120\nimage_w = 128\nimage_h = 128\n\n\ndef train(model):\n    (True)\n     = ()\n     = ((), lr=0.001)\n\n    one_hot_indices = torch.LongTensor(batch_size) \\\n                           .random_(0, num_classes) \\\n                           .view(batch_size, 1)\n\n    for _ in range(num_batches):\n        # generate random inputs and labels\n        inputs = (batch_size, 3, image_w, image_h)\n         = (batch_size, num_classes) \\\n                      .scatter_(1, one_hot_indices, 1)\n\n        # run forward pass\n        ()\n         = model(inputs.to('cuda:0'))\n\n        # run backward pass\n         = .to()\n        (, ).backward()\n        ()",
                        "code"
                    ],
                    [
                        "The train(model) method above uses nn.MSELoss as the loss function,\nand optim.SGD as the optimizer. It mimics training on 128 X 128\nimages which are organized into 3 batches where each batch contains 120\nimages. Then, we use timeit to run the train(model) method 10 times\nand plot the execution times with standard deviations.",
                        "markdown"
                    ],
                    [
                        "import matplotlib.pyplot as plt\nplt.switch_backend('Agg')\nimport numpy as np\nimport timeit\n\nnum_repeat = 10\n\nstmt = \"train(model)\"\n\nsetup = \"model = ModelParallelResNet50()\"\nmp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\nmp_mean, mp_std = np.mean(mp_run_times), np.std(mp_run_times)\n\nsetup = \"import torchvision.models as models;\" + \\\n        \"model = models.resnet50(num_classes=num_classes).to('cuda:0')\"\nrn_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\nrn_mean, rn_std = np.mean(rn_run_times), np.std(rn_run_times)\n\n\ndef plot(means, stds, , fig_name):\n    fig, ax = plt.subplots()\n    ax.bar(np.arange(len(means)), means, yerr=stds,\n           align='center', alpha=0.5, ecolor='red', capsize=10, width=0.6)\n    ax.set_ylabel('ResNet50 Execution Time (Second)')\n    ax.set_xticks(np.arange(len(means)))\n    ax.set_xticklabels()\n    ax.yaxis.grid(True)\n    plt.tight_layout()\n    plt.savefig(fig_name)\n    plt.close(fig)\n\n\nplot([mp_mean, rn_mean],\n     [mp_std, rn_std],\n     ['Model Parallel', 'Single GPU'],\n     'mp_vs_rn.png')\n\n\n\n<img alt=\"\" src=\"../_images/mp_vs_rn.png\"/>",
                        "code"
                    ],
                    [
                        "The result shows that the execution time of model parallel implementation is\n4.02/3.75-1=7% longer than the existing single-GPU implementation. So we\ncan conclude there is roughly 7% overhead in copying tensors back and forth\nacross the GPUs. There are rooms for improvements, as we know one of the two\nGPUs is sitting idle throughout the execution. One option is to further\ndivide each batch into a pipeline of splits, such that when one split reaches\nthe second sub-network, the following split can be fed into the first\nsub-network. In this way, two consecutive splits can run concurrently on two\nGPUs.",
                        "markdown"
                    ]
                ]
            },
            {
                "Speed Up by Pipelining Inputs": [
                    [
                        "In the following experiments, we further divide each 120-image batch into\n20-image splits. As PyTorch launches CUDA operations asynchronously, the\nimplementation does not need to spawn multiple threads to achieve\nconcurrency.",
                        "markdown"
                    ],
                    [
                        "class PipelineParallelResNet50():\n    def __init__(self, split_size=20, *args, **kwargs):\n        super(, self).__init__(*args, **kwargs)\n        self.split_size = split_size\n\n    def forward(self, x):\n        splits = iter(x.split(self.split_size, dim=0))\n        s_next = next(splits)\n        s_prev = self.seq1(s_next).to('cuda:1')\n        ret = []\n\n        for s_next in splits:\n            # A. s_prev runs on cuda:1\n            s_prev = self.seq2(s_prev)\n            ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n            # B. s_next runs on cuda:0, which can run concurrently with A\n            s_prev = self.seq1(s_next).to('cuda:1')\n\n        s_prev = self.seq2(s_prev)\n        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n        return (ret)\n\n\nsetup = \"model = PipelineParallelResNet50()\"\npp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\npp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)\n\nplot([mp_mean, rn_mean, pp_mean],\n     [mp_std, rn_std, pp_std],\n     ['Model Parallel', 'Single GPU', 'Pipelining Model Parallel'],\n     'mp_vs_rn_vs_pp.png')",
                        "code"
                    ],
                    [
                        "Please note, device-to-device tensor copy operations are synchronized on\ncurrent streams on the source and the destination devices. If you create\nmultiple streams, you have to make sure that copy operations are properly\nsynchronized. Writing the source tensor or reading/writing the destination\ntensor before finishing the copy operation can lead to undefined behavior.\nThe above implementation only uses default streams on both source and\ndestination devices, hence it is not necessary to enforce additional\nsynchronizations.\n\n<img alt=\"\" src=\"../_images/mp_vs_rn_vs_pp.png\"/>",
                        "markdown"
                    ],
                    [
                        "The experiment result shows that, pipelining inputs to model parallel\nResNet50 speeds up the training process by roughly 3.75/2.51-1=49%. It is\nstill quite far away from the ideal 100% speedup. As we have introduced a new\nparameter split_sizes in our pipeline parallel implementation, it is\nunclear how the new parameter affects the overall training time. Intuitively\nspeaking, using small split_size leads to many tiny CUDA kernel launch,\nwhile using large split_size results to relatively long idle times during\nthe first and last splits. Neither are optimal. There might be an optimal\nsplit_size configuration for this specific experiment. Let us try to find\nit by running experiments using several different split_size values.",
                        "markdown"
                    ],
                    [
                        "means = []\nstds = []\nsplit_sizes = [1, 3, 5, 8, 10, 12, 20, 40, 60]\n\nfor split_size in split_sizes:\n    setup = \"model = PipelineParallelResNet50(split_size=%d)\" % split_size\n    pp_run_times = timeit.repeat(\n        stmt, setup, number=1, repeat=num_repeat, globals=globals())\n    means.append(np.mean(pp_run_times))\n    stds.append(np.std(pp_run_times))\n\nfig, ax = plt.subplots()\nax.plot(split_sizes, means)\nax.errorbar(split_sizes, means, yerr=stds, ecolor='red', fmt='ro')\nax.set_ylabel('ResNet50 Execution Time (Second)')\nax.set_xlabel('Pipeline Split Size')\nax.set_xticks(split_sizes)\nax.yaxis.grid(True)\nplt.tight_layout()\nplt.savefig(\"split_size_tradeoff.png\")\nplt.close(fig)\n\n\n\n<img alt=\"\" src=\"../_images/split_size_tradeoff.png\"/>",
                        "code"
                    ],
                    [
                        "The result shows that setting split_size to 12 achieves the fastest\ntraining speed, which leads to 3.75/2.43-1=54% speedup. There are\nstill opportunities to further accelerate the training process. For example,\nall operations on cuda:0 is placed on its default stream. It means that\ncomputations on the next split cannot overlap with the copy operation of the\nprev split. However, as prev and next splits are different tensors, there is\nno problem to overlap one\u2019s computation with the other one\u2019s copy. The\nimplementation need to use multiple streams on both GPUs, and different\nsub-network structures require different stream management strategies. As no\ngeneral multi-stream solution works for all model parallel use cases, we will\nnot discuss it in this tutorial.",
                        "markdown"
                    ],
                    [
                        "<strong>Note:</strong>",
                        "markdown"
                    ],
                    [
                        "This post shows several performance measurements. You might see different\nnumbers when running the same code on your own machine, because the result\ndepends on the underlying hardware and software. To get the best performance\nfor your environment, a proper approach is to first generate the curve to\nfigure out the best split size, and then use that split size to pipeline\ninputs.",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 3 minutes  50.597 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Getting Started with Distributed Data Parallel": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "<strong>Edited by</strong>: ",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                " View and edit this tutorial in .",
                "markdown"
            ],
            [
                "Prerequisites:",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "(DDP) implements data parallelism at the module level which can run across\nmultiple machines. Applications using DDP should spawn multiple processes and\ncreate a single DDP instance per process. DDP uses collective communications in the\n\npackage to synchronize gradients and buffers. More specifically, DDP registers\nan autograd hook for each parameter given by model.parameters() and the\nhook will fire when the corresponding gradient is computed in the backward\npass. Then DDP uses that signal to trigger gradient synchronization across\nprocesses. Please refer to\n for more details.",
                "markdown"
            ],
            [
                "The recommended way to use DDP is to spawn one process for each model replica,\nwhere a model replica can span multiple devices. DDP processes can be\nplaced on the same machine or across machines, but GPU devices cannot be\nshared across processes. This tutorial starts from a basic DDP use case and\nthen demonstrates more advanced use cases including checkpointing models and\ncombining DDP with model parallel.",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                "The code in this tutorial runs on an 8-GPU server, but it can be easily\ngeneralized to other environments.",
                "markdown"
            ],
            {
                "Comparison between DataParallel and DistributedDataParallel": [
                    [
                        "Before we dive in, let\u2019s clarify why, despite the added complexity, you would\nconsider using DistributedDataParallel over DataParallel:",
                        "markdown"
                    ],
                    [
                        "First, DataParallel is single-process, multi-thread, and only works on a\nsingle machine, while DistributedDataParallel is multi-process and works\nfor both single- and multi- machine training. DataParallel is usually\nslower than DistributedDataParallel even on a single machine due to GIL\ncontention across threads, per-iteration replicated model, and additional\noverhead introduced by scattering inputs and gathering outputs.",
                        "markdown"
                    ],
                    [
                        "Recall from the\n\nthat if your model is too large to fit on a single GPU, you must use <strong>model parallel</strong>\nto split it across multiple GPUs. DistributedDataParallel works with\n<strong>model parallel</strong>; DataParallel does not at this time. When DDP is combined\nwith model parallel, each DDP process would use model parallel, and all processes\ncollectively would use data parallel.",
                        "markdown"
                    ],
                    [
                        "If your model needs to span multiple machines or if your use case does not fit\ninto data parallelism paradigm, please see \nfor more generic distributed training support.",
                        "markdown"
                    ]
                ]
            },
            {
                "Basic Use Case": [
                    [
                        "To create a DDP module, you must first set up process groups properly. More details can\nbe found in\n.",
                        "markdown"
                    ],
                    [
                        "import os\nimport sys\nimport tempfile\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.multiprocessing as mp\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\n# On Windows platform, the torch.distributed package only\n# supports Gloo backend, FileStore and TcpStore.\n# For FileStore, set init_method parameter in init_process_group\n# to a local file. Example as follow:\n# init_method=\"file:///f:/libtmp/some_file\"\n# dist.init_process_group(\n#    \"gloo\",\n#    rank=rank,\n#    init_method=init_method,\n#    world_size=world_size)\n# For TcpStore, same way as on Linux.\n\ndef setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()",
                        "code"
                    ],
                    [
                        "Now, let\u2019s create a toy module, wrap it with DDP, and feed it some dummy\ninput data. Please note, as DDP broadcasts model states from rank 0 process to\nall other processes in the DDP constructor, you do not need to worry about\ndifferent DDP processes starting from different initial model parameter values.",
                        "markdown"
                    ],
                    [
                        "class ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef demo_basic(rank, world_size):\n    print(f\"Running basic DDP example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # create model and move it to GPU with id rank\n    model = ToyModel().to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(rank)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\n\ndef run_demo(demo_fn, world_size):\n    mp.spawn(demo_fn,\n             args=(world_size,),\n             nprocs=world_size,\n             join=True)",
                        "code"
                    ],
                    [
                        "As you can see, DDP wraps lower-level distributed communication details and\nprovides a clean API as if it were a local model. Gradient synchronization\ncommunications take place during the backward pass and overlap with the\nbackward computation. When the backward() returns, param.grad already\ncontains the synchronized gradient tensor. For basic use cases, DDP only\nrequires a few more LoCs to set up the process group. When applying DDP to more\nadvanced use cases, some caveats require caution.",
                        "markdown"
                    ]
                ]
            },
            {
                "Skewed Processing Speeds": [
                    [
                        "In DDP, the constructor, the forward pass, and the backward pass are\ndistributed synchronization points. Different processes are expected to launch\nthe same number of synchronizations and reach these synchronization points in\nthe same order and enter each synchronization point at roughly the same time.\nOtherwise, fast processes might arrive early and timeout while waiting for\nstragglers. Hence, users are responsible for balancing workload distributions\nacross processes. Sometimes, skewed processing speeds are inevitable due to,\ne.g., network delays, resource contentions, or unpredictable workload spikes. To\navoid timeouts in these situations, make sure that you pass a sufficiently\nlarge timeout value when calling\n.",
                        "markdown"
                    ]
                ]
            },
            {
                "Save and Load Checkpoints": [
                    [
                        "It\u2019s common to use torch.save and torch.load to checkpoint modules\nduring training and recover from checkpoints. See\n\nfor more details. When using DDP, one optimization is to save the model in\nonly one process and then load it to all processes, reducing write overhead.\nThis is correct because all processes start from the same parameters and\ngradients are synchronized in backward passes, and hence optimizers should keep\nsetting parameters to the same values. If you use this optimization, make sure no process starts\nloading before the saving is finished. Additionally, when\nloading the module, you need to provide an appropriate map_location\nargument to prevent a process from stepping into others\u2019 devices. If map_location\nis missing, torch.load will first load the module to CPU and then copy each\nparameter to where it was saved, which would result in all processes on the\nsame machine using the same set of devices. For more advanced failure recovery\nand elasticity support, please refer to .",
                        "markdown"
                    ],
                    [
                        "def demo_checkpoint(rank, world_size):\n    print(f\"Running DDP checkpoint example on rank {rank}.\")\n    setup(rank, world_size)\n\n    model = ToyModel().to(rank)\n    ddp_model = DDP(model, device_ids=[rank])\n\n\n    CHECKPOINT_PATH = tempfile.gettempdir() + \"/model.checkpoint\"\n    if rank == 0:\n        # All processes should see same parameters as they all start from same\n        # random parameters and gradients are synchronized in backward passes.\n        # Therefore, saving it in one process is sufficient.\n        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)\n\n    # Use a barrier() to make sure that process 1 loads the model after process\n    # 0 saves it.\n    dist.barrier()\n    # configure map_location properly\n    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}\n    ddp_model.load_state_dict(\n        torch.load(CHECKPOINT_PATH, map_location=map_location))\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(rank)\n\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    # Not necessary to use a dist.barrier() to guard the file deletion below\n    # as the AllReduce ops in the backward pass of DDP already served as\n    # a synchronization.\n\n    if rank == 0:\n        os.remove(CHECKPOINT_PATH)\n\n    cleanup()",
                        "code"
                    ]
                ]
            },
            {
                "Combining DDP with Model Parallelism": [
                    [
                        "DDP also works with multi-GPU models. DDP wrapping multi-GPU models is especially\nhelpful when training large models with a huge amount of data.",
                        "markdown"
                    ],
                    [
                        "class ToyMpModel(nn.Module):\n    def __init__(self, dev0, dev1):\n        super(ToyMpModel, self).__init__()\n        self.dev0 = dev0\n        self.dev1 = dev1\n        self.net1 = torch.nn.Linear(10, 10).to(dev0)\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to(dev1)\n\n    def forward(self, x):\n        x = x.to(self.dev0)\n        x = self.relu(self.net1(x))\n        x = x.to(self.dev1)\n        return self.net2(x)",
                        "code"
                    ],
                    [
                        "When passing a multi-GPU model to DDP, device_ids and output_device\nmust NOT be set. Input and output data will be placed in proper devices by\neither the application or the model forward() method.",
                        "markdown"
                    ],
                    [
                        "def demo_model_parallel(rank, world_size):\n    print(f\"Running DDP with model parallel example on rank {rank}.\")\n    setup(rank, world_size)\n\n    # setup mp_model and devices for this process\n    dev0 = (rank * 2) % world_size\n    dev1 = (rank * 2 + 1) % world_size\n    mp_model = ToyMpModel(dev0, dev1)\n    ddp_mp_model = DDP(mp_model)\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_mp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    # outputs will be on dev1\n    outputs = ddp_mp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(dev1)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\n    cleanup()\n\n\nif __name__ == \"__main__\":\n    n_gpus = torch.cuda.device_count()\n    assert n_gpus &gt;= 2, f\"Requires at least 2 GPUs to run, but got {n_gpus}\"\n    world_size = n_gpus\n    run_demo(demo_basic, world_size)\n    run_demo(demo_checkpoint, world_size)\n    run_demo(demo_model_parallel, world_size)",
                        "code"
                    ]
                ]
            },
            {
                "Initialize DDP with torch.distributed.run/torchrun": [
                    [
                        "We can leverage PyTorch Elastic to simplify the DDP code and initialize the job more easily.\nLet\u2019s still use the Toymodel example and create a file named elastic_ddp.py.",
                        "markdown"
                    ],
                    [
                        "import torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = nn.Linear(10, 10)\n        self.relu = nn.ReLU()\n        self.net2 = nn.Linear(10, 5)\n\n    def forward(self, x):\n        return self.net2(self.relu(self.net1(x)))\n\n\ndef demo_basic():\n    dist.init_process_group(\"nccl\")\n    rank = dist.get_rank()\n    print(f\"Start running basic DDP example on rank {rank}.\")\n\n    # create model and move it to GPU with id rank\n    device_id = rank % torch.cuda.device_count()\n    model = ToyModel().to(device_id)\n    ddp_model = DDP(model, device_ids=[device_id])\n\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n\n    optimizer.zero_grad()\n    outputs = ddp_model(torch.randn(20, 10))\n    labels = torch.randn(20, 5).to(device_id)\n    loss_fn(outputs, labels).backward()\n    optimizer.step()\n\nif __name__ == \"__main__\":\n    demo_basic()",
                        "code"
                    ],
                    [
                        "One can then run a  command\non all nodes to initialize the DDP job created above:",
                        "markdown"
                    ],
                    [
                        "torchrun --nnodes=2 --nproc_per_node=8 --rdzv_id=100 --rdzv_backend=c10d --rdzv_endpoint=$MASTER_ADDR:29400 elastic_ddp.py",
                        "code"
                    ],
                    [
                        "We are running the DDP script on two hosts, and each host we run with 8 processes, aka, we\nare running it on 16 GPUs. Note that $MASTER_ADDR must be the same across all nodes.",
                        "markdown"
                    ],
                    [
                        "Here torchrun will launch 8 process and invoke elastic_ddp.py\non each process on the node it is launched on, but user also needs to apply cluster\nmanagement tools like slurm to actually run this command on 2 nodes.",
                        "markdown"
                    ],
                    [
                        "For example, on a SLURM enabled cluster, we can write a script to run the command above\nand set MASTER_ADDR as:",
                        "markdown"
                    ],
                    [
                        "export MASTER_ADDR=$(scontrol show hostname ${SLURM_NODELIST} | head -n 1)",
                        "code"
                    ],
                    [
                        "Then we can just run this script using the SLURM command: srun --nodes=2 ./torchrun_script.sh.\nOf course, this is just an example; you can choose your own cluster scheduling tools\nto initiate the torchrun job.",
                        "markdown"
                    ],
                    [
                        "For more information about Elastic run, one can check this\n to learn more.",
                        "markdown"
                    ]
                ]
            }
        ],
        "Writing Distributed Applications with PyTorch": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                " View and edit this tutorial in .",
                "markdown"
            ],
            [
                "Prerequisites:",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "In this short tutorial, we will be going over the distributed package\nof PyTorch. We\u2019ll see how to set up the distributed setting, use the\ndifferent communication strategies, and go over some of the internals of\nthe package.",
                "markdown"
            ],
            {
                "Setup\n<!--\n* Processes & machines\n* variables and init_process_group\n-->": [
                    [
                        "The distributed package included in PyTorch (i.e.,\ntorch.distributed) enables researchers and practitioners to easily\nparallelize their computations across processes and clusters of\nmachines. To do so, it leverages message passing semantics\nallowing each process to communicate data to any of the other processes.\nAs opposed to the multiprocessing (torch.multiprocessing) package,\nprocesses can use different communication backends and are not\nrestricted to being executed on the same machine.",
                        "markdown"
                    ],
                    [
                        "In order to get started we need the ability to run multiple processes\nsimultaneously. If you have access to compute cluster you should check\nwith your local sysadmin or use your favorite coordination tool (e.g.,\n,\n, or\n). For the purpose of this\ntutorial, we will use a single machine and spawn multiple processes using\nthe following template.",
                        "markdown"
                    ],
                    [
                        "\"\"\"run.py:\"\"\"\n#!/usr/bin/env python\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\n\ndef run(rank, size):\n    \"\"\" Distributed function to be implemented later. \"\"\"\n    pass\n\ndef init_process(rank, size, fn, backend='gloo'):\n    \"\"\" Initialize the distributed environment. \"\"\"\n    os.environ['MASTER_ADDR'] = '127.0.0.1'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(backend, rank=rank, world_size=size)\n    fn(rank, size)\n\n\nif __name__ == \"__main__\":\n    size = 2\n    processes = []\n    mp.set_start_method(\"spawn\")\n    for rank in range(size):\n        p = mp.Process(target=init_process, args=(rank, size, run))\n        p.start()\n        processes.append(p)\n\n    for p in processes:\n        p.join()",
                        "code"
                    ],
                    [
                        "The above script spawns two processes who will each setup the\ndistributed environment, initialize the process group\n(dist.init_process_group), and finally execute the given run\nfunction.",
                        "markdown"
                    ],
                    [
                        "Let\u2019s have a look at the init_process function. It ensures that\nevery process will be able to coordinate through a master, using the\nsame ip address and port. Note that we used the gloo backend but\nother backends are available. (c.f.\n) We will go over the magic\nhappening in dist.init_process_group at the end of this tutorial,\nbut it essentially allows processes to communicate with each other by\nsharing their locations.",
                        "markdown"
                    ]
                ]
            },
            {
                "Point-to-Point Communication": [
                    [
                        "Send and Recv",
                        "markdown"
                    ],
                    [
                        "A transfer of data from one process to another is called a\npoint-to-point communication. These are achieved through the send\nand recv functions or their <em>immediate</em> counter-parts, isend and\nirecv.",
                        "markdown"
                    ],
                    [
                        "\"\"\"Blocking point-to-point communication.\"\"\"\n\ndef run(rank, size):\n    tensor = torch.zeros(1)\n    if rank == 0:\n        tensor += 1\n        # Send the tensor to process 1\n        dist.send(tensor=tensor, dst=1)\n    else:\n        # Receive tensor from process 0\n        dist.recv(tensor=tensor, src=0)\n    print('Rank ', rank, ' has data ', tensor[0])",
                        "code"
                    ],
                    [
                        "In the above example, both processes start with a zero tensor, then\nprocess 0 increments the tensor and sends it to process 1 so that they\nboth end up with 1.0. Notice that process 1 needs to allocate memory in\norder to store the data it will receive.",
                        "markdown"
                    ],
                    [
                        "Also notice that send/recv are <strong>blocking</strong>: both processes stop\nuntil the communication is completed. On the other hand immediates are\n<strong>non-blocking</strong>; the script continues its execution and the methods\nreturn a Work object upon which we can choose to\nwait().",
                        "markdown"
                    ],
                    [
                        "\"\"\"Non-blocking point-to-point communication.\"\"\"\n\ndef run(rank, size):\n    tensor = torch.zeros(1)\n    req = None\n    if rank == 0:\n        tensor += 1\n        # Send the tensor to process 1\n        req = dist.isend(tensor=tensor, dst=1)\n        print('Rank 0 started sending')\n    else:\n        # Receive tensor from process 0\n        req = dist.irecv(tensor=tensor, src=0)\n        print('Rank 1 started receiving')\n    req.wait()\n    print('Rank ', rank, ' has data ', tensor[0])",
                        "code"
                    ],
                    [
                        "When using immediates we have to be careful about how we use the sent and received tensors.\nSince we do not know when the data will be communicated to the other process,\nwe should not modify the sent tensor nor access the received tensor before req.wait() has completed.\nIn other words,",
                        "markdown"
                    ],
                    [
                        "writing to tensor after dist.isend() will result in undefined behaviour.",
                        "markdown"
                    ],
                    [
                        "reading from tensor after dist.irecv() will result in undefined behaviour.",
                        "markdown"
                    ],
                    [
                        "However, after req.wait()\nhas been executed we are guaranteed that the communication took place,\nand that the value stored in tensor[0] is 1.0.",
                        "markdown"
                    ],
                    [
                        "Point-to-point communication is useful when we want more fine-grained\ncontrol over the communication of our processes. They can be used to\nimplement fancy algorithms, such as the one used in  or\n.(c.f.\n)",
                        "markdown"
                    ]
                ]
            },
            {
                "Collective Communication": [
                    [
                        "As opposed to point-to-point communcation, collectives allow for\ncommunication patterns across all processes in a <strong>group</strong>. A group is a\nsubset of all our processes. To create a group, we can pass a list of\nranks to dist.new_group(group). By default, collectives are executed\non all processes, also known as the <strong>world</strong>. For example, in order\nto obtain the sum of all tensors on all processes, we can use the\ndist.all_reduce(tensor, op, group) collective.",
                        "markdown"
                    ],
                    [
                        "\"\"\" All-Reduce example.\"\"\"\ndef run(rank, size):\n    \"\"\" Simple collective communication. \"\"\"\n    group = dist.new_group([0, 1])\n    tensor = torch.ones(1)\n    dist.all_reduce(tensor, op=dist.ReduceOp.SUM, group=group)\n    print('Rank ', rank, ' has data ', tensor[0])",
                        "code"
                    ],
                    [
                        "Since we want the sum of all tensors in the group, we use\ndist.ReduceOp.SUM as the reduce operator. Generally speaking, any\ncommutative mathematical operation can be used as an operator.\nOut-of-the-box, PyTorch comes with 4 such operators, all working at the\nelement-wise level:",
                        "markdown"
                    ],
                    [
                        "dist.ReduceOp.SUM,",
                        "markdown"
                    ],
                    [
                        "dist.ReduceOp.PRODUCT,",
                        "markdown"
                    ],
                    [
                        "dist.ReduceOp.MAX,",
                        "markdown"
                    ],
                    [
                        "dist.ReduceOp.MIN.",
                        "markdown"
                    ],
                    [
                        "In addition to dist.all_reduce(tensor, op, group), there are a total\nof 6 collectives currently implemented in PyTorch.",
                        "markdown"
                    ],
                    [
                        "dist.broadcast(tensor, src, group): Copies tensor from\nsrc to all other processes.",
                        "markdown"
                    ],
                    [
                        "dist.reduce(tensor, dst, op, group): Applies op to every\ntensor and stores the result in dst.",
                        "markdown"
                    ],
                    [
                        "dist.all_reduce(tensor, op, group): Same as reduce, but the\nresult is stored in all processes.",
                        "markdown"
                    ],
                    [
                        "dist.scatter(tensor, scatter_list, src, group): Copies the\n\\(i^{\\text{th}}\\) tensor scatter_list[i] to the\n\\(i^{\\text{th}}\\) process.",
                        "markdown"
                    ],
                    [
                        "dist.gather(tensor, gather_list, dst, group): Copies tensor\nfrom all processes in dst.",
                        "markdown"
                    ],
                    [
                        "dist.all_gather(tensor_list, tensor, group): Copies tensor\nfrom all processes to tensor_list, on all processes.",
                        "markdown"
                    ],
                    [
                        "dist.barrier(group): Blocks all processes in <cite>group</cite> until each one has entered this function.",
                        "markdown"
                    ]
                ]
            },
            {
                "Distributed Training\n<!--\n* Gloo Backend\n* Simple all_reduce on the gradients\n* Point to optimized DistributedDataParallel\n\nTODO: Custom ring-allreduce\n-->": [
                    [
                        "<strong>Note:</strong> You can find the example script of this section in .",
                        "markdown"
                    ],
                    [
                        "Now that we understand how the distributed module works, let us write\nsomething useful with it. Our goal will be to replicate the\nfunctionality of\n.\nOf course, this will be a didactic example and in a real-world\nsituation you should use the official, well-tested and well-optimized\nversion linked above.",
                        "markdown"
                    ],
                    [
                        "Quite simply we want to implement a distributed version of stochastic\ngradient descent. Our script will let all processes compute the\ngradients of their model on their batch of data and then average their\ngradients. In order to ensure similar convergence results when changing\nthe number of processes, we will first have to partition our dataset.\n(You could also use\n,\ninstead of the snippet below.)",
                        "markdown"
                    ],
                    [
                        "\"\"\" Dataset partitioning helper \"\"\"\nclass Partition(object):\n\n    def __init__(self, data, index):\n        self.data = data\n        self.index = index\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, index):\n        data_idx = self.index[index]\n        return self.data[data_idx]\n\n\nclass DataPartitioner(object):\n\n    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n        self.data = data\n        self.partitions = []\n        rng = Random()\n        rng.seed(seed)\n        data_len = len(data)\n        indexes = [x for x in range(0, data_len)]\n        rng.shuffle(indexes)\n\n        for frac in sizes:\n            part_len = int(frac * data_len)\n            self.partitions.append(indexes[0:part_len])\n            indexes = indexes[part_len:]\n\n    def use(self, partition):\n        return Partition(self.data, self.partitions[partition])",
                        "code"
                    ],
                    [
                        "With the above snippet, we can now simply partition any dataset using\nthe following few lines:",
                        "markdown"
                    ],
                    [
                        "\"\"\" Partitioning MNIST \"\"\"\ndef partition_dataset():\n    dataset = datasets.MNIST('./data', train=True, download=True,\n                             transform=transforms.Compose([\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.1307,), (0.3081,))\n                             ]))\n    size = dist.get_world_size()\n    bsz = 128 / float(size)\n    partition_sizes = [1.0 / size for _ in range(size)]\n    partition = DataPartitioner(dataset, partition_sizes)\n    partition = partition.use(dist.get_rank())\n    train_set = torch.utils.data.DataLoader(partition,\n                                         batch_size=bsz,\n                                         shuffle=True)\n    return train_set, bsz",
                        "code"
                    ],
                    [
                        "Assuming we have 2 replicas, then each process will have a train_set\nof 60000 / 2 = 30000 samples. We also divide the batch size by the\nnumber of replicas in order to maintain the <em>overall</em> batch size of 128.",
                        "markdown"
                    ],
                    [
                        "We can now write our usual forward-backward-optimize training code, and\nadd a function call to average the gradients of our models. (The\nfollowing is largely inspired by the official .)",
                        "markdown"
                    ],
                    [
                        "\"\"\" Distributed Synchronous SGD Example \"\"\"\ndef run(rank, size):\n    torch.manual_seed(1234)\n    train_set, bsz = partition_dataset()\n    model = Net()\n    optimizer = optim.SGD(model.parameters(),\n                          lr=0.01, momentum=0.5)\n\n    num_batches = ceil(len(train_set.dataset) / float(bsz))\n    for epoch in range(10):\n        epoch_loss = 0.0\n        for data, target in train_set:\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            average_gradients(model)\n            optimizer.step()\n        print('Rank ', dist.get_rank(), ', epoch ',\n              epoch, ': ', epoch_loss / num_batches)",
                        "code"
                    ],
                    [
                        "It remains to implement the average_gradients(model) function, which\nsimply takes in a model and averages its gradients across the whole\nworld.",
                        "markdown"
                    ],
                    [
                        "\"\"\" Gradient averaging. \"\"\"\ndef average_gradients(model):\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n        param.grad.data /= size",
                        "code"
                    ],
                    [
                        "<em>Et voil\u00e0</em>! We successfully implemented distributed synchronous SGD and\ncould train any model on a large computer cluster.",
                        "markdown"
                    ],
                    [
                        "<strong>Note:</strong> While the last sentence is <em>technically</em> true, there are  required to\nimplement a production-level implementation of synchronous SGD. Again,\nuse what .",
                        "markdown"
                    ],
                    {
                        "Our Own Ring-Allreduce": [
                            [
                                "As an additional challenge, imagine that we wanted to implement\nDeepSpeech\u2019s efficient ring allreduce. This is fairly easy to implement\nusing point-to-point collectives.",
                                "markdown"
                            ],
                            [
                                "\"\"\" Implementation of a ring-reduce with addition. \"\"\"\ndef allreduce(send, recv):\n   rank = dist.get_rank()\n   size = dist.get_world_size()\n   send_buff = send.clone()\n   recv_buff = send.clone()\n   accum = send.clone()\n\n   left = ((rank - 1) + size) % size\n   right = (rank + 1) % size\n\n   for i in range(size - 1):\n       if i % 2 == 0:\n           # Send send_buff\n           send_req = dist.isend(send_buff, right)\n           dist.recv(recv_buff, left)\n           accum[:] += recv_buff[:]\n       else:\n           # Send recv_buff\n           send_req = dist.isend(recv_buff, right)\n           dist.recv(send_buff, left)\n           accum[:] += send_buff[:]\n       send_req.wait()\n   recv[:] = accum[:]",
                                "code"
                            ],
                            [
                                "In the above script, the allreduce(send, recv) function has a\nslightly different signature than the ones in PyTorch. It takes a\nrecv tensor and will store the sum of all send tensors in it. As\nan exercise left to the reader, there is still one difference between\nour version and the one in DeepSpeech: their implementation divides the\ngradient tensor into <em>chunks</em>, so as to optimally utilize the\ncommunication bandwidth. (Hint:\n)",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Advanced Topics": [
                    [
                        "We are now ready to discover some of the more advanced functionalities\nof torch.distributed. Since there is a lot to cover, this section is\ndivided into two subsections:",
                        "markdown"
                    ],
                    [
                        "Communication Backends: where we learn how to use MPI and Gloo for\nGPU-GPU communication.",
                        "markdown"
                    ],
                    [
                        "Initialization Methods: where we understand how to best set up the\ninitial coordination phase in dist.init_process_group().",
                        "markdown"
                    ],
                    {
                        "Communication Backends": [
                            [
                                "One of the most elegant aspects of torch.distributed is its ability\nto abstract and build on top of different backends. As mentioned before,\nthere are currently three backends implemented in PyTorch: Gloo, NCCL, and\nMPI. They each have different specifications and tradeoffs, depending\non the desired use case. A comparative table of supported functions can\nbe found\n.",
                                "markdown"
                            ],
                            [
                                "<strong>Gloo Backend</strong>",
                                "markdown"
                            ],
                            [
                                "So far we have made extensive usage of the .\nIt is quite handy as a development platform, as it is included in\nthe pre-compiled PyTorch binaries and works on both Linux (since 0.2)\nand macOS (since 1.3). It supports all point-to-point and collective\noperations on CPU, and all collective operations on GPU. The\nimplementation of the collective operations for CUDA tensors is not as\noptimized as the ones provided by the NCCL backend.",
                                "markdown"
                            ],
                            [
                                "As you have surely noticed, our\ndistributed SGD example does not work if you put model on the GPU.\nIn order to use multiple GPUs, let us also make the following\nmodifications:",
                                "markdown"
                            ],
                            [
                                "Use device = torch.device(\"cuda:{}\".format(rank))",
                                "markdown"
                            ],
                            [
                                "model = Net() \\(\\rightarrow\\) model = Net().to(device)",
                                "markdown"
                            ],
                            [
                                "Use data, target = data.to(device), target.to(device)",
                                "markdown"
                            ],
                            [
                                "With the above modifications, our model is now training on two GPUs and\nyou can monitor their utilization with watch nvidia-smi.",
                                "markdown"
                            ],
                            [
                                "<strong>MPI Backend</strong>",
                                "markdown"
                            ],
                            [
                                "The Message Passing Interface (MPI) is a standardized tool from the\nfield of high-performance computing. It allows to do point-to-point and\ncollective communications and was the main inspiration for the API of\ntorch.distributed. Several implementations of MPI exist (e.g.\n,\n, ) each\noptimized for different purposes. The advantage of using the MPI backend\nlies in MPI\u2019s wide availability - and high-level of optimization - on\nlarge computer clusters. \n\n are also able to take\nadvantage of CUDA IPC and GPU Direct technologies in order to avoid\nmemory copies through the CPU.",
                                "markdown"
                            ],
                            [
                                "Unfortunately, PyTorch\u2019s binaries cannot include an MPI implementation\nand we\u2019ll have to recompile it by hand. Fortunately, this process is\nfairly simple given that upon compilation, PyTorch will look <em>by itself</em>\nfor an available MPI implementation. The following steps install the MPI\nbackend, by installing PyTorch .",
                                "markdown"
                            ],
                            [
                                "Create and activate your Anaconda environment, install all the\npre-requisites following , but do\n<strong>not</strong> run python setup.py install yet.",
                                "markdown"
                            ],
                            [
                                "Choose and install your favorite MPI implementation. Note that\nenabling CUDA-aware MPI might require some additional steps. In our\ncase, we\u2019ll stick to Open-MPI <em>without</em> GPU support:\nconda install -c conda-forge openmpi",
                                "markdown"
                            ],
                            [
                                "Now, go to your cloned PyTorch repo and execute\npython setup.py install.",
                                "markdown"
                            ],
                            [
                                "In order to test our newly installed backend, a few modifications are\nrequired.",
                                "markdown"
                            ],
                            [
                                "Replace the content under if __name__ == '__main__': with\ninit_process(0, 0, run, backend='mpi').",
                                "markdown"
                            ],
                            [
                                "Run mpirun -n 4 python myscript.py.",
                                "markdown"
                            ],
                            [
                                "The reason for these changes is that MPI needs to create its own\nenvironment before spawning the processes. MPI will also spawn its own\nprocesses and perform the handshake described in , making the rankand size\narguments of init_process_group superfluous. This is actually quite\npowerful as you can pass additional arguments to mpirun in order to\ntailor computational resources for each process. (Things like number of\ncores per process, hand-assigning machines to specific ranks, and )\nDoing so, you should obtain the same familiar output as with the other\ncommunication backends.",
                                "markdown"
                            ],
                            [
                                "<strong>NCCL Backend</strong>",
                                "markdown"
                            ],
                            [
                                "The  provides an\noptimized implementation of collective operations against CUDA\ntensors. If you only use CUDA tensors for your collective operations,\nconsider using this backend for the best in class performance. The\nNCCL backend is included in the pre-built binaries with CUDA support.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Initialization Methods": [
                            [
                                "To finish this tutorial, let\u2019s talk about the very first function we\ncalled: dist.init_process_group(backend, init_method). In\nparticular, we will go over the different initialization methods which\nare responsible for the initial coordination step between each process.\nThose methods allow you to define how this coordination is done.\nDepending on your hardware setup, one of these methods should be\nnaturally more suitable than the others. In addition to the following\nsections, you should also have a look at the .",
                                "markdown"
                            ],
                            [
                                "<strong>Environment Variable</strong>",
                                "markdown"
                            ],
                            [
                                "We have been using the environment variable initialization method\nthroughout this tutorial. By setting the following four environment\nvariables on all machines, all processes will be able to properly\nconnect to the master, obtain information about the other processes, and\nfinally handshake with them.",
                                "markdown"
                            ],
                            [
                                "MASTER_PORT: A free port on the machine that will host the\nprocess with rank 0.",
                                "markdown"
                            ],
                            [
                                "MASTER_ADDR: IP address of the machine that will host the process\nwith rank 0.",
                                "markdown"
                            ],
                            [
                                "WORLD_SIZE: The total number of processes, so that the master\nknows how many workers to wait for.",
                                "markdown"
                            ],
                            [
                                "RANK: Rank of each process, so they will know whether it is the\nmaster of a worker.",
                                "markdown"
                            ],
                            [
                                "<strong>Shared File System</strong>",
                                "markdown"
                            ],
                            [
                                "The shared filesystem requires all processes to have access to a shared\nfile system, and will coordinate them through a shared file. This means\nthat each process will open the file, write its information, and wait\nuntil everybody did so. After that all required information will be\nreadily available to all processes. In order to avoid race conditions,\nthe file system must support locking through\n.",
                                "markdown"
                            ],
                            [
                                "dist.init_process_group(\n    init_method='file:///mnt/nfs/sharedfile',\n    rank=args.rank,\n    world_size=4)",
                                "code"
                            ],
                            [
                                "<strong>TCP</strong>",
                                "markdown"
                            ],
                            [
                                "Initializing via TCP can be achieved by providing the IP address of the process with rank 0 and a reachable port number.\nHere, all workers will be able to connect to the process\nwith rank 0 and exchange information on how to reach each other.",
                                "markdown"
                            ],
                            [
                                "dist.init_process_group(\n    init_method='tcp://10.1.1.20:23456',\n    rank=args.rank,\n    world_size=4)\n\n\n<!--\n## Internals\n* The magic behind init_process_group:\n\n1. validate and parse the arguments\n2. resolve the backend: name2channel.at()\n3. Drop GIL & THDProcessGroupInit: instantiate the channel and add address of master from config\n4. rank 0 inits master, others workers\n5. master: create sockets for all workers -> wait for all workers to connect -> send them each the info about location of other processes\n6. worker: create socket to master, send own info, receive info about each worker, and then handshake with each of them\n7. By this time everyone has handshake with everyone.\n--><center>",
                                "code"
                            ],
                            [
                                "<strong>Acknowledgements</strong>\n</center>",
                                "markdown"
                            ],
                            [
                                "I\u2019d like to thank the PyTorch developers for doing such a good job on\ntheir implementation, documentation, and tests. When the code was\nunclear, I could always count on the\n or the\n\nto find an answer. In particular, I\u2019d like to thank Soumith Chintala,\nAdam Paszke, and Natalia Gimelshein for providing insightful comments\nand answering questions on early drafts.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            }
        ],
        "Getting Started with Fully Sharded Data Parallel(FSDP)": [
            [
                "<strong>Author</strong>: , , ",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                " View and edit this tutorial in .",
                "markdown"
            ],
            [
                "Training AI models at a large scale is a challenging task that requires a lot of compute power and resources.\nIt also comes with considerable engineering complexity to handle the training of these very large models.\n, released in PyTorch 1.11 makes this easier.",
                "markdown"
            ],
            [
                "In this tutorial, we show how to use , for simple MNIST models that can be extended to other larger models such as ,\n . The sample DDP MNIST code has been borrowed from .",
                "markdown"
            ],
            {
                "How FSDP works": [
                    [
                        "In , (DDP) training, each process/ worker owns a replica of the model and processes a batch of data, finally it uses all-reduce to sum up gradients over different workers. In DDP the model weights and optimizer states are replicated across all workers. FSDP is a type of data parallelism that shards model parameters, optimizer states and gradients across DDP ranks.",
                        "markdown"
                    ],
                    [
                        "FSDP GPU memory footprint would be smaller than DDP across all workers. This makes the training of some very large models feasible and helps to fit larger models or batch sizes for our training job. This would come with the cost of increased communication volume. The communication overhead is reduced by internal optimizations like communication and computation overlapping.",
                        "markdown"
                    ],
                    [
                        "FSDP Workflow",
                        "markdown"
                    ],
                    [
                        "At high level FSDP works as follow:",
                        "markdown"
                    ],
                    [
                        "<em>In constructor</em>",
                        "markdown"
                    ],
                    [
                        "Shard model parameters and each rank only keeps its own shard",
                        "markdown"
                    ],
                    [
                        "<em>In forward path</em>",
                        "markdown"
                    ],
                    [
                        "Run all_gather to collect all shards from all ranks to recover the full parameter in this FSDP unit",
                        "markdown"
                    ],
                    [
                        "Run forward computation",
                        "markdown"
                    ],
                    [
                        "Discard parameter shards it has just collected",
                        "markdown"
                    ],
                    [
                        "<em>In backward path</em>",
                        "markdown"
                    ],
                    [
                        "Run all_gather to collect all shards from all ranks to recover the full parameter in this FSDP unit",
                        "markdown"
                    ],
                    [
                        "Run backward computation",
                        "markdown"
                    ],
                    [
                        "Run reduce_scatter to sync gradients",
                        "markdown"
                    ],
                    [
                        "Discard parameters.",
                        "markdown"
                    ]
                ]
            },
            {
                "How to use FSDP": [
                    [
                        "Here we use a toy model to run training on MNIST dataset for demonstration purposes. Similarly the APIs and logic can be applied to larger models for training.",
                        "markdown"
                    ],
                    [
                        "<em>Setup</em>",
                        "markdown"
                    ],
                    [
                        "1.1 Install Pytorch along with Torchvision",
                        "markdown"
                    ],
                    [
                        "pip3 install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cu113/torch_nightly.html",
                        "code"
                    ],
                    [
                        "We add the following code snippets to a python script \u201cFSDP_mnist.py\u201d.",
                        "markdown"
                    ],
                    [
                        "1.2  Import necessary packages",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "This tutorial is intended for PyTorch versions 1.12 and later. If you are using an earlier version, replace all instances of <cite>size_based_auto_wrap_policy</cite> with <cite>default_auto_wrap_policy</cite>.",
                        "markdown"
                    ],
                    [
                        "# Based on: https://github.com/pytorch/examples/blob/master/mnist/main.py\nimport os\nimport argparse\nimport functools\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\n\nfrom torch.optim.lr_scheduler import StepLR\n\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\nfrom torch.distributed.fsdp.fully_sharded_data_parallel import (\n    CPUOffload,\n    BackwardPrefetch,\n)\nfrom torch.distributed.fsdp.wrap import (\n    size_based_auto_wrap_policy,\n    enable_wrap,\n    wrap,\n)",
                        "code"
                    ],
                    [
                        "1.3 Distributed training setup. As we mentioned FSDP is a type of data parallelism which requires a distributed training environment, so here we use two helper functions to initialize the processes for distributed training and clean up.",
                        "markdown"
                    ],
                    [
                        "def setup(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12355'\n\n    # initialize the process group\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\ndef cleanup():\n    dist.destroy_process_group()",
                        "code"
                    ],
                    [
                        "2.1  Define our toy model for handwritten digit classification.",
                        "markdown"
                    ],
                    [
                        "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output",
                        "code"
                    ],
                    [
                        "2.2 define a train function",
                        "markdown"
                    ],
                    [
                        "def train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n    model.train()\n    ddp_loss = torch.zeros(2).to(rank)\n    if sampler:\n        sampler.set_epoch(epoch)\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(rank), target.to(rank)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target, reduction='sum')\n        loss.backward()\n        optimizer.step()\n        ddp_loss[0] += loss.item()\n        ddp_loss[1] += len(data)\n\n    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n    if rank == 0:\n        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, ddp_loss[0] / ddp_loss[1]))",
                        "code"
                    ],
                    [
                        "2.3 Define a validation function",
                        "markdown"
                    ],
                    [
                        "def test(model, rank, world_size, test_loader):\n    model.eval()\n    correct = 0\n    ddp_loss = torch.zeros(3).to(rank)\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(rank), target.to(rank)\n            output = model(data)\n            ddp_loss[0] += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n            ddp_loss[1] += pred.eq(target.view_as(pred)).sum().item()\n            ddp_loss[2] += len(data)\n\n    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n\n    if rank == 0:\n        test_loss = ddp_loss[0] / ddp_loss[2]\n        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n            test_loss, int(ddp_loss[1]), int(ddp_loss[2]),\n            100. * ddp_loss[1] / ddp_loss[2]))",
                        "code"
                    ],
                    [
                        "2.4 Define a distributed train function that wraps the model in FSDP",
                        "markdown"
                    ],
                    [
                        "<strong>Note: to save the FSDP model, we need to call the state_dict on each rank then on Rank 0 save the overall states. This is only available in Pytorch nightlies, current Pytorch release is 1.11 at the moment.</strong>",
                        "markdown"
                    ],
                    [
                        "def fsdp_main(rank, world_size, args):\n    setup(rank, world_size)\n\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n    dataset1 = datasets.MNIST('../data', train=True, download=True,\n                        transform=transform)\n    dataset2 = datasets.MNIST('../data', train=False,\n                        transform=transform)\n\n    sampler1 = DistributedSampler(dataset1, rank=rank, num_replicas=world_size, shuffle=True)\n    sampler2 = DistributedSampler(dataset2, rank=rank, num_replicas=world_size)\n\n    train_kwargs = {'batch_size': args.batch_size, 'sampler': sampler1}\n    test_kwargs = {'batch_size': args.test_batch_size, 'sampler': sampler2}\n    cuda_kwargs = {'num_workers': 2,\n                    'pin_memory': True,\n                    'shuffle': False}\n    train_kwargs.update(cuda_kwargs)\n    test_kwargs.update(cuda_kwargs)\n\n    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n    my_auto_wrap_policy = functools.partial(\n        size_based_auto_wrap_policy, min_num_params=100\n    )\n    torch.cuda.set_device(rank)\n\n\n    init_start_event = torch.cuda.Event(enable_timing=True)\n    init_end_event = torch.cuda.Event(enable_timing=True)\n\n    model = Net().to(rank)\n\n    model = FSDP(model)\n\n    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    init_start_event.record()\n    for epoch in range(1, args.epochs + 1):\n        train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)\n        test(model, rank, world_size, test_loader)\n        scheduler.step()\n\n    init_end_event.record()\n\n    if rank == 0:\n        print(f\"CUDA event elapsed time: {init_start_event.elapsed_time(init_end_event) / 1000}sec\")\n        print(f\"{model}\")\n\n    if args.save_model:\n        # use a barrier to make sure training is done on all ranks\n        dist_barrier()\n        # state_dict for FSDP model is only available on Nightlies for now\n        states = model.state_dict()\n        if rank == 0:\n            torch.save(states, \"mnist_cnn.pt\")\n\n    cleanup()",
                        "code"
                    ],
                    [
                        "2.5 Finally parsing the arguments and setting the main function",
                        "markdown"
                    ],
                    [
                        "if __name__ == '__main__':\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n                        help='number of epochs to train (default: 14)')\n    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n                        help='learning rate (default: 1.0)')\n    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--save-model', action='store_true', default=False,\n                        help='For Saving the current Model')\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n\n    WORLD_SIZE = torch.cuda.device_count()\n    mp.spawn(fsdp_main,\n        args=(WORLD_SIZE, args),\n        nprocs=WORLD_SIZE,\n        join=True)",
                        "code"
                    ],
                    [
                        "We have recorded cuda events to measure the time of FSDP model specifics. The CUDA event time was 110.85 seconds.",
                        "markdown"
                    ],
                    [
                        "python FSDP_mnist.py\n\nCUDA event elapsed time on training loop 40.67462890625sec",
                        "code"
                    ],
                    [
                        "Wrapping the model with FSDP, the model will look as follows, we can see the model has been wrapped in one FSDP unit.\nAlternatively, we will look at adding the fsdp_auto_wrap_policy next and will discuss the differences.",
                        "markdown"
                    ],
                    [
                        "   FullyShardedDataParallel(\n   (_fsdp_wrapped_module): FlattenParamsWrapper(\n       (_fpw_module): Net(\n       (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n       (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n       (dropout1): Dropout(p=0.25, inplace=False)\n       (dropout2): Dropout(p=0.5, inplace=False)\n       (fc1): Linear(in_features=9216, out_features=128, bias=True)\n       (fc2): Linear(in_features=128, out_features=10, bias=True)\n       )\n   )\n)",
                        "code"
                    ],
                    [
                        "Following is the peak memory usage from FSDP MNIST training on g4dn.12.xlarge AWS EC2 instance with 4 gpus captured from Pytorch Profiler.",
                        "markdown"
                    ],
                    [
                        "FSDP Peak Memory Usage",
                        "markdown"
                    ],
                    [
                        "<em>Applying fsdp_auto_wrap_policy</em> in FSDP otherwise, FSDP will put the entire model in one FSDP unit, which will reduce computation efficiency and memory efficiency.\nThe way it works is that, suppose your model contains 100 Linear layers. If you do FSDP(model), there will only be one FSDP unit which wraps the entire model.\nIn that case, the allgather would collect the full parameters for all 100 linear layers, and hence won\u2019t save CUDA memory for parameter sharding.\nAlso, there is only one blocking allgather call for the all 100 linear layers, there will not be communication and computation overlapping between layers.",
                        "markdown"
                    ],
                    [
                        "To avoid that, you can pass in an fsdp_auto_wrap_policy, which will seal the current FSDP unit and start a new one automatically when the specified condition is met (e.g., size limit).\nIn that way you will have multiple FSDP units, and only one FSDP unit needs to collect full parameters at a time. E.g., suppose you have 5 FSDP units, and each wraps 20 linear layers.\nThen, in the forward, the 1st FSDP unit will allgather parameters for the first 20 linear layers, do computation, discard the parameters and then move on to the next 20 linear layers. So, at any point in time, each rank only materializes parameters/grads for 20 linear layers instead of 100.",
                        "markdown"
                    ],
                    [
                        "To do so in 2.4 we define the auto_wrap_policy and pass it to FSDP wrapper, in the following example, my_auto_wrap_policy defines that a layer could be wrapped or sharded by FSDP if the number of parameters in this layer is larger than 100.\nIf the number of parameters in this layer is smaller than 100, it will be wrapped with other small layers together by FSDP.\nFinding an optimal auto wrap policy is challenging, PyTorch will add auto tuning for this config in the future. Without an auto tuning tool, it is good to profile your workflow using different auto wrap policies experimentally and find the optimal one.",
                        "markdown"
                    ],
                    [
                        "my_auto_wrap_policy = functools.partial(\n        size_based_auto_wrap_policy, min_num_params=20000\n    )\ntorch.cuda.set_device(rank)\nmodel = Net().to(rank)\n\nmodel = FSDP(model,\n    fsdp_auto_wrap_policy=my_auto_wrap_policy)",
                        "code"
                    ],
                    [
                        "Applying the FSDP_auto_wrap_policy, the model would be as follows:",
                        "markdown"
                    ],
                    [
                        "  FullyShardedDataParallel(\n(_fsdp_wrapped_module): FlattenParamsWrapper(\n  (_fpw_module): Net(\n    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n    (dropout1): Dropout(p=0.25, inplace=False)\n    (dropout2): Dropout(p=0.5, inplace=False)\n    (fc1): FullyShardedDataParallel(\n      (_fsdp_wrapped_module): FlattenParamsWrapper(\n        (_fpw_module): Linear(in_features=9216, out_features=128, bias=True)\n      )\n    )\n    (fc2): Linear(in_features=128, out_features=10, bias=True)\n  )\n)",
                        "code"
                    ],
                    [
                        "python FSDP_mnist.py\n\nCUDA event elapsed time on training loop 41.89130859375sec",
                        "code"
                    ],
                    [
                        "Following is the peak memory usage from FSDP with auto_wrap policy of MNIST training on g4dn.12.xlarge AWS EC2 instance with 4 gpus captured from Pytorch Profiler.\nIt can be observed that the peak memory usage on each device is smaller compared to FSDP without auto wrap policy applied, from ~75 MB to 66 MB.",
                        "markdown"
                    ],
                    [
                        "FSDP Peak Memory Usage using Auto_wrap policy",
                        "markdown"
                    ],
                    [
                        "<em>CPU Off-loading</em>: In case the model is very large that even with FSDP wouldn\u2019t fit into gpus, then CPU offload can be helpful here.",
                        "markdown"
                    ],
                    [
                        "Currently, only parameter and gradient CPU offload is supported. It can be enabled via passing in cpu_offload=CPUOffload(offload_params=True).",
                        "markdown"
                    ],
                    [
                        "Note that this currently implicitly enables gradient offloading to CPU in order for params and grads to be on the same device to work with the optimizer. This API is subject to change. Default is None in which case there will be no offloading.",
                        "markdown"
                    ],
                    [
                        "Using this feature may slow down the training considerably, due to frequent copying of tensors from host to device, but it could help improve memory efficiency and train larger scale models.",
                        "markdown"
                    ],
                    [
                        "In 2.4 we just add it to the FSDP wrapper",
                        "markdown"
                    ],
                    [
                        "model = FSDP(model,\n    fsdp_auto_wrap_policy=my_auto_wrap_policy,\n    cpu_offload=CPUOffload(offload_params=True))",
                        "code"
                    ],
                    [
                        "Compare it with DDP, if in 2.4 we just normally wrap the model in ddp, saving the changes in \u201cDDP_mnist.py\u201d.",
                        "markdown"
                    ],
                    [
                        "model = Net().to(rank)\nmodel = DDP(model)",
                        "code"
                    ],
                    [
                        "python DDP_mnist.py\n\nCUDA event elapsed time on training loop 39.77766015625sec",
                        "code"
                    ],
                    [
                        "Following is the peak memory usage from DDP MNIST training on g4dn.12.xlarge AWS EC2 instance with 4 gpus captured from Pytorch profiler.",
                        "markdown"
                    ],
                    [
                        "DDP Peak Memory Usage using Auto_wrap policy",
                        "markdown"
                    ],
                    [
                        "Considering the toy example and tiny MNIST model we defined here, we can observe the difference between peak memory usage of DDP and FSDP.\nIn DDP each process holds a replica of the model, so the memory footprint is higher compared to FSDP that shards the model parameter, optimizer states and gradients over DDP ranks.\nThe peak memory usage using FSDP with auto_wrap policy is the lowest followed by FSDP and DDP.",
                        "markdown"
                    ],
                    [
                        "Also, looking at timings, considering the small model and running the training on a single machine, FSDP with/out auto_wrap policy performed almost as fast as DDP.\nThis example does not represent most of the real applications, for detailed analysis and comparison between DDP and FSDP please refer to this  .",
                        "markdown"
                    ]
                ]
            }
        ],
        "Advanced Model Training with Fully Sharded Data Parallel (FSDP)": [
            [
                "<strong>Author</strong>: , , , ",
                "markdown"
            ],
            [
                "This tutorial introduces more advanced features of Fully Sharded Data Parallel\n(FSDP) as part of the PyTorch 1.12 release. To get familiar with FSDP, please\nrefer to the .",
                "markdown"
            ],
            [
                "In this tutorial, we fine-tune a HuggingFace (HF) T5 model with FSDP for text\nsummarization as a working example.",
                "markdown"
            ],
            [
                "The example uses Wikihow and for simplicity, we will showcase the training on a\nsingle node, P4dn instance with 8 A100 GPUs. We will soon have a blog post on\nlarge scale FSDP training on a multi-node cluster, please stay tuned for that on\nthe PyTorch medium channel.",
                "markdown"
            ],
            [
                "FSDP is a production ready package with focus on ease of use, performance, and\nlong-term support.  One of the main benefits of FSDP is reducing the memory\nfootprint on each GPU. This enables training of larger models with lower total\nmemory vs DDP, and leverages the overlap of computation and communication to\ntrain models efficiently.\nThis reduced memory pressure can be leveraged to either train larger models or\nincrease batch size, potentially helping overall training throughput.  You can\nread more about PyTorch FSDP .",
                "markdown"
            ],
            {
                "FSDP Features in This Tutorial": [
                    [
                        "Transformer Auto Wrap Policy",
                        "markdown"
                    ],
                    [
                        "Mixed Precision",
                        "markdown"
                    ],
                    [
                        "Initializing FSDP Model on Device",
                        "markdown"
                    ],
                    [
                        "Sharding Strategy",
                        "markdown"
                    ],
                    [
                        "Backward Prefetch",
                        "markdown"
                    ],
                    [
                        "Model Checkpoint Saving via Streaming to CPU",
                        "markdown"
                    ]
                ]
            },
            {
                "Recap on How FSDP Works": [
                    [
                        "At a high level FDSP works as follow:",
                        "markdown"
                    ],
                    [
                        "<em>In constructor</em>",
                        "markdown"
                    ],
                    [
                        "Shard model parameters and each rank only keeps its own shard",
                        "markdown"
                    ],
                    [
                        "<em>In forward pass</em>",
                        "markdown"
                    ],
                    [
                        "Run <cite>all_gather</cite> to collect all shards from all ranks to recover the full\nparameter for this FSDP unit Run forward computation",
                        "markdown"
                    ],
                    [
                        "Discard non-owned parameter shards it has just collected to free memory",
                        "markdown"
                    ],
                    [
                        "<em>In backward pass</em>",
                        "markdown"
                    ],
                    [
                        "Run <cite>all_gather</cite> to collect all shards from all ranks to recover the full\nparameter in this FSDP unit Run backward computation",
                        "markdown"
                    ],
                    [
                        "Discard non-owned parameters to free memory.",
                        "markdown"
                    ],
                    [
                        "Run reduce_scatter to sync gradients",
                        "markdown"
                    ]
                ]
            },
            {
                "Fine-tuning HF T5": [
                    [
                        "HF T5 pre-trained models are available in four different sizes, ranging from\nsmall with 60 Million parameters to XXL with 11 Billion parameters. In this\ntutorial, we demonstrate the fine-tuning of a T5 3B with FSDP for text\nsummarization using WikiHow dataset.  The main focus of this tutorial is to\nhighlight different available features in FSDP that are helpful for training\nlarge scale model above 3B parameters. Also, we cover specific features for\nTransformer based models. The code for this tutorial is available in  .",
                        "markdown"
                    ],
                    [
                        "<em>Setup</em>",
                        "markdown"
                    ],
                    [
                        "1.1 Install PyTorch Nightlies",
                        "markdown"
                    ],
                    [
                        "We will install PyTorch nightlies, as some of the features such as activation\ncheckpointing is available in nightlies and will be added in next PyTorch\nrelease after 1.12.",
                        "markdown"
                    ],
                    [
                        "pip3 install --pre torch torchvision torchaudio -f https://download.pytorch.org/whl/nightly/cu113/torch_nightly.html",
                        "code"
                    ],
                    [
                        "1.2 Dataset Setup",
                        "markdown"
                    ],
                    [
                        "Please create a <cite>data</cite> folder, download the WikiHow dataset from   and\n,\nand place them in the <cite>data</cite> folder.  We will use the wikihow dataset from\n.",
                        "markdown"
                    ],
                    [
                        "Next, we add the following code snippets to a Python script \u201cT5_training.py\u201d.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "The full source code for this tutorial is available in .",
                        "markdown"
                    ],
                    [
                        "1.3  Import necessary packages:",
                        "markdown"
                    ],
                    [
                        "import os\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom transformers import AutoTokenizer, GPT2TokenizerFast\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nimport functools\nfrom torch.optim.lr_scheduler import StepLR\nimport torch.nn.functional as F\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom transformers.models.t5.modeling_t5 import T5Block\n\nfrom torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n checkpoint_wrapper,\n CheckpointImpl,\n apply_activation_checkpointing_wrapper)\n\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    MixedPrecision,\n    BackwardPrefetch,\n    ShardingStrategy,\n    FullStateDictConfig,\n    StateDictType,\n)\nfrom torch.distributed.fsdp.wrap import (\n    transformer_auto_wrap_policy,\n    enable_wrap,\n    wrap,\n)\nfrom functools import partial\nfrom torch.utils.data import DataLoader\nfrom pathlib import Path\nfrom summarization_dataset import *\nfrom transformers.models.t5.modeling_t5 import T5Block\nfrom typing import Type\nimport time\nimport tqdm\nfrom datetime import datetime",
                        "code"
                    ],
                    [
                        "1.4 Distributed training setup.\nHere we use two helper functions to initialize the processes for distributed\ntraining,  and then to clean up after training completion.  In this tutorial, we\nare going to use torch elastic, using  , which will set the\nworker <cite>RANK</cite> and <cite>WORLD_SIZE</cite> automatically.",
                        "markdown"
                    ],
                    [
                        "def setup():\n    # initialize the process group\n    dist.init_process_group(\"nccl\")\n\ndef cleanup():\n    dist.destroy_process_group()",
                        "code"
                    ],
                    [
                        "2.1  Set up the HuggingFace T5 model:",
                        "markdown"
                    ],
                    [
                        "def setup_model(model_name):\n    model = T5ForConditionalGeneration.from_pretrained(model_name)\n    tokenizer =  T5Tokenizer.from_pretrained(model_name)\n    return model, tokenizer",
                        "code"
                    ],
                    [
                        "We also, add couple of helper functions here for date and formatting memory\nmetrics.",
                        "markdown"
                    ],
                    [
                        "def get_date_of_run():\n    \"\"\"create date and time for file save uniqueness\n    example: 2022-05-07-08:31:12_PM'\n    \"\"\"\n    date_of_run = datetime.now().strftime(\"%Y-%m-%d-%I:%M:%S_%p\")\n    print(f\"--&gt; current date and time of run = {date_of_run}\")\n    return date_of_run\n\ndef format_metrics_to_gb(item):\n    \"\"\"quick function to format numbers to gigabyte and round to 4 digit precision\"\"\"\n    metric_num = item / g_gigabyte\n    metric_num = round(metric_num, ndigits=4)\n    return metric_num",
                        "code"
                    ],
                    [
                        "2.2 Define a train function:",
                        "markdown"
                    ],
                    [
                        "def train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=None):\n    model.train()\n    local_rank = int(os.environ['LOCAL_RANK'])\n    fsdp_loss = torch.zeros(2).to(local_rank)\n\n    if sampler:\n        sampler.set_epoch(epoch)\n    if rank==0:\n        inner_pbar = tqdm.tqdm(\n            range(len(train_loader)), colour=\"blue\", desc=\"r0 Training Epoch\"\n        )\n    for batch in train_loader:\n        for key in batch.keys():\n            batch[key] = batch[key].to(local_rank)\n        optimizer.zero_grad()\n        output = model(input_ids=batch[\"source_ids\"],attention_mask=batch[\"source_mask\"],labels=batch[\"target_ids\"] )\n        loss = output[\"loss\"]\n        loss.backward()\n        optimizer.step()\n        fsdp_loss[0] += loss.item()\n        fsdp_loss[1] += len(batch)\n        if rank==0:\n            inner_pbar.update(1)\n\n    dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)\n    train_accuracy = fsdp_loss[0] / fsdp_loss[1]\n\n\n    if rank == 0:\n        inner_pbar.close()\n        print(\n                f\"Train Epoch: \\t{epoch}, Loss: \\t{train_accuracy:.4f}\"\n            )\n    return train_accuracy",
                        "code"
                    ],
                    [
                        "2.3 Define a validation function:",
                        "markdown"
                    ],
                    [
                        "def validation(model, rank, world_size, val_loader):\n    model.eval()\n    correct = 0\n    local_rank = int(os.environ['LOCAL_RANK'])\n    fsdp_loss = torch.zeros(3).to(local_rank)\n    if rank == 0:\n        inner_pbar = tqdm.tqdm(\n            range(len(val_loader)), colour=\"green\", desc=\"Validation Epoch\"\n        )\n    with torch.no_grad():\n        for batch in val_loader:\n            for key in batch.keys():\n                batch[key] = batch[key].to(local_rank)\n            output = model(input_ids=batch[\"source_ids\"],attention_mask=batch[\"source_mask\"],labels=batch[\"target_ids\"])\n            fsdp_loss[0] += output[\"loss\"].item()  # sum up batch loss\n            fsdp_loss[1] += len(batch)\n\n            if rank==0:\n                inner_pbar.update(1)\n\n    dist.all_reduce(fsdp_loss, op=dist.ReduceOp.SUM)\n    val_loss = fsdp_loss[0] / fsdp_loss[1]\n    if rank == 0:\n        inner_pbar.close()\n        print(f\"Validation Loss: {val_loss:.4f}\")\n    return val_loss",
                        "code"
                    ],
                    [
                        "2.4 Define a distributed train function that wraps the model in FSDP:",
                        "markdown"
                    ],
                    [
                        "def fsdp_main(args):\n\n    model, tokenizer = setup_model(\"t5-base\")\n\n    local_rank = int(os.environ['LOCAL_RANK'])\n    rank = int(os.environ['RANK'])\n    world_size = int(os.environ['WORLD_SIZE'])\n\n\n    dataset = load_dataset('wikihow', 'all', data_dir='data/')\n    print(dataset.keys())\n    print(\"Size of train dataset: \", dataset['train'].shape)\n    print(\"Size of Validation dataset: \", dataset['validation'].shape)\n\n\n    #wikihow(tokenizer, type_path, num_samples, input_length, output_length, print_text=False)\n    train_dataset = wikihow(tokenizer, 'train', 1500, 512, 150, False)\n    val_dataset = wikihow(tokenizer, 'validation', 300, 512, 150, False)\n\n    sampler1 = DistributedSampler(train_dataset, rank=rank, num_replicas=world_size, shuffle=True)\n    sampler2 = DistributedSampler(val_dataset, rank=rank, num_replicas=world_size)\n\n    setup()\n\n\n    train_kwargs = {'batch_size': args.batch_size, 'sampler': sampler1}\n    test_kwargs = {'batch_size': args.test_batch_size, 'sampler': sampler2}\n    cuda_kwargs = {'num_workers': 2,\n                    'pin_memory': True,\n                    'shuffle': False}\n    train_kwargs.update(cuda_kwargs)\n    test_kwargs.update(cuda_kwargs)\n\n    train_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs)\n    val_loader = torch.utils.data.DataLoader(val_dataset, **test_kwargs)\n\n    t5_auto_wrap_policy = functools.partial(\n        transformer_auto_wrap_policy,\n        transformer_layer_cls={\n            T5Block,\n        },\n    )\n    sharding_strategy: ShardingStrategy = ShardingStrategy.SHARD_GRAD_OP #for Zero2 and FULL_SHARD for Zero3\n    torch.cuda.set_device(local_rank)\n\n\n    #init_start_event = torch.cuda.Event(enable_timing=True)\n    #init_end_event = torch.cuda.Event(enable_timing=True)\n\n    #init_start_event.record()\n\n    bf16_ready = (\n    torch.version.cuda\n    and torch.cuda.is_bf16_supported()\n    and LooseVersion(torch.version.cuda) &gt;= \"11.0\"\n    and dist.is_nccl_available()\n    and nccl.version() &gt;= (2, 10)\n    )\n\n    if bf16_ready:\n        mp_policy = bfSixteen\n    else:\n        mp_policy = None # defaults to fp32\n\n    # model is on CPU before input to FSDP\n    model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=mp_policy,\n        #sharding_strategy=sharding_strategy,\n        device_id=torch.cuda.current_device())\n\n    optimizer = optim.AdamW(model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    best_val_loss = float(\"inf\")\n    curr_val_loss = float(\"inf\")\n    file_save_name = \"T5-model-\"\n\n    if rank == 0:\n        time_of_run = get_date_of_run()\n        dur = []\n        train_acc_tracking = []\n        val_acc_tracking = []\n        training_start_time = time.time()\n\n    if rank == 0 and args.track_memory:\n        mem_alloc_tracker = []\n        mem_reserved_tracker = []\n\n    for epoch in range(1, args.epochs + 1):\n        t0 = time.time()\n        train_accuracy = train(args, model, rank, world_size, train_loader, optimizer, epoch, sampler=sampler1)\n        if args.run_validation:\n            curr_val_loss = validation(model, rank, world_size, val_loader)\n        scheduler.step()\n\n        if rank == 0:\n\n            print(f\"--&gt; epoch {epoch} completed...entering save and stats zone\")\n\n            dur.append(time.time() - t0)\n            train_acc_tracking.append(train_accuracy.item())\n\n            if args.run_validation:\n                val_acc_tracking.append(curr_val_loss.item())\n\n            if args.track_memory:\n                mem_alloc_tracker.append(\n                    format_metrics_to_gb(torch.cuda.memory_allocated())\n                )\n                mem_reserved_tracker.append(\n                    format_metrics_to_gb(torch.cuda.memory_reserved())\n                )\n            print(f\"completed save and stats zone...\")\n\n        if args.save_model and curr_val_loss &lt; best_val_loss:\n\n            # save\n            if rank == 0:\n                print(f\"--&gt; entering save model state\")\n\n            save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\n            with FSDP.state_dict_type(\n                model, StateDictType.FULL_STATE_DICT, save_policy\n            ):\n                cpu_state = model.state_dict()\n            #print(f\"saving process: rank {rank}  done w state_dict\")\n\n\n            if rank == 0:\n                print(f\"--&gt; saving model ...\")\n                currEpoch = (\n                    \"-\" + str(epoch) + \"-\" + str(round(curr_val_loss.item(), 4)) + \".pt\"\n                )\n                print(f\"--&gt; attempting to save model prefix {currEpoch}\")\n                save_name = file_save_name + \"-\" + time_of_run + \"-\" + currEpoch\n                print(f\"--&gt; saving as model name {save_name}\")\n\n                torch.save(cpu_state, save_name)\n\n        if curr_val_loss &lt; best_val_loss:\n\n            best_val_loss = curr_val_loss\n            if rank==0:\n                print(f\"--&gt;&gt;&gt;&gt; New Val Loss Record: {best_val_loss}\")\n\n    dist.barrier()\n    cleanup()",
                        "code"
                    ],
                    [
                        "2.5 Parse the arguments and set the main function:",
                        "markdown"
                    ],
                    [
                        "if __name__ == '__main__':\n    # Training settings\n    parser = argparse.ArgumentParser(description='PyTorch T5 FSDP Example')\n    parser.add_argument('--batch-size', type=int, default=4, metavar='N',\n                        help='input batch size for training (default: 64)')\n    parser.add_argument('--test-batch-size', type=int, default=4, metavar='N',\n                        help='input batch size for testing (default: 1000)')\n    parser.add_argument('--epochs', type=int, default=2, metavar='N',\n                        help='number of epochs to train (default: 3)')\n    parser.add_argument('--lr', type=float, default=.002, metavar='LR',\n                        help='learning rate (default: .002)')\n    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n                        help='Learning rate step gamma (default: 0.7)')\n    parser.add_argument('--no-cuda', action='store_true', default=False,\n                        help='disables CUDA training')\n    parser.add_argument('--seed', type=int, default=1, metavar='S',\n                        help='random seed (default: 1)')\n    parser.add_argument('--track_memory', action='store_false', default=True,\n                        help='track the gpu memory')\n    parser.add_argument('--run_validation', action='store_false', default=True,\n                        help='running the validation')\n    parser.add_argument('--save-model', action='store_false', default=True,\n                        help='For Saving the current Model')\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n\n    fsdp_main(args)",
                        "code"
                    ],
                    [
                        "To run the the training using torchrun:",
                        "markdown"
                    ],
                    [
                        "torchrun --nnodes 1 --nproc_per_node 4  T5_training.py",
                        "code"
                    ]
                ]
            },
            {
                "Transformer Wrapping Policy": [
                    [
                        "As discussed in the ,\nauto_wrap_policy is one of the FSDP features that make it easy to automatically\nshard a given model and put the model, optimizer and gradient shards into\ndistinct FSDP units.",
                        "markdown"
                    ],
                    [
                        "For some architectures such as Transformer encoder-decoders, some parts of the\nmodel such as embedding table is being shared with both encoder and decoder.  In\nthis case, we need to place the embedding table in the outer FSDP unit so that\nit could be accessed from both encoder and decoder.  In addition, by registering\nthe layer class for a transformer, the sharding plan can be made much more\ncommunication efficient.  In PyTorch 1.12, FSDP added this support and now we\nhave a wrapping policy for transfomers.",
                        "markdown"
                    ],
                    [
                        "It can be created as follows, where the T5Block represents the T5 transformer\nlayer class (holding MHSA and FFN).",
                        "markdown"
                    ],
                    [
                        "t5_auto_wrap_policy = functools.partial(\n        transformer_auto_wrap_policy,\n        transformer_layer_cls={\n            T5Block,\n        },\n    )\ntorch.cuda.set_device(local_rank)\n\n\nmodel = FSDP(model,\n    fsdp_auto_wrap_policy=t5_auto_wrap_policy)",
                        "code"
                    ],
                    [
                        "To see the wrapped model, you can easily print the model and visually inspect\nthe sharding and FSDP units as well.",
                        "markdown"
                    ]
                ]
            },
            {
                "Mixed Precision": [
                    [
                        "FSDP supports flexible mixed precision training allowing for arbitrary reduced\nprecision types (such as fp16 or bfloat16). Currently BFloat16 is only available\non Ampere GPUs, so you need to confirm native support before you use it. On\nV100s for example, BFloat16 can still be run but due to it running non-natively,\nit can result in significant slowdowns.",
                        "markdown"
                    ],
                    [
                        "To check if BFloat16 is natively supported, you can use the following :",
                        "markdown"
                    ],
                    [
                        "bf16_ready = (\n    torch.version.cuda\n    and torch.cuda.is_bf16_supported()\n    and LooseVersion(torch.version.cuda) &gt;= \"11.0\"\n    and dist.is_nccl_available()\n    and nccl.version() &gt;= (2, 10)\n)",
                        "code"
                    ],
                    [
                        "One of the advantages of mixed percision in FSDP is providing granular control\nover different precision levels for parameters, gradients, and buffers as\nfollows:",
                        "markdown"
                    ],
                    [
                        "fpSixteen = MixedPrecision(\n    param_dtype=torch.float16,\n    # Gradient communication precision.\n    reduce_dtype=torch.float16,\n    # Buffer precision.\n    buffer_dtype=torch.float16,\n)\n\nbfSixteen = MixedPrecision(\n    param_dtype=torch.bfloat16,\n    # Gradient communication precision.\n    reduce_dtype=torch.bfloat16,\n    # Buffer precision.\n    buffer_dtype=torch.bfloat16,\n)\n\nfp32_policy = MixedPrecision(\n    param_dtype=torch.float32,\n    # Gradient communication precision.\n    reduce_dtype=torch.float32,\n    # Buffer precision.\n    buffer_dtype=torch.float32,\n)",
                        "code"
                    ],
                    [
                        "Note that if a certain type (parameter, reduce, buffer) is not specified, they\nwill not be casted at all.",
                        "markdown"
                    ],
                    [
                        "This flexibility allows users fine grained control, such as only setting\ngradient communication to happen in reduced precision, and all parameters /\nbuffer computation to be done in full precision. This is potentially useful in\ncases where intra-node communication is the main bottleneck and parameters /\nbuffers must be in full precision to avoid accuracy issues. This can be done\nwith the following policy:",
                        "markdown"
                    ],
                    [
                        "grad_bf16 = MixedPrecision(reduce_dtype=torch.bfloat16)",
                        "code"
                    ],
                    [
                        "In 2.4 we just add the relevant mixed precision policy to the FSDP wrapper:",
                        "markdown"
                    ],
                    [
                        "model = FSDP(model,\n       auto_wrap_policy=t5_auto_wrap_policy,\n       mixed_precision=bfSixteen)",
                        "code"
                    ],
                    [
                        "In our experiments, we have observed up to 4x speed up by using BFloat16 for\ntraining and memory reduction of approximately 30% in some experiments that can\nbe used for batch size increases.",
                        "markdown"
                    ]
                ]
            },
            {
                "Intializing FSDP Model on Device": [
                    [
                        "In 1.12, FSDP supports a <cite>device_id</cite> argument meant to initialize input CPU\nmodule on the device given by <cite>device_id</cite>. This is useful when the entire model\ndoes not fit on a single GPU, but fits in a host\u2019s CPU memory. When <cite>device_id</cite>\nis specified, FSDP will move the model to the specified device on a per-FSDP\nunit basis, avoiding GPU OOM issues while initializing several times faster than\nCPU-based initialization:",
                        "markdown"
                    ],
                    [
                        "torch.cuda.set_device(local_rank)\n\n model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=bfSixteen,\n        device_id=torch.cuda.current_device())",
                        "code"
                    ]
                ]
            },
            {
                "Sharding Strategy": [
                    [
                        "FSDP sharding strategy by default is set to fully shard the model parameters,\ngradients and optimizer states get sharded across all ranks. (also termed Zero3\nsharding). In case you are interested to have the Zero2 sharding strategy, where\nonly optimizer states and gradients are sharded, FSDP support this feature by\npassing the Sharding strategy by using  \u201cShardingStrategy.SHARD_GRAD_OP\u201d,\ninstead of \u201cShardingStrategy.FULL_SHARD\u201d to the FSDP initialization  as follows:",
                        "markdown"
                    ],
                    [
                        "torch.cuda.set_device(local_rank)\n\n model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=bfSixteen,\n        device_id=torch.cuda.current_device(),\n        sharding_strategy=ShardingStrategy.SHARD_GRAD_OP # ZERO2)",
                        "code"
                    ],
                    [
                        "This will reduce the communication overhead in FSDP, in this case, it holds full\nparameters after forward and through the backwards pass.",
                        "markdown"
                    ],
                    [
                        "This saves an all_gather during backwards so there is less communication at the\ncost of a higher memory footprint. Note that full model params are freed at the\nend of backwards and all_gather will happen on the next forward pass.",
                        "markdown"
                    ]
                ]
            },
            {
                "Backward Prefetch": [
                    [
                        "The backward prefetch setting controls the timing of when the next FSDP unit\u2019s\nparameters should be requested.  By setting it to <cite>BACKWARD_PRE</cite>, the next\nFSDP\u2019s unit params can begin to be requested and arrive sooner before the\ncomputation of the current unit starts. This overlaps the <cite>all_gather</cite>\ncommunication and gradient computation which can increase the training speed in\nexchange for slightly higher memory consumption. It can be utilized in the FSDP\nwrapper in 2.4 as follows:",
                        "markdown"
                    ],
                    [
                        "torch.cuda.set_device(local_rank)\n\n model = FSDP(model,\n        auto_wrap_policy=t5_auto_wrap_policy,\n        mixed_precision=bfSixteen,\n        device_id=torch.cuda.current_device(),\n        backward_prefetch = BackwardPrefetch.BACKWARD_PRE)",
                        "code"
                    ],
                    [
                        "<cite>backward_prefetch</cite> has two modes, <cite>BACKWARD_PRE</cite> and <cite>BACKWARD_POST</cite>.\n<cite>BACKWARD_POST</cite> means that the next FSDP unit\u2019s params will not be requested\nuntil the current FSDP unit processing is complete, thus minimizing memory\noverhead.  In some cases, using <cite>BACKWARD_PRE</cite> can increase model training speed\nup to 2-10%, with even higher speed improvements noted for larger models.",
                        "markdown"
                    ]
                ]
            },
            {
                "Model Checkpoint Saving, by streaming to the Rank0 CPU": [
                    [
                        "To save model checkpoints using FULL_STATE_DICT saving which saves model in the\nsame fashion as a local model, PyTorch 1.12 offers a few utilities to support\nthe saving of larger models.",
                        "markdown"
                    ],
                    [
                        "First, a FullStateDictConfig can be specified, allowing the state_dict to be\npopulated on rank 0 only and offloaded to the CPU.",
                        "markdown"
                    ],
                    [
                        "When using this configuration, FSDP will allgather model parameters, offloading\nthem to the CPU one by one, only on rank 0. When the state_dict is finally\nsaved, it will only be populated on rank 0 and contain CPU tensors. This avoids\npotential OOM for models that are larger than a single GPU memory and allows\nusers to checkpoint models whose size is roughly the available CPU RAM on the\nuser\u2019s machine.",
                        "markdown"
                    ],
                    [
                        "This feature can be run as follows:",
                        "markdown"
                    ],
                    [
                        "save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)\nwith FSDP.state_dict_type(\n            model, StateDictType.FULL_STATE_DICT, save_policy\n        ):\n            cpu_state = model.state_dict()\nif rank == 0:\n save_name = file_save_name + \"-\" + time_of_run + \"-\" + currEpoch\n torch.save(cpu_state, save_name)",
                        "code"
                    ]
                ]
            },
            {
                "Summary": [
                    [
                        "In this tutorial, we have introduced many new features for FSDP available in\nPytorch 1.12 and used HF T5 as the running example.  Using the proper wrapping\npolicy especially for transformer models, along with mixed precision and\nbackward prefetch should speed up your training runs. Also, features such as\ninitializing the model on device, and checkpoint saving via streaming to CPU\nshould help to avoid OOM error in dealing with large models.",
                        "markdown"
                    ],
                    [
                        "We are actively working to add new features to FSDP for the next release. If\nyou have feedback, feature requests, questions or are encountering issues\nusing FSDP, please feel free to contact us by opening an issue in the\n.",
                        "markdown"
                    ]
                ]
            }
        ],
        "Customize Process Group Backends Using Cpp Extensions": [
            [
                "<strong>Author</strong>: , , ",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                " View and edit this tutorial in .",
                "markdown"
            ],
            [
                "Prerequisites:",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "This tutorial demonstrates how to implement a custom ProcessGroup\nbackend and plug that into\n using\n. This is helpful when you need a specialized software\nstack for your hardware, or when you would like to experiment with new\ncollective communication algorithms.",
                "markdown"
            ],
            {
                "Basics": [
                    [
                        "PyTorch collective communications power several widely adopted distributed\ntraining features, including\n,\n,\n.\nIn order to make the same collective communication API work with\ndifferent communication backends, the distributed package abstracts collective\ncommunication operations into a\n\nclass. Different backends can\nthen be implemented as subclasses of ProcessGroup using preferred\nthird-party libraries. PyTorch distributed comes with three default backends,\nProcessGroupNCCL, ProcessGroupGloo, and ProcessGroupMPI. However,\nbeyond these three backends, there are also other communication libraries\n(e.g., ,\n), different types of hardware\n(e.g., ,\n), and emerging\ncommunication algorithms (e.g.,\n,\n).\nTherefore, the distributed package exposes extension APIs to allow customizing\ncollective communication backends.",
                        "markdown"
                    ],
                    [
                        "The 4 steps below show how to implement a dummy ProcessGroup backend\nand use that in Python application code. Please note that this tutorial focuses\non demonstrating the extension APIs, instead of developing a functioning\ncommunication backend. Hence, the dummy backend just covers a subset of the\nAPIs (all_reduce and all_gather), and simply sets the values of tensors\nto 0.",
                        "markdown"
                    ]
                ]
            },
            {
                "Step 1: Implement a Subclass of ProcessGroup": [
                    [
                        "This first step is to implement a ProcessGroup subclass that overrides\ntarget collective communication APIs and runs the custom communication algorithm.\nThe extension also needs to implement a Work subclass, which\nserves as a future of communication results and allows asynchronous execution in\napplication code. If the extension uses third-party libraries, it can\ninclude the headers and call into the library APIs from the ProcessGroupDummy\nsubclass. The two code snippets below present the implementation of dummy.h and\ndummy.cpp. See the \nrepository for the full implementation.",
                        "markdown"
                    ],
                    [
                        "// file name: dummy.hpp\n#include &lt;torch/python.h&gt;\n\n#include &lt;torch/csrc/distributed/c10d/ProcessGroup.hpp&gt;\n#include &lt;torch/csrc/distributed/c10d/Work.hpp&gt;\n#include &lt;torch/csrc/distributed/c10d/Store.hpp&gt;\n#include &lt;torch/csrc/distributed/c10d/Types.hpp&gt;\n#include &lt;torch/csrc/distributed/c10d/Utils.hpp&gt;\n\n#include &lt;pybind11/chrono.h&gt;\n\nnamespace c10d {\n\nclass ProcessGroupDummy : public ProcessGroup {\n  public:\n    ProcessGroupDummy(int rank, int size);\n\n    c10::intrusive_ptr&lt;Work&gt; allgather(\n        std::vector&lt;std::vector&lt;at::Tensor&gt;&gt;&amp; outputTensors,\n        std::vector&lt;at::Tensor&gt;&amp; inputTensors,\n        const AllgatherOptions&amp; opts = AllgatherOptions()) override;\n\n    c10::intrusive_ptr&lt;Work&gt; allreduce(\n        std::vector&lt;at::Tensor&gt;&amp; tensors,\n        const AllreduceOptions&amp; opts = AllreduceOptions()) override;\n\n    // The collective communication APIs without a custom implementation\n    // will error out if invoked by application code.\n};\n\nclass WorkDummy : public Work {\n  public:\n    WorkDummy(\n      OpType opType,\n      c10::intrusive_ptr&lt;c10::ivalue::Future&gt; future) // future of the output\n      : Work(\n          -1, // rank, only used by recvAnySource, irrelevant in this demo\n          opType),\n      future_(std::move(future)) {}\n    // There are several additional helper functions that need to be\n    // implemented. Please refer to https://github.com/mrshenli/dummy_collectives\n    // for the full implementation.\n\n  private:\n    c10::intrusive_ptr&lt;c10::ivalue::Future&gt; future_;\n};\n} // namespace c10d",
                        "code"
                    ],
                    [
                        "// file name: dummy.cpp\n#include \"dummy.hpp\"\n\nnamespace c10d {\n\n// This is a dummy allgather that sets all output tensors to zero\n// Modify the implementation to conduct real communication asynchronously\nc10::intrusive_ptr&lt;Work&gt; ProcessGroupDummy::allgather(\n        std::vector&lt;std::vector&lt;at::Tensor&gt;&gt;&amp; outputTensors,\n        std::vector&lt;at::Tensor&gt;&amp; inputTensors,\n        const AllgatherOptions&amp; /* unused */) {\n    for (auto&amp; outputTensorVec : outputTensors) {\n        for (auto&amp; outputTensor : outputTensorVec) {\n            outputTensor.zero_();\n        }\n    }\n\n    auto future = c10::make_intrusive&lt;c10::ivalue::Future&gt;(\n        c10::ListType::create(c10::ListType::create(c10::TensorType::get())));\n    future-&gt;markCompleted(c10::IValue(outputTensors));\n    return c10::make_intrusive&lt;WorkDummy&gt;(OpType::ALLGATHER, std::move(future));\n}\n\n// This is a dummy allreduce that sets all output tensors to zero\n// Modify the implementation to conduct real communication asynchronously\nc10::intrusive_ptr&lt;Work&gt; ProcessGroupDummy::allreduce(\n        std::vector&lt;at::Tensor&gt;&amp; tensors,\n        const AllreduceOptions&amp; opts) {\n    for (auto&amp; tensor : tensors) {\n        tensor.zero_();\n    }\n\n    auto future = c10::make_intrusive&lt;c10::ivalue::Future&gt;(\n        c10::ListType::create(c10::TensorType::get()));\n    future-&gt;markCompleted(c10::IValue(tensors));\n    return c10::make_intrusive&lt;WorkDummy&gt;(OpType::ALLGATHER, std::move(future));\n}\n} // namespace c10d",
                        "code"
                    ]
                ]
            },
            {
                "Step 2: Expose The Extension Python APIs": [
                    [
                        "The backend constructors are called\n,\nso the extension also needs to expose the constructor APIs to Python. This can\nbe done by adding the following methods. In this example, store and\ntimeout are ignored by the ProcessGroupDummy instantiation method, as\nthose are not used in this dummy implementation. However, real-world extensions\nshould consider using the store to perform rendezvous and supporting the\ntimeout argument.",
                        "markdown"
                    ],
                    [
                        "class ProcessGroupDummy : public ProcessGroup {\n    static c10::intrusive_ptr&lt;ProcessGroup&gt; createProcessGroupDummy(\n        const c10::intrusive_ptr&lt;::c10d::Store&gt;&amp; store,\n        int rank,\n        int size,\n        const std::chrono::duration&lt;float&gt;&amp; timeout);\n\n    static void ProcessGroupDummyConstructor() __attribute__((constructor)) {\n        py::object module = py::module::import(\"torch.distributed\");\n        py::object register_backend =\n            module.attr(\"Backend\").attr(\"register_backend\");\n        // torch.distributed.Backend.register_backend will add `dummy` as a\n        // new valid backend.\n        register_backend(\"dummy\", py::cpp_function(createProcessGroupDummy));\n    }\n}",
                        "code"
                    ],
                    [
                        "c10::intrusive_ptr&lt;ProcessGroup&gt; ProcessGroupDummy::createProcessGroupDummy(\n        const c10::intrusive_ptr&lt;::c10d::Store&gt;&amp; /* unused */,\n        int rank,\n        int size,\n        const std::chrono::duration&lt;float&gt;&amp; /* unused */) {\n    return c10::make_intrusive&lt;ProcessGroupDummy&gt;(rank, size);\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"createProcessGroupDummy\", &amp;ProcessGroupDummy::createProcessGroupDummy);\n}",
                        "code"
                    ]
                ]
            },
            {
                "Step 3: Build The Custom Extension": [
                    [
                        "Now, the extension source code files are ready. We can then use\n\nto build it. To do that, create a setup.py file that prepares the paths and\ncommands. Then call python setup.py install to install the extension.",
                        "markdown"
                    ],
                    [
                        "If the extension depends on third-party libraries, you can also specify\nlibraries_dirs and libraries to the cpp extension APIs. See the\n\nproject as a real-world example.",
                        "markdown"
                    ],
                    [
                        "# file name: setup.py\nimport os\nimport sys\nimport torch\nfrom setuptools import setup\nfrom torch.utils import cpp_extension\n\nsources = [\"src/dummy.cpp\"]\ninclude_dirs = [f\"{os.path.dirname(os.path.abspath(__file__))}/include/\"]\n\nif torch.cuda.is_available():\n    module = cpp_extension.CUDAExtension(\n        name = \"dummy_collectives\",\n        sources = sources,\n        include_dirs = include_dirs,\n    )\nelse:\n    module = cpp_extension.CppExtension(\n        name = \"dummy_collectives\",\n        sources = sources,\n        include_dirs = include_dirs,\n    )\n\nsetup(\n    name = \"Dummy-Collectives\",\n    version = \"0.0.1\",\n    ext_modules = [module],\n    cmdclass={'build_ext': cpp_extension.BuildExtension}\n)",
                        "code"
                    ]
                ]
            },
            {
                "Step 4: Use The Extension in Application": [
                    [
                        "After installation, you can conveniently use the dummy backend when calling\n\nas if it is an builtin backend.",
                        "markdown"
                    ],
                    [
                        "import os\n\nimport torch\n# importing dummy_collectives makes torch.distributed recognize `dummy`\n# as a valid backend.\nimport dummy_collectives\n\nimport torch.distributed as dist\n\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '29500'\n\ndist.init_process_group(\"dummy\", rank=0, world_size=1)\n\nx = torch.ones(6)\ndist.all_reduce(x)\nprint(f\"cpu allreduce: {x}\")\nif torch.cuda.is_available():\n    y = x.cuda()\n    dist.all_reduce(y)\n    print(f\"cuda allreduce: {y}\")\n\ntry:\n    dist.broadcast(x, 0)\nexcept RuntimeError:\n    print(\"got RuntimeError as broadcast is not implemented in Dummy ProcessGroup\")",
                        "code"
                    ]
                ]
            }
        ],
        "Getting Started with Distributed RPC Framework": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                " View and edit this tutorial in .",
                "markdown"
            ],
            [
                "Prerequisites:",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "This tutorial uses two simple examples to demonstrate how to build distributed\ntraining with the \npackage which was first introduced as an experimental feature in PyTorch v1.4.\nSource code of the two examples can be found in\n.",
                "markdown"
            ],
            [
                "Previous tutorials,\n\nand ,\ndescribed \nwhich supports a specific training paradigm where the model is replicated across\nmultiple processes and each process handles a split of the input data.\nSometimes, you might run into scenarios that require different training\nparadigms. For example:",
                "markdown"
            ],
            [
                "In reinforcement learning, it might be relatively expensive to acquire\ntraining data from environments while the model itself can be quite small. In\nthis case, it might be useful to spawn multiple observers running in parallel\nand share a single agent. In this case, the agent takes care of the training\nlocally, but the application would still need libraries to send and receive\ndata between observers and the trainer.",
                "markdown"
            ],
            [
                "Your model might be too large to fit in GPUs on a single machine, and hence\nwould need a library to help split the model onto multiple machines. Or you\nmight be implementing a \ntraining framework, where model parameters and trainers live on different\nmachines.",
                "markdown"
            ],
            [
                "The  package\ncan help with the above scenarios. In case 1, \nand  allow sending data\nfrom one worker to another while easily referencing remote data objects. In\ncase 2, \nand \nmake executing backward pass and optimizer step as if it is local training. In\nthe next two sections, we will demonstrate APIs of\n using a\nreinforcement learning example and a language model example. Please note, this\ntutorial does not aim at building the most accurate or efficient models to\nsolve given problems, instead, the main goal here is to show how to use the\n package to\nbuild distributed training applications.",
                "markdown"
            ],
            {
                "Distributed Reinforcement Learning using RPC and RRef": [
                    [
                        "This section describes steps to build a toy distributed reinforcement learning\nmodel using RPC to solve CartPole-v1 from .\nThe policy code is mostly borrowed from the existing single-thread\n\nas shown below. We will skip details of the Policy design, and focus on RPC\nusages.",
                        "markdown"
                    ],
                    [
                        "import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Policy(nn.Module):\n\n    def __init__(self):\n        super(Policy, self).__init__()\n        self.affine1 = nn.Linear(4, 128)\n        self.dropout = nn.Dropout(p=0.6)\n        self.affine2 = nn.Linear(128, 2)\n\n    def forward(self, x):\n        x = self.affine1(x)\n        x = self.dropout(x)\n        x = F.relu(x)\n        action_scores = self.affine2(x)\n        return F.softmax(action_scores, dim=1)",
                        "code"
                    ],
                    [
                        "We are ready to present the observer. In this example, each observer creates its\nown environment, and waits for the agent\u2019s command to run an episode. In each\nepisode, one observer loops at most n_steps iterations, and in each\niteration, it uses RPC to pass its environment state to the agent and gets an\naction back. Then it applies that action to its environment, and gets the reward\nand the next state from the environment. After that, the observer uses another\nRPC to report the reward to the agent. Again, please note that, this is\nobviously not the most efficient observer implementation. For example, one\nsimple optimization could be packing current state and last reward in one RPC to\nreduce the communication overhead. However, the goal is to demonstrate RPC API\ninstead of building the best solver for CartPole. So, let\u2019s keep the logic\nsimple and the two steps explicit in this example.",
                        "markdown"
                    ],
                    [
                        "import argparse\nimport gym\nimport torch.distributed.rpc as rpc\n\nparser = argparse.ArgumentParser(\n    description=\"RPC Reinforcement Learning Example\",\n    formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n)\n\nparser.add_argument('--world_size', default=2, type=int, metavar='W',\n                    help='number of workers')\nparser.add_argument('--log_interval', type=int, default=10, metavar='N',\n                    help='interval between training status logs')\nparser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n                    help='how much to value future rewards')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed  for reproducibility')\nargs = parser.parse_args()\n\nclass Observer:\n\n    def __init__(self):\n        self.id = rpc.get_worker_info().id\n        self.env = gym.make('CartPole-v1')\n        self.env.seed(args.seed)\n\n    def run_episode(self, agent_rref):\n        state, ep_reward = self.env.reset(), 0\n        for _ in range(10000):\n            # send the state to the agent to get an action\n            action = agent_rref.rpc_sync().select_action(self.id, state)\n\n            # apply the action to the environment, and get the reward\n            state, reward, done, _ = self.env.step(action)\n\n            # report the reward to the agent for training purpose\n            agent_rref.rpc_sync().report_reward(self.id, reward)\n\n            # finishes after the number of self.env._max_episode_steps\n            if done:\n                break",
                        "code"
                    ],
                    [
                        "The code for agent is a little more complex, and we will break it into multiple\npieces. In this example, the agent serves as both the trainer and the master,\nsuch that it sends command to multiple distributed observers to run episodes,\nand it also records all actions and rewards locally which will be used during\nthe training phase after each episode. The code below shows Agent\nconstructor where most lines are initializing various components. The loop at\nthe end initializes observers remotely on other workers, and holds RRefs to\nthose observers locally. The agent will use those observer RRefs later to\nsend commands. Applications don\u2019t need to worry about the lifetime of RRefs.\nThe owner of each RRef maintains a reference counting map to track its\nlifetime, and guarantees the remote data object will not be deleted as long as\nthere is any live user of that RRef. Please refer to the RRef\n for details.",
                        "markdown"
                    ],
                    [
                        "import gym\nimport numpy as np\n\nimport torch\nimport torch.distributed.rpc as rpc\nimport torch.optim as optim\nfrom torch.distributed.rpc import RRef, rpc_async, remote\nfrom torch.distributions import Categorical\n\nclass Agent:\n    def __init__(self, world_size):\n        self.ob_rrefs = []\n        self.agent_rref = RRef(self)\n        self.rewards = {}\n        self.saved_log_probs = {}\n        self.policy = Policy()\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n        self.eps = np.finfo(np.float32).eps.item()\n        self.running_reward = 0\n        self.reward_threshold = gym.make('CartPole-v1').spec.reward_threshold\n        for ob_rank in range(1, world_size):\n            ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))\n            self.ob_rrefs.append(remote(ob_info, Observer))\n            self.rewards[ob_info.id] = []\n            self.saved_log_probs[ob_info.id] = []",
                        "code"
                    ],
                    [
                        "Next, the agent exposes two APIs to observers for selecting actions and\nreporting rewards. Those functions only run locally on the agent, but will\nbe triggered by observers through RPC.",
                        "markdown"
                    ],
                    [
                        "class Agent:\n    ...\n    def select_action(self, ob_id, state):\n        state = torch.from_numpy(state).float().unsqueeze(0)\n        probs = self.policy(state)\n        m = Categorical(probs)\n        action = m.sample()\n        self.saved_log_probs[ob_id].append(m.log_prob(action))\n        return action.item()\n\n    def report_reward(self, ob_id, reward):\n        self.rewards[ob_id].append(reward)",
                        "code"
                    ],
                    [
                        "Let\u2019s add a run_episode function on agent which tells all observers\nto execute an episode. In this function, it first creates a list to collect\nfutures from asynchronous RPCs, and then loop over all observer RRefs to\nmake asynchronous RPCs. In these RPCs, the agent also passes an RRef of\nitself to the observer, so that the observer can call functions on the agent as\nwell. As shown above, each observer will make RPCs back to the agent, which are\nnested RPCs. After each episode, the saved_log_probs and rewards will\ncontain the recorded action probs and rewards.",
                        "markdown"
                    ],
                    [
                        "class Agent:\n    ...\n    def run_episode(self):\n        futs = []\n        for ob_rref in self.ob_rrefs:\n            # make async RPC to kick off an episode on all observers\n            futs.append(\n                rpc_async(\n                    ob_rref.owner(),\n                    ob_rref.rpc_sync().run_episode,\n                    args=(self.agent_rref,)\n                )\n            )\n\n        # wait until all obervers have finished this episode\n        for fut in futs:\n            fut.wait()",
                        "code"
                    ],
                    [
                        "Finally, after one episode, the agent needs to train the model, which\nis implemented in the finish_episode function below. There is no RPCs in\nthis function and it is mostly borrowed from the single-thread\n.\nHence, we skip describing its contents.",
                        "markdown"
                    ],
                    [
                        "class Agent:\n    ...\n    def finish_episode(self):\n      # joins probs and rewards from different observers into lists\n      R, probs, rewards = 0, [], []\n      for ob_id in self.rewards:\n          probs.extend(self.saved_log_probs[ob_id])\n          rewards.extend(self.rewards[ob_id])\n\n      # use the minimum observer reward to calculate the running reward\n      min_reward = min([sum(self.rewards[ob_id]) for ob_id in self.rewards])\n      self.running_reward = 0.05 * min_reward + (1 - 0.05) * self.running_reward\n\n      # clear saved probs and rewards\n      for ob_id in self.rewards:\n          self.rewards[ob_id] = []\n          self.saved_log_probs[ob_id] = []\n\n      policy_loss, returns = [], []\n      for r in rewards[::-1]:\n          R = r + args.gamma * R\n          returns.insert(0, R)\n      returns = torch.tensor(returns)\n      returns = (returns - returns.mean()) / (returns.std() + self.eps)\n      for log_prob, R in zip(probs, returns):\n          policy_loss.append(-log_prob * R)\n      self.optimizer.zero_grad()\n      policy_loss = torch.cat(policy_loss).sum()\n      policy_loss.backward()\n      self.optimizer.step()\n      return min_reward",
                        "code"
                    ],
                    [
                        "With Policy, Observer, and Agent classes, we are ready to launch\nmultiple processes to perform the distributed training. In this example, all\nprocesses run the same run_worker function, and they use the rank to\ndistinguish their role. Rank 0 is always the agent, and all other ranks are\nobservers. The agent serves as master by repeatedly calling run_episode and\nfinish_episode until the running reward surpasses the reward threshold\nspecified by the environment. All observers passively waiting for commands\nfrom the agent. The code is wrapped by\n and\n,\nwhich initializes and terminates RPC instances respectively. More details are\navailable in the .",
                        "markdown"
                    ],
                    [
                        "import os\nfrom itertools import count\n\nimport torch.multiprocessing as mp\n\nAGENT_NAME = \"agent\"\nOBSERVER_NAME=\"obs{}\"\n\ndef run_worker(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    if rank == 0:\n        # rank0 is the agent\n        rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)\n\n        agent = Agent(world_size)\n        print(f\"This will run until reward threshold of {agent.reward_threshold}\"\n                \" is reached. Ctrl+C to exit.\")\n        for i_episode in count(1):\n            agent.run_episode()\n            last_reward = agent.finish_episode()\n\n            if i_episode % args.log_interval == 0:\n                print(f\"Episode {i_episode}\\tLast reward: {last_reward:.2f}\\tAverage reward: \"\n                    f\"{agent.running_reward:.2f}\")\n            if agent.running_reward &gt; agent.reward_threshold:\n                print(f\"Solved! Running reward is now {agent.running_reward}!\")\n                break\n    else:\n        # other ranks are the observer\n        rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)\n        # observers passively waiting for instructions from the agent\n\n    # block until all rpcs finish, and shutdown the RPC instance\n    rpc.shutdown()\n\n\nmp.spawn(\n    run_worker,\n    args=(args.world_size, ),\n    nprocs=args.world_size,\n    join=True\n)",
                        "code"
                    ],
                    [
                        "Below are some sample outputs when training with <cite>world_size=2</cite>.",
                        "markdown"
                    ],
                    [
                        "This will run until reward threshold of 475.0 is reached. Ctrl+C to exit.\nEpisode 10      Last reward: 26.00      Average reward: 10.01\nEpisode 20      Last reward: 16.00      Average reward: 11.27\nEpisode 30      Last reward: 49.00      Average reward: 18.62\nEpisode 40      Last reward: 45.00      Average reward: 26.09\nEpisode 50      Last reward: 44.00      Average reward: 30.03\nEpisode 60      Last reward: 111.00     Average reward: 42.23\nEpisode 70      Last reward: 131.00     Average reward: 70.11\nEpisode 80      Last reward: 87.00      Average reward: 76.51\nEpisode 90      Last reward: 86.00      Average reward: 95.93\nEpisode 100     Last reward: 13.00      Average reward: 123.93\nEpisode 110     Last reward: 33.00      Average reward: 91.39\nEpisode 120     Last reward: 73.00      Average reward: 76.38\nEpisode 130     Last reward: 137.00     Average reward: 88.08\nEpisode 140     Last reward: 89.00      Average reward: 104.96\nEpisode 150     Last reward: 97.00      Average reward: 98.74\nEpisode 160     Last reward: 150.00     Average reward: 100.87\nEpisode 170     Last reward: 126.00     Average reward: 104.38\nEpisode 180     Last reward: 500.00     Average reward: 213.74\nEpisode 190     Last reward: 322.00     Average reward: 300.22\nEpisode 200     Last reward: 165.00     Average reward: 272.71\nEpisode 210     Last reward: 168.00     Average reward: 233.11\nEpisode 220     Last reward: 184.00     Average reward: 195.02\nEpisode 230     Last reward: 284.00     Average reward: 208.32\nEpisode 240     Last reward: 395.00     Average reward: 247.37\nEpisode 250     Last reward: 500.00     Average reward: 335.42\nEpisode 260     Last reward: 500.00     Average reward: 386.30\nEpisode 270     Last reward: 500.00     Average reward: 405.29\nEpisode 280     Last reward: 500.00     Average reward: 443.29\nEpisode 290     Last reward: 500.00     Average reward: 464.65\nSolved! Running reward is now 475.3163778435275!",
                        "code"
                    ],
                    [
                        "In this example, we show how to use RPC as the communication vehicle to pass\ndata across workers, and how to use RRef to reference remote objects. It is true\nthat you could build the entire structure directly on top of ProcessGroup\nsend and recv APIs or use other communication/RPC libraries. However,\nby using <cite>torch.distributed.rpc</cite>, you can get the native support and\ncontinuously optimized performance under the hood.",
                        "markdown"
                    ],
                    [
                        "Next, we will show how to combine RPC and RRef with distributed autograd and\ndistributed optimizer to perform distributed model parallel training.",
                        "markdown"
                    ]
                ]
            },
            {
                "Distributed RNN using Distributed Autograd and Distributed Optimizer": [
                    [
                        "In this section, we use an RNN model to show how to build distributed model\nparallel training with the RPC API. The example RNN model is very small and\ncan easily fit into a single GPU, but we still divide its layers onto two\ndifferent workers to demonstrate the idea. Developer can apply the similar\ntechniques to distribute much larger models across multiple devices and\nmachines.",
                        "markdown"
                    ],
                    [
                        "The RNN model design is borrowed from the word language model in PyTorch\n\nrepository, which contains three main components, an embedding table, an\nLSTM layer, and a decoder. The code below wraps the embedding table and the\ndecoder into sub-modules, so that their constructors can be passed to the RPC\nAPI. In the EmbeddingTable sub-module, we intentionally put the\nEmbedding layer on GPU to cover the use case. In v1.4, RPC always creates\nCPU tensor arguments or return values on the destination worker. If the function\ntakes a GPU tensor, you need to move it to the proper device explicitly.",
                        "markdown"
                    ],
                    [
                        "class EmbeddingTable(nn.Module):\n    r\"\"\"\n    Encoding layers of the RNNModel\n    \"\"\"\n    def __init__(self, ntoken, ninp, dropout):\n        super(EmbeddingTable, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp).cuda()\n        self.encoder.weight.data.uniform_(-0.1, 0.1)\n\n    def forward(self, input):\n        return self.drop(self.encoder(input.cuda()).cpu()\n\n\nclass Decoder(nn.Module):\n    def __init__(self, ntoken, nhid, dropout):\n        super(Decoder, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.decoder = nn.Linear(nhid, ntoken)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-0.1, 0.1)\n\n    def forward(self, output):\n        return self.decoder(self.drop(output))",
                        "code"
                    ],
                    [
                        "With the above sub-modules, we can now piece them together using RPC to\ncreate an RNN model. In the code below ps represents a parameter server,\nwhich hosts parameters of the embedding table and the decoder. The constructor\nuses the \nAPI to create an EmbeddingTable object and a Decoder object on the\nparameter server, and locally creates the LSTM sub-module. During the\nforward pass, the trainer uses the EmbeddingTable RRef to find the\nremote sub-module and passes the input data to the EmbeddingTable using RPC\nand fetches the lookup results. Then, it runs the embedding through the local\nLSTM layer, and finally uses another RPC to send the output to the\nDecoder sub-module. In general, to implement distributed model parallel\ntraining, developers can divide the model into sub-modules, invoke RPC to create\nsub-module instances remotely, and use on RRef to find them when necessary.\nAs you can see in the code below, it looks very similar to single-machine model\nparallel training. The main difference is replacing Tensor.to(device) with\nRPC functions.",
                        "markdown"
                    ],
                    [
                        "class RNNModel(nn.Module):\n    def __init__(self, ps, ntoken, ninp, nhid, nlayers, dropout=0.5):\n        super(RNNModel, self).__init__()\n\n        # setup embedding table remotely\n        self.emb_table_rref = rpc.remote(ps, EmbeddingTable, args=(ntoken, ninp, dropout))\n        # setup LSTM locally\n        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n        # setup decoder remotely\n        self.decoder_rref = rpc.remote(ps, Decoder, args=(ntoken, nhid, dropout))\n\n    def forward(self, input, hidden):\n        # pass input to the remote embedding table and fetch emb tensor back\n        emb = _remote_method(EmbeddingTable.forward, self.emb_table_rref, input)\n        output, hidden = self.rnn(emb, hidden)\n        # pass output to the rremote decoder and get the decoded output back\n        decoded = _remote_method(Decoder.forward, self.decoder_rref, output)\n        return decoded, hidden",
                        "code"
                    ],
                    [
                        "Before introducing the distributed optimizer, let\u2019s add a helper function to\ngenerate a list of RRefs of model parameters, which will be consumed by the\ndistributed optimizer. In local training, applications could call\nModule.parameters() to grab references to all parameter tensors, and pass it\nto the local optimizer for subsequent updates. However, the same API does not\nwork in distributed training scenarios as some parameters live on remote\nmachines. Therefore, instead of taking a list of parameter Tensors, the\ndistributed optimizer takes a list of RRefs, one RRef per model\nparameter for both local and remote model parameters. The helper function is\npretty simple, just call Module.parameters() and creates a local RRef on\neach of the parameters.",
                        "markdown"
                    ],
                    [
                        "def _parameter_rrefs(module):\n    param_rrefs = []\n    for param in module.parameters():\n        param_rrefs.append(RRef(param))\n    return param_rrefs",
                        "code"
                    ],
                    [
                        "Then, as the RNNModel contains three sub-modules, we need to call\n_parameter_rrefs three times, and wrap that into another helper function.",
                        "markdown"
                    ],
                    [
                        "class RNNModel(nn.Module):\n    ...\n    def parameter_rrefs(self):\n        remote_params = []\n        # get RRefs of embedding table\n        remote_params.extend(_remote_method(_parameter_rrefs, self.emb_table_rref))\n        # create RRefs for local parameters\n        remote_params.extend(_parameter_rrefs(self.rnn))\n        # get RRefs of decoder\n        remote_params.extend(_remote_method(_parameter_rrefs, self.decoder_rref))\n        return remote_params",
                        "code"
                    ],
                    [
                        "Now, we are ready to implement the training loop. After initializing model\narguments, we create the RNNModel and the DistributedOptimizer. The\ndistributed optimizer will take a list of parameter RRefs, find all distinct\nowner workers, and create the given local optimizer (i.e., SGD in this case,\nyou can use other local optimizers as well) on each of the owner worker using\nthe given arguments (i.e., lr=0.05).",
                        "markdown"
                    ],
                    [
                        "In the training loop, it first creates a distributed autograd context, which\nwill help the distributed autograd engine to find gradients and involved RPC\nsend/recv functions. The design details of the distributed autograd engine can\nbe found in its .\nThen, it kicks off the forward pass as if it is a local\nmodel, and run the distributed backward pass. For the distributed backward, you\nonly need to specify a list of roots, in this case, it is the loss Tensor.\nThe distributed autograd engine will traverse the distributed graph\nautomatically and write gradients properly. Next, it runs the step\nfunction on the distributed optimizer, which will reach out to all involved\nlocal optimizers to update model parameters. Compared to local training, one\nminor difference is that you don\u2019t need to run zero_grad() because each\nautograd context has dedicated space to store gradients, and as we create a\ncontext per iteration, those gradients from different iterations will not\naccumulate to the same set of Tensors.",
                        "markdown"
                    ],
                    [
                        "def run_trainer():\n    batch = 5\n    ntoken = 10\n    ninp = 2\n\n    nhid = 3\n    nindices = 3\n    nlayers = 4\n    hidden = (\n        torch.randn(nlayers, nindices, nhid),\n        torch.randn(nlayers, nindices, nhid)\n    )\n\n    model = rnn.RNNModel('ps', ntoken, ninp, nhid, nlayers)\n\n    # setup distributed optimizer\n    opt = DistributedOptimizer(\n        optim.SGD,\n        model.parameter_rrefs(),\n        lr=0.05,\n    )\n\n    criterion = torch.nn.CrossEntropyLoss()\n\n    def get_next_batch():\n        for _ in range(5):\n            data = torch.LongTensor(batch, nindices) % ntoken\n            target = torch.LongTensor(batch, ntoken) % nindices\n            yield data, target\n\n    # train for 10 iterations\n    for epoch in range(10):\n        for data, target in get_next_batch():\n            # create distributed autograd context\n            with dist_autograd.context() as context_id:\n                hidden[0].detach_()\n                hidden[1].detach_()\n                output, hidden = model(data, hidden)\n                loss = criterion(output, target)\n                # run distributed backward pass\n                dist_autograd.backward(context_id, [loss])\n                # run distributed optimizer\n                opt.step(context_id)\n                # not necessary to zero grads since they are\n                # accumulated into the distributed autograd context\n                # which is reset every iteration.\n        print(\"Training epoch {}\".format(epoch))",
                        "code"
                    ],
                    [
                        "Finally, let\u2019s add some glue code to launch the parameter server and the trainer\nprocesses.",
                        "markdown"
                    ],
                    [
                        "def run_worker(rank, world_size):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    if rank == 1:\n        rpc.init_rpc(\"trainer\", rank=rank, world_size=world_size)\n        _run_trainer()\n    else:\n        rpc.init_rpc(\"ps\", rank=rank, world_size=world_size)\n        # parameter server do nothing\n        pass\n\n    # block until all rpcs finish\n    rpc.shutdown()\n\n\nif __name__==\"__main__\":\n    world_size = 2\n    mp.spawn(run_worker, args=(world_size, ), nprocs=world_size, join=True)",
                        "code"
                    ]
                ]
            }
        ],
        "Implementing a Parameter Server Using Distributed RPC Framework": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                " View and edit this tutorial in .",
                "markdown"
            ],
            [
                "Prerequisites:",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "This tutorial walks through a simple example of implementing a parameter server using PyTorch\u2019s . The parameter server framework is a paradigm in which a set of servers store parameters, such as large embedding tables, and several trainers query the parameter servers in order to retrieve the most up to date parameters. These trainers can run a training loop locally and occasionally synchronize with the parameter server to get the latest parameters. For more reading on the parameter server approach, check out .",
                "markdown"
            ],
            [
                "Using the Distributed RPC Framework, we\u2019ll build an example where multiple trainers use RPC to communicate with the same parameter server and use  to access states on the remote parameter server instance. Each trainer will launch its dedicated backward pass in a distributed fashion through stitching of the autograd graph across multiple nodes using distributed autograd.",
                "markdown"
            ],
            [
                "<strong>Note</strong>: This tutorial covers the use of the Distributed RPC Framework, which is useful for splitting a model onto multiple machines, or for implementing a parameter-server training strategy where network trainers fetch parameters hosted on a different machine. If instead you are looking for replicating your model across many GPUs, please see the . There is also another  that covers reinforcement learning and RNN use cases.",
                "markdown"
            ],
            [
                "Let\u2019s start with the familiar: importing our required modules and defining a simple ConvNet that will train on the MNIST dataset. The below network is largely adopted from the network defined in the .",
                "markdown"
            ],
            [
                "import argparse\nimport os\nimport time\nfrom threading import Lock\n\nimport torch\nimport torch.distributed.autograd as dist_autograd\nimport torch.distributed.rpc as rpc\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.distributed.optim import DistributedOptimizer\nfrom torchvision import datasets, transforms\n\n# --------- MNIST Network to train, from pytorch/examples -----\n\nclass Net(nn.Module):\n    def __init__(self, num_gpus=0):\n        super(Net, self).__init__()\n        print(f\"Using {num_gpus} GPUs to train\")\n        self.num_gpus = num_gpus\n        device = torch.device(\n            \"cuda:0\" if torch.cuda.is_available() and self.num_gpus &gt; 0 else \"cpu\")\n        print(f\"Putting first 2 convs on {str(device)}\")\n        # Put conv layers on the first cuda device, or CPU if no cuda device\n        self.conv1 = nn.Conv2d(1, 32, 3, 1).to(device)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1).to(device)\n        # Put rest of the network on the 2nd cuda device, if there is one\n        if \"cuda\" in str(device) and num_gpus &gt; 1:\n            device = torch.device(\"cuda:1\")\n\n        print(f\"Putting rest of layers on {str(device)}\")\n        self.dropout1 = nn.Dropout2d(0.25).to(device)\n        self.dropout2 = nn.Dropout2d(0.5).to(device)\n        self.fc1 = nn.Linear(9216, 128).to(device)\n        self.fc2 = nn.Linear(128, 10).to(device)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        # Move tensor to next device if necessary\n        next_device = next(self.fc1.parameters()).device\n        x = x.to(next_device)\n\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output",
                "code"
            ],
            [
                "Next, let\u2019s define some helper functions that will be useful for the rest of our script. The following uses  and  in order to define a function that invokes a given method on an object living on a remote node. Below, our handle to the remote object is given by the rref argument, and we run it on its owning node: rref.owner(). On the caller node, we run this command synchronously through the use of rpc_sync, meaning that we will block until a response is received.",
                "markdown"
            ],
            [
                "# --------- Helper Methods --------------------\n\n# On the local node, call a method with first arg as the value held by the\n# RRef. Other args are passed in as arguments to the function called.\n# Useful for calling instance methods. method could be any matching function, including\n# class methods.\ndef call_method(method, rref, *args, **kwargs):\n    return method(rref.local_value(), *args, **kwargs)\n\n# Given an RRef, return the result of calling the passed in method on the value\n# held by the RRef. This call is done on the remote node that owns\n# the RRef and passes along the given argument.\n# Example: If the value held by the RRef is of type Foo, then\n# remote_method(Foo.bar, rref, arg1, arg2) is equivalent to calling\n# &lt;foo_instance&gt;.bar(arg1, arg2) on the remote node and getting the result\n# back.\n\ndef remote_method(method, rref, *args, **kwargs):\n    args = [method, rref] + list(args)\n    return rpc.rpc_sync(rref.owner(), call_method, args=args, kwargs=kwargs)",
                "code"
            ],
            [
                "Now, we\u2019re ready to define our parameter server. We will subclass nn.Module and save a handle to our network defined above. We\u2019ll also save an input device which will be the device our input is transferred to before invoking the model.",
                "markdown"
            ],
            [
                "# --------- Parameter Server --------------------\nclass ParameterServer(nn.Module):\n    def __init__(self, num_gpus=0):\n        super().__init__()\n        model = Net(num_gpus=num_gpus)\n        self.model = model\n        self.input_device = torch.device(\n            \"cuda:0\" if torch.cuda.is_available() and num_gpus &gt; 0 else \"cpu\")",
                "code"
            ],
            [
                "Next, we\u2019ll define our forward pass. Note that regardless of the device of the model output, we move the output to CPU, as the Distributed RPC Framework currently only supports sending CPU tensors over RPC. We have intentionally disabled sending CUDA tensors over RPC due to the potential for different devices (CPU/GPU) on on the caller/callee, but may support this in future releases.",
                "markdown"
            ],
            [
                "class ParameterServer(nn.Module):\n...\n    def forward(self, inp):\n        inp = inp.to(self.input_device)\n        out = self.model(inp)\n        # This output is forwarded over RPC, which as of 1.5.0 only accepts CPU tensors.\n        # Tensors must be moved in and out of GPU memory due to this.\n        out = out.to(\"cpu\")\n        return out",
                "code"
            ],
            [
                "Next, we\u2019ll define a few miscellaneous functions useful for training and verification purposes. The first, get_dist_gradients, will take in a Distributed Autograd context ID and call into the dist_autograd.get_gradients API in order to retrieve gradients computed by distributed autograd. More information can be found in the . Note that we also iterate through the resulting dictionary and convert each tensor to a CPU tensor, as the framework currently only supports sending tensors over RPC. Next, get_param_rrefs will iterate through our model parameters and wrap them as a (local) . This method will be invoked over RPC by trainer nodes and will return a list of the parameters to be optimized. This is required as input to the , which requires all parameters it must optimize as a list of RRefs.",
                "markdown"
            ],
            [
                "# Use dist autograd to retrieve gradients accumulated for this model.\n# Primarily used for verification.\ndef get_dist_gradients(self, cid):\n    grads = dist_autograd.get_gradients(cid)\n    # This output is forwarded over RPC, which as of 1.5.0 only accepts CPU tensors.\n    # Tensors must be moved in and out of GPU memory due to this.\n    cpu_grads = {}\n    for k, v in grads.items():\n        k_cpu, v_cpu = k.to(\"cpu\"), v.to(\"cpu\")\n        cpu_grads[k_cpu] = v_cpu\n    return cpu_grads\n\n# Wrap local parameters in a RRef. Needed for building the\n# DistributedOptimizer which optimizes paramters remotely.\ndef get_param_rrefs(self):\n    param_rrefs = [rpc.RRef(param) for param in self.model.parameters()]\n    return param_rrefs",
                "code"
            ],
            [
                "Finally, we\u2019ll create methods to initialize our parameter server. Note that there will only be one instance of a parameter server across all processes, and all trainers will talk to the same parameter server and update the same stored model. As seen in run_parameter_server, the server itself does not take any independent actions; it waits for requests from trainers (which are yet to be defined) and responds to them by running the requested function.",
                "markdown"
            ],
            [
                "# The global parameter server instance.\nparam_server = None\n# A lock to ensure we only have one parameter server.\nglobal_lock = Lock()\n\n\ndef get_parameter_server(num_gpus=0):\n    \"\"\"\n    Returns a singleton parameter server to all trainer processes\n    \"\"\"\n    global param_server\n    # Ensure that we get only one handle to the ParameterServer.\n    with global_lock:\n        if not param_server:\n            # construct it once\n            param_server = ParameterServer(num_gpus=num_gpus)\n        return param_server\n\ndef run_parameter_server(rank, world_size):\n    # The parameter server just acts as a host for the model and responds to\n    # requests from trainers.\n    # rpc.shutdown() will wait for all workers to complete by default, which\n    # in this case means that the parameter server will wait for all trainers\n    # to complete, and then exit.\n    print(\"PS master initializing RPC\")\n    rpc.init_rpc(name=\"parameter_server\", rank=rank, world_size=world_size)\n    print(\"RPC initialized! Running parameter server...\")\n    rpc.shutdown()\n    print(\"RPC shutdown on parameter server.\")",
                "code"
            ],
            [
                "Note that above, rpc.shutdown() will not immediately shut down the Parameter Server. Instead, it will wait for all workers (trainers in this case) to also call into rpc.shutdown(). This gives us the guarantee that the parameter server will not go offline before all trainers (yet to be define) have completed their training process.",
                "markdown"
            ],
            [
                "Next, we\u2019ll define our TrainerNet class. This will also be a subclass of nn.Module, and our __init__ method will use the rpc.remote API to obtain an RRef, or Remote Reference, to our parameter server. Note that here we are not copying the parameter server to our local process, instead, we can think of self.param_server_rref as a distributed shared pointer to the parameter server that lives on a separate process.",
                "markdown"
            ],
            [
                "# --------- Trainers --------------------\n\n# nn.Module corresponding to the network trained by this trainer. The\n# forward() method simply invokes the network on the given parameter\n# server.\nclass TrainerNet(nn.Module):\n    def __init__(self, num_gpus=0):\n        super().__init__()\n        self.num_gpus = num_gpus\n        self.param_server_rref = rpc.remote(\n            \"parameter_server\", get_parameter_server, args=(num_gpus,))",
                "code"
            ],
            [
                "Next, we\u2019ll define a method called get_global_param_rrefs. To motivate the need for this method, it is worth it to read through the documentation on , specifically the API signature.  The optimizer must be passed a list of RRefs corresponding to the remote parameters to be optimized, so here we obtain the necessary RRefs. Since the only remote worker that a given TrainerNet interacts with is the ParameterServer, we simply invoke a remote_method on the ParameterServer. We use the get_param_rrefs method which we defined in the ParameterServer class. This method will return a list of RRefs to the parameters that need to be optimized. Note that in this case our TrainerNet does not define its own paramaters; if it did, we would need to wrap each parameter in an RRef as well and include it into our input to DistributedOptimizer.",
                "markdown"
            ],
            [
                "class TrainerNet(nn.Module):\n...\n    def get_global_param_rrefs(self):\n        remote_params = remote_method(\n            ParameterServer.get_param_rrefs,\n            self.param_server_rref)\n        return remote_params",
                "code"
            ],
            [
                "Now, we\u2019re ready to define our forward method, which will invoke (synchronous) RPC to run the forward pass of the network defined on the ParameterServer. Note that we pass in self.param_server_rref, which is a remote handle to our ParameterServer, to our RPC call. This call will send an RPC to the node on which our ParameterServer is running, invoke the forward pass, and return the Tensor corresponding to the model\u2019s output.",
                "markdown"
            ],
            [
                "class TrainerNet(nn.Module):\n...\n    def forward(self, x):\n        model_output = remote_method(\n            ParameterServer.forward, self.param_server_rref, x)\n        return model_output",
                "code"
            ],
            [
                "With our trainer fully defined, it\u2019s now time to write our neural network training loop that will create our network and optimizer, run some inputs through the network and compute the loss. The training loop looks a lot like that of a local training program, with some modifications due to the nature of our network being distributed across machines.",
                "markdown"
            ],
            [
                "Below, we initialize our TrainerNet and build a DistributedOptimizer. Note that as mentioned above, we must pass in all of the global (across all nodes participating in distributed training) parameters that we want to be optimized. In addition, we pass in the local optimizer to be used, in this case, SGD. Note that we can configure the underlying optimizer algorithm in the same way as creating a local optimizer - all arguments for optimizer.SGD will be forwarded properly. As an example, we pass in a custom learning rate that will be used as the learning rate for all local optimizers.",
                "markdown"
            ],
            [
                "def run_training_loop(rank, num_gpus, train_loader, test_loader):\n    # Runs the typical nueral network forward + backward + optimizer step, but\n    # in a distributed fashion.\n    net = TrainerNet(num_gpus=num_gpus)\n    # Build DistributedOptimizer.\n    param_rrefs = net.get_global_param_rrefs()\n    opt = DistributedOptimizer(optim.SGD, param_rrefs, lr=0.03)",
                "code"
            ],
            [
                "Next, we define our main training loop. We loop through iterables given by PyTorch\u2019s . Before writing our typical forward/backward/optimizer loop, we first wrap the logic within a . Note that this is needed to record RPCs invoked in the model\u2019s forward pass, so that an appropriate graph can be constructed which includes all participating distributed workers in the backward pass. The distributed autograd context returns a context_id which serves as an identifier for accumulating and optimizing gradients corresponding to a particular iteration.",
                "markdown"
            ],
            [
                "As opposed to calling the typical loss.backward() which would kick off the backward pass on this local worker, we call dist_autograd.backward() and pass in our context_id as well as loss, which is the root at which we want the backward pass to begin. In addition, we pass this context_id into our optimizer call, which is required to be able to look up the corresponding gradients computed by this particular backwards pass across all nodes.",
                "markdown"
            ],
            [
                "def run_training_loop(rank, num_gpus, train_loader, test_loader):\n...\n    for i, (data, target) in enumerate(train_loader):\n        with dist_autograd.context() as cid:\n            model_output = net(data)\n            target = target.to(model_output.device)\n            loss = F.nll_loss(model_output, target)\n            if i % 5 == 0:\n                print(f\"Rank {rank} training batch {i} loss {loss.item()}\")\n            dist_autograd.backward(cid, [loss])\n            # Ensure that dist autograd ran successfully and gradients were\n            # returned.\n            assert remote_method(\n                ParameterServer.get_dist_gradients,\n                net.param_server_rref,\n                cid) != {}\n            opt.step(cid)\n\n     print(\"Training complete!\")\n     print(\"Getting accuracy....\")\n     get_accuracy(test_loader, net)",
                "code"
            ],
            [
                "The following simply computes the accuracy of our model after we\u2019re done training, much like a traditional local model. However, note that the net we pass into this function above is an instance of TrainerNet and therefore the forward pass invokes RPC in a transparent fashion.",
                "markdown"
            ],
            [
                "def get_accuracy(test_loader, model):\n    model.eval()\n    correct_sum = 0\n    # Use GPU to evaluate if possible\n    device = torch.device(\"cuda:0\" if model.num_gpus &gt; 0\n        and torch.cuda.is_available() else \"cpu\")\n    with torch.no_grad():\n        for i, (data, target) in enumerate(test_loader):\n            out = model(data, -1)\n            pred = out.argmax(dim=1, keepdim=True)\n            pred, target = pred.to(device), target.to(device)\n            correct = pred.eq(target.view_as(pred)).sum().item()\n            correct_sum += correct\n\n    print(f\"Accuracy {correct_sum / len(test_loader.dataset)}\")",
                "code"
            ],
            [
                "Next, similar to how we defined run_parameter_server as the main loop for our ParameterServer that is responsible for initializing RPC, let\u2019s define a similar loop for our trainers. The difference will be that our trainers must run the training loop we defined above:",
                "markdown"
            ],
            [
                "# Main loop for trainers.\ndef run_worker(rank, world_size, num_gpus, train_loader, test_loader):\n    print(f\"Worker rank {rank} initializing RPC\")\n    rpc.init_rpc(\n        name=f\"trainer_{rank}\",\n        rank=rank,\n        world_size=world_size)\n\n    print(f\"Worker {rank} done initializing RPC\")\n\n    run_training_loop(rank, num_gpus, train_loader, test_loader)\n    rpc.shutdown()",
                "code"
            ],
            [
                "Note that similar to run_parameter_server, rpc.shutdown() will by default wait for all workers, both trainers and ParameterServers, to call into rpc.shutdown() before this node exits. This ensures that nodes are terminated gracefully and no node goes offline while another is expecting it to be online.",
                "markdown"
            ],
            [
                "We\u2019ve now completed our trainer and parameter server specific code, and all that\u2019s left is to add code to launch trainers and parameter servers. First, we must take in various arguments that apply to our parameter server and trainers. world_size corresponds to the total number of nodes that will participate in training, and is the sum of all trainers and the parameter server. We also must pass in a unique rank for each individual process, from 0 (where we will run our single parameter server) to world_size - 1. master_addr and master_port are arguments that can be used to identify where the rank 0 process is running, and will be used by individual nodes to discover each other. To test this example out locally, simply pass in localhost and the same master_port to all instances spawned. Note that for demonstration purposes, this example supports only between 0-2 GPUs, although the pattern can be extended to make use of additional GPUs.",
                "markdown"
            ],
            [
                "if __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=\"Parameter-Server RPC based training\")\n    parser.add_argument(\n        \"--world_size\",\n        type=int,\n        default=4,\n        help=\"\"\"Total number of participating processes. Should be the sum of\n        master node and all training nodes.\"\"\")\n    parser.add_argument(\n        \"rank\",\n        type=int,\n        default=None,\n        help=\"Global rank of this process. Pass in 0 for master.\")\n    parser.add_argument(\n        \"num_gpus\",\n        type=int,\n        default=0,\n        help=\"\"\"Number of GPUs to use for training, Currently supports between 0\n         and 2 GPUs. Note that this argument will be passed to the parameter servers.\"\"\")\n    parser.add_argument(\n        \"--master_addr\",\n        type=str,\n        default=\"localhost\",\n        help=\"\"\"Address of master, will default to localhost if not provided.\n        Master must be able to accept network traffic on the address + port.\"\"\")\n    parser.add_argument(\n        \"--master_port\",\n        type=str,\n        default=\"29500\",\n        help=\"\"\"Port that master is listening on, will default to 29500 if not\n        provided. Master must be able to accept network traffic on the host and port.\"\"\")\n\n    args = parser.parse_args()\n    assert args.rank is not None, \"must provide rank argument.\"\n    assert args.num_gpus &lt;= 3, f\"Only 0-2 GPUs currently supported (got {args.num_gpus}).\"\n    os.environ['MASTER_ADDR'] = args.master_addr\n    os.environ[\"MASTER_PORT\"] = args.master_port",
                "code"
            ],
            [
                "Now, we\u2019ll create a process corresponding to either a parameter server or trainer depending on our command line arguments. We\u2019ll create a ParameterServer if our passed in rank is 0, and a TrainerNet otherwise. Note that we\u2019re using torch.multiprocessing to launch a subprocess corresponding to the function that we want to execute, and waiting on this process\u2019s completion from the main thread with p.join(). In the case of initializing our trainers, we also use PyTorch\u2019s  in order to specify train and test data loaders on the MNIST dataset.",
                "markdown"
            ],
            [
                "processes = []\nworld_size = args.world_size\nif args.rank == 0:\n    p = mp.Process(target=run_parameter_server, args=(0, world_size))\n    p.start()\n    processes.append(p)\nelse:\n    # Get data to train on\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST('../data', train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=32, shuffle=True,)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\n            '../data',\n            train=False,\n            transform=transforms.Compose([\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.1307,), (0.3081,))\n                        ])),\n        batch_size=32,\n        shuffle=True,\n    )\n    # start training worker on this node\n    p = mp.Process(\n        target=run_worker,\n        args=(\n            args.rank,\n            world_size, args.num_gpus,\n            train_loader,\n            test_loader))\n    p.start()\n    processes.append(p)\n\nfor p in processes:\n    p.join()",
                "code"
            ],
            [
                "To run the example locally, run the following command worker for the server and each worker you wish to spawn, in separate terminal windows: python rpc_parameter_server.py --world_size=WORLD_SIZE --rank=RANK. For example, for a master node with world size of 2, the command would be python rpc_parameter_server.py --world_size=2 --rank=0. The trainer can then be launched with the command python rpc_parameter_server.py --world_size=2 --rank=1 in a separate window, and this will begin training with one server and a single trainer. Note that this tutorial assumes that training occurs using between 0 and 2 GPUs, and this argument can be configured by passing --num_gpus=N into the training script.",
                "markdown"
            ],
            [
                "You can pass in the command line arguments --master_addr=ADDRESS and --master_port=PORT to indicate the address and port that the master worker is listening on, for example, to test functionality where trainers and master nodes run on different machines.",
                "markdown"
            ]
        ],
        "Distributed Pipeline Parallelism Using RPC": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                " View and edit this tutorial in .",
                "markdown"
            ],
            [
                "Prerequisites:",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "RRef helper functions:\n,\n, and",
                "markdown"
            ],
            [
                "This tutorial uses a Resnet50 model to demonstrate implementing distributed\npipeline parallelism with \nAPIs. This can be viewed as the distributed counterpart of the multi-GPU\npipeline parallelism discussed in\n.",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                "This tutorial requires PyTorch v1.6.0 or above.",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                "Full source code of this tutorial can be found at\n.",
                "markdown"
            ],
            {
                "Basics": [
                    [
                        "The previous tutorial, \nshows how to use \nto implement distributed model parallelism for an RNN model. That tutorial uses\none GPU to host the EmbeddingTable, and the provided code works fine.\nHowever, if a model lives on multiple GPUs, it would require some extra steps to\nincrease the amortized utilization of all GPUs. Pipeline parallelism is one type\nof paradigm that can help in this case.",
                        "markdown"
                    ],
                    [
                        "In this tutorial, we use ResNet50 as an example model which is also used by\nthe \ntutorial. Similarly, the ResNet50 model is divided into two shards and\nthe input batch is partitioned into multiple splits and fed into the two model\nshards in a pipelined fashion. The difference is that, instead of parallelizing\nthe execution using CUDA streams, this tutorial invokes asynchronous RPCs. So,\nthe solution presented in this tutorial also works across machine boundaries.\nThe remainder of this tutorial presents the implementation in four steps.",
                        "markdown"
                    ]
                ]
            },
            {
                "Step 1: Partition ResNet50 Model": [
                    [
                        "This is the preparation step which implements ResNet50 in two model shards.\nThe code below is borrowed from the\n.\nThe ResNetBase module contains the common building blocks and attributes for\nthe two ResNet shards.",
                        "markdown"
                    ],
                    [
                        "import threading\n\nimport torch\nimport torch.nn as nn\n\nfrom torchvision.models.resnet import Bottleneck\n\nnum_classes = 1000\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass ResNetBase(nn.Module):\n    def __init__(self, block, inplanes, num_classes=1000,\n                groups=1, width_per_group=64, norm_layer=None):\n        super(ResNetBase, self).__init__()\n\n        self._lock = threading.Lock()\n        self._block = block\n        self._norm_layer = nn.BatchNorm2d\n        self.inplanes = inplanes\n        self.dilation = 1\n        self.groups = groups\n        self.base_width = width_per_group\n\n    def _make_layer(self, planes, blocks, stride=1):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if stride != 1 or self.inplanes != planes * self._block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * self._block.expansion, stride),\n                norm_layer(planes * self._block.expansion),\n            )\n\n        layers = []\n        layers.append(self._block(self.inplanes, planes, stride, downsample, self.groups,\n                                self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * self._block.expansion\n        for _ in range(1, blocks):\n            layers.append(self._block(self.inplanes, planes, groups=self.groups,\n                                    base_width=self.base_width, dilation=self.dilation,\n                                    norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def parameter_rrefs(self):\n        return [RRef(p) for p in self.parameters()]",
                        "code"
                    ],
                    [
                        "Now, we are ready to define the two model shards. For the constructor, we\nsimply split all ResNet50 layers into two parts and move each part into the\nprovided device. The forward functions of both shards take an RRef of\nthe input data, fetch the data locally, and then move it to the expected device.\nAfter applying all layers to the input, it moves the output to CPU and returns.\nIt is because the RPC API requires tensors to reside on CPU to avoid invalid\ndevice errors when the numbers of devices in the caller and the callee do not\nmatch.",
                        "markdown"
                    ],
                    [
                        "class ResNetShard1(ResNetBase):\n    def __init__(self, device, *args, **kwargs):\n        super(ResNetShard1, self).__init__(\n            Bottleneck, 64, num_classes=num_classes, *args, **kwargs)\n\n        self.device = device\n        self.seq = nn.Sequential(\n            nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False),\n            self._norm_layer(self.inplanes),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            self._make_layer(64, 3),\n            self._make_layer(128, 4, stride=2)\n        ).to(self.device)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x_rref):\n        x = x_rref.to_here().to(self.device)\n        with self._lock:\n            out =  self.seq(x)\n        return out.cpu()\n\n\nclass ResNetShard2(ResNetBase):\n    def __init__(self, device, *args, **kwargs):\n        super(ResNetShard2, self).__init__(\n            Bottleneck, 512, num_classes=num_classes, *args, **kwargs)\n\n        self.device = device\n        self.seq = nn.Sequential(\n            self._make_layer(256, 6, stride=2),\n            self._make_layer(512, 3, stride=2),\n            nn.AdaptiveAvgPool2d((1, 1)),\n        ).to(self.device)\n\n        self.fc =  nn.Linear(512 * self._block.expansion, num_classes).to(self.device)\n\n    def forward(self, x_rref):\n        x = x_rref.to_here().to(self.device)\n        with self._lock:\n            out = self.fc(torch.flatten(self.seq(x), 1))\n        return out.cpu()",
                        "code"
                    ]
                ]
            },
            {
                "Step 2: Stitch ResNet50 Model Shards Into One Module": [
                    [
                        "Then, we create a DistResNet50 module to assemble the two shards and\nimplement the pipeline parallel logic. In the constructor, we use two\nrpc.remote calls to put the two shards on two different RPC workers\nrespectively and hold on to the RRef to the two model parts so that they\ncan be referenced in the forward pass.  The forward function\nsplits the input batch into multiple micro-batches, and feeds these\nmicro-batches to the two model parts in a pipelined fashion. It first uses an\nrpc.remote call to apply the first shard to a micro-batch and then forwards\nthe returned intermediate output RRef to the second model shard. After that,\nit collects the Future of all micro-outputs, and waits for all of them after\nthe loop. Note that both remote() and rpc_async() return immediately and\nrun asynchronously. Therefore, the entire loop is non-blocking, and will launch\nmultiple RPCs concurrently. The execution order of one micro-batch on two model\nparts are preserved by intermediate output y_rref. The execution order\nacross micro-batches does not matter. In the end, the forward function\nconcatenates outputs of all micro-batches into one single output tensor and\nreturns. The parameter_rrefs function is a helper to\nsimplify distributed optimizer construction, which will be used later.",
                        "markdown"
                    ],
                    [
                        "class DistResNet50(nn.Module):\n    def __init__(self, num_split, workers, *args, **kwargs):\n        super(DistResNet50, self).__init__()\n\n        self.num_split = num_split\n\n        # Put the first part of the ResNet50 on workers[0]\n        self.p1_rref = rpc.remote(\n            workers[0],\n            ResNetShard1,\n            args = (\"cuda:0\",) + args,\n            kwargs = kwargs\n        )\n\n        # Put the second part of the ResNet50 on workers[1]\n        self.p2_rref = rpc.remote(\n            workers[1],\n            ResNetShard2,\n            args = (\"cuda:1\",) + args,\n            kwargs = kwargs\n        )\n\n    def forward(self, xs):\n        out_futures = []\n        for x in iter(xs.split(self.num_split, dim=0)):\n            x_rref = RRef(x)\n            y_rref = self.p1_rref.remote().forward(x_rref)\n            z_fut = self.p2_rref.rpc_async().forward(y_rref)\n            out_futures.append(z_fut)\n\n        return torch.cat(torch.futures.wait_all(out_futures))\n\n    def parameter_rrefs(self):\n        remote_params = []\n        remote_params.extend(self.p1_rref.remote().parameter_rrefs().to_here())\n        remote_params.extend(self.p2_rref.remote().parameter_rrefs().to_here())\n        return remote_params",
                        "code"
                    ]
                ]
            },
            {
                "Step 3: Define The Training Loop": [
                    [
                        "After defining the model, let us implement the training loop. We use a\ndedicated \u201cmaster\u201d worker to prepare random inputs and labels, and control the\ndistributed backward pass and distributed optimizer step. It first creates an\ninstance of the DistResNet50 module. It specifies the number of\nmicro-batches for each batch, and also provides the name of the two RPC workers\n(i.e., \u201cworker1\u201d, and \u201cworker2\u201d). Then it defines the loss function and creates\na DistributedOptimizer using the parameter_rrefs() helper to acquire a\nlist of parameter RRefs. Then, the main training loop is very similar to\nregular local training, except that it uses dist_autograd to launch\nbackward and provides the context_id for both backward and optimizer\nstep().",
                        "markdown"
                    ],
                    [
                        "import torch.distributed.autograd as dist_autograd\nimport torch.optim as optim\nfrom torch.distributed.optim import DistributedOptimizer\n\nnum_batches = 3\nbatch_size = 120\nimage_w = 128\nimage_h = 128\n\n\ndef run_master(num_split):\n    # put the two model parts on worker1 and worker2 respectively\n    model = DistResNet50(num_split, [\"worker1\", \"worker2\"])\n    loss_fn = nn.MSELoss()\n    opt = DistributedOptimizer(\n        optim.SGD,\n        model.parameter_rrefs(),\n        lr=0.05,\n    )\n\n    one_hot_indices = torch.LongTensor(batch_size) \\\n                        .random_(0, num_classes) \\\n                        .view(batch_size, 1)\n\n    for i in range(num_batches):\n        print(f\"Processing batch {i}\")\n        # generate random inputs and labels\n        inputs = torch.randn(batch_size, 3, image_w, image_h)\n        labels = torch.zeros(batch_size, num_classes) \\\n                    .scatter_(1, one_hot_indices, 1)\n\n        with dist_autograd.context() as context_id:\n            outputs = model(inputs)\n            dist_autograd.backward(context_id, [loss_fn(outputs, labels)])\n            opt.step(context_id)",
                        "code"
                    ]
                ]
            },
            {
                "Step 4: Launch RPC Processes": [
                    [
                        "Finally, the code below shows the target function for all processes. The main\nlogic is defined in run_master. The workers passively waiting for\ncommands from the master, and hence simply runs init_rpc and shutdown,\nwhere the shutdown by default will block until all RPC participants finish.",
                        "markdown"
                    ],
                    [
                        "import os\nimport time\n\nimport torch.multiprocessing as mp\n\n\ndef run_worker(rank, world_size, num_split):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    options = rpc.TensorPipeRpcBackendOptions(num_worker_threads=128)\n\n    if rank == 0:\n        rpc.init_rpc(\n            \"master\",\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=options\n        )\n        run_master(num_split)\n    else:\n        rpc.init_rpc(\n            f\"worker{rank}\",\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=options\n        )\n        pass\n\n    # block until all rpcs finish\n    rpc.shutdown()\n\n\nif __name__==\"__main__\":\n    world_size = 3\n    for num_split in [1, 2, 4, 8]:\n        tik = time.time()\n        mp.spawn(run_worker, args=(world_size, num_split), nprocs=world_size, join=True)\n        tok = time.time()\n        print(f\"number of splits = {num_split}, execution time = {tok - tik}\")",
                        "code"
                    ]
                ]
            }
        ],
        "Implementing Batch RPC Processing Using Asynchronous Executions": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                " View and edit this tutorial in .",
                "markdown"
            ],
            [
                "Prerequisites:",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "This tutorial demonstrates how to build batch-processing RPC applications with\nthe \ndecorator, which helps to speed up training by reducing the number of blocked\nRPC threads and consolidating CUDA operations on the callee. This shares the\nsame idea as .",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                "This tutorial requires PyTorch v1.6.0 or above.",
                "markdown"
            ],
            {
                "Basics": [
                    [
                        "Previous tutorials have shown the steps to build distributed training\napplications using ,\nbut they didn\u2019t elaborate on what happens on the callee side when processing an\nRPC request. As of PyTorch v1.5, each RPC request will block one thread on the\ncallee to execute the function in that request until that function returns.\nThis works for many use cases, but there is one caveat. If the user function\nblocks on IO, e.g., with nested RPC invocation, or signaling, e.g., waiting for\na different RPC request to unblock, the RPC thread on the callee will have to\nidle waiting until the IO finishes or the signaling event occurs. As a result,\nRPC callees are likely to use more threads than necessary. The cause of this\nproblem is that RPC treats user functions as black boxes, and knows very little\nabout what happens in the function. To allow user functions to yield and free\nRPC threads, more hints need to be provided to the RPC system.",
                        "markdown"
                    ],
                    [
                        "Since v1.6.0, PyTorch addresses this problem by introducing two new concepts:",
                        "markdown"
                    ],
                    [
                        "A  type\nthat encapsulates an asynchronous execution, which also supports installing\ncallback functions.",
                        "markdown"
                    ],
                    [
                        "An \ndecorator that allows applications to tell the callee that the target function\nwill return a future and can pause and yield multiple times during execution.",
                        "markdown"
                    ],
                    [
                        "With these two tools, the application code can break a user function into\nmultiple smaller functions, chain them together as callbacks on Future\nobjects, and return the Future that contains the final result. On the callee\nside, when getting the Future object, it installs subsequent RPC response\npreparation and communication as callbacks as well, which will be triggered\nwhen the final result is ready. In this way, the callee no longer needs to block\none thread and wait until the final return value is ready. Please refer to the\nAPI doc of\n\nfor simple examples.",
                        "markdown"
                    ],
                    [
                        "Besides reducing the number of idle threads on the callee, these tools also help\nto make batch RPC processing easier and faster. The following two sections of\nthis tutorial demonstrate how to build distributed batch-updating parameter\nserver and batch-processing reinforcement learning applications using the\n\ndecorator.",
                        "markdown"
                    ]
                ]
            },
            {
                "Batch-Updating Parameter Server": [
                    [
                        "Consider a synchronized parameter server training application with one parameter\nserver (PS) and multiple trainers. In this application, the PS holds the\nparameters and waits for all trainers to report gradients. In every iteration,\nit waits until receiving gradients from all trainers and then updates all\nparameters in one shot. The code below shows the implementation of the PS class.\nThe update_and_fetch_model method is decorated using\n@rpc.functions.async_execution and will be called by trainers. Each\ninvocation returns a Future object that will be populated with the updated\nmodel. Invocations launched by most trainers just accumulate gradients to the\n.grad field, return immediately, and yield the RPC thread on the PS. The\nlast arriving trainer will trigger the optimizer step and consume all previously\nreported gradients. Then it sets the future_model with the updated model,\nwhich in turn notifies all previous requests from other trainers through the\nFuture object and sends out the updated model to all trainers.",
                        "markdown"
                    ],
                    [
                        "import threading\nimport torchvision\nimport torch\nimport torch.distributed.rpc as rpc\nfrom torch import optim\n\nnum_classes, batch_update_size = 30, 5\n\nclass BatchUpdateParameterServer(object):\n    def __init__(self, batch_update_size=batch_update_size):\n        self.model = torchvision.models.resnet50(num_classes=num_classes)\n        self.lock = threading.Lock()\n        self.future_model = torch.futures.Future()\n        self.batch_update_size = batch_update_size\n        self.curr_update_size = 0\n        self.optimizer = optim.SGD(self.model.parameters(), lr=0.001, momentum=0.9)\n        for p in self.model.parameters():\n            p.grad = torch.zeros_like(p)\n\n    def get_model(self):\n        return self.model\n\n    @staticmethod\n    @rpc.functions.async_execution\n    def update_and_fetch_model(ps_rref, grads):\n        # Using the RRef to retrieve the local PS instance\n        self = ps_rref.local_value()\n        with self.lock:\n            self.curr_update_size += 1\n            # accumulate gradients into .grad field\n            for p, g in zip(self.model.parameters(), grads):\n                p.grad += g\n\n            # Save the current future_model and return it to make sure the\n            # returned Future object holds the correct model even if another\n            # thread modifies future_model before this thread returns.\n            fut = self.future_model\n\n            if self.curr_update_size &gt;= self.batch_update_size:\n                # update the model\n                for p in self.model.parameters():\n                    p.grad /= self.batch_update_size\n                self.curr_update_size = 0\n                self.optimizer.step()\n                self.optimizer.zero_grad()\n                # by settiing the result on the Future object, all previous\n                # requests expecting this updated model will be notified and\n                # the their responses will be sent accordingly.\n                fut.set_result(self.model)\n                self.future_model = torch.futures.Future()\n\n        return fut",
                        "code"
                    ],
                    [
                        "For the trainers, they are all initialized using the same set of\nparameters from the PS. In every iteration, each trainer first runs the forward\nand the backward passes to generate gradients locally. Then, each trainer\nreports its gradients to the PS using RPC, and fetches back the updated\nparameters through the return value of the same RPC request. In the trainer\u2019s\nimplementation, whether the target function is marked with\n@rpc.functions.async_execution or not makes no difference. The\ntrainer simply calls update_and_fetch_model using rpc_sync which will\nblock on the trainer until the updated model is returned.",
                        "markdown"
                    ],
                    [
                        "batch_size, image_w, image_h  = 20, 64, 64\n\nclass Trainer(object):\n    def __init__(self, ps_rref):\n        self.ps_rref, self.loss_fn = ps_rref, torch.nn.MSELoss()\n        self.one_hot_indices = torch.LongTensor(batch_size) \\\n                                    .random_(0, num_classes) \\\n                                    .view(batch_size, 1)\n\n    def get_next_batch(self):\n        for _ in range(6):\n            inputs = torch.randn(batch_size, 3, image_w, image_h)\n            labels = torch.zeros(batch_size, num_classes) \\\n                        .scatter_(1, self.one_hot_indices, 1)\n            yield inputs.cuda(), labels.cuda()\n\n    def train(self):\n        name = rpc.get_worker_info().name\n        # get initial model parameters\n        m = self.ps_rref.rpc_sync().get_model().cuda()\n        # start training\n        for inputs, labels in self.get_next_batch():\n            self.loss_fn(m(inputs), labels).backward()\n            m = rpc.rpc_sync(\n                self.ps_rref.owner(),\n                BatchUpdateParameterServer.update_and_fetch_model,\n                args=(self.ps_rref, [p.grad for p in m.cpu().parameters()]),\n            ).cuda()",
                        "code"
                    ],
                    [
                        "We skip the code that launches multiple processes in this tutorial and please\nrefer to the \nrepo for the full implementation. Note that, it is possible to implement batch\nprocessing without the\n\ndecorator. However, that would require either blocking more RPC threads on\nthe PS or use another round of RPC to fetch updated models, where the latter\nwould add both more code complexity and more communication overhead.",
                        "markdown"
                    ],
                    [
                        "This section uses a simple parameter sever training example to show how to\nimplement batch RPC applications using the\n\ndecorator. In the next section, we re-implement the reinforcement learning\nexample in the previous\n\ntutorial using batch processing, and demonstrate its impact on the training\nspeed.",
                        "markdown"
                    ]
                ]
            },
            {
                "Batch-Processing CartPole Solver": [
                    [
                        "This section uses CartPole-v1 from  as\nan example to show the performance impact of batch processing RPC. Please note\nthat since the goal is to demonstrate the usage of\n\ninstead of building the best CartPole solver or solving most different RL\nproblems, we use very simple policies and reward calculation strategies and\nfocus on the multi-observer single-agent batch RPC implementation. We use a\nsimilar Policy model as the previous tutorial which is shown below. Compared\nto the previous tutorial, the difference is that its constructor takes an\nadditional batch argument which controls the dim parameter for\nF.softmax because with batching, the x argument in the forward\nfunction contains states from multiple observers and hence the dimension needs\nto change properly. Everything else stays intact.",
                        "markdown"
                    ],
                    [
                        "import argparse\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nparser = argparse.ArgumentParser(description='PyTorch RPC Batch RL example')\nparser.add_argument('--gamma', type=float, default=1.0, metavar='G',\n                    help='discount factor (default: 1.0)')\nparser.add_argument('--seed', type=int, default=543, metavar='N',\n                    help='random seed (default: 543)')\nparser.add_argument('--num-episode', type=int, default=10, metavar='E',\n                    help='number of episodes (default: 10)')\nargs = parser.parse_args()\n\ntorch.manual_seed(args.seed)\n\nclass Policy(nn.Module):\n    def __init__(self, batch=True):\n        super(Policy, self).__init__()\n        self.affine1 = nn.Linear(4, 128)\n        self.dropout = nn.Dropout(p=0.6)\n        self.affine2 = nn.Linear(128, 2)\n        self.dim = 2 if batch else 1\n\n    def forward(self, x):\n        x = self.affine1(x)\n        x = self.dropout(x)\n        x = F.relu(x)\n        action_scores = self.affine2(x)\n        return F.softmax(action_scores, dim=self.dim)",
                        "code"
                    ],
                    [
                        "The constructor of the Observer adjusts accordingly as well. It also takes a\nbatch argument, which governs which Agent function it uses to select\nactions. In batch mode, it calls select_action_batch function on Agent\nwhich will be presented shortly, and this function will be decorated with\n.",
                        "markdown"
                    ],
                    [
                        "import gym\nimport torch.distributed.rpc as rpc\n\nclass Observer:\n    def __init__(self, batch=True):\n        self.id = rpc.get_worker_info().id - 1\n        self.env = gym.make('CartPole-v1')\n        self.env.seed(args.seed)\n        self.select_action = Agent.select_action_batch if batch else Agent.select_action",
                        "code"
                    ],
                    [
                        "Compared to the previous tutorial\n,\nobservers behave a little differently. Instead of exiting when the environment\nis stopped, it always runs n_steps iterations in every episode. When the\nenvironment returns, the observer simply resets the environment and start over\nagain. With this design, the agent will receive a fixed number of states from\nevery observer and hence can pack them into a fixed-size tensor. In every\nstep, the Observer uses RPC to send its state to the Agent and fetches\nthe action through the return value. At the end of every episode, it returns the\nrewards of all steps to Agent. Note that this run_episode function will\nbe called by the Agent using RPC. So the rpc_sync call in this function\nwill be a nested RPC invocation. We could mark this function as @rpc.functions.async_execution\ntoo to avoid blocking one thread on the Observer. However, as the bottleneck\nis the Agent instead of the Observer, it should be OK to block one\nthread on the Observer process.",
                        "markdown"
                    ],
                    [
                        "import torch\n\nclass Observer:\n    ...\n\n    def run_episode(self, agent_rref, n_steps):\n        state, ep_reward = self.env.reset(), NUM_STEPS\n        rewards = torch.zeros(n_steps)\n        start_step = 0\n        for step in range(n_steps):\n            state = torch.from_numpy(state).float().unsqueeze(0)\n            # send the state to the agent to get an action\n            action = rpc.rpc_sync(\n                agent_rref.owner(),\n                self.select_action,\n                args=(agent_rref, self.id, state)\n            )\n\n            # apply the action to the environment, and get the reward\n            state, reward, done, _ = self.env.step(action)\n            rewards[step] = reward\n\n            if done or step + 1 &gt;= n_steps:\n                curr_rewards = rewards[start_step:(step + 1)]\n                R = 0\n                for i in range(curr_rewards.numel() -1, -1, -1):\n                    R = curr_rewards[i] + args.gamma * R\n                    curr_rewards[i] = R\n                state = self.env.reset()\n                if start_step == 0:\n                    ep_reward = min(ep_reward, step - start_step + 1)\n                start_step = step + 1\n\n        return [rewards, ep_reward]",
                        "code"
                    ],
                    [
                        "The constructor of the Agent also takes a batch argument, which controls\nhow action probs are batched. In batch mode, the saved_log_probs contains a\nlist of tensors, where each tensor contains action robs from all observers in\none step. Without batching, the saved_log_probs is a dictionary where the\nkey is the observer id and the value is a list of action probs for that\nobserver.",
                        "markdown"
                    ],
                    [
                        "import threading\nfrom torch.distributed.rpc import RRef\n\nclass Agent:\n    def __init__(self, world_size, batch=True):\n        self.ob_rrefs = []\n        self.agent_rref = RRef(self)\n        self.rewards = {}\n        self.policy = Policy(batch).cuda()\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=1e-2)\n        self.running_reward = 0\n\n        for ob_rank in range(1, world_size):\n            ob_info = rpc.get_worker_info(OBSERVER_NAME.format(ob_rank))\n            self.ob_rrefs.append(rpc.remote(ob_info, Observer, args=(batch,)))\n            self.rewards[ob_info.id] = []\n\n        self.states = torch.zeros(len(self.ob_rrefs), 1, 4)\n        self.batch = batch\n        self.saved_log_probs = [] if batch else {k:[] for k in range(len(self.ob_rrefs))}\n        self.future_actions = torch.futures.Future()\n        self.lock = threading.Lock()\n        self.pending_states = len(self.ob_rrefs)",
                        "code"
                    ],
                    [
                        "The non-batching select_acion simply runs the state throw the policy, saves\nthe action prob, and returns the action to the observer right away.",
                        "markdown"
                    ],
                    [
                        "from torch.distributions import Categorical\n\nclass Agent:\n    ...\n\n    @staticmethod\n    def select_action(agent_rref, ob_id, state):\n        self = agent_rref.local_value()\n        probs = self.policy(state.cuda())\n        m = Categorical(probs)\n        action = m.sample()\n        self.saved_log_probs[ob_id].append(m.log_prob(action))\n        return action.item()",
                        "code"
                    ],
                    [
                        "With batching, the state is stored in a 2D tensor self.states, using the\nobserver id as the row id. Then, it chains a Future by installing a callback\nfunction to the batch-generated self.future_actions Future object, which\nwill be populated with the specific row indexed using the id of that observer.\nThe last arriving observer runs all batched states through the policy in one\nshot and set  self.future_actions accordingly. When this occurs, all the\ncallback functions installed on self.future_actions will be triggered and\ntheir return values will be used to populate the chained Future object,\nwhich in turn notifies the Agent to prepare and communicate responses for\nall previous RPC requests from other observers.",
                        "markdown"
                    ],
                    [
                        "class Agent:\n    ...\n\n    @staticmethod\n    @rpc.functions.async_execution\n    def select_action_batch(agent_rref, ob_id, state):\n        self = agent_rref.local_value()\n        self.states[ob_id].copy_(state)\n        future_action = self.future_actions.then(\n            lambda future_actions: future_actions.wait()[ob_id].item()\n        )\n\n        with self.lock:\n            self.pending_states -= 1\n            if self.pending_states == 0:\n                self.pending_states = len(self.ob_rrefs)\n                probs = self.policy(self.states.cuda())\n                m = Categorical(probs)\n                actions = m.sample()\n                self.saved_log_probs.append(m.log_prob(actions).t()[0])\n                future_actions = self.future_actions\n                self.future_actions = torch.futures.Future()\n                future_actions.set_result(actions.cpu())\n        return future_action",
                        "code"
                    ],
                    [
                        "Now let\u2019s define how different RPC functions are stitched together. The Agent\ncontrols the execution of every episode. It first uses rpc_async to kick off\nthe episode on all observers and block on the returned futures which will be\npopulated with observer rewards. Note that the code below uses the RRef helper\nob_rref.rpc_async() to launch the run_episode function on the owner\nof the ob_rref RRef with the provided arguments.\nIt then converts the saved action probs and returned observer rewards into\nexpected data format, and launch the training step. Finally, it resets all\nstates and returns the reward of the current episode. This function is the entry\npoint to run one episode.",
                        "markdown"
                    ],
                    [
                        "class Agent:\n    ...\n\n    def run_episode(self, n_steps=0):\n        futs = []\n        for ob_rref in self.ob_rrefs:\n            # make async RPC to kick off an episode on all observers\n            futs.append(ob_rref.rpc_async().run_episode(self.agent_rref, n_steps))\n\n        # wait until all obervers have finished this episode\n        rets = torch.futures.wait_all(futs)\n        rewards = torch.stack([ret[0] for ret in rets]).cuda().t()\n        ep_rewards = sum([ret[1] for ret in rets]) / len(rets)\n\n        # stack saved probs into one tensor\n        if self.batch:\n            probs = torch.stack(self.saved_log_probs)\n        else:\n            probs = [torch.stack(self.saved_log_probs[i]) for i in range(len(rets))]\n            probs = torch.stack(probs)\n\n        policy_loss = -probs * rewards / len(rets)\n        policy_loss.sum().backward()\n        self.optimizer.step()\n        self.optimizer.zero_grad()\n\n        # reset variables\n        self.saved_log_probs = [] if self.batch else {k:[] for k in range(len(self.ob_rrefs))}\n        self.states = torch.zeros(len(self.ob_rrefs), 1, 4)\n\n        # calculate running rewards\n        self.running_reward = 0.5 * ep_rewards + 0.5 * self.running_reward\n        return ep_rewards, self.running_reward",
                        "code"
                    ],
                    [
                        "The rest of the code is normal processes launching and logging which are\nsimilar to other RPC tutorials. In this tutorial, all observers passively\nwaiting for commands from the agent. Please refer to the\n\nrepo for the full implementation.",
                        "markdown"
                    ],
                    [
                        "def run_worker(rank, world_size, n_episode, batch, print_log=True):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    if rank == 0:\n        # rank0 is the agent\n        rpc.init_rpc(AGENT_NAME, rank=rank, world_size=world_size)\n\n        agent = Agent(world_size, batch)\n        for i_episode in range(n_episode):\n            last_reward, running_reward = agent.run_episode(n_steps=NUM_STEPS)\n\n            if print_log:\n                print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n                    i_episode, last_reward, running_reward))\n    else:\n        # other ranks are the observer\n        rpc.init_rpc(OBSERVER_NAME.format(rank), rank=rank, world_size=world_size)\n        # observers passively waiting for instructions from agents\n    rpc.shutdown()\n\n\ndef main():\n    for world_size in range(2, 12):\n        delays = []\n        for batch in [True, False]:\n            tik = time.time()\n            mp.spawn(\n                run_worker,\n                args=(world_size, args.num_episode, batch),\n                nprocs=world_size,\n                join=True\n            )\n            tok = time.time()\n            delays.append(tok - tik)\n\n        print(f\"{world_size}, {delays[0]}, {delays[1]}\")\n\n\nif __name__ == '__main__':\n    main()",
                        "code"
                    ],
                    [
                        "Batch RPC helps to consolidate the action inference into less CUDA operations,\nand hence reduces the amortized overhead. The above main function runs the\nsame code on both batch and no-batch modes using different numbers of observers,\nranging from 1 to 10. The figure below plots the execution time of different\nworld sizes using default argument values. The results confirmed our expectation\nthat batch processing helped to speed up training.\n\n<img alt=\"\" src=\"../_images/batch.png\"/>",
                        "markdown"
                    ]
                ]
            },
            {
                "Learn More": [
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Combining Distributed DataParallel with Distributed RPC Framework": [
            [
                "<strong>Authors</strong>:  and ",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                " View and edit this tutorial in .",
                "markdown"
            ],
            [
                "This tutorial uses a simple example to demonstrate how you can combine\n (DDP)\nwith the \nto combine distributed data parallelism with distributed model parallelism to\ntrain a simple model. Source code of the example can be found .",
                "markdown"
            ],
            [
                "Previous tutorials,\n\nand ,\ndescribed how to perform distributed data parallel and distributed model\nparallel training respectively. Although, there are several training paradigms\nwhere you might want to combine these two techniques. For example:",
                "markdown"
            ],
            [
                "If we have a model with a sparse part (large embedding table) and a dense\npart (FC layers), we might want to put the embedding table on a parameter\nserver and replicate the FC layer across multiple trainers using .\nThe \ncan be used to perform embedding lookups on the parameter server.",
                "markdown"
            ],
            [
                "Enable hybrid parallelism as described in the  paper.\nWe can use the \nto pipeline stages of the model across multiple workers and replicate each\nstage (if needed) using .\n\n\n<br/>",
                "markdown"
            ],
            [
                "In this tutorial we will cover case 1 mentioned above. We have a total of 4\nworkers in our setup as follows:",
                "markdown"
            ],
            [
                "1 Master, which is responsible for creating an embedding table\n(nn.EmbeddingBag) on the parameter server. The master also drives the\ntraining loop on the two trainers.",
                "markdown"
            ],
            [
                "1 Parameter Server, which basically holds the embedding table in memory and\nresponds to RPCs from the Master and Trainers.",
                "markdown"
            ],
            [
                "2 Trainers, which store an FC layer (nn.Linear) which is replicated amongst\nthemselves using .\nThe trainers are also responsible for executing the forward pass, backward\npass and optimizer step.\n\n\n<br/>",
                "markdown"
            ],
            [
                "The entire training process is executed as follows:",
                "markdown"
            ],
            [
                "The master creates a \nthat holds an embedding table on the Parameter Server.",
                "markdown"
            ],
            [
                "The master, then kicks off the training loop on the trainers and passes the\nremote module to the trainers.",
                "markdown"
            ],
            [
                "The trainers create a HybridModel which first performs an embedding lookup\nusing the remote module provided by the master and then executes the\nFC layer which is wrapped inside DDP.",
                "markdown"
            ],
            [
                "The trainer executes the forward pass of the model and uses the loss to\nexecute the backward pass using .",
                "markdown"
            ],
            [
                "As part of the backward pass, the gradients for the FC layer are computed\nfirst and synced to all trainers via allreduce in DDP.",
                "markdown"
            ],
            [
                "Next, Distributed Autograd propagates the gradients to the parameter server,\nwhere the gradients for the embedding table are updated.",
                "markdown"
            ],
            [
                "Finally, the  is used to update all the parameters.",
                "markdown"
            ],
            [
                "Attention",
                "markdown"
            ],
            [
                "You should always use \nfor the backward pass if you\u2019re combining DDP and RPC.",
                "markdown"
            ],
            [
                "Now, let\u2019s go through each part in detail. Firstly, we need to setup all of our\nworkers before we can perform any training. We create 4 processes such that\nranks 0 and 1 are our trainers, rank 2 is the master and rank 3 is the\nparameter server.",
                "markdown"
            ],
            [
                "We initialize the RPC framework on all 4 workers using the TCP init_method.\nOnce RPC initialization is done, the master creates a remote module that holds an \nlayer on the Parameter Server using .\nThe master then loops through each trainer and kicks off the training loop by\ncalling _run_trainer on each trainer using .\nFinally, the master waits for all training to finish before exiting.",
                "markdown"
            ],
            [
                "The trainers first initialize a ProcessGroup for DDP with world_size=2\n(for two trainers) using .\nNext, they initialize the RPC framework using the TCP init_method. Note that\nthe ports are different in RPC initialization and ProcessGroup initialization.\nThis is to avoid port conflicts between initialization of both frameworks.\nOnce the initialization is done, the trainers just wait for the _run_trainer\nRPC from the master.",
                "markdown"
            ],
            [
                "The parameter server just initializes the RPC framework and waits for RPCs from\nthe trainers and master.",
                "markdown"
            ],
            [
                "def run_worker(rank, world_size):\n    r\"\"\"\n    A wrapper function that initializes RPC, calls the function, and shuts down\n    RPC.\n    \"\"\"\n\n    # We need to use different port numbers in TCP init_method for init_rpc and\n    # init_process_group to avoid port conflicts.\n    rpc_backend_options = TensorPipeRpcBackendOptions()\n    rpc_backend_options.init_method = \"tcp://localhost:29501\"\n\n    # Rank 2 is master, 3 is ps and 0 and 1 are trainers.\n    if rank == 2:\n        rpc.init_rpc(\n            \"master\",\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=rpc_backend_options,\n        )\n\n        remote_emb_module = RemoteModule(\n            \"ps\",\n            torch.nn.EmbeddingBag,\n            args=(NUM_EMBEDDINGS, EMBEDDING_DIM),\n            kwargs={\"mode\": \"sum\"},\n        )\n\n        # Run the training loop on trainers.\n        futs = []\n        for trainer_rank in [0, 1]:\n            trainer_name = \"trainer{}\".format(trainer_rank)\n            fut = rpc.rpc_async(\n                trainer_name, _run_trainer, args=(remote_emb_module, trainer_rank)\n            )\n            futs.append(fut)\n\n        # Wait for all training to finish.\n        for fut in futs:\n            fut.wait()\n    elif rank &lt;= 1:\n        # Initialize process group for Distributed DataParallel on trainers.\n        dist.init_process_group(\n            backend=\"gloo\", rank=rank, world_size=2, init_method=\"tcp://localhost:29500\"\n        )\n\n        # Initialize RPC.\n        trainer_name = \"trainer{}\".format(rank)\n        rpc.init_rpc(\n            trainer_name,\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=rpc_backend_options,\n        )\n\n        # Trainer just waits for RPCs from master.\n    else:\n        rpc.init_rpc(\n            \"ps\",\n            rank=rank,\n            world_size=world_size,\n            rpc_backend_options=rpc_backend_options,\n        )\n        # parameter server do nothing\n        pass\n\n    # block until all rpcs finish\n    rpc.shutdown()\n\n\nif __name__ == \"__main__\":\n    # 2 trainers, 1 parameter server, 1 master.\n    world_size = 4\n    mp.spawn(run_worker, args=(world_size,), nprocs=world_size, join=True)",
                "code"
            ],
            [
                "Before we discuss details of the Trainer, let\u2019s introduce the HybridModel that\nthe trainer uses. As described below, the HybridModel is initialized using a\nremote module that holds an embedding table (remote_emb_module) on the parameter server and the device\nto use for DDP. The initialization of the model wraps an\n\nlayer inside DDP to replicate and synchronize this layer across all trainers.",
                "markdown"
            ],
            [
                "The forward method of the model is pretty straightforward. It performs an\nembedding lookup on the parameter server using RemoteModule\u2019s forward\nand passes its output onto the FC layer.",
                "markdown"
            ],
            [
                "class HybridModel(torch.nn.Module):\n    r\"\"\"\n    The model consists of a sparse part and a dense part.\n    1) The dense part is an nn.Linear module that is replicated across all trainers using DistributedDataParallel.\n    2) The sparse part is a Remote Module that holds an nn.EmbeddingBag on the parameter server.\n    This remote model can get a Remote Reference to the embedding table on the parameter server.\n    \"\"\"\n\n    def __init__(self, remote_emb_module, device):\n        super(HybridModel, self).__init__()\n        self.remote_emb_module = remote_emb_module\n        self.fc = DDP(torch.nn.Linear(16, 8).cuda(device), device_ids=[device])\n        self.device = device\n\n    def forward(self, indices, offsets):\n        emb_lookup = self.remote_emb_module.forward(indices, offsets)\n        return self.fc(emb_lookup.cuda(self.device))",
                "code"
            ],
            [
                "Next, let\u2019s look at the setup on the Trainer. The trainer first creates the\nHybridModel described above using a remote module that holds the embedding table on the\nparameter server and its own rank.",
                "markdown"
            ],
            [
                "Now, we need to retrieve a list of RRefs to all the parameters that we would\nlike to optimize with .\nTo retrieve the parameters for the embedding table from the parameter server,\nwe can call RemoteModule\u2019s ,\nwhich basically walks through all the parameters for the embedding table and returns\na list of RRefs. The trainer calls this method on the parameter server via RPC\nto receive a list of RRefs to the desired parameters. Since the\nDistributedOptimizer always takes a list of RRefs to parameters that need to\nbe optimized, we need to create RRefs even for the local parameters for our\nFC layers. This is done by walking model.fc.parameters(), creating an RRef for\neach parameter and appending it to the list returned from remote_parameters().\nNote that we cannnot use model.parameters(),\nbecause it will recursively call model.remote_emb_module.parameters(),\nwhich is not supported by RemoteModule.",
                "markdown"
            ],
            [
                "Finally, we create our DistributedOptimizer using all the RRefs and define a\nCrossEntropyLoss function.",
                "markdown"
            ],
            [
                "def _run_trainer(remote_emb_module, rank):\n    r\"\"\"\n    Each trainer runs a forward pass which involves an embedding lookup on the\n    parameter server and running nn.Linear locally. During the backward pass,\n    DDP is responsible for aggregating the gradients for the dense part\n    (nn.Linear) and distributed autograd ensures gradients updates are\n    propagated to the parameter server.\n    \"\"\"\n\n    # Setup the model.\n    model = HybridModel(remote_emb_module, rank)\n\n    # Retrieve all model parameters as rrefs for DistributedOptimizer.\n\n    # Retrieve parameters for embedding table.\n    model_parameter_rrefs = model.remote_emb_module.remote_parameters()\n\n    # model.fc.parameters() only includes local parameters.\n    # NOTE: Cannot call model.parameters() here,\n    # because this will call remote_emb_module.parameters(),\n    # which supports remote_parameters() but not parameters().\n    for param in model.fc.parameters():\n        model_parameter_rrefs.append(RRef(param))\n\n    # Setup distributed optimizer\n    opt = DistributedOptimizer(\n        optim.SGD,\n        model_parameter_rrefs,\n        lr=0.05,\n    )\n\n    criterion = torch.nn.CrossEntropyLoss()",
                "code"
            ],
            [
                "Now we\u2019re ready to introduce the main training loop that is run on each trainer.\nget_next_batch is just a helper function to generate random inputs and\ntargets for training. We run the training loop for multiple epochs and for each\nbatch:",
                "markdown"
            ],
            [
                "Setup a \nfor Distributed Autograd.",
                "markdown"
            ],
            [
                "Run the forward pass of the model and retrieve its output.",
                "markdown"
            ],
            [
                "Compute the loss based on our outputs and targets using the loss function.",
                "markdown"
            ],
            [
                "Use Distributed Autograd to execute a distributed backward pass using the loss.",
                "markdown"
            ],
            [
                "Finally, run a Distributed Optimizer step to optimize all the parameters.",
                "markdown"
            ],
            [
                "    def get_next_batch(rank):\n        for _ in range(10):\n            num_indices = random.randint(20, 50)\n            indices = torch.LongTensor(num_indices).random_(0, NUM_EMBEDDINGS)\n\n            # Generate offsets.\n            offsets = []\n            start = 0\n            batch_size = 0\n            while start &lt; num_indices:\n                offsets.append(start)\n                start += random.randint(1, 10)\n                batch_size += 1\n\n            offsets_tensor = torch.LongTensor(offsets)\n            target = torch.LongTensor(batch_size).random_(8).cuda(rank)\n            yield indices, offsets_tensor, target\n\n    # Train for 100 epochs\n    for epoch in range(100):\n        # create distributed autograd context\n        for indices, offsets, target in get_next_batch(rank):\n            with dist_autograd.context() as context_id:\n                output = model(indices, offsets)\n                loss = criterion(output, target)\n\n                # Run distributed backward pass\n                dist_autograd.backward(context_id, [loss])\n\n                # Tun distributed optimizer\n                opt.step(context_id)\n\n                # Not necessary to zero grads as each iteration creates a different\n                # distributed autograd context which hosts different grads\n        print(\"Training done for epoch {}\".format(epoch))",
                "code"
            ],
            [
                "Source code for the entire example can be found .",
                "markdown"
            ]
        ],
        "Training Transformer models using Pipeline Parallelism": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "This tutorial demonstrates how to train a large Transformer model across\nmultiple GPUs using pipeline parallelism. This tutorial is an extension of the\n tutorial\nand scales up the same model to demonstrate how pipeline parallelism can be\nused to train Transformer models.",
                "markdown"
            ],
            [
                "Prerequisites:\n<blockquote>",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "</blockquote>",
                "markdown"
            ],
            {
                "Define the model": [
                    [
                        "In this tutorial, we will split a Transformer model across two GPUs and use\npipeline parallelism to train the model. The model is exactly the same model\nused in the  tutorial,\nbut is split into two stages. The largest number of parameters belong to the\n layer.\nThe \nitself consists of nlayers of .\nAs a result, our focus is on nn.TransformerEncoder and we split the model\nsuch that half of the nn.TransformerEncoderLayer are on one GPU and the\nother half are on another. To do this, we pull out the Encoder and\nDecoder sections into seperate modules and then build an nn.Sequential\nrepresenting the original Transformer module.",
                        "markdown"
                    ],
                    [
                        "import sys\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport tempfile\nfrom torch.nn import TransformerEncoder, \n\nif sys.platform == 'win32':\n    print('Windows platform is not supported for pipeline parallelism')\n    sys.exit(0)\nif () &lt; 2:\n    print('Need at least two GPU devices for this tutorial')\n    sys.exit(0)\n\nclass Encoder():\n    def __init__(self, ntoken, ninp, dropout=0.5):\n        super(, self).__init__()\n        self.pos_encoder = (ninp, dropout)\n        self.encoder = (ntoken, ninp)\n        self.ninp = ninp\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        # Need (S, N) format for encoder.\n        src = src.t()\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        return self.pos_encoder(src)\n\nclass Decoder():\n    def __init__(self, ntoken, ninp):\n        super(, self).__init__()\n        self.decoder = (ninp, ntoken)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, inp):\n        # Need batch dimension first for output of pipeline.\n        return self.decoder(inp).permute(1, 0, 2)",
                        "code"
                    ],
                    [
                        "PositionalEncoding module injects some information about the\nrelative or absolute position of the tokens in the sequence. The\npositional encodings have the same dimension as the embeddings so that\nthe two can be summed. Here, we use sine and cosine functions of\ndifferent frequencies.",
                        "markdown"
                    ],
                    [
                        "class PositionalEncoding():\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(, self).__init__()\n        self.dropout = (p=dropout)\n\n        pe = (max_len, d_model)\n        position = (0, max_len, dtype=).unsqueeze(1)\n        div_term = ((0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = (position * div_term)\n        pe[:, 1::2] = (position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)",
                        "code"
                    ]
                ]
            },
            {
                "Load and batch data": [
                    [
                        "The training process uses Wikitext-2 dataset from torchtext.\nTo access torchtext datasets, please install torchdata following instructions at .",
                        "markdown"
                    ],
                    [
                        "The vocab object is built based on the train dataset and is used to numericalize\ntokens into tensors. Starting from sequential data, the batchify()\nfunction arranges the dataset into columns, trimming off any tokens remaining\nafter the data has been divided into batches of size batch_size.\nFor instance, with the alphabet as the sequence (total length of 26)\nand a batch size of 4, we would divide the alphabet into 4 sequences of\nlength 6:\n\n\\[\\begin{bmatrix}\n\\text{A} &amp; \\text{B} &amp; \\text{C} &amp; \\ldots &amp; \\text{X} &amp; \\text{Y} &amp; \\text{Z}\n\\end{bmatrix}\n\\Rightarrow\n\\begin{bmatrix}\n\\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n\\end{bmatrix}\n\n\\]",
                        "markdown"
                    ],
                    [
                        "These columns are treated as independent by the model, which means that\nthe dependence of G and F can not be learned, but allows more\nefficient batch processing.",
                        "markdown"
                    ],
                    [
                        "import torch\nfrom torchtext.datasets import \nfrom torchtext.data.utils import \nfrom torchtext.vocab import \n\ntrain_iter = (split='train')\ntokenizer = ('basic_english')\n = (map(tokenizer, train_iter), specials=[\"&lt;unk&gt;\"])\n([\"&lt;unk&gt;\"])\n\ndef data_process(raw_text_iter):\n  data = [((tokenizer(item)), dtype=) for item in raw_text_iter]\n  return (tuple(filter(lambda t: t.numel() &gt; 0, data)))\n\ntrain_iter, val_iter, test_iter = ()\n = data_process(train_iter)\n = data_process(val_iter)\n = data_process(test_iter)\n\ndevice = (\"cuda\")\n\ndef batchify(data, bsz):\n    # Divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\nbatch_size = 20\neval_batch_size = 10\n = batchify(, batch_size)\n = batchify(, eval_batch_size)\n = batchify(, eval_batch_size)",
                        "code"
                    ],
                    {
                        "Functions to generate input and target sequence": [
                            [
                                "get_batch() function generates the input and target sequence for\nthe transformer model. It subdivides the source data into chunks of\nlength bptt. For the language modeling task, the model needs the\nfollowing words as Target. For example, with a bptt value of 2,\nwe\u2019d get the following two Variables for i = 0:\n<img alt=\"../_images/transformer_input_target.png\" src=\"../_images/transformer_input_target.png\"/>",
                                "markdown"
                            ],
                            [
                                "It should be noted that the chunks are along dimension 0, consistent\nwith the S dimension in the Transformer model. The batch dimension\nN is along dimension 1.",
                                "markdown"
                            ],
                            [
                                "bptt = 25\ndef get_batch(source, i):\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].view(-1)\n    # Need batch dimension first for pipeline parallelism.\n    return data.t(), target",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Model scale and Pipe initialization": [
                    [
                        "To demonstrate training large Transformer models using pipeline parallelism,\nwe scale up the Transformer layers appropriately. We use an embedding\ndimension of 4096, hidden size of 4096, 16 attention heads and 12 total\ntransformer layers (nn.TransformerEncoderLayer). This creates a model with\n<strong>~1.4 billion</strong> parameters.",
                        "markdown"
                    ],
                    [
                        "We need to initialize the \nsince Pipe depends on the RPC framework via \nwhich allows for future expansion to cross host pipelining. We need to\ninitialize the RPC framework with only a single worker since we\u2019re using a\nsingle process to drive multiple GPUs.",
                        "markdown"
                    ],
                    [
                        "The pipeline is then initialized with 8 transformer layers on one GPU and 8\ntransformer layers on the other GPU.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "For efficiency purposes we ensure that the nn.Sequential passed to\nPipe only consists of two elements (corresponding to two GPUs), this\nallows the Pipe to work with only two partitions and avoid any\ncross-partition overheads.",
                        "markdown"
                    ],
                    [
                        "ntokens = len() # the size of vocabulary\nemsize = 4096 # embedding dimension\nnhid = 4096 # the dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 12 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 16 # the number of heads in the multiheadattention models\ndropout = 0.2 # the dropout value\n\nfrom torch.distributed import rpc\ntmpfile = tempfile.NamedTemporaryFile()\n(\n    name=\"worker\",\n    rank=0,\n    world_size=1,\n    rpc_backend_options=(\n        init_method=\"file://{}\".format(tmpfile.name),\n        # Specifying _transports and _channels is a workaround and we no longer\n        # will have to specify _transports and _channels for PyTorch\n        # versions &gt;= 1.8.1\n        _transports=[\"ibv\", \"uv\"],\n        _channels=[\"cuda_ipc\", \"cuda_basic\"],\n    )\n)\n\nnum_gpus = 2\npartition_len = ((nlayers - 1) // num_gpus) + 1\n\n# Add encoder in the beginning.\ntmp_list = [(ntokens, emsize, dropout).cuda(0)]\nmodule_list = []\n\n# Add all the necessary transformer blocks.\nfor i in range(nlayers):\n     = (emsize, nhead, nhid, dropout)\n    if i != 0 and i % (partition_len) == 0:\n        module_list.append((*tmp_list))\n        tmp_list = []\n    device = i // (partition_len)\n    tmp_list.append((device))\n\n# Add decoder in the end.\ntmp_list.append((ntokens, emsize).cuda(num_gpus - 1))\nmodule_list.append((*tmp_list))\n\nfrom torch.distributed.pipeline.sync import \n\n# Build the pipeline.\nchunks = 8\n = ((*module_list), chunks = chunks)\n\n\ndef get_total_params(module: ):\n    total_params = 0\n    for param in module.parameters():\n        total_params += param.numel()\n    return total_params\n\nprint ('Total parameters in model: {:,}'.format(get_total_params()))",
                        "code"
                    ],
                    [
                        "Total parameters in model: 1,444,261,998",
                        "code"
                    ]
                ]
            },
            {
                "Run the model": [
                    [
                        "is applied to track the loss and\n\nimplements stochastic gradient descent method as the optimizer. The initial\nlearning rate is set to 5.0.  is\napplied to adjust the learn rate through epochs. During the\ntraining, we use\n\nfunction to scale all the gradient together to prevent exploding.",
                        "markdown"
                    ],
                    [
                        " = ()\nlr = 5.0 # learning rate\n = ((), lr=lr)\n = (, 1.0, gamma=0.95)\n\nimport time\ndef train():\n    () # Turn on the train mode\n    total_loss = 0.\n    start_time = time.time()\n    ntokens = len()\n\n    # Train only for 50 batches to keep script execution time low.\n    nbatches = min(50 * bptt, .size(0) - 1)\n\n    for batch, i in enumerate(range(0, nbatches, bptt)):\n        data, targets = get_batch(, i)\n        ()\n        # Since the Pipe is only within a single host and process the ``RRef``\n        # returned by forward method is local to this node and can simply\n        # retrieved via ``RRef.local_value()``.\n        output = (data).local_value()\n        # Need to move targets to the device where the output of the\n        # pipeline resides.\n        loss = (output.view(-1, ntokens), targets.cuda(1))\n        loss.backward()\n        ((), 0.5)\n        .step()\n\n        total_loss += loss.item()\n        log_interval = 10\n        if batch % log_interval == 0 and batch &gt; 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n                  'lr {:02.2f} | ms/batch {:5.2f} | '\n                  'loss {:5.2f} | ppl {:8.2f}'.format(\n                    epoch, batch, nbatches // bptt, .get_lr()[0],\n                    elapsed * 1000 / log_interval,\n                    cur_loss, math.exp(cur_loss)))\n            total_loss = 0\n            start_time = time.time()\n\ndef evaluate(eval_model, data_source):\n    eval_model.eval() # Turn on the evaluation mode\n    total_loss = 0.\n    ntokens = len()\n    # Evaluate only for 50 batches to keep script execution time low.\n    nbatches = min(50 * bptt, data_source.size(0) - 1)\n    with ():\n        for i in range(0, nbatches, bptt):\n            data, targets = get_batch(data_source, i)\n            output = eval_model(data).local_value()\n            output_flat = output.view(-1, ntokens)\n            # Need to move targets to the device where the output of the\n            # pipeline resides.\n            total_loss += len(data) * (output_flat, targets.cuda(1)).item()\n    return total_loss / (len(data_source) - 1)",
                        "code"
                    ],
                    [
                        "Loop over epochs. Save the model if the validation loss is the best\nwe\u2019ve seen so far. Adjust the learning rate after each epoch.",
                        "markdown"
                    ],
                    [
                        "best_val_loss = float(\"inf\")\nepochs = 3 # The number of epochs\n = None\n\nfor epoch in range(1, epochs + 1):\n    epoch_start_time = time.time()\n    train()\n    val_loss = evaluate(, )\n    print('-' * 89)\n    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                     val_loss, math.exp(val_loss)))\n    print('-' * 89)\n\n    if val_loss &lt; best_val_loss:\n        best_val_loss = val_loss\n         = \n\n    .step()",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:389: UserWarning:\n\nTo get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n\n| epoch   1 |    10/   50 batches | lr 5.00 | ms/batch 1816.43 | loss 48.60 | ppl 1277713776368723165184.00\n| epoch   1 |    20/   50 batches | lr 5.00 | ms/batch 1692.90 | loss 36.95 | ppl 11126355217385184.00\n| epoch   1 |    30/   50 batches | lr 5.00 | ms/batch 1653.26 | loss 39.14 | ppl 99769255234084784.00\n| epoch   1 |    40/   50 batches | lr 5.00 | ms/batch 1618.83 | loss 43.02 | ppl 4838079630300874752.00\n-----------------------------------------------------------------------------------------\n| end of epoch   1 | time: 92.53s | valid loss  0.92 | valid ppl     2.52\n-----------------------------------------------------------------------------------------\n| epoch   2 |    10/   50 batches | lr 4.51 | ms/batch 1816.67 | loss 37.51 | ppl 19522289362883428.00\n| epoch   2 |    20/   50 batches | lr 4.51 | ms/batch 1656.04 | loss 30.85 | ppl 24890930028738.51\n| epoch   2 |    30/   50 batches | lr 4.51 | ms/batch 1647.42 | loss 28.68 | ppl 2846180117003.57\n| epoch   2 |    40/   50 batches | lr 4.51 | ms/batch 1636.76 | loss 22.94 | ppl 9142782978.44\n-----------------------------------------------------------------------------------------\n| end of epoch   2 | time: 92.37s | valid loss  0.35 | valid ppl     1.43\n-----------------------------------------------------------------------------------------\n| epoch   3 |    10/   50 batches | lr 4.29 | ms/batch 1798.89 | loss 14.40 | ppl 1800868.66\n| epoch   3 |    20/   50 batches | lr 4.29 | ms/batch 1646.24 | loss 10.98 | ppl 58598.56\n| epoch   3 |    30/   50 batches | lr 4.29 | ms/batch 1645.90 | loss 10.66 | ppl 42600.28\n| epoch   3 |    40/   50 batches | lr 4.29 | ms/batch 1645.18 | loss 10.81 | ppl 49456.98\n-----------------------------------------------------------------------------------------\n| end of epoch   3 | time: 92.29s | valid loss  0.21 | valid ppl     1.23\n-----------------------------------------------------------------------------------------",
                        "code"
                    ]
                ]
            },
            {
                "Evaluate the model with the test dataset": [
                    [
                        "Apply the best model to check the result with the test dataset.",
                        "markdown"
                    ],
                    [
                        "test_loss = evaluate(, )\nprint('=' * 89)\nprint('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n    test_loss, math.exp(test_loss)))\nprint('=' * 89)",
                        "code"
                    ],
                    [
                        "=========================================================================================\n| End of training | test loss  0.18 | test ppl     1.20\n=========================================================================================",
                        "code"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 5 minutes  11.429 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Training Transformer models using Distributed Data Parallel and Pipeline Parallelism": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "This tutorial demonstrates how to train a large Transformer model across\nmultiple GPUs using  and\n. This tutorial is an extension of the\n tutorial\nand scales up the same model to demonstrate how Distributed Data Parallel and\nPipeline Parallelism can be used to train Transformer models.",
                "markdown"
            ],
            [
                "Prerequisites:\n<blockquote>",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "",
                "markdown"
            ],
            [
                "</blockquote>",
                "markdown"
            ],
            {
                "Define the model": [
                    [
                        "PositionalEncoding module injects some information about the\nrelative or absolute position of the tokens in the sequence. The\npositional encodings have the same dimension as the embeddings so that\nthe two can be summed. Here, we use sine and cosine functions of\ndifferent frequencies.",
                        "markdown"
                    ],
                    [
                        "import sys\nimport os\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport tempfile\nfrom torch.nn import TransformerEncoder, \n\nclass PositionalEncoding():\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = (p=dropout)\n\n        pe = (max_len, d_model)\n        position = (0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = ((0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = (position * div_term)\n        pe[:, 1::2] = (position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.pe = nn.Parameter(pe, requires_grad=False)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)",
                        "code"
                    ],
                    [
                        "In this tutorial, we will split a Transformer model across two GPUs and use\npipeline parallelism to train the model. In addition to this, we use\n\nto train two replicas of this pipeline. We have one process driving a pipe across\nGPUs 0 and 1 and another process driving a pipe across GPUs 2 and 3. Both these\nprocesses then use Distributed Data Parallel to train the two replicas. The\nmodel is exactly the same model used in the  tutorial,\nbut is split into two stages. The largest number of parameters belong to the\n layer.\nThe \nitself consists of nlayers of .\nAs a result, our focus is on nn.TransformerEncoder and we split the model\nsuch that half of the nn.TransformerEncoderLayer are on one GPU and the\nother half are on another. To do this, we pull out the Encoder and\nDecoder sections into seperate modules and then build an nn.Sequential\nrepresenting the original Transformer module.",
                        "markdown"
                    ],
                    [
                        "if sys.platform == 'win32':\n    print('Windows platform is not supported for pipeline parallelism')\n    sys.exit(0)\nif () &lt; 4:\n    print('Need at least four GPU devices for this tutorial')\n    sys.exit(0)\n\nclass Encoder():\n    def __init__(self, ntoken, ninp, dropout=0.5):\n        super(Encoder, self).__init__()\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        self.encoder = (ntoken, ninp)\n        self.ninp = ninp\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        # Need (S, N) format for encoder.\n        src = src.t()\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        return self.pos_encoder(src)\n\nclass Decoder():\n    def __init__(self, ntoken, ninp):\n        super(Decoder, self).__init__()\n        self.decoder = (ninp, ntoken)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, inp):\n        # Need batch dimension first for output of pipeline.\n        return self.decoder(inp).permute(1, 0, 2)",
                        "code"
                    ]
                ]
            },
            {
                "Start multiple processes for training": [
                    [
                        "We start two processes where each process drives its own pipeline across two\nGPUs. run_worker is executed for each process.",
                        "markdown"
                    ],
                    [
                        "def run_worker(rank, world_size):",
                        "code"
                    ]
                ]
            },
            {
                "Load and batch data": [
                    [
                        "The training process uses Wikitext-2 dataset from torchtext.\nTo access torchtext datasets, please install torchdata following instructions at .",
                        "markdown"
                    ],
                    [
                        "The vocab object is built based on the train dataset and is used to numericalize\ntokens into tensors. Starting from sequential data, the batchify()\nfunction arranges the dataset into columns, trimming off any tokens remaining\nafter the data has been divided into batches of size batch_size.\nFor instance, with the alphabet as the sequence (total length of 26)\nand a batch size of 4, we would divide the alphabet into 4 sequences of\nlength 6:\n\n\\[\\begin{bmatrix}\n\\text{A} &amp; \\text{B} &amp; \\text{C} &amp; \\ldots &amp; \\text{X} &amp; \\text{Y} &amp; \\text{Z}\n\\end{bmatrix}\n\\Rightarrow\n\\begin{bmatrix}\n\\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &amp;\n\\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n\\end{bmatrix}\n\n\\]",
                        "markdown"
                    ],
                    [
                        "These columns are treated as independent by the model, which means that\nthe dependence of G and F can not be learned, but allows more\nefficient batch processing.",
                        "markdown"
                    ],
                    [
                        "# In 'run_worker'\n    def print_with_rank(msg):\n        print('[RANK {}]: {}'.format(rank, msg))\n\n    from torchtext.datasets import \n    from torchtext.data.utils import \n    from torchtext.vocab import \n\n    train_iter = (split='train')\n    tokenizer = ('basic_english')\n    vocab = (map(tokenizer, train_iter), specials=[\"&lt;unk&gt;\"])\n    vocab.set_default_index(vocab[\"&lt;unk&gt;\"])\n\n    def data_process(raw_text_iter):\n      data = [(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n      return (tuple(filter(lambda t: t.numel() &gt; 0, data)))\n\n    train_iter, val_iter, test_iter = ()\n    train_data = data_process(train_iter)\n    val_data = data_process(val_iter)\n    test_data = data_process(test_iter)\n\n    device = (2 * rank)\n\n    def batchify(data, bsz, rank, world_size, is_train=False):\n        # Divide the dataset into bsz parts.\n        nbatch = data.size(0) // bsz\n        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n        data = data.narrow(0, 0, nbatch * bsz)\n        # Evenly divide the data across the bsz batches.\n        data = data.view(bsz, -1).t().contiguous()\n        # Divide the data across the ranks only for training data.\n        if is_train:\n            data_per_rank = data.size(0) // world_size\n            data = data[rank * data_per_rank : (rank + 1) * data_per_rank]\n        return data.to(device)\n\n    batch_size = 20\n    eval_batch_size = 10\n    train_data = batchify(train_data, batch_size, rank, world_size, True)\n    val_data = batchify(val_data, eval_batch_size, rank, world_size)\n    test_data = batchify(test_data, eval_batch_size, rank, world_size)",
                        "code"
                    ],
                    {
                        "Functions to generate input and target sequence": [
                            [
                                "get_batch() function generates the input and target sequence for\nthe transformer model. It subdivides the source data into chunks of\nlength bptt. For the language modeling task, the model needs the\nfollowing words as Target. For example, with a bptt value of 2,\nwe\u2019d get the following two Variables for i = 0:\n<img alt=\"../_images/transformer_input_target.png\" src=\"../_images/transformer_input_target.png\"/>",
                                "markdown"
                            ],
                            [
                                "It should be noted that the chunks are along dimension 0, consistent\nwith the S dimension in the Transformer model. The batch dimension\nN is along dimension 1.",
                                "markdown"
                            ],
                            [
                                "# In 'run_worker'\n    bptt = 35\n    def get_batch(source, i):\n        seq_len = min(bptt, len(source) - 1 - i)\n        data = source[i:i+seq_len]\n        target = source[i+1:i+1+seq_len].view(-1)\n        # Need batch dimension first for pipeline parallelism.\n        return data.t(), target",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "Model scale and Pipe initialization": [
                    [
                        "To demonstrate training large Transformer models using pipeline parallelism,\nwe scale up the Transformer layers appropriately. We use an embedding\ndimension of 4096, hidden size of 4096, 16 attention heads and 8 total\ntransformer layers (nn.TransformerEncoderLayer). This creates a model with\n<strong>~1 billion</strong> parameters.",
                        "markdown"
                    ],
                    [
                        "We need to initialize the \nsince Pipe depends on the RPC framework via \nwhich allows for future expansion to cross host pipelining. We need to\ninitialize the RPC framework with only a single worker since we\u2019re using a\nsingle process to drive multiple GPUs.",
                        "markdown"
                    ],
                    [
                        "The pipeline is then initialized with 8 transformer layers on one GPU and 8\ntransformer layers on the other GPU. One pipe is setup across GPUs 0 and 1 and\nanother across GPUs 2 and 3. Both pipes are then replicated using DistributedDataParallel.",
                        "markdown"
                    ],
                    [
                        "# In 'run_worker'\n    ntokens = len(vocab) # the size of vocabulary\n    emsize = 4096 # embedding dimension\n    nhid = 4096 # the dimension of the feedforward network model in nn.TransformerEncoder\n    nlayers = 8 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n    nhead = 16 # the number of heads in the multiheadattention models\n    dropout = 0.2 # the dropout value\n\n    from torch.distributed import rpc\n    tmpfile = tempfile.NamedTemporaryFile()\n    (\n        name=\"worker\",\n        rank=0,\n        world_size=1,\n        rpc_backend_options=(\n            init_method=\"file://{}\".format(tmpfile.name),\n            # Specifying _transports and _channels is a workaround and we no longer\n            # will have to specify _transports and _channels for PyTorch\n            # versions &gt;= 1.8.1\n            _transports=[\"ibv\", \"uv\"],\n            _channels=[\"cuda_ipc\", \"cuda_basic\"],\n        )\n    )\n\n    # Num gpus for model parallelism.\n    num_gpus = 2\n    partition_len = ((nlayers - 1) // num_gpus) + 1\n\n    # Add encoder in the beginning.\n    tmp_list = [Encoder(ntokens, emsize, dropout).cuda(2 * rank)]\n    module_list = []\n\n    # Add all the necessary transformer blocks.\n    for i in range(nlayers):\n        transformer_block = (emsize, nhead, nhid, dropout)\n        if i != 0 and i % (partition_len) == 0:\n            module_list.append((*tmp_list))\n            tmp_list = []\n        device = i // (partition_len)\n        tmp_list.append(transformer_block.to(2 * rank + device))\n\n    # Add decoder in the end.\n    tmp_list.append(Decoder(ntokens, emsize).cuda(2 * rank + num_gpus - 1))\n    module_list.append((*tmp_list))\n\n    # Need to use 'checkpoint=never' since as of PyTorch 1.8, Pipe checkpointing\n    # doesn't work with DDP.\n    from torch.distributed.pipeline.sync import \n    chunks = 8\n    model = ((\n        *module_list), chunks = chunks, checkpoint=\"never\")\n\n    # Initialize process group and wrap model in DDP.\n    from torch.nn.parallel import \n    import torch.distributed as dist\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    (\n                backend=\"nccl\", rank=rank, world_size=world_size)\n    model = (model)\n\n    def get_total_params(module: ):\n        total_params = 0\n        for param in module.parameters():\n            total_params += param.numel()\n        return total_params\n\n    print_with_rank('Total parameters in model: {:,}'.format(get_total_params(model)))",
                        "code"
                    ]
                ]
            },
            {
                "Run the model": [
                    [
                        "is applied to track the loss and\n\nimplements stochastic gradient descent method as the optimizer. The initial\nlearning rate is set to 5.0.  is\napplied to adjust the learn rate through epochs. During the\ntraining, we use\n\nfunction to scale all the gradient together to prevent exploding.",
                        "markdown"
                    ],
                    [
                        "# In 'run_worker'\n    criterion = ()\n    lr = 5.0 # learning rate\n    optimizer = (model.parameters(), lr=lr)\n    scheduler = (optimizer, 1.0, gamma=0.95)\n\n    import time\n    def train():\n        model.train() # Turn on the train mode\n        total_loss = 0.\n        start_time = time.time()\n        ntokens = len(vocab)\n\n        # Train only for 50 batches to keep script execution time low.\n        nbatches = min(50 * bptt, train_data.size(0) - 1)\n\n        for batch, i in enumerate(range(0, nbatches, bptt)):\n            data, targets = get_batch(train_data, i)\n            optimizer.zero_grad()\n            # Since the Pipe is only within a single host and process the ``RRef``\n            # returned by forward method is local to this node and can simply\n            # retrieved via ``RRef.local_value()``.\n            output = model(data).local_value()\n            # Need to move targets to the device where the output of the\n            # pipeline resides.\n            loss = criterion(output.view(-1, ntokens), targets.cuda(2 * rank + 1))\n            loss.backward()\n            (model.parameters(), 0.5)\n            optimizer.step()\n\n            total_loss += loss.item()\n            log_interval = 10\n            if batch % log_interval == 0 and batch &gt; 0:\n                cur_loss = total_loss / log_interval\n                elapsed = time.time() - start_time\n                print_with_rank('| epoch {:3d} | {:5d}/{:5d} batches | '\n                      'lr {:02.2f} | ms/batch {:5.2f} | '\n                      'loss {:5.2f} | ppl {:8.2f}'.format(\n                        epoch, batch, nbatches // bptt, scheduler.get_last_lr()[0],\n                        elapsed * 1000 / log_interval,\n                        cur_loss, math.exp(cur_loss)))\n                total_loss = 0\n                start_time = time.time()\n\n    def evaluate(eval_model, data_source):\n        eval_model.eval() # Turn on the evaluation mode\n        total_loss = 0.\n        ntokens = len(vocab)\n        # Evaluate only for 50 batches to keep script execution time low.\n        nbatches = min(50 * bptt, data_source.size(0) - 1)\n        with ():\n            for i in range(0, nbatches, bptt):\n                data, targets = get_batch(data_source, i)\n                output = eval_model(data).local_value()\n                output_flat = output.view(-1, ntokens)\n                # Need to move targets to the device where the output of the\n                # pipeline resides.\n                total_loss += len(data) * criterion(output_flat, targets.cuda(2 * rank + 1)).item()\n        return total_loss / (len(data_source) - 1)",
                        "code"
                    ],
                    [
                        "Loop over epochs. Save the model if the validation loss is the best\nwe\u2019ve seen so far. Adjust the learning rate after each epoch.",
                        "markdown"
                    ],
                    [
                        "# In 'run_worker'\n    best_val_loss = float(\"inf\")\n    epochs = 3 # The number of epochs\n    best_model = None\n\n    for epoch in range(1, epochs + 1):\n        epoch_start_time = time.time()\n        train()\n        val_loss = evaluate(model, val_data)\n        print_with_rank('-' * 89)\n        print_with_rank('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                         val_loss, math.exp(val_loss)))\n        print_with_rank('-' * 89)\n\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            best_model = model\n\n        scheduler.step()",
                        "code"
                    ]
                ]
            },
            {
                "Evaluate the model with the test dataset": [
                    [
                        "Apply the best model to check the result with the test dataset.",
                        "markdown"
                    ],
                    [
                        "# In 'run_worker'\n    test_loss = evaluate(best_model, test_data)\n    print_with_rank('=' * 89)\n    print_with_rank('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n        test_loss, math.exp(test_loss)))\n    print_with_rank('=' * 89)\n\n# Main execution\nimport torch.multiprocessing as mp\n\nif __name__==\"__main__\":\n    world_size = 2\n    (run_worker, args=(world_size, ), nprocs=world_size, join=True)",
                        "code"
                    ]
                ]
            },
            {
                "Output": [
                    [
                        "[RANK 0]: | epoch   1 |    10/   50 batches | lr 5.00 | ms/batch 778.97 | loss 43.31 | ppl 6432469059895903232.00\n[RANK 1]: | epoch   1 |    10/   50 batches | lr 5.00 | ms/batch 778.90 | loss 44.50 | ppl 21245447128217366528.00\n[RANK 0]: | epoch   1 |    20/   50 batches | lr 5.00 | ms/batch 699.89 | loss 44.50 | ppl 21176949187407757312.00\n[RANK 1]: | epoch   1 |    20/   50 batches | lr 5.00 | ms/batch 699.87 | loss 44.62 | ppl 23975861229620961280.00\n[RANK 0]: | epoch   1 |    30/   50 batches | lr 5.00 | ms/batch 698.86 | loss 41.62 | ppl 1193312915629888256.00\n[RANK 1]: | epoch   1 |    30/   50 batches | lr 5.00 | ms/batch 698.87 | loss 40.69 | ppl 471605759847546240.00\n[RANK 0]: | epoch   1 |    40/   50 batches | lr 5.00 | ms/batch 698.34 | loss 45.20 | ppl 42812308420836458496.00\n[RANK 1]: | epoch   1 |    40/   50 batches | lr 5.00 | ms/batch 698.33 | loss 45.68 | ppl 68839569686012223488.00\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 1]: | end of epoch   1 | time: 40.08s | valid loss  0.80 | valid ppl     2.22\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 0]: | end of epoch   1 | time: 40.09s | valid loss  0.80 | valid ppl     2.22\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 0]: | epoch   2 |    10/   50 batches | lr 4.75 | ms/batch 768.51 | loss 36.34 | ppl 6063529544668166.00\n[RANK 1]: | epoch   2 |    10/   50 batches | lr 4.75 | ms/batch 769.23 | loss 37.41 | ppl 17651211266236086.00\n[RANK 0]: | epoch   2 |    20/   50 batches | lr 4.75 | ms/batch 699.57 | loss 28.97 | ppl 3798441739584.11\n[RANK 1]: | epoch   2 |    20/   50 batches | lr 4.75 | ms/batch 699.56 | loss 29.28 | ppl 5203636967575.47\n[RANK 0]: | epoch   2 |    30/   50 batches | lr 4.75 | ms/batch 699.04 | loss 28.43 | ppl 2212498693571.25\n[RANK 1]: | epoch   2 |    30/   50 batches | lr 4.75 | ms/batch 699.05 | loss 28.33 | ppl 2015144761281.48\n[RANK 0]: | epoch   2 |    40/   50 batches | lr 4.75 | ms/batch 699.10 | loss 23.30 | ppl 13121380184.92\n[RANK 1]: | epoch   2 |    40/   50 batches | lr 4.75 | ms/batch 699.09 | loss 23.41 | ppl 14653799192.87\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 0]: | end of epoch   2 | time: 39.97s | valid loss  0.24 | valid ppl     1.27\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 1]: | end of epoch   2 | time: 39.98s | valid loss  0.24 | valid ppl     1.27\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 0]: | epoch   3 |    10/   50 batches | lr 4.51 | ms/batch 769.36 | loss 12.80 | ppl 361681.11\n[RANK 1]: | epoch   3 |    10/   50 batches | lr 4.51 | ms/batch 768.97 | loss 12.57 | ppl 287876.61\n[RANK 0]: | epoch   3 |    20/   50 batches | lr 4.51 | ms/batch 698.27 | loss 12.01 | ppl 164364.60\n[RANK 1]: | epoch   3 |    20/   50 batches | lr 4.51 | ms/batch 698.30 | loss 11.98 | ppl 159095.89\n[RANK 0]: | epoch   3 |    30/   50 batches | lr 4.51 | ms/batch 697.75 | loss 10.90 | ppl 54261.91\n[RANK 1]: | epoch   3 |    30/   50 batches | lr 4.51 | ms/batch 697.72 | loss 10.89 | ppl 53372.39\n[RANK 0]: | epoch   3 |    40/   50 batches | lr 4.51 | ms/batch 699.49 | loss 10.78 | ppl 47948.35\n[RANK 1]: | epoch   3 |    40/   50 batches | lr 4.51 | ms/batch 699.50 | loss 10.79 | ppl 48664.42\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 0]: | end of epoch   3 | time: 39.96s | valid loss  0.38 | valid ppl     1.46\n[RANK 0]: -----------------------------------------------------------------------------------------\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 1]: | end of epoch   3 | time: 39.96s | valid loss  0.38 | valid ppl     1.46\n[RANK 1]: -----------------------------------------------------------------------------------------\n[RANK 0]: =========================================================================================\n[RANK 0]: | End of training | test loss  0.33 | test ppl     1.39\n[RANK 0]: =========================================================================================\n[RANK 1]: =========================================================================================\n[RANK 1]: | End of training | test loss  0.33 | test ppl     1.39\n[RANK 1]: =========================================================================================",
                        "code"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Distributed Training with Uneven Inputs Using the Join Context Manager": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                " View and edit this tutorial in .",
                "markdown"
            ],
            [
                "Note",
                "markdown"
            ],
            [
                "Join is introduced in PyTorch 1.10 as a prototype feature. This\nAPI is subject to change.",
                "markdown"
            ],
            [
                "In this tutorial, you will see:",
                "markdown"
            ],
            [
                "An overview of the  context manager.",
                "markdown"
            ],
            [
                "An example of how to use the context manager with DistributedDataParallel.",
                "markdown"
            ],
            [
                "An example of how to use the context manager with both\nDistributedDataParallel and ZeroRedundancyOptimizer.",
                "markdown"
            ],
            [
                "An example of passing in keyword arguments to the context manager.",
                "markdown"
            ],
            [
                "A dive into how the  context manager works.",
                "markdown"
            ],
            [
                "An example showing how to make a toy class compatible with the context\nmanager.",
                "markdown"
            ],
            {
                "Requirements": [
                    [
                        "PyTorch 1.10+",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            },
            {
                "What is Join?": [
                    [
                        "In , you saw\nthe general skeleton for using  to perform data\nparallel training. This implicitly schedules all-reduces in each backward pass\nto synchronize gradients across ranks. Such  require participation\nfrom all ranks in the process group, so if a rank has fewer inputs, then the\nother ranks will hang or error (depending on the backend). More generally, this\nproblem persists for any class that performs per-iteration synchronous\ncollective communications.",
                        "markdown"
                    ],
                    [
                        "Join is a context manager to be used around your per-rank training loop to\nfacilitate training with uneven inputs. The context manager allows the ranks\nthat exhaust their inputs early (i.e. <em>join</em> early) to shadow the collective\ncommunications performed by those that have not yet joined. The ways in which\nthe communications are shadowed are specified by hooks.",
                        "markdown"
                    ]
                ]
            },
            {
                "Using Join with DistributedDataParallel": [
                    [
                        "PyTorch\u2019s  works out-of-the-box with the Join\ncontext manager. Here is an example usage:",
                        "markdown"
                    ],
                    [
                        "import os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.distributed.algorithms.join import Join\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nBACKEND = \"nccl\"\nWORLD_SIZE = 2\nNUM_INPUTS = 5\n\ndef worker(rank):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(BACKEND, rank=rank, world_size=WORLD_SIZE)\n\n    model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])\n    # Rank 1 gets one more input than rank 0\n    inputs = [torch.tensor([1]).float() for _ in range(NUM_INPUTS + rank)]\n\n    num_inputs = 0\n    with Join([model]):\n        for input in inputs:\n            num_inputs += 1\n            loss = model(input).sum()\n            loss.backward()\n\n    print(f\"Rank {rank} has exhausted all {num_inputs} of its inputs!\")\n\ndef main():\n    mp.spawn(worker, nprocs=WORLD_SIZE, join=True)\n\nif __name__ == \"__main__\":\n    main()",
                        "code"
                    ],
                    [
                        "This produces the following output (where the print() s from rank 0 and\nrank 1 may be arbitrarily ordered):",
                        "markdown"
                    ],
                    [
                        "Rank 0 has exhausted all 5 of its inputs!\nRank 1 has exhausted all 6 of its inputs!",
                        "code"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        " provided its own  context manager\nprior to the introduction of this generic Join context manager. In the\nabove example, using with Join([model]): is equivalent to using\nwith model.join():. One limitation of the existing\nDistributedDataParallel.join() is that it does not allow multiple\nparticipating classes, e.g. DistributedDataParallel and\n together.",
                        "markdown"
                    ]
                ]
            },
            {
                "Using Join with DistributedDataParallel and ZeroRedundancyOptimizer": [
                    [
                        "The Join context manager works not only with a single class but also with\nmultiple classes together. PyTorch\u2019s ZeroRedundancyOptimizer is also\ncompatible with the context manager, so here, we examine how to modify the\nprevious example to use both DistributedDataParallel and\nZeroRedundancyOptimizer:",
                        "markdown"
                    ],
                    [
                        "from torch.distributed.optim import ZeroRedundancyOptimizer as ZeRO\nfrom torch.optim import Adam\n\ndef worker(rank):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(BACKEND, rank=rank, world_size=WORLD_SIZE)\n\n    model = DDP(torch.nn.Linear(1, 1).to(rank), device_ids=[rank])\n    optim = ZeRO(model.parameters(), Adam, lr=0.01)\n    # Rank 1 gets one more input than rank 0\n    inputs = [torch.tensor([1]).float() for _ in range(NUM_INPUTS + rank)]\n\n    num_inputs = 0\n    # Pass both `model` and `optim` into `Join()`\n    with Join([model, optim]):\n        for input in inputs:\n            num_inputs += 1\n            loss = model(input).sum()\n            loss.backward()\n            optim.step()\n\n    print(f\"Rank {rank} has exhausted all {num_inputs} of its inputs!\")",
                        "code"
                    ],
                    [
                        "This will yield the same output as before. The notable change was\nadditionally passing in the ZeroRedundancyOptimizer instance into\nJoin().",
                        "markdown"
                    ]
                ]
            },
            {
                "Passing Keyword Arguments": [
                    [
                        "Classes may provide keyword arguments that modify their behavior in the context\nmanager at run time. For example, DistributedDataParallel provides an\nargument divide_by_initial_world_size, which determines if gradients are\ndivided by the initial world size or by the effective world size (i.e. number\nof non-joined ranks). Such keyword arguments can be passed directly into the\ncontext manager.",
                        "markdown"
                    ],
                    [
                        "with Join([model, optim], divide_by_initial_world_size=False):\n    for input in inputs:\n        ...",
                        "code"
                    ],
                    [
                        "Warning",
                        "markdown"
                    ],
                    [
                        "The keyword arguments passed into the context manager are shared across\nall participating classes. This should not be a limitation since we do\nnot expect cases where multiple Joinable s need differing settings\nof the same argument. Nonetheless, this is something to keep in mind.",
                        "markdown"
                    ]
                ]
            },
            {
                "How Does Join Work?": [
                    [
                        "Now that we have seen some preliminary examples of how to use the Join\ncontext manager, let us delve deeper into how it works. This will provide a\ngreater insight into the full capability that it offers and prepare you to make\nyour own custom classes compatible. Here, we will go over the Join class as\nwell as the supporting classes Joinable and JoinHook.",
                        "markdown"
                    ],
                    {
                        "Joinable": [
                            [
                                "To begin, classes compatible with the Join context manager must inherit\nfrom the abstract base class Joinable. In particular, a Joinable must\nimplement:",
                                "markdown"
                            ],
                            [
                                "join_hook(self, **kwargs) -&gt; JoinHook",
                                "markdown"
                            ],
                            [
                                "This returns the JoinHook instance for the Joinable, determining how\njoined processes should shadow the per-iteration collective communications\nperformed by the Joinable.",
                                "markdown"
                            ],
                            [
                                "join_device(self) -&gt; torch.device",
                                "markdown"
                            ],
                            [
                                "This returns a device to be used by the Join context manager to perform\ncollective communications, e.g. torch.device(\"cuda:0\") or\ntorch.device(\"cpu\").",
                                "markdown"
                            ],
                            [
                                "join_process_group(self) -&gt; ProcessGroup",
                                "markdown"
                            ],
                            [
                                "This returns the process group to be used by the Join context manager to\nperform collective communications.",
                                "markdown"
                            ],
                            [
                                "In particular, the join_device and join_process_group are required\nattributes to ensure that the context manager can schedule collective\ncommunications between joined and non-joined processes. One usage is to count\nthe number of non-joined processes on each iteration using an all-reduce.\nAnother usage is for implementing the mechanism required for\nthrow_on_early_termination=True, which we will explain later below.",
                                "markdown"
                            ],
                            [
                                "DistributedDataParallel and ZeroRedundancyOptimizer already inherit\nfrom Joinable and implement the above methods, which is why we could\ndirectly use them in the previous examples.",
                                "markdown"
                            ],
                            [
                                "Joinable classes should make sure to call the Joinable constructor\nsince it initializes a JoinConfig instance, which is used internally by\nthe context manager to ensure correctness. This will be saved in each\nJoinable as a field _join_config.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "JoinHook": [
                            [
                                "Next, let us break down the JoinHook class. A JoinHook provides two\nentry points into a context manager:",
                                "markdown"
                            ],
                            [
                                "main_hook(self) -&gt; None",
                                "markdown"
                            ],
                            [
                                "This hook is called repeatedly by each joined rank while there exists a rank\nthat has not yet joined. It is meant to shadow the collective communications\nperformed by the Joinable in each training iteration (e.g. in one forward\npass, backward pass, and optimizer step).",
                                "markdown"
                            ],
                            [
                                "post_hook(self, is_last_joiner: bool) -&gt; None",
                                "markdown"
                            ],
                            [
                                "This hook is called once all ranks have joined. It is passed an additional\nbool argument is_last_joiner, which indicates if the rank was one of\nthe last to join. The argument may be useful for synchronization.",
                                "markdown"
                            ],
                            [
                                "To give concrete examples of what these hooks may look like, the provided\nZeroRedundancyOptimizer main hook performs an optimizer step per normal\nsince the joined rank is still responsible for updating and synchronizing its\nshard of the parameters, and the provided DistributedDataParallel post-hook\nbroadcasts the final updated model from one of the last joining ranks to ensure\nthat it is the same across all ranks.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Join": [
                            [
                                "Finally, let us examine how these fit into the Join class itself.",
                                "markdown"
                            ],
                            [
                                "__init__(self, joinables: List[Joinable], enable: bool = True, throw_on_early_termination: bool = False)",
                                "markdown"
                            ],
                            [
                                "As we saw in the previous examples, the constructor takes in a list of the\nJoinable s that participate in the training loop. These should be the\nclasses that perform collective communications in each iteration.",
                                "markdown"
                            ],
                            [
                                "enable is a bool that can be set to False if you know that there\nwill not be uneven inputs, in which case the context manager becomes vacuous\nsimilar to contextlib.nullcontext(). This also may disable join-related\ncomputation in the participating Joinable s.",
                                "markdown"
                            ],
                            [
                                "throw_on_early_termination is a bool that can be set to True to\nhave each rank raise an exception the moment that uneven inputs are detected.\nThis is useful for cases that do not conform to the context manager\u2019s\nrequirements, which is most typically when there are collective communications\nfrom different classes that may be arbitrarily interleaved, such as when using\nDistributedDataParallel with a model that has SyncBatchNorm layers. In\nsuch cases, this argument should be set to True so that the application\nlogic can catch the exception and determine how to proceed.",
                                "markdown"
                            ],
                            [
                                "The core logic occurs in the __exit__() method, which loops while there\nexists a non-joined rank, calling each Joinable \u2018s main hook, and\nthen once all ranks have joined, calls their post hooks. Both the main hooks\nand post-hooks are iterated over in the order that the Joinable s are\npassed in.",
                                "markdown"
                            ],
                            [
                                "The context manager requires a heartbeat from non-joined processes. As such,\neach Joinable class should make a call to Join.notify_join_context()\nbefore its per-iteration collective communications. The context manager will\nensure that only the first Joinable passed in actually sends the\nheartbeat.",
                                "markdown"
                            ],
                            [
                                "Warning",
                                "markdown"
                            ],
                            [
                                "As mentioned above regarding throw_on_early_termination, the\nJoin context manager is not compatible with certain compositions of\nclasses. The Joinable \u2018s JoinHook s must be serializable since each\nhook is fully executed before proceeding to the next. In other words, two\nhooks cannot overlap. Moreover, currently, both the main hooks and post-\nhooks are iterated over in the same deterministic order. If this appears to\nbe a major limitation, we may modify the API to permit a customizable\nordering.",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Making a Toy Class Work with Join": [
                    [
                        "Since the previous section introduced several concepts, let us see them in\npractice with a toy example. Here, we will implement a class that counts the\nnumber of inputs that are seen across all ranks before its rank joins. This\nshould provide a basic idea of how you may make your own class compatible\nwith the Join context manager.",
                        "markdown"
                    ],
                    [
                        "Specifically, the following code has each rank print out (1) the number of\ninputs across all ranks that seen before it joins and (2) the total number\nof inputs across all ranks.",
                        "markdown"
                    ],
                    [
                        "import os\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.distributed.algorithms.join import Join, Joinable, JoinHook\n\nBACKEND = \"nccl\"\nWORLD_SIZE = 2\nNUM_INPUTS = 5\n\nclass CounterJoinHook(JoinHook):\n    r\"\"\"\n    Join hook for :class:`Counter`.\n\n    Arguments:\n        counter (Counter): the :class:`Counter` object using this hook.\n        sync_max_count (bool): whether to sync the max count once all ranks\n            join.\n    \"\"\"\n    def __init__(\n        self,\n        counter,\n        sync_max_count\n    ):\n        self.counter = counter\n        self.sync_max_count = sync_max_count\n\n    def main_hook(self):\n        r\"\"\"\n        Shadows the counter's all-reduce by all-reducing a dim-1 zero tensor.\n        \"\"\"\n        t = torch.zeros(1, device=self.counter.device)\n        dist.all_reduce(t)\n\n    def post_hook(self, is_last_joiner: bool):\n        r\"\"\"\n        Synchronizes the max count across all :class:`Counter` s if\n        ``sync_max_count=True``.\n        \"\"\"\n        if not self.sync_max_count:\n            return\n        rank = dist.get_rank(self.counter.process_group)\n        common_rank = self.counter.find_common_rank(rank, is_last_joiner)\n        if rank == common_rank:\n            self.counter.max_count = self.counter.count.detach().clone()\n        dist.broadcast(self.counter.max_count, src=common_rank)\n\nclass Counter(Joinable):\n    r\"\"\"\n    Example :class:`Joinable` that counts the number of training iterations\n    that it participates in.\n    \"\"\"\n    def __init__(self, device, process_group):\n        super(Counter, self).__init__()\n        self.device = device\n        self.process_group = process_group\n        self.count = torch.tensor([0], device=device).float()\n        self.max_count = torch.tensor([0], device=device).float()\n\n    def __call__(self):\n        r\"\"\"\n        Counts the number of inputs processed on this iteration by all ranks\n        by all-reducing a dim-1 one tensor; increments its own internal count.\n        \"\"\"\n        Join.notify_join_context(self)\n        t = torch.ones(1, device=self.device).float()\n        dist.all_reduce(t)\n        self.count += t\n\n    def join_hook(self, **kwargs) -&gt; JoinHook:\n        r\"\"\"\n        Return a join hook that shadows the all-reduce in :meth:`__call__`.\n\n        This join hook supports the following keyword arguments:\n            sync_max_count (bool, optional): whether to synchronize the maximum\n                count across all ranks once all ranks join; default is ``False``.\n        \"\"\"\n        sync_max_count = kwargs.get(\"sync_max_count\", False)\n        return CounterJoinHook(self, sync_max_count)\n\n    @property\n    def join_device(self) -&gt; torch.device:\n        return self.device\n\n    @property\n    def join_process_group(self):\n        return self.process_group\n\n    def find_common_rank(self, rank, to_consider):\n        r\"\"\"\n        Returns the max rank of the ones to consider over the process group.\n        \"\"\"\n        common_rank = torch.tensor([rank if to_consider else -1], device=self.device)\n        dist.all_reduce(common_rank, op=dist.ReduceOp.MAX, group=self.process_group)\n        common_rank = common_rank.item()\n        return common_rank\n\ndef worker(rank):\n    assert torch.cuda.device_count() &gt;= WORLD_SIZE\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29500'\n    dist.init_process_group(BACKEND, rank=rank, world_size=WORLD_SIZE)\n\n    counter = Counter(torch.device(f\"cuda:{rank}\"), dist.group.WORLD)\n    inputs = [torch.tensor([1]).float() for _ in range(NUM_INPUTS + rank)]\n\n    with Join([counter], sync_max_count=True):\n        for _ in inputs:\n            counter()\n\n    print(f\"{int(counter.count.item())} inputs processed before rank {rank} joined!\")\n    print(f\"{int(counter.max_count.item())} inputs processed across all ranks!\")\n\ndef main():\n    mp.spawn(worker, nprocs=WORLD_SIZE, join=True)\n\nif __name__ == \"__main__\":\n    main()",
                        "code"
                    ],
                    [
                        "Since rank 0 sees 5 inputs and rank 1 sees 6, this yields the output:",
                        "markdown"
                    ],
                    [
                        "10 inputs processed before rank 0 joined!\n11 inputs processed across all ranks!\n11 inputs processed before rank 1 joined!\n11 inputs processed across all ranks!",
                        "code"
                    ],
                    [
                        "Some key points to highlight:",
                        "markdown"
                    ],
                    [
                        "A Counter instance performs a single all-reduce per iteration, so the\nmain hook performs a single all-reduce as well to shadow it.",
                        "markdown"
                    ],
                    [
                        "The Counter class makes a call to Join.notify_join_context() at the\nbeginning of its __call__() method since that is a place before its per-\niteration collective communications (i.e. its all-reduce).",
                        "markdown"
                    ],
                    [
                        "The is_last_joiner argument is used to determine the broadcast source in\nthe post-hooks.",
                        "markdown"
                    ],
                    [
                        "We pass in the sync_max_count keyword argument to the context manager,\nwhich is then forwarded to Counter \u2018s join hook.",
                        "markdown"
                    ]
                ]
            }
        ]
    },
    "Mobile": {
        "Image Segmentation DeepLabV3 on iOS": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "<strong>Reviewed by</strong>: ",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "Semantic image segmentation is a computer vision task that uses semantic labels to mark specific regions of an input image. The PyTorch semantic image segmentation  can be used to label image regions with  including, for example, bicycle, bus, car, dog, and person. Image segmentation models can be very useful in applications such as autonomous driving and scene understanding.",
                        "markdown"
                    ],
                    [
                        "In this tutorial, we will provide a step-by-step guide on how to prepare and run the PyTorch DeepLabV3 model on iOS, taking you from the beginning of having a model you may want to use on iOS to the end of having a complete iOS app using the model. We will also cover practical and general tips on how to check if your next favorite pre-trained PyTorch models can run on iOS, and how to avoid pitfalls.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "Before going through this tutorial, you should check out  and give the PyTorch iOS  example app a quick try. This tutorial will go beyond the image classification model, usually the first kind of model deployed on mobile. The complete code repo for this tutorial is available .",
                        "markdown"
                    ]
                ]
            },
            {
                "Learning Objectives": [
                    [
                        "In this tutorial, you will learn how to:",
                        "markdown"
                    ],
                    [
                        "Convert the DeepLabV3 model for iOS deployment.",
                        "markdown"
                    ],
                    [
                        "Get the output of the model for the example input image in Python and compare it to the output from the iOS app.",
                        "markdown"
                    ],
                    [
                        "Build a new iOS app or reuse an iOS example app to load the converted model.",
                        "markdown"
                    ],
                    [
                        "Prepare the input into the format that the model expects and process the model output.",
                        "markdown"
                    ],
                    [
                        "Complete the UI, refactor, build and run the app to see image segmentation in action.",
                        "markdown"
                    ]
                ]
            },
            {
                "Pre-requisites": [
                    [
                        "PyTorch 1.6 or 1.7",
                        "markdown"
                    ],
                    [
                        "torchvision 0.7 or 0.8",
                        "markdown"
                    ],
                    [
                        "Xcode 11 or 12",
                        "markdown"
                    ]
                ]
            },
            {
                "Steps": [
                    {
                        "1. Convert the DeepLabV3 model for iOS deployment": [
                            [
                                "The first step to deploying a model on iOS is to convert the model into the  format.",
                                "markdown"
                            ],
                            [
                                "Note",
                                "markdown"
                            ],
                            [
                                "Not all PyTorch models can be converted to TorchScript at this time because a model definition may use language features that are not in TorchScript, which is a subset of Python. See the  for more details.",
                                "markdown"
                            ],
                            [
                                "Simply run the script below to generate the scripted model <cite>deeplabv3_scripted.pt</cite>:",
                                "markdown"
                            ],
                            [
                                "import torch\n\n# use deeplabv3_resnet50 instead of deeplabv3_resnet101 to reduce the model size\nmodel = torch.hub.load('pytorch/vision:v0.8.0', 'deeplabv3_resnet50', pretrained=True)\nmodel.eval()\n\nscriptedm = torch.jit.script(model)\ntorch.jit.save(scriptedm, \"deeplabv3_scripted.pt\")",
                                "code"
                            ],
                            [
                                "The size of the generated <cite>deeplabv3_scripted.pt</cite> model file should be around 168MB. Ideally, a model should also be quantized for significant size reduction and faster inference before being deployed on an iOS app. To have a general understanding of quantization, see the  and the resource links there. We will cover in detail how to correctly apply a quantization workflow called Post Training  to the DeepLabV3 model in a future tutorial or recipe.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "2. Get example input and output of the model in Python": [
                            [
                                "Now that we have a scripted PyTorch model, let\u2019s test with some example inputs to make sure the model works correctly on iOS. First, let\u2019s write a Python script that uses the model to make inferences and examine inputs and outputs. For this example of the DeepLabV3 model, we can reuse the code in Step 1 and in the . Add the following code snippet to the code above:",
                                "markdown"
                            ],
                            [
                                "from PIL import Image\nfrom torchvision import transforms\ninput_image = Image.open(\"deeplab.jpg\")\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\nwith torch.no_grad():\n    output = model(input_batch)['out'][0]\n\nprint(input_batch.shape)\nprint(output.shape)",
                                "code"
                            ],
                            [
                                "Download <cite>deeplab.jpg</cite> from  and run the script above to see the shapes of the input and output of the model:",
                                "markdown"
                            ],
                            [
                                "torch.Size([1, 3, 400, 400])\ntorch.Size([21, 400, 400])",
                                "code"
                            ],
                            [
                                "So if you provide the same image input <cite>deeplab.jpg</cite> of size 400x400 to the model on iOS, the output of the model should have the size [21, 400, 400]. You should also print out at least the beginning parts of the actual data of the input and output, to be used in Step 4 below to compare with the actual input and output of the model when running in the iOS app.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "3. Build a new iOS app or reuse an example app and load the model": [
                            [
                                "First, follow Step 3 of the  to use our model in an Xcode project with PyTorch Mobile enabled. Because both the DeepLabV3 model used in this tutorial and the MobileNet v2 model used in the PyTorch HelloWorld iOS example are computer vision models, you may choose to start with the  as a template to reuse the code that loads the model and processes the input and output.",
                                "markdown"
                            ],
                            [
                                "Now let\u2019s add <cite>deeplabv3_scripted.pt</cite> and <cite>deeplab.jpg</cite> used in Step 2 to the Xcode project and modify <cite>ViewController.swift</cite> to resemble:",
                                "markdown"
                            ],
                            [
                                "class ViewController: UIViewController {\n    var image = UIImage(named: \"deeplab.jpg\")!\n\n    override func viewDidLoad() {\n        super.viewDidLoad()\n    }\n\n    private lazy var module: TorchModule = {\n        if let filePath = Bundle.main.path(forResource: \"deeplabv3_scripted\",\n              ofType: \"pt\"),\n            let module = TorchModule(fileAtPath: filePath) {\n            return module\n        } else {\n            fatalError(\"Can't load the model file!\")\n        }\n    }()\n}",
                                "code"
                            ],
                            [
                                "Then set a breakpoint at the line <cite>return module</cite> and build and run the app. The app should stop at the breakpoint, meaning that the scripted model in Step 1 has been successfully loaded on iOS.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "4. Process the model input and output for model inference": [
                            [
                                "After the model loads in the previous step, let\u2019s verify that it works with expected inputs and can generate expected outputs. As the model input for the DeepLabV3 model is an image, the same as that of the MobileNet v2 in the HelloWorld example, we will reuse some of the code in the  file from HelloWorld for input processing. Replace the <cite>predictImage</cite> method implementation in <cite>TorchModule.mm</cite> with the following code:",
                                "markdown"
                            ],
                            [
                                "- (unsigned char*)predictImage:(void*)imageBuffer {\n    // 1. the example deeplab.jpg size is size 400x400 and there are 21 semantic classes\n    const int WIDTH = 400;\n    const int HEIGHT = 400;\n    const int CLASSNUM = 21;\n\n    at::Tensor tensor = torch::from_blob(imageBuffer, {1, 3, WIDTH, HEIGHT}, at::kFloat);\n    torch::autograd::AutoGradMode guard(false);\n    at::AutoNonVariableTypeMode non_var_type_mode(true);\n\n    // 2. convert the input tensor to an NSMutableArray for debugging\n    float* floatInput = tensor.data_ptr&lt;float&gt;();\n    if (!floatInput) {\n        return nil;\n    }\n    NSMutableArray* inputs = [[NSMutableArray alloc] init];\n    for (int i = 0; i &lt; 3 * WIDTH * HEIGHT; i++) {\n        [inputs addObject:@(floatInput[i])];\n    }\n\n    // 3. the output of the model is a dictionary of string and tensor, as\n    // specified at https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101\n    auto outputDict = _impl.forward({tensor}).toGenericDict();\n\n    // 4. convert the output to another NSMutableArray for easy debugging\n    auto outputTensor = outputDict.at(\"out\").toTensor();\n    float* floatBuffer = outputTensor.data_ptr&lt;float&gt;();\n    if (!floatBuffer) {\n      return nil;\n    }\n    NSMutableArray* results = [[NSMutableArray alloc] init];\n    for (int i = 0; i &lt; CLASSNUM * WIDTH * HEIGHT; i++) {\n      [results addObject:@(floatBuffer[i])];\n    }\n\n    return nil;\n}",
                                "code"
                            ],
                            [
                                "Note",
                                "markdown"
                            ],
                            [
                                "The model output is a dictionary for the DeepLabV3 model so we use <cite>toGenericDict</cite> to correctly extract the result. For other models, the model output may also be a single tensor or a tuple of tensors, among other things.",
                                "markdown"
                            ],
                            [
                                "With the code changes shown above, you can set breakpoints after the two for loops that populate <cite>inputs</cite> and <cite>results</cite> and compare them with the model input and output data you saw in Step 2 to see if they match. For the same inputs to the models running on iOS and Python, you should get the same outputs.",
                                "markdown"
                            ],
                            [
                                "All we have done so far is to confirm that the model of our interest can be scripted and run correctly in our iOS app as in Python. The steps we walked through so far for using a model in an iOS app consumes the bulk, if not most, of our app development time, similar to how data preprocessing is the heaviest lift for a typical machine learning project.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "5. Complete the UI, refactor, build and run the app": [
                            [
                                "Now we are ready to complete the app and the UI to actually see the processed result as a new image. The output processing code should be like this, added to the end of the code snippet in Step 4 in <cite>TorchModule.mm</cite> - remember to first remove the line <cite>return nil;</cite> temporarily put there to make the code build and run:",
                                "markdown"
                            ],
                            [
                                "// see the 20 semantic classes link in Introduction\nconst int DOG = 12;\nconst int PERSON = 15;\nconst int SHEEP = 17;\n\nNSMutableData* data = [NSMutableData dataWithLength:\n    sizeof(unsigned char) * 3 * WIDTH * HEIGHT];\nunsigned char* buffer = (unsigned char*)[data mutableBytes];\n// go through each element in the output of size [WIDTH, HEIGHT] and\n// set different color for different classnum\nfor (int j = 0; j &lt; WIDTH; j++) {\n    for (int k = 0; k &lt; HEIGHT; k++) {\n        // maxi: the index of the 21 CLASSNUM with the max probability\n        int maxi = 0, maxj = 0, maxk = 0;\n        float maxnum = -100000.0;\n        for (int i = 0; i &lt; CLASSNUM; i++) {\n            if ([results[i * (WIDTH * HEIGHT) + j * WIDTH + k] floatValue] &gt; maxnum) {\n                maxnum = [results[i * (WIDTH * HEIGHT) + j * WIDTH + k] floatValue];\n                maxi = i; maxj = j; maxk = k;\n            }\n        }\n        int n = 3 * (maxj * width + maxk);\n        // color coding for person (red), dog (green), sheep (blue)\n        // black color for background and other classes\n        buffer[n] = 0; buffer[n+1] = 0; buffer[n+2] = 0;\n        if (maxi == PERSON) buffer[n] = 255;\n        else if (maxi == DOG) buffer[n+1] = 255;\n        else if (maxi == SHEEP) buffer[n+2] = 255;\n    }\n}\nreturn buffer;",
                                "code"
                            ],
                            [
                                "The implementation here is based on the understanding of the DeepLabV3 model which outputs a tensor of size [21, width, height] for an input image of width*height. Each element in the width*height output array is a value between 0 and 20 (for a total of 21 semantic labels described in Introduction) and the value is used to set a specific color. Color coding of the segmentation here is based on the class with the highest probability, and you can extend the color coding for all classes in your own dataset.",
                                "markdown"
                            ],
                            [
                                "After the output processing, you will also need to call a helper function to convert the RGB <cite>buffer</cite> to an <cite>UIImage</cite> instance to be shown on <cite>UIImageView</cite>. You can refer to the example code <cite>convertRGBBufferToUIImage</cite> defined in <cite>UIImageHelper.mm</cite> in the code repo.",
                                "markdown"
                            ],
                            [
                                "The UI for this app is also similar to that for HelloWorld, except that you do not need the <cite>UITextView</cite> to show the image classification result. You can also add two buttons <cite>Segment</cite> and <cite>Restart</cite> as shown in the code repo to run the model inference and to show back the original image after the segmentation result is shown.",
                                "markdown"
                            ],
                            [
                                "The last step before we can run the app is to connect all the pieces together. Modify the <cite>ViewController.swift</cite> file to use the <cite>predictImage</cite>, which is refactored and changed to <cite>segmentImage</cite> in the repo, and helper functions you built as shown in the example code in the repo in <cite>ViewController.swift</cite>. Connect the buttons to the actions and you should be good to go.",
                                "markdown"
                            ],
                            [
                                "Now when you run the app on an iOS simulator or an actual iOS device, you will see the following screens:",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Recap": [
                    [
                        "In this tutorial, we described what it takes to convert a pre-trained PyTorch DeepLabV3 model for iOS and how to make sure the model can run successfully on iOS. Our focus was to help you understand the process of confirming that a model can indeed run on iOS. The complete code repo is available .",
                        "markdown"
                    ],
                    [
                        "More advanced topics such as quantization and using models via transfer learning or of your own on iOS will be covered soon in future demo apps and tutorials.",
                        "markdown"
                    ]
                ]
            },
            {
                "Learn More": [
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ],
        "Image Segmentation DeepLabV3 on Android": [
            [
                "<strong>Author</strong>: ",
                "markdown"
            ],
            [
                "<strong>Reviewed by</strong>: ",
                "markdown"
            ],
            {
                "Introduction": [
                    [
                        "Semantic image segmentation is a computer vision task that uses semantic labels to mark specific regions of an input image. The PyTorch semantic image segmentation  can be used to label image regions with  including, for example, bicycle, bus, car, dog, and person. Image segmentation models can be very useful in applications such as autonomous driving and scene understanding.",
                        "markdown"
                    ],
                    [
                        "In this tutorial, we will provide a step-by-step guide on how to prepare and run the PyTorch DeepLabV3 model on Android, taking you from the beginning of having a model you may want to use on Android to the end of having a complete Android app using the model. We will also cover practical and general tips on how to check if your next favorable pre-trained PyTorch models can run on Android, and how to avoid pitfalls.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "Before going through this tutorial, you should check out  and give the PyTorch Android  example app a quick try. This tutorial will go beyond the image classification model, usually the first kind of model deployed on mobile. The complete code repo for this tutorial is available .",
                        "markdown"
                    ]
                ]
            },
            {
                "Learning Objectives": [
                    [
                        "In this tutorial, you will learn how to:",
                        "markdown"
                    ],
                    [
                        "Convert the DeepLabV3 model for Android deployment.",
                        "markdown"
                    ],
                    [
                        "Get the output of the model for the example input image in Python and compare it to the output from the Android app.",
                        "markdown"
                    ],
                    [
                        "Build a new Android app or reuse an Android example app to load the converted model.",
                        "markdown"
                    ],
                    [
                        "Prepare the input into the format that the model expects and process the model output.",
                        "markdown"
                    ],
                    [
                        "Complete the UI, refactor, build and run the app to see image segmentation in action.",
                        "markdown"
                    ]
                ]
            },
            {
                "Pre-requisites": [
                    [
                        "PyTorch 1.6 or 1.7",
                        "markdown"
                    ],
                    [
                        "torchvision 0.7 or 0.8",
                        "markdown"
                    ],
                    [
                        "Android Studio 3.5.1 or above with NDK installed",
                        "markdown"
                    ]
                ]
            },
            {
                "Steps": [
                    {
                        "1. Convert the DeepLabV3 model for Android deployment": [
                            [
                                "The first step to deploying a model on Android is to convert the model into the  format.",
                                "markdown"
                            ],
                            [
                                "Note",
                                "markdown"
                            ],
                            [
                                "Not all PyTorch models can be converted to TorchScript at this time because a model definition may use language features that are not in TorchScript, which is a subset of Python. See the  for more details.",
                                "markdown"
                            ],
                            [
                                "Simply run the script below to generate the scripted model <cite>deeplabv3_scripted.pt</cite>:",
                                "markdown"
                            ],
                            [
                                "import torch\n\n# use deeplabv3_resnet50 instead of resnet101 to reduce the model size\nmodel = torch.hub.load('pytorch/vision:v0.7.0', 'deeplabv3_resnet50', pretrained=True)\nmodel.eval()\n\nscriptedm = torch.jit.script(model)\ntorch.jit.save(scriptedm, \"deeplabv3_scripted.pt\")",
                                "code"
                            ],
                            [
                                "The size of the generated <cite>deeplabv3_scripted.pt</cite> model file should be around 168MB. Ideally, a model should also be quantized for significant size reduction and faster inference before being deployed on an Android app. To have a general understanding of quantization, see the  and the resource links there. We will cover in detail how to correctly apply a quantization workflow called Post Training  to the DeepLabV3 model in a future tutorial or recipe.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "2. Get example input and output of the model in Python": [
                            [
                                "Now that we have a scripted PyTorch model, let\u2019s test with some example inputs to make sure the model works correctly on Android. First, let\u2019s write a Python script that uses the model to make inferences and examine inputs and outputs. For this example of the DeepLabV3 model, we can reuse the code in Step 1 and in the . Add the following code snippet to the code above:",
                                "markdown"
                            ],
                            [
                                "from PIL import Image\nfrom torchvision import transforms\ninput_image = Image.open(\"deeplab.jpg\")\npreprocess = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ninput_tensor = preprocess(input_image)\ninput_batch = input_tensor.unsqueeze(0)\nwith torch.no_grad():\n    output = model(input_batch)['out'][0]\n\nprint(input_batch.shape)\nprint(output.shape)",
                                "code"
                            ],
                            [
                                "Download <cite>deeplab.jpg</cite> from , then run the script above and you will see the shapes of the input and output of the model:",
                                "markdown"
                            ],
                            [
                                "torch.Size([1, 3, 400, 400])\ntorch.Size([21, 400, 400])",
                                "code"
                            ],
                            [
                                "So if you provide the same image input <cite>deeplab.jpg</cite> of size 400x400 to the model on Android, the output of the model should have the size [21, 400, 400]. You should also print out at least the beginning parts of the actual data of the input and output, to be used in Step 4 below to compare with the actual input and output of the model when running in the Android app.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "3. Build a new Android app or reuse an example app and load the model": [
                            [
                                "First, follow Step 3 of the  to use our model in an Android Studio project with PyTorch Mobile enabled. Because both DeepLabV3 used in this tutorial and MobileNet v2 used in the PyTorch HelloWorld Android example are computer vision models, you can also get the  to make it easier to modify the code that loads the model and processes the input and output. The main goal in this step and Step 4 is to make sure the model <cite>deeplabv3_scripted.pt</cite> generated in Step 1 can indeed work correctly on Android.",
                                "markdown"
                            ],
                            [
                                "Now let\u2019s add <cite>deeplabv3_scripted.pt</cite> and <cite>deeplab.jpg</cite> used in Step 2 to the Android Studio project and modify the <cite>onCreate</cite> method in the <cite>MainActivity</cite> to resemble:",
                                "markdown"
                            ],
                            [
                                "Module module = null;\ntry {\n  module = Module.load(assetFilePath(this, \"deeplabv3_scripted.pt\"));\n} catch (IOException e) {\n  Log.e(\"ImageSegmentation\", \"Error loading model!\", e);\n  finish();\n}",
                                "code"
                            ],
                            [
                                "Then set a breakpoint at the line <cite>finish()</cite> and build and run the app. If the app doesn\u2019t stop at the breakpoint, it means  that the scripted model in Step 1 has been successfully loaded on Android.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "4. Process the model input and output for model inference": [
                            [
                                "After the model loads in the previous step, let\u2019s verify that it works with expected inputs and can generate expected outputs. As the model input for the DeepLabV3 model is an image the same as that of the MobileNet v2 in the HelloWorld example, we will reuse some of the code in the  file from HelloWorld for input processing. Replace the code snippet between  and 73 in <cite>MainActivity.java</cite> with the following code:",
                                "markdown"
                            ],
                            [
                                "final Tensor inputTensor = TensorImageUtils.bitmapToFloat32Tensor(bitmap,\n        TensorImageUtils.TORCHVISION_NORM_MEAN_RGB,\n        TensorImageUtils.TORCHVISION_NORM_STD_RGB);\nfinal float[] inputs = inputTensor.getDataAsFloatArray();\n\nMap&lt;String, IValue&gt; outTensors =\n    module.forward(IValue.from(inputTensor)).toDictStringKey();\n\n// the key \"out\" of the output tensor contains the semantic masks\n// see https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101\nfinal Tensor outputTensor = outTensors.get(\"out\").toTensor();\nfinal float[] outputs = outputTensor.getDataAsFloatArray();\n\nint width = bitmap.getWidth();\nint height = bitmap.getHeight();",
                                "code"
                            ],
                            [
                                "Note",
                                "markdown"
                            ],
                            [
                                "The model output is a dictionary for the DeepLabV3 model so we use <cite>toDictStringKey</cite> to correctly extract the result. For other models, the model output may also be a single tensor or a tuple of tensors, among other things.",
                                "markdown"
                            ],
                            [
                                "With the code changes shown above, you can set breakpoints after <cite>final float[] inputs</cite> and <cite>final float[] outputs</cite>, which populate the input tensor and output tensor data to float arrays for easy debugging. Run the app and when it stops at the breakpoints, compare the numbers in <cite>inputs</cite> and <cite>outputs</cite> with the model input and output data you see in Step 2 to see if they match. For the same inputs to the models running on Android and Python, you should get the same outputs.",
                                "markdown"
                            ],
                            [
                                "Warning",
                                "markdown"
                            ],
                            [
                                "You may see different model outputs with the same image input when running on an Android emulator due to some Android emulator\u2019s floating point implementation issue. So it is best to test the app on a real Android device.",
                                "markdown"
                            ],
                            [
                                "All we have done so far is to confirm that the model of our interest can be scripted and run correctly in our Android app as in Python. The steps we walked through so far for using a model in an iOS app consumes the bulk, if not most, of our app development time, similar to how data preprocessing is the heaviest lift for a typical machine learning project.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "5. Complete the UI, refactor, build and run the app": [
                            [
                                "Now we are ready to complete the app and the UI to actually see the processed result as a new image. The output processing code should be like this, added to the end of the code snippet in Step 4:",
                                "markdown"
                            ],
                            [
                                "int[] intValues = new int[width * height];\n// go through each element in the output of size [WIDTH, HEIGHT] and\n// set different color for different classnum\nfor (int j = 0; j &lt; width; j++) {\n    for (int k = 0; k &lt; height; k++) {\n        // maxi: the index of the 21 CLASSNUM with the max probability\n        int maxi = 0, maxj = 0, maxk = 0;\n        double maxnum = -100000.0;\n        for (int i=0; i &lt; CLASSNUM; i++) {\n            if (outputs[i*(width*height) + j*width + k] &gt; maxnum) {\n                maxnum = outputs[i*(width*height) + j*width + k];\n                maxi = i; maxj = j; maxk= k;\n            }\n        }\n        // color coding for person (red), dog (green), sheep (blue)\n        // black color for background and other classes\n        if (maxi == PERSON)\n            intValues[maxj*width + maxk] = 0xFFFF0000; // red\n        else if (maxi == DOG)\n            intValues[maxj*width + maxk] = 0xFF00FF00; // green\n        else if (maxi == SHEEP)\n            intValues[maxj*width + maxk] = 0xFF0000FF; // blue\n        else\n            intValues[maxj*width + maxk] = 0xFF000000; // black\n    }\n}",
                                "code"
                            ],
                            [
                                "The constants used in the code above are defined in the beginning of the class <cite>MainActivity</cite>:",
                                "markdown"
                            ],
                            [
                                "private static final int CLASSNUM = 21;\nprivate static final int DOG = 12;\nprivate static final int PERSON = 15;\nprivate static final int SHEEP = 17;",
                                "code"
                            ],
                            [
                                "The implementation here is based on the understanding of the DeepLabV3 model which outputs a tensor of size [21, width, height] for an input image of width*height. Each element in the width*height output array is a value between 0 and 20 (for a total of 21 semantic labels described in Introduction) and the value is used to set a specific color. Color coding of the segmentation here is based on the class with the highest probability, and you can extend the color coding for all classes in your own dataset.",
                                "markdown"
                            ],
                            [
                                "After the output processing, you will also need to call the code below to render the RGB <cite>intValues</cite> array to a bitmap instance <cite>outputBitmap</cite> before displaying it on an <cite>ImageView</cite>:",
                                "markdown"
                            ],
                            [
                                "Bitmap bmpSegmentation = Bitmap.createScaledBitmap(bitmap, width, height, true);\nBitmap outputBitmap = bmpSegmentation.copy(bmpSegmentation.getConfig(), true);\noutputBitmap.setPixels(intValues, 0, outputBitmap.getWidth(), 0, 0,\n    outputBitmap.getWidth(), outputBitmap.getHeight());\nimageView.setImageBitmap(outputBitmap);",
                                "code"
                            ],
                            [
                                "The UI for this app is also similar to that for HelloWorld, except that you do not need the <cite>TextView</cite> to show the image classification result. You can also add two buttons <cite>Segment</cite> and <cite>Restart</cite> as shown in the code repo to run the model inference and to show back the original image after the segmentation result is shown.",
                                "markdown"
                            ],
                            [
                                "Now when you run the app on an Android emulator or preferably an actual device, you will see screens like the following:",
                                "markdown"
                            ]
                        ]
                    }
                ]
            },
            {
                "Recap": [
                    [
                        "In this tutorial, we described what it takes to convert a pre-trained PyTorch DeepLabV3 model for Android and how to make sure the model can run successfully on Android. Our focus was to help you understand the process of confirming that a model can indeed run on Android. The complete code repo is available .",
                        "markdown"
                    ],
                    [
                        "More advanced topics such as quantization and using models via transfer learning or of your own on Android will be covered soon in future demo apps and tutorials.",
                        "markdown"
                    ]
                ]
            },
            {
                "Learn More": [
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ]
    },
    "Recommendation Systems": {
        "Introduction to TorchRec": [
            [
                "Tip",
                "markdown"
            ],
            [
                "To get the most of this tutorial, we suggest using this\n.\nThis will allow you to experiment with the information presented below.",
                "markdown"
            ],
            [
                "Follow along with the video below or on .\n\n<iframe allow=\"accelerometer; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen=\"\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/cjgj41dvSeQ\" width=\"560\"></iframe>",
                "markdown"
            ],
            [
                "When building recommendation systems, we frequently want to represent\nentities like products or pages with embeddings. For example, see Meta\nAI\u2019s , or DLRM. As the number of\nentities grow, the size of the embedding tables can exceed a single\nGPU\u2019s memory. A common practice is to shard the embedding table across\ndevices, a type of model parallelism. To that end, TorchRec introduces\nits primary API\ncalled ,\nor DMP. Like PyTorch\u2019s DistributedDataParallel, DMP wraps a model to\nenable distributed training.",
                "markdown"
            ],
            {
                "Installation": [
                    [
                        "Requirements: python &gt;= 3.7",
                        "markdown"
                    ],
                    [
                        "We highly recommend CUDA when using TorchRec (If using CUDA: cuda &gt;= 11.0).",
                        "markdown"
                    ],
                    [
                        "# install pytorch with cudatoolkit 11.3\nconda install pytorch cudatoolkit=11.3 -c pytorch-nightly -y\n# install TorchRec\npip3 install torchrec-nightly",
                        "code"
                    ]
                ]
            },
            {
                "Overview": [
                    [
                        "This tutorial will cover three pieces of TorchRec: the nn.module , the  API, and\nthe datastructure .",
                        "markdown"
                    ],
                    {
                        "Distributed Setup": [
                            [
                                "We setup our environment with torch.distributed. For more info on\ndistributed, see this\n.",
                                "markdown"
                            ],
                            [
                                "Here, we use one rank (the colab process) corresponding to our 1 colab\nGPU.",
                                "markdown"
                            ],
                            [
                                "import os\nimport torch\nimport torchrec\nimport torch.distributed as dist\n\nos.environ[\"RANK\"] = \"0\"\nos.environ[\"WORLD_SIZE\"] = \"1\"\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"29500\"\n\n# Note - you will need a V100 or A100 to run tutorial as as!\n# If using an older GPU (such as colab free K80),\n# you will need to compile fbgemm with the appripriate CUDA architecture\n# or run with \"gloo\" on CPUs\ndist.init_process_group(backend=\"nccl\")",
                                "code"
                            ]
                        ]
                    },
                    {
                        "From EmbeddingBag to EmbeddingBagCollection": [
                            [
                                "PyTorch represents embeddings through  and .\nEmbeddingBag is a pooled version of Embedding.",
                                "markdown"
                            ],
                            [
                                "TorchRec extends these modules by creating collections of embeddings. We\nwill use  to represent a group of EmbeddingBags.",
                                "markdown"
                            ],
                            [
                                "Here, we create an EmbeddingBagCollection (EBC) with two embedding bags.\nEach table, product_table and user_table, is represented by a 64\ndimension embedding of size 4096. Note how we initially allocate the EBC\non device \u201cmeta\u201d. This will tell EBC to not allocate memory yet.",
                                "markdown"
                            ],
                            [
                                "ebc = torchrec.EmbeddingBagCollection(\n    device=\"meta\",\n    tables=[\n        torchrec.EmbeddingBagConfig(\n            name=\"product_table\",\n            embedding_dim=64,\n            num_embeddings=4096,\n            feature_names=[\"product\"],\n            pooling=torchrec.PoolingType.SUM,\n        ),\n        torchrec.EmbeddingBagConfig(\n            name=\"user_table\",\n            embedding_dim=64,\n            num_embeddings=4096,\n            feature_names=[\"user\"],\n            pooling=torchrec.PoolingType.SUM,\n        )\n    ]\n)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "DistributedModelParallel": [
                            [
                                "Now, we\u2019re ready to wrap our model with  (DMP). Instantiating DMP will:",
                                "markdown"
                            ],
                            [
                                "Decide how to shard the model. DMP will collect the available\n\u2018sharders\u2019 and come up with a \u2018plan\u2019 of the optimal way to shard the\nembedding table(s) (i.e., the EmbeddingBagCollection).",
                                "markdown"
                            ],
                            [
                                "Actually shard the model. This includes allocating memory for each\nembedding table on the appropriate device(s).",
                                "markdown"
                            ],
                            [
                                "In this toy example, since we have two EmbeddingTables and one GPU,\nTorchRec will place both on the single GPU.",
                                "markdown"
                            ],
                            [
                                "model = torchrec.distributed.DistributedModelParallel(ebc, device=torch.device(\"cuda\"))\nprint(model)\nprint(model.plan)",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Query vanilla nn.EmbeddingBag with input and offsets": [
                            [
                                "We query  and \nwith input and offsets. Input is a 1-D tensor containing the\nlookup values. Offsets is a 1-D tensor where the sequence is a\ncumulative sum of the number of values to pool per example.",
                                "markdown"
                            ],
                            [
                                "Let\u2019s look at an example, recreating the product EmbeddingBag above:",
                                "markdown"
                            ],
                            [
                                "|------------|\n| product ID |\n|------------|\n| [101, 202] |\n| []         |\n| [303]      |\n|------------|",
                                "code"
                            ],
                            [
                                "product_eb = torch.nn.EmbeddingBag(4096, 64)\nproduct_eb(input=torch.tensor([101, 202, 303]), offsets=torch.tensor([0, 2, 2]))",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Representing minibatches with KeyedJaggedTensor": [
                            [
                                "We need an efficient representation of multiple examples of an arbitrary\nnumber of entity IDs per feature per example. In order to enable this\n\u201cjagged\u201d representation, we use the TorchRec datastructure\n (KJT).",
                                "markdown"
                            ],
                            [
                                "Let\u2019s take a look at how to lookup a collection of two embedding\nbags, \u201cproduct\u201d and \u201cuser\u201d. Assume the minibatch is made up of three\nexamples for three users. The first of which has two product IDs, the\nsecond with none, and the third with one product ID.",
                                "markdown"
                            ],
                            [
                                "|------------|------------|\n| product ID | user ID    |\n|------------|------------|\n| [101, 202] | [404]      |\n| []         | [505]      |\n| [303]      | [606]      |\n|------------|------------|",
                                "code"
                            ],
                            [
                                "The query should be:",
                                "markdown"
                            ],
                            [
                                "mb = torchrec.KeyedJaggedTensor(\n    keys = [\"product\", \"user\"],\n    values = torch.tensor([101, 202, 303, 404, 505, 606]).cuda(),\n    lengths = torch.tensor([2, 0, 1, 1, 1, 1], dtype=torch.int64).cuda(),\n)\n\nprint(mb.to(torch.device(\"cpu\")))",
                                "code"
                            ],
                            [
                                "Note that the KJT batch size is\nbatch_size = len(lengths)//len(keys). In the above example,\nbatch_size is 3.",
                                "markdown"
                            ]
                        ]
                    },
                    {
                        "Putting it all together, querying our distributed model with a KJT minibatch": [
                            [
                                "Finally, we can query our model using our minibatch of products and\nusers.",
                                "markdown"
                            ],
                            [
                                "The resulting lookup will contain a KeyedTensor, where each key (or\nfeature) contains a 2D tensor of size 3x64 (batch_size x embedding_dim).",
                                "markdown"
                            ],
                            [
                                "pooled_embeddings = model(mb)\nprint(pooled_embeddings)",
                                "code"
                            ]
                        ]
                    }
                ]
            },
            {
                "More resources": [
                    [
                        "For more information, please see our\n\nexample, which includes multinode training on the criteo terabyte\ndataset, using Meta\u2019s .",
                        "markdown"
                    ]
                ]
            }
        ],
        "Exploring TorchRec sharding": [
            [
                "This tutorial will mainly cover the sharding schemes of embedding tables\nvia EmbeddingPlanner and DistributedModelParallel API and\nexplore the benefits of different sharding schemes for the embedding\ntables by explicitly configuring them.",
                "markdown"
            ],
            {
                "Installation": [
                    [
                        "Requirements: - python &gt;= 3.7",
                        "markdown"
                    ],
                    [
                        "We highly recommend CUDA when using torchRec. If using CUDA: - cuda &gt;=\n11.0",
                        "markdown"
                    ],
                    [
                        "# install conda to make installying pytorch with cudatoolkit 11.3 easier.\n!sudo rm Miniconda3-py37_4.9.2-Linux-x86_64.sh Miniconda3-py37_4.9.2-Linux-x86_64.sh.*\n!sudo wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.9.2-Linux-x86_64.sh\n!sudo chmod +x Miniconda3-py37_4.9.2-Linux-x86_64.sh\n!sudo bash ./Miniconda3-py37_4.9.2-Linux-x86_64.sh -b -f -p /usr/local",
                        "code"
                    ],
                    [
                        "# install pytorch with cudatoolkit 11.3\n!sudo conda install pytorch cudatoolkit=11.3 -c pytorch-nightly -y",
                        "code"
                    ],
                    [
                        "Installing torchRec will also install\n, a collection of CUDA\nkernels and GPU enabled operations to run",
                        "markdown"
                    ],
                    [
                        "# install torchrec\n!pip3 install torchrec-nightly",
                        "code"
                    ],
                    [
                        "Install multiprocess which works with ipython to for multi-processing\nprogramming within colab",
                        "markdown"
                    ],
                    [
                        "!pip3 install multiprocess",
                        "code"
                    ],
                    [
                        "The following steps are needed for the Colab runtime to detect the added\nshared libraries. The runtime searches for shared libraries in /usr/lib,\nso we copy over the libraries which were installed in /usr/local/lib/.\n<strong>This is a very necessary step, only in the colab runtime</strong>.",
                        "markdown"
                    ],
                    [
                        "!sudo cp /usr/local/lib/lib* /usr/lib/",
                        "code"
                    ],
                    [
                        "<strong>Restart your runtime at this point for the newly installed packages\nto be seen.</strong> Run the step below immediately after restarting so that\npython knows where to look for packages. <strong>Always run this step after\nrestarting the runtime.</strong>",
                        "markdown"
                    ],
                    [
                        "import sys\nsys.path = ['', '/env/python', '/usr/local/lib/python37.zip', '/usr/local/lib/python3.7', '/usr/local/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/site-packages', './.local/lib/python3.7/site-packages']",
                        "code"
                    ]
                ]
            },
            {
                "Distributed Setup": [
                    [
                        "Due to the notebook enviroment, we cannot run\n program here but we\ncan do multiprocessing inside the notebook to mimic the setup. Users\nshould be responsible for setting up their own\n launcher when using\nTorchrec. We setup our environment so that torch distributed based\ncommunication backend can work.",
                        "markdown"
                    ],
                    [
                        "import os\nimport torch\nimport torchrec\n\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"29500\"",
                        "code"
                    ]
                ]
            },
            {
                "Constructing our embedding model": [
                    [
                        "Here we use TorchRec offering of\n\nto construct our embedding bag model with embedding tables.",
                        "markdown"
                    ],
                    [
                        "Here, we create an EmbeddingBagCollection (EBC) with four embedding\nbags. We have two types of tables: large tables and small tables\ndifferentiated by their row size difference: 4096 vs 1024. Each table is\nstill represented by 64 dimension embedding.",
                        "markdown"
                    ],
                    [
                        "We configure the ParameterConstraints data structure for the tables,\nwhich provides hints for the model parallel API to help decide the\nsharding and placement strategy for the tables. In TorchRec, we support\n* table-wise: place the entire table on one device; *\nrow-wise: shard the table evenly by row dimension and place one\nshard on each device of the communication world; * column-wise:\nshard the table evenly by embedding dimension, and place one shard on\neach device of the communication world; * table-row-wise: special\nsharding optimized for intra-host communication for available fast\nintra-machine device interconnect, e.g. NVLink; * data_parallel:\nreplicate the tables for every device;",
                        "markdown"
                    ],
                    [
                        "Note how we initially allocate the EBC on device \u201cmeta\u201d. This will tell\nEBC to not allocate memory yet.",
                        "markdown"
                    ],
                    [
                        "from torchrec.distributed.planner.types import ParameterConstraints\nfrom torchrec.distributed.embedding_types import EmbeddingComputeKernel\nfrom torchrec.distributed.types import ShardingType\nfrom typing import Dict\n\nlarge_table_cnt = 2\nsmall_table_cnt = 2\nlarge_tables=[\n  torchrec.EmbeddingBagConfig(\n    name=\"large_table_\" + str(i),\n    embedding_dim=64,\n    num_embeddings=4096,\n    feature_names=[\"large_table_feature_\" + str(i)],\n    pooling=torchrec.PoolingType.SUM,\n  ) for i in range(large_table_cnt)\n]\nsmall_tables=[\n  torchrec.EmbeddingBagConfig(\n    name=\"small_table_\" + str(i),\n    embedding_dim=64,\n    num_embeddings=1024,\n    feature_names=[\"small_table_feature_\" + str(i)],\n    pooling=torchrec.PoolingType.SUM,\n  ) for i in range(small_table_cnt)\n]\n\ndef gen_constraints(sharding_type: ShardingType = ShardingType.TABLE_WISE) -&gt; Dict[str, ParameterConstraints]:\n  large_table_constraints = {\n    \"large_table_\" + str(i): ParameterConstraints(\n      sharding_types=[sharding_type.value],\n    ) for i in range(large_table_cnt)\n  }\n  small_table_constraints = {\n    \"small_table_\" + str(i): ParameterConstraints(\n      sharding_types=[sharding_type.value],\n    ) for i in range(small_table_cnt)\n  }\n  constraints = {**large_table_constraints, **small_table_constraints}\n  return constraints",
                        "code"
                    ],
                    [
                        "ebc = torchrec.EmbeddingBagCollection(\n    device=\"cuda\",\n    tables=large_tables + small_tables\n)",
                        "code"
                    ]
                ]
            },
            {
                "DistributedModelParallel in multiprocessing": [
                    [
                        "Now, we have a single process execution function for mimicking one\nrank\u2019s work during \nexecution.",
                        "markdown"
                    ],
                    [
                        "This code will shard the model collectively with other processes and\nallocate memories accordingly. It first sets up process groups and do\nembedding table placement using planner and generate sharded model using\nDistributedModelParallel.",
                        "markdown"
                    ],
                    [
                        "def single_rank_execution(\n    rank: int,\n    world_size: int,\n    constraints: Dict[str, ParameterConstraints],\n    module: torch.nn.Module,\n    backend: str,\n) -&gt; None:\n    import os\n    import torch\n    import torch.distributed as dist\n    from torchrec.distributed.embeddingbag import EmbeddingBagCollectionSharder\n    from torchrec.distributed.model_parallel import DistributedModelParallel\n    from torchrec.distributed.planner import EmbeddingShardingPlanner, Topology\n    from torchrec.distributed.types import ModuleSharder, ShardingEnv\n    from typing import cast\n\n    def init_distributed_single_host(\n        rank: int,\n        world_size: int,\n        backend: str,\n        # pyre-fixme[11]: Annotation `ProcessGroup` is not defined as a type.\n    ) -&gt; dist.ProcessGroup:\n        os.environ[\"RANK\"] = f\"{rank}\"\n        os.environ[\"WORLD_SIZE\"] = f\"{world_size}\"\n        dist.init_process_group(rank=rank, world_size=world_size, backend=backend)\n        return dist.group.WORLD\n\n    if backend == \"nccl\":\n        device = torch.device(f\"cuda:{rank}\")\n        torch.cuda.set_device(device)\n    else:\n        device = torch.device(\"cpu\")\n    topology = Topology(world_size=world_size, compute_device=\"cuda\")\n    pg = init_distributed_single_host(rank, world_size, backend)\n    planner = EmbeddingShardingPlanner(\n        topology=topology,\n        constraints=constraints,\n    )\n    sharders = [cast(ModuleSharder[torch.nn.Module], EmbeddingBagCollectionSharder())]\n    plan: ShardingPlan = planner.collective_plan(module, sharders, pg)\n\n    sharded_model = DistributedModelParallel(\n        module,\n        env=ShardingEnv.from_process_group(pg),\n        plan=plan,\n        sharders=sharders,\n        device=device,\n    )\n    print(f\"rank:{rank},sharding plan: {plan}\")\n    return sharded_model",
                        "code"
                    ],
                    {
                        "Multiprocessing Execution": [
                            [
                                "Now let\u2019s execute the code in multi-processes representing multiple GPU\nranks.",
                                "markdown"
                            ],
                            [
                                "import multiprocess\n\ndef spmd_sharing_simulation(\n    sharding_type: ShardingType = ShardingType.TABLE_WISE,\n    world_size = 2,\n):\n  ctx = multiprocess.get_context(\"spawn\")\n  processes = []\n  for rank in range(world_size):\n      p = ctx.Process(\n          target=single_rank_execution,\n          args=(\n              rank,\n              world_size,\n              gen_constraints(sharding_type),\n              ebc,\n              \"nccl\"\n          ),\n      )\n      p.start()\n      processes.append(p)\n\n  for p in processes:\n      p.join()\n      assert 0 == p.exitcode",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Table Wise Sharding": [
                            [
                                "Now let\u2019s execute the code in two processes for 2 GPUs. We can see in\nthe plan print that how our tables are sharded across GPUs. Each node\nwill have one large table and one small which shows our planner tries\nfor load balance for the embedding tables. Table-wise is the de-factor\ngo-to sharding schemes for many small-medium size tables for load\nbalancing over the devices.",
                                "markdown"
                            ],
                            [
                                "spmd_sharing_simulation(ShardingType.TABLE_WISE)",
                                "code"
                            ],
                            [
                                "rank:1,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 64], placement=rank:0/cuda:0)])), 'large_table_1': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 64], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 64], placement=rank:0/cuda:0)])), 'small_table_1': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 64], placement=rank:1/cuda:1)]))}}\nrank:0,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 64], placement=rank:0/cuda:0)])), 'large_table_1': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 64], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[0], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 64], placement=rank:0/cuda:0)])), 'small_table_1': ParameterSharding(sharding_type='table_wise', compute_kernel='batched_fused', ranks=[1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 64], placement=rank:1/cuda:1)]))}}",
                                "code"
                            ]
                        ]
                    },
                    {
                        "Explore other sharding modes": [
                            [
                                "We have initially explored what table-wise sharding would look like and\nhow it balances the tables placement. Now we explore sharding modes with\nfiner focus on load balance: row-wise. Row-wise is specifically\naddressing large tables which a single device cannot hold due to the\nmemory size increase from large embedding row numbers. It can address\nthe placement of the super large tables in your models. Users can see\nthat in the shard_sizes section in the printed plan log, the tables\nare halved by row dimension to be distributed onto two GPUs.",
                                "markdown"
                            ],
                            [
                                "spmd_sharing_simulation(ShardingType.ROW_WISE)",
                                "code"
                            ],
                            [
                                "rank:1,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[2048, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[2048, 0], shard_sizes=[2048, 64], placement=rank:1/cuda:1)])), 'large_table_1': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[2048, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[2048, 0], shard_sizes=[2048, 64], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[512, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[512, 0], shard_sizes=[512, 64], placement=rank:1/cuda:1)])), 'small_table_1': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[512, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[512, 0], shard_sizes=[512, 64], placement=rank:1/cuda:1)]))}}\nrank:0,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[2048, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[2048, 0], shard_sizes=[2048, 64], placement=rank:1/cuda:1)])), 'large_table_1': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[2048, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[2048, 0], shard_sizes=[2048, 64], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[512, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[512, 0], shard_sizes=[512, 64], placement=rank:1/cuda:1)])), 'small_table_1': ParameterSharding(sharding_type='row_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[512, 64], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[512, 0], shard_sizes=[512, 64], placement=rank:1/cuda:1)]))}}",
                                "code"
                            ],
                            [
                                "Column-wise on the other hand, address the load imbalance problems for\ntables with large embedding dimensions. We will split the table\nvertically. Users can see that in the shard_sizes section in the\nprinted plan log, the tables are halved by embedding dimension to be\ndistributed onto two GPUs.",
                                "markdown"
                            ],
                            [
                                "spmd_sharing_simulation(ShardingType.COLUMN_WISE)",
                                "code"
                            ],
                            [
                                "rank:0,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[4096, 32], placement=rank:1/cuda:1)])), 'large_table_1': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[4096, 32], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[1024, 32], placement=rank:1/cuda:1)])), 'small_table_1': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[1024, 32], placement=rank:1/cuda:1)]))}}\nrank:1,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[4096, 32], placement=rank:1/cuda:1)])), 'large_table_1': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[4096, 32], placement=rank:1/cuda:1)])), 'small_table_0': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[1024, 32], placement=rank:1/cuda:1)])), 'small_table_1': ParameterSharding(sharding_type='column_wise', compute_kernel='batched_fused', ranks=[0, 1], sharding_spec=EnumerableShardingSpec(shards=[ShardMetadata(shard_offsets=[0, 0], shard_sizes=[1024, 32], placement=rank:0/cuda:0), ShardMetadata(shard_offsets=[0, 32], shard_sizes=[1024, 32], placement=rank:1/cuda:1)]))}}",
                                "code"
                            ],
                            [
                                "For table-row-wise, unfortuately we cannot simulate it due to its\nnature of operating under multi-host setup. We will present a python\n example in the future\nto train models with table-row-wise.",
                                "markdown"
                            ],
                            [
                                "With data parallel, we will repeat the tables for all devices.",
                                "markdown"
                            ],
                            [
                                "spmd_sharing_simulation(ShardingType.DATA_PARALLEL)",
                                "code"
                            ],
                            [
                                "rank:0,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'large_table_1': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'small_table_0': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'small_table_1': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None)}}\nrank:1,sharding plan: {'': {'large_table_0': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'large_table_1': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'small_table_0': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None), 'small_table_1': ParameterSharding(sharding_type='data_parallel', compute_kernel='batched_dense', ranks=[0, 1], sharding_spec=None)}}",
                                "code"
                            ]
                        ]
                    }
                ]
            }
        ]
    },
    "Multimodality": {
        "TorchMultimodal Tutorial: Finetuning FLAVA": [
            [
                "Multimodal AI has recently become very popular owing to its ubiquitous\nnature, from use cases like image captioning and visual search to more\nrecent applications like image generation from text. <strong>TorchMultimodal\nis a library powered by Pytorch consisting of building blocks and end to\nend examples, aiming to enable and accelerate research in\nmultimodality</strong>.",
                "markdown"
            ],
            [
                "In this tutorial, we will demonstrate how to use a <strong>pretrained SoTA\nmodel called</strong>  <strong>from\nTorchMultimodal library to finetune on a multimodal task i.e. visual\nquestion answering</strong> (VQA). The model consists of two unimodal transformer\nbased encoders for text and image and a multimodal encoder to combine\nthe two embeddings. It is pretrained using contrastive, image text matching and\ntext, image and multimodal masking losses.",
                "markdown"
            ],
            {
                "Installation": [
                    [
                        "We will use TextVQA dataset and bert tokenizer from HuggingFace for this\ntutorial. So you need to install datasets and transformers in addition to TorchMultimodal.",
                        "markdown"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "When running this tutorial in Google Colab, install the required packages by\ncreating a new cell and running the following commands:",
                        "markdown"
                    ],
                    [
                        "!pip install torchmultimodal-nightly\n!pip install datasets\n!pip install transformers",
                        "code"
                    ]
                ]
            },
            {
                "Steps": [
                    [
                        "Download the HuggingFace dataset to a directory on your computer by running the following command:",
                        "markdown"
                    ],
                    [
                        "wget http://dl.fbaipublicfiles.com/pythia/data/vocab.tar.gz\ntar xf vocab.tar.gz",
                        "code"
                    ],
                    [
                        "Note",
                        "markdown"
                    ],
                    [
                        "If you are running this tutorial in Google Colab, run these commands\nin a new cell and prepend these commands with an exclamation mark (!)",
                        "markdown"
                    ],
                    [
                        "For this tutorial, we treat VQA as a classification task where\nthe inputs are images and question (text) and the output is an answer class.\nSo we need to download the vocab file with answer classes and create the answer to\nlabel mapping.",
                        "markdown"
                    ],
                    [
                        "We also load the  containing 34602 training samples\n(images,questions and answers) from HuggingFace",
                        "markdown"
                    ],
                    [
                        "We see there are 3997 answer classes including a class representing\nunknown answers.",
                        "markdown"
                    ],
                    [
                        "with open(\"data/vocabs/answers_textvqa_more_than_1.txt\") as f:\n  vocab = f.readlines()\n\nanswer_to_idx = {}\nfor idx, entry in enumerate(vocab):\n  answer_to_idx[entry.strip(\"\\n\")] = idx\nprint(len(vocab))\nprint(vocab[:5])\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"textvqa\")",
                        "code"
                    ],
                    [
                        "3997\n['&lt;unk&gt;\\n', 'nokia\\n', 'ec\\n', 'virgin\\n', '2011\\n']\n\nDownloading builder script:   0%|          | 0.00/5.02k [00:00&lt;?, ?B/s]\nDownloading builder script: 100%|##########| 5.02k/5.02k [00:00&lt;00:00, 5.47MB/s]\n\nDownloading metadata:   0%|          | 0.00/13.3k [00:00&lt;?, ?B/s]\nDownloading metadata: 100%|##########| 13.3k/13.3k [00:00&lt;00:00, 12.9MB/s]\n\nDownloading readme:   0%|          | 0.00/13.2k [00:00&lt;?, ?B/s]\nDownloading readme: 100%|##########| 13.2k/13.2k [00:00&lt;00:00, 9.70MB/s]\nNo config specified, defaulting to: textvqa/textvqa\nDownloading and preparing dataset textvqa/textvqa to /var/lib/jenkins/.cache/huggingface/datasets/textvqa/textvqa/0.5.1/9b89037cc122c3b495b155a1bce4170851829843454e88f236bb8715d977c027...\n\nDownloading data files:   0%|          | 0/5 [00:00&lt;?, ?it/s]\n\nDownloading data:   0%|          | 0.00/21.6M [00:00&lt;?, ?B/s]\n\nDownloading data:   0%|          | 104k/21.6M [00:00&lt;00:21, 1.00MB/s]\n\nDownloading data:   1%|1         | 239k/21.6M [00:00&lt;00:21, 987kB/s]\n\nDownloading data:   2%|1         | 387k/21.6M [00:00&lt;00:17, 1.18MB/s]\n\nDownloading data:   3%|2         | 617k/21.6M [00:00&lt;00:14, 1.40MB/s]\n\nDownloading data:   4%|4         | 880k/21.6M [00:00&lt;00:11, 1.79MB/s]\n\nDownloading data:   5%|5         | 1.10M/21.6M [00:00&lt;00:12, 1.69MB/s]\n\nDownloading data:   6%|6         | 1.40M/21.6M [00:00&lt;00:09, 2.08MB/s]\n\nDownloading data:   7%|7         | 1.62M/21.6M [00:00&lt;00:09, 2.10MB/s]\n\nDownloading data:   9%|9         | 1.98M/21.6M [00:01&lt;00:08, 2.29MB/s]\n\nDownloading data:  11%|#1        | 2.46M/21.6M [00:01&lt;00:06, 2.97MB/s]\n\nDownloading data:  13%|#3        | 2.83M/21.6M [00:01&lt;00:06, 2.87MB/s]\n\nDownloading data:  15%|#5        | 3.29M/21.6M [00:01&lt;00:05, 3.30MB/s]\n\nDownloading data:  18%|#8        | 3.93M/21.6M [00:01&lt;00:04, 3.78MB/s]\n\nDownloading data:  21%|##        | 4.44M/21.6M [00:01&lt;00:04, 4.11MB/s]\n\nDownloading data:  24%|##4       | 5.23M/21.6M [00:01&lt;00:03, 5.16MB/s]\n\nDownloading data:  28%|##7       | 5.97M/21.6M [00:01&lt;00:03, 5.18MB/s]\n\nDownloading data:  32%|###1      | 6.82M/21.6M [00:01&lt;00:02, 6.06MB/s]\n\nDownloading data:  37%|###6      | 7.92M/21.6M [00:02&lt;00:02, 6.67MB/s]\n\nDownloading data:  41%|####1     | 8.98M/21.6M [00:02&lt;00:01, 7.69MB/s]\n\nDownloading data:  47%|####7     | 10.2M/21.6M [00:02&lt;00:01, 9.02MB/s]\n\nDownloading data:  54%|#####3    | 11.6M/21.6M [00:02&lt;00:01, 9.19MB/s]\n\nDownloading data:  62%|######1   | 13.3M/21.6M [00:02&lt;00:00, 11.4MB/s]\n\nDownloading data:  68%|######8   | 14.8M/21.6M [00:02&lt;00:00, 12.3MB/s]\n\nDownloading data:  77%|#######7  | 16.8M/21.6M [00:02&lt;00:00, 13.1MB/s]\n\nDownloading data:  86%|########6 | 18.6M/21.6M [00:02&lt;00:00, 14.6MB/s]\n\nDownloading data:  96%|#########6| 20.8M/21.6M [00:02&lt;00:00, 16.5MB/s]\nDownloading data: 100%|##########| 21.6M/21.6M [00:03&lt;00:00, 7.20MB/s]\n\nDownloading data files:  20%|##        | 1/5 [00:03&lt;00:14,  3.60s/it]\n\nDownloading data: 0.00B [00:00, ?B/s]\nDownloading data: 3.12MB [00:00, 63.1MB/s]\n\nDownloading data files:  40%|####      | 2/5 [00:04&lt;00:05,  1.76s/it]\n\nDownloading data: 0.00B [00:00, ?B/s]\nDownloading data: 2.77MB [00:00, 69.4MB/s]\n\nDownloading data files:  60%|######    | 3/5 [00:04&lt;00:02,  1.18s/it]\n\nDownloading data:   0%|          | 0.00/7.07G [00:00&lt;?, ?B/s]\n\nDownloading data:   0%|          | 88.1k/7.07G [00:00&lt;2:13:52, 880kB/s]\n\nDownloading data:   0%|          | 461k/7.07G [00:00&lt;46:10, 2.55MB/s]\n\nDownloading data:   0%|          | 1.92M/7.07G [00:00&lt;14:36, 8.07MB/s]\n\nDownloading data:   0%|          | 6.13M/7.07G [00:00&lt;05:28, 21.5MB/s]\n\nDownloading data:   0%|          | 12.8M/7.07G [00:00&lt;03:07, 37.7MB/s]\n\nDownloading data:   0%|          | 19.3M/7.07G [00:00&lt;02:30, 46.9MB/s]\n\nDownloading data:   0%|          | 25.4M/7.07G [00:00&lt;02:16, 51.6MB/s]\n\nDownloading data:   0%|          | 31.8M/7.07G [00:00&lt;02:06, 55.6MB/s]\n\nDownloading data:   1%|          | 38.1M/7.07G [00:00&lt;02:01, 58.1MB/s]\n\nDownloading data:   1%|          | 44.8M/7.07G [00:01&lt;01:55, 60.7MB/s]\n\nDownloading data:   1%|          | 51.3M/7.07G [00:01&lt;01:53, 62.1MB/s]\n\nDownloading data:   1%|          | 57.9M/7.07G [00:01&lt;01:50, 63.3MB/s]\n\nDownloading data:   1%|          | 64.3M/7.07G [00:01&lt;01:51, 62.7MB/s]\n\nDownloading data:   1%|          | 70.6M/7.07G [00:01&lt;01:51, 62.8MB/s]\n\nDownloading data:   1%|1         | 76.9M/7.07G [00:01&lt;01:51, 62.9MB/s]\n\nDownloading data:   1%|1         | 83.2M/7.07G [00:01&lt;01:50, 63.0MB/s]\n\nDownloading data:   1%|1         | 89.6M/7.07G [00:01&lt;01:50, 63.3MB/s]\n\nDownloading data:   1%|1         | 96.1M/7.07G [00:01&lt;01:49, 63.7MB/s]\n\nDownloading data:   1%|1         | 102M/7.07G [00:01&lt;01:49, 63.6MB/s]\n\nDownloading data:   2%|1         | 109M/7.07G [00:02&lt;01:49, 63.7MB/s]\n\nDownloading data:   2%|1         | 115M/7.07G [00:02&lt;01:49, 63.5MB/s]\n\nDownloading data:   2%|1         | 122M/7.07G [00:02&lt;01:49, 63.4MB/s]\n\nDownloading data:   2%|1         | 128M/7.07G [00:02&lt;01:49, 63.7MB/s]\n\nDownloading data:   2%|1         | 134M/7.07G [00:02&lt;01:48, 63.9MB/s]\n\nDownloading data:   2%|1         | 141M/7.07G [00:02&lt;01:47, 64.5MB/s]\n\nDownloading data:   2%|2         | 148M/7.07G [00:02&lt;01:47, 64.5MB/s]\n\nDownloading data:   2%|2         | 154M/7.07G [00:02&lt;01:47, 64.6MB/s]\n\nDownloading data:   2%|2         | 161M/7.07G [00:02&lt;01:46, 64.9MB/s]\n\nDownloading data:   2%|2         | 167M/7.07G [00:02&lt;01:46, 64.7MB/s]\n\nDownloading data:   2%|2         | 174M/7.07G [00:03&lt;01:46, 64.6MB/s]\n\nDownloading data:   3%|2         | 180M/7.07G [00:03&lt;01:47, 64.2MB/s]\n\nDownloading data:   3%|2         | 186M/7.07G [00:03&lt;01:49, 63.1MB/s]\n\nDownloading data:   3%|2         | 193M/7.07G [00:03&lt;01:49, 62.9MB/s]\n\nDownloading data:   3%|2         | 199M/7.07G [00:03&lt;01:49, 62.7MB/s]\n\nDownloading data:   3%|2         | 206M/7.07G [00:03&lt;01:48, 63.5MB/s]\n\nDownloading data:   3%|2         | 212M/7.07G [00:03&lt;01:47, 63.6MB/s]\n\nDownloading data:   3%|3         | 218M/7.07G [00:03&lt;01:46, 64.2MB/s]\n\nDownloading data:   3%|3         | 225M/7.07G [00:03&lt;01:47, 63.5MB/s]\n\nDownloading data:   3%|3         | 232M/7.07G [00:03&lt;01:46, 64.4MB/s]\n\nDownloading data:   3%|3         | 238M/7.07G [00:04&lt;01:46, 64.5MB/s]\n\nDownloading data:   3%|3         | 245M/7.07G [00:04&lt;01:45, 64.7MB/s]\n\nDownloading data:   4%|3         | 251M/7.07G [00:04&lt;01:45, 64.7MB/s]\n\nDownloading data:   4%|3         | 257M/7.07G [00:04&lt;01:45, 64.6MB/s]\n\nDownloading data:   4%|3         | 264M/7.07G [00:04&lt;01:44, 64.8MB/s]\n\nDownloading data:   4%|3         | 271M/7.07G [00:04&lt;01:44, 65.2MB/s]\n\nDownloading data:   4%|3         | 277M/7.07G [00:04&lt;01:43, 65.5MB/s]\n\nDownloading data:   4%|4         | 284M/7.07G [00:04&lt;01:43, 65.8MB/s]\n\nDownloading data:   4%|4         | 290M/7.07G [00:04&lt;01:42, 65.9MB/s]\n\nDownloading data:   4%|4         | 297M/7.07G [00:04&lt;01:43, 65.3MB/s]\n\nDownloading data:   4%|4         | 304M/7.07G [00:05&lt;01:45, 64.4MB/s]\n\nDownloading data:   4%|4         | 310M/7.07G [00:05&lt;01:44, 64.5MB/s]\n\nDownloading data:   4%|4         | 317M/7.07G [00:05&lt;01:43, 65.2MB/s]\n\nDownloading data:   5%|4         | 323M/7.07G [00:05&lt;01:42, 65.6MB/s]\n\nDownloading data:   5%|4         | 330M/7.07G [00:05&lt;01:42, 65.9MB/s]\n\nDownloading data:   5%|4         | 337M/7.07G [00:05&lt;01:41, 66.1MB/s]\n\nDownloading data:   5%|4         | 343M/7.07G [00:05&lt;01:41, 66.1MB/s]\n\nDownloading data:   5%|4         | 350M/7.07G [00:05&lt;01:41, 66.2MB/s]\n\nDownloading data:   5%|5         | 357M/7.07G [00:05&lt;01:41, 66.4MB/s]\n\nDownloading data:   5%|5         | 363M/7.07G [00:05&lt;01:40, 66.6MB/s]\n\nDownloading data:   5%|5         | 370M/7.07G [00:06&lt;01:40, 66.6MB/s]\n\nDownloading data:   5%|5         | 377M/7.07G [00:06&lt;01:40, 66.7MB/s]\n\nDownloading data:   5%|5         | 383M/7.07G [00:06&lt;01:41, 65.9MB/s]\n\nDownloading data:   6%|5         | 390M/7.07G [00:06&lt;01:41, 66.0MB/s]\n\nDownloading data:   6%|5         | 397M/7.07G [00:06&lt;01:40, 66.2MB/s]\n\nDownloading data:   6%|5         | 403M/7.07G [00:06&lt;01:40, 66.4MB/s]\n\nDownloading data:   6%|5         | 410M/7.07G [00:06&lt;01:40, 66.4MB/s]\n\nDownloading data:   6%|5         | 417M/7.07G [00:06&lt;01:40, 66.2MB/s]\n\nDownloading data:   6%|5         | 423M/7.07G [00:06&lt;01:40, 66.4MB/s]\n\nDownloading data:   6%|6         | 430M/7.07G [00:06&lt;01:39, 66.6MB/s]\n\nDownloading data:   6%|6         | 437M/7.07G [00:07&lt;01:39, 66.8MB/s]\n\nDownloading data:   6%|6         | 443M/7.07G [00:07&lt;01:39, 66.6MB/s]\n\nDownloading data:   6%|6         | 450M/7.07G [00:07&lt;01:39, 66.8MB/s]\n\nDownloading data:   6%|6         | 457M/7.07G [00:07&lt;01:38, 66.9MB/s]\n\nDownloading data:   7%|6         | 464M/7.07G [00:07&lt;01:38, 67.1MB/s]\n\nDownloading data:   7%|6         | 470M/7.07G [00:07&lt;01:38, 67.2MB/s]\n\nDownloading data:   7%|6         | 477M/7.07G [00:07&lt;01:38, 67.2MB/s]\n\nDownloading data:   7%|6         | 484M/7.07G [00:07&lt;01:38, 66.9MB/s]\n\nDownloading data:   7%|6         | 491M/7.07G [00:07&lt;01:38, 66.9MB/s]\n\nDownloading data:   7%|7         | 497M/7.07G [00:07&lt;01:38, 66.8MB/s]\n\nDownloading data:   7%|7         | 504M/7.07G [00:08&lt;01:39, 66.3MB/s]\n\nDownloading data:   7%|7         | 511M/7.07G [00:08&lt;01:39, 65.7MB/s]\n\nDownloading data:   7%|7         | 517M/7.07G [00:08&lt;01:39, 65.9MB/s]\n\nDownloading data:   7%|7         | 524M/7.07G [00:08&lt;01:38, 66.3MB/s]\n\nDownloading data:   8%|7         | 531M/7.07G [00:08&lt;01:38, 66.3MB/s]\n\nDownloading data:   8%|7         | 537M/7.07G [00:08&lt;01:38, 66.7MB/s]\n\nDownloading data:   8%|7         | 544M/7.07G [00:08&lt;01:37, 66.8MB/s]\n\nDownloading data:   8%|7         | 551M/7.07G [00:08&lt;01:37, 66.8MB/s]\n\nDownloading data:   8%|7         | 557M/7.07G [00:08&lt;01:37, 66.7MB/s]\n\nDownloading data:   8%|7         | 564M/7.07G [00:08&lt;01:37, 66.8MB/s]\n\nDownloading data:   8%|8         | 571M/7.07G [00:09&lt;01:37, 66.9MB/s]\n\nDownloading data:   8%|8         | 578M/7.07G [00:09&lt;01:37, 66.9MB/s]\n\nDownloading data:   8%|8         | 584M/7.07G [00:09&lt;01:37, 66.5MB/s]\n\nDownloading data:   8%|8         | 591M/7.07G [00:09&lt;01:39, 65.2MB/s]\n\nDownloading data:   8%|8         | 597M/7.07G [00:09&lt;01:39, 65.3MB/s]\n\nDownloading data:   9%|8         | 604M/7.07G [00:09&lt;01:38, 65.7MB/s]\n\nDownloading data:   9%|8         | 611M/7.07G [00:09&lt;01:37, 66.0MB/s]\n\nDownloading data:   9%|8         | 617M/7.07G [00:09&lt;01:38, 65.6MB/s]\n\nDownloading data:   9%|8         | 624M/7.07G [00:09&lt;01:39, 64.8MB/s]\n\nDownloading data:   9%|8         | 631M/7.07G [00:09&lt;01:38, 65.5MB/s]\n\nDownloading data:   9%|9         | 637M/7.07G [00:10&lt;01:37, 65.9MB/s]\n\nDownloading data:   9%|9         | 644M/7.07G [00:10&lt;01:37, 66.0MB/s]\n\nDownloading data:   9%|9         | 651M/7.07G [00:10&lt;02:26, 43.8MB/s]\n\nDownloading data:   9%|9         | 657M/7.07G [00:10&lt;02:13, 48.1MB/s]\n\nDownloading data:   9%|9         | 663M/7.07G [00:10&lt;02:04, 51.3MB/s]\n\nDownloading data:   9%|9         | 669M/7.07G [00:10&lt;01:57, 54.5MB/s]\n\nDownloading data:  10%|9         | 676M/7.07G [00:10&lt;01:51, 57.5MB/s]\n\nDownloading data:  10%|9         | 683M/7.07G [00:10&lt;01:46, 60.1MB/s]\n\nDownloading data:  10%|9         | 689M/7.07G [00:11&lt;01:44, 60.9MB/s]\n\nDownloading data:  10%|9         | 696M/7.07G [00:11&lt;01:41, 62.5MB/s]\n\nDownloading data:  10%|9         | 702M/7.07G [00:11&lt;01:40, 63.5MB/s]\n\nDownloading data:  10%|#         | 709M/7.07G [00:11&lt;01:39, 64.0MB/s]\n\nDownloading data:  10%|#         | 715M/7.07G [00:11&lt;01:38, 64.6MB/s]\n\nDownloading data:  10%|#         | 722M/7.07G [00:11&lt;01:37, 65.4MB/s]\n\nDownloading data:  10%|#         | 729M/7.07G [00:11&lt;01:39, 63.8MB/s]\n\nDownloading data:  10%|#         | 735M/7.07G [00:11&lt;01:41, 62.7MB/s]\n\nDownloading data:  10%|#         | 741M/7.07G [00:11&lt;01:40, 62.8MB/s]\n\nDownloading data:  11%|#         | 748M/7.07G [00:11&lt;01:40, 62.8MB/s]\n\nDownloading data:  11%|#         | 754M/7.07G [00:12&lt;01:41, 62.5MB/s]\n\nDownloading data:  11%|#         | 760M/7.07G [00:12&lt;01:41, 62.1MB/s]\n\nDownloading data:  11%|#         | 766M/7.07G [00:12&lt;01:41, 62.2MB/s]\n\nDownloading data:  11%|#         | 773M/7.07G [00:12&lt;01:41, 62.3MB/s]\n\nDownloading data:  11%|#1        | 779M/7.07G [00:12&lt;01:41, 62.1MB/s]\n\nDownloading data:  11%|#1        | 785M/7.07G [00:12&lt;01:41, 61.9MB/s]\n\nDownloading data:  11%|#1        | 791M/7.07G [00:12&lt;01:40, 62.2MB/s]\n\nDownloading data:  11%|#1        | 798M/7.07G [00:12&lt;01:40, 62.3MB/s]\n\nDownloading data:  11%|#1        | 804M/7.07G [00:12&lt;01:40, 62.6MB/s]\n\nDownloading data:  11%|#1        | 810M/7.07G [00:12&lt;01:53, 55.3MB/s]\n\nDownloading data:  12%|#1        | 816M/7.07G [00:13&lt;01:53, 55.3MB/s]\n\nDownloading data:  12%|#1        | 822M/7.07G [00:13&lt;01:52, 55.4MB/s]\n\nDownloading data:  12%|#1        | 827M/7.07G [00:13&lt;01:51, 56.0MB/s]\n\nDownloading data:  12%|#1        | 833M/7.07G [00:13&lt;01:50, 56.4MB/s]\n\nDownloading data:  12%|#1        | 839M/7.07G [00:13&lt;01:49, 56.7MB/s]\n\nDownloading data:  12%|#1        | 845M/7.07G [00:13&lt;01:49, 56.8MB/s]\n\nDownloading data:  12%|#2        | 850M/7.07G [00:13&lt;01:49, 56.8MB/s]\n\nDownloading data:  12%|#2        | 856M/7.07G [00:13&lt;01:49, 56.9MB/s]\n\nDownloading data:  12%|#2        | 862M/7.07G [00:13&lt;01:47, 57.6MB/s]\n\nDownloading data:  12%|#2        | 868M/7.07G [00:14&lt;01:46, 58.5MB/s]\n\nDownloading data:  12%|#2        | 874M/7.07G [00:14&lt;01:45, 58.9MB/s]\n\nDownloading data:  12%|#2        | 880M/7.07G [00:14&lt;01:44, 59.3MB/s]\n\nDownloading data:  13%|#2        | 886M/7.07G [00:14&lt;01:44, 59.3MB/s]\n\nDownloading data:  13%|#2        | 892M/7.07G [00:14&lt;01:44, 59.1MB/s]\n\nDownloading data:  13%|#2        | 898M/7.07G [00:14&lt;01:43, 59.7MB/s]\n\nDownloading data:  13%|#2        | 904M/7.07G [00:14&lt;01:42, 60.1MB/s]\n\nDownloading data:  13%|#2        | 910M/7.07G [00:14&lt;01:42, 59.9MB/s]\n\nDownloading data:  13%|#2        | 916M/7.07G [00:14&lt;01:42, 59.9MB/s]\n\nDownloading data:  13%|#3        | 922M/7.07G [00:14&lt;01:42, 60.0MB/s]\n\nDownloading data:  13%|#3        | 928M/7.07G [00:15&lt;01:42, 59.9MB/s]\n\nDownloading data:  13%|#3        | 934M/7.07G [00:15&lt;01:42, 59.9MB/s]\n\nDownloading data:  13%|#3        | 940M/7.07G [00:15&lt;01:41, 60.5MB/s]\n\nDownloading data:  13%|#3        | 946M/7.07G [00:15&lt;01:41, 60.4MB/s]\n\nDownloading data:  13%|#3        | 952M/7.07G [00:15&lt;01:40, 60.6MB/s]\n\nDownloading data:  14%|#3        | 959M/7.07G [00:15&lt;01:40, 60.9MB/s]\n\nDownloading data:  14%|#3        | 965M/7.07G [00:15&lt;01:40, 60.6MB/s]\n\nDownloading data:  14%|#3        | 971M/7.07G [00:15&lt;01:41, 60.4MB/s]\n\nDownloading data:  14%|#3        | 977M/7.07G [00:15&lt;01:40, 60.5MB/s]\n\nDownloading data:  14%|#3        | 983M/7.07G [00:15&lt;01:39, 61.0MB/s]\n\nDownloading data:  14%|#3        | 989M/7.07G [00:16&lt;01:40, 60.6MB/s]\n\nDownloading data:  14%|#4        | 995M/7.07G [00:16&lt;01:39, 60.9MB/s]\n\nDownloading data:  14%|#4        | 1.00G/7.07G [00:16&lt;01:39, 60.8MB/s]\n\nDownloading data:  14%|#4        | 1.01G/7.07G [00:16&lt;01:40, 60.4MB/s]\n\nDownloading data:  14%|#4        | 1.01G/7.07G [00:16&lt;01:40, 60.3MB/s]\n\nDownloading data:  14%|#4        | 1.02G/7.07G [00:16&lt;01:40, 60.2MB/s]\n\nDownloading data:  15%|#4        | 1.03G/7.07G [00:16&lt;01:41, 59.5MB/s]\n\nDownloading data:  15%|#4        | 1.03G/7.07G [00:16&lt;01:41, 59.7MB/s]\n\nDownloading data:  15%|#4        | 1.04G/7.07G [00:16&lt;01:41, 59.7MB/s]\n\nDownloading data:  15%|#4        | 1.04G/7.07G [00:16&lt;01:41, 59.1MB/s]\n\nDownloading data:  15%|#4        | 1.05G/7.07G [00:17&lt;01:41, 59.3MB/s]\n\nDownloading data:  15%|#4        | 1.06G/7.07G [00:17&lt;01:42, 58.7MB/s]\n\nDownloading data:  15%|#5        | 1.06G/7.07G [00:17&lt;01:41, 59.1MB/s]\n\nDownloading data:  15%|#5        | 1.07G/7.07G [00:17&lt;01:40, 59.7MB/s]\n\nDownloading data:  15%|#5        | 1.07G/7.07G [00:17&lt;01:40, 59.5MB/s]\n\nDownloading data:  15%|#5        | 1.08G/7.07G [00:17&lt;01:40, 59.8MB/s]\n\nDownloading data:  15%|#5        | 1.09G/7.07G [00:17&lt;01:40, 59.6MB/s]\n\nDownloading data:  15%|#5        | 1.09G/7.07G [00:17&lt;01:40, 59.6MB/s]\n\nDownloading data:  16%|#5        | 1.10G/7.07G [00:17&lt;01:40, 59.5MB/s]\n\nDownloading data:  16%|#5        | 1.10G/7.07G [00:17&lt;01:40, 59.7MB/s]\n\nDownloading data:  16%|#5        | 1.11G/7.07G [00:18&lt;01:54, 52.0MB/s]\n\nDownloading data:  16%|#5        | 1.12G/7.07G [00:18&lt;01:50, 54.0MB/s]\n\nDownloading data:  16%|#5        | 1.12G/7.07G [00:18&lt;01:47, 55.6MB/s]\n\nDownloading data:  16%|#5        | 1.13G/7.07G [00:18&lt;01:44, 56.9MB/s]\n\nDownloading data:  16%|#6        | 1.13G/7.07G [00:18&lt;01:39, 59.4MB/s]\n\nDownloading data:  16%|#6        | 1.14G/7.07G [00:18&lt;01:36, 61.6MB/s]\n\nDownloading data:  16%|#6        | 1.15G/7.07G [00:18&lt;01:34, 62.7MB/s]\n\nDownloading data:  16%|#6        | 1.15G/7.07G [00:18&lt;01:33, 63.4MB/s]\n\nDownloading data:  16%|#6        | 1.16G/7.07G [00:18&lt;01:32, 63.9MB/s]\n\nDownloading data:  16%|#6        | 1.17G/7.07G [00:18&lt;01:31, 64.7MB/s]\n\nDownloading data:  17%|#6        | 1.17G/7.07G [00:19&lt;01:30, 65.0MB/s]\n\nDownloading data:  17%|#6        | 1.18G/7.07G [00:19&lt;01:29, 65.6MB/s]\n\nDownloading data:  17%|#6        | 1.19G/7.07G [00:19&lt;01:29, 66.1MB/s]\n\nDownloading data:  17%|#6        | 1.19G/7.07G [00:19&lt;01:28, 66.3MB/s]\n\nDownloading data:  17%|#6        | 1.20G/7.07G [00:19&lt;01:28, 66.3MB/s]\n\nDownloading data:  17%|#7        | 1.21G/7.07G [00:19&lt;01:28, 66.5MB/s]\n\nDownloading data:  17%|#7        | 1.21G/7.07G [00:19&lt;01:27, 66.7MB/s]\n\nDownloading data:  17%|#7        | 1.22G/7.07G [00:19&lt;01:27, 66.7MB/s]\n\nDownloading data:  17%|#7        | 1.23G/7.07G [00:19&lt;01:27, 66.6MB/s]\n\nDownloading data:  17%|#7        | 1.23G/7.07G [00:19&lt;01:27, 66.6MB/s]\n\nDownloading data:  18%|#7        | 1.24G/7.07G [00:20&lt;01:27, 66.7MB/s]\n\nDownloading data:  18%|#7        | 1.25G/7.07G [00:20&lt;01:27, 66.5MB/s]\n\nDownloading data:  18%|#7        | 1.25G/7.07G [00:20&lt;01:27, 66.6MB/s]\n\nDownloading data:  18%|#7        | 1.26G/7.07G [00:20&lt;01:27, 66.7MB/s]\n\nDownloading data:  18%|#7        | 1.27G/7.07G [00:20&lt;01:27, 66.5MB/s]\n\nDownloading data:  18%|#8        | 1.27G/7.07G [00:20&lt;01:27, 66.5MB/s]\n\nDownloading data:  18%|#8        | 1.28G/7.07G [00:20&lt;01:27, 66.3MB/s]\n\nDownloading data:  18%|#8        | 1.29G/7.07G [00:20&lt;01:27, 66.5MB/s]\n\nDownloading data:  18%|#8        | 1.29G/7.07G [00:20&lt;01:27, 66.4MB/s]\n\nDownloading data:  18%|#8        | 1.30G/7.07G [00:20&lt;01:26, 66.5MB/s]\n\nDownloading data:  18%|#8        | 1.31G/7.07G [00:21&lt;01:26, 66.5MB/s]\n\nDownloading data:  19%|#8        | 1.31G/7.07G [00:21&lt;01:26, 66.4MB/s]\n\nDownloading data:  19%|#8        | 1.32G/7.07G [00:21&lt;01:26, 66.5MB/s]\n\nDownloading data:  19%|#8        | 1.33G/7.07G [00:21&lt;01:26, 66.6MB/s]\n\nDownloading data:  19%|#8        | 1.33G/7.07G [00:21&lt;01:26, 66.5MB/s]\n\nDownloading data:  19%|#8        | 1.34G/7.07G [00:21&lt;01:26, 66.5MB/s]\n\nDownloading data:  19%|#9        | 1.35G/7.07G [00:21&lt;01:26, 66.2MB/s]\n\nDownloading data:  19%|#9        | 1.35G/7.07G [00:21&lt;01:26, 66.3MB/s]\n\nDownloading data:  19%|#9        | 1.36G/7.07G [00:21&lt;01:25, 66.5MB/s]\n\nDownloading data:  19%|#9        | 1.37G/7.07G [00:21&lt;01:25, 66.6MB/s]\n\nDownloading data:  19%|#9        | 1.37G/7.07G [00:22&lt;01:25, 66.5MB/s]\n\nDownloading data:  20%|#9        | 1.38G/7.07G [00:22&lt;01:25, 66.5MB/s]\n\nDownloading data:  20%|#9        | 1.39G/7.07G [00:22&lt;01:25, 66.6MB/s]\n\nDownloading data:  20%|#9        | 1.39G/7.07G [00:22&lt;01:26, 65.8MB/s]\n\nDownloading data:  20%|#9        | 1.40G/7.07G [00:22&lt;01:27, 65.1MB/s]\n\nDownloading data:  20%|#9        | 1.41G/7.07G [00:22&lt;01:27, 64.5MB/s]\n\nDownloading data:  20%|#9        | 1.41G/7.07G [00:22&lt;01:29, 63.1MB/s]\n\nDownloading data:  20%|##        | 1.42G/7.07G [00:22&lt;01:30, 62.4MB/s]\n\nDownloading data:  20%|##        | 1.43G/7.07G [00:22&lt;01:29, 62.8MB/s]\n\nDownloading data:  20%|##        | 1.43G/7.07G [00:23&lt;01:30, 62.3MB/s]\n\nDownloading data:  20%|##        | 1.44G/7.07G [00:23&lt;01:30, 62.1MB/s]\n\nDownloading data:  20%|##        | 1.44G/7.07G [00:23&lt;01:30, 62.0MB/s]\n\nDownloading data:  21%|##        | 1.45G/7.07G [00:23&lt;01:30, 61.9MB/s]\n\nDownloading data:  21%|##        | 1.46G/7.07G [00:23&lt;01:30, 62.0MB/s]\n\nDownloading data:  21%|##        | 1.46G/7.07G [00:23&lt;01:30, 61.9MB/s]\n\nDownloading data:  21%|##        | 1.47G/7.07G [00:23&lt;01:30, 61.9MB/s]\n\nDownloading data:  21%|##        | 1.48G/7.07G [00:23&lt;01:29, 62.2MB/s]\n\nDownloading data:  21%|##        | 1.48G/7.07G [00:23&lt;01:30, 62.1MB/s]\n\nDownloading data:  21%|##1       | 1.49G/7.07G [00:23&lt;01:29, 62.1MB/s]\n\nDownloading data:  21%|##1       | 1.49G/7.07G [00:24&lt;01:29, 62.2MB/s]\n\nDownloading data:  21%|##1       | 1.50G/7.07G [00:24&lt;01:30, 61.7MB/s]\n\nDownloading data:  21%|##1       | 1.51G/7.07G [00:24&lt;01:30, 61.4MB/s]\n\nDownloading data:  21%|##1       | 1.51G/7.07G [00:24&lt;01:30, 61.6MB/s]\n\nDownloading data:  21%|##1       | 1.52G/7.07G [00:24&lt;01:29, 62.0MB/s]\n\nDownloading data:  22%|##1       | 1.53G/7.07G [00:24&lt;01:29, 62.3MB/s]\n\nDownloading data:  22%|##1       | 1.53G/7.07G [00:24&lt;01:28, 62.4MB/s]\n\nDownloading data:  22%|##1       | 1.54G/7.07G [00:24&lt;01:28, 62.5MB/s]\n\nDownloading data:  22%|##1       | 1.54G/7.07G [00:24&lt;01:29, 61.8MB/s]\n\nDownloading data:  22%|##1       | 1.55G/7.07G [00:24&lt;01:30, 61.1MB/s]\n\nDownloading data:  22%|##2       | 1.56G/7.07G [00:25&lt;01:29, 61.5MB/s]\n\nDownloading data:  22%|##2       | 1.56G/7.07G [00:25&lt;01:30, 60.6MB/s]\n\nDownloading data:  22%|##2       | 1.57G/7.07G [00:25&lt;01:31, 60.4MB/s]\n\nDownloading data:  22%|##2       | 1.58G/7.07G [00:25&lt;01:30, 61.0MB/s]\n\nDownloading data:  22%|##2       | 1.58G/7.07G [00:25&lt;01:29, 61.1MB/s]\n\nDownloading data:  22%|##2       | 1.59G/7.07G [00:25&lt;01:29, 61.4MB/s]\n\nDownloading data:  23%|##2       | 1.59G/7.07G [00:25&lt;01:27, 62.7MB/s]\n\nDownloading data:  23%|##2       | 1.60G/7.07G [00:25&lt;01:31, 60.0MB/s]\n\nDownloading data:  23%|##2       | 1.61G/7.07G [00:25&lt;01:30, 60.6MB/s]\n\nDownloading data:  23%|##2       | 1.61G/7.07G [00:25&lt;01:29, 61.1MB/s]\n\nDownloading data:  23%|##2       | 1.62G/7.07G [00:26&lt;01:29, 61.2MB/s]\n\nDownloading data:  23%|##2       | 1.63G/7.07G [00:26&lt;01:29, 60.9MB/s]\n\nDownloading data:  23%|##3       | 1.63G/7.07G [00:26&lt;01:29, 60.7MB/s]\n\nDownloading data:  23%|##3       | 1.64G/7.07G [00:26&lt;01:28, 61.1MB/s]\n\nDownloading data:  23%|##3       | 1.64G/7.07G [00:26&lt;01:28, 61.5MB/s]\n\nDownloading data:  23%|##3       | 1.65G/7.07G [00:26&lt;01:29, 60.9MB/s]\n\nDownloading data:  23%|##3       | 1.66G/7.07G [00:26&lt;01:28, 61.1MB/s]\n\nDownloading data:  24%|##3       | 1.66G/7.07G [00:26&lt;01:29, 60.6MB/s]\n\nDownloading data:  24%|##3       | 1.67G/7.07G [00:26&lt;01:29, 60.3MB/s]\n\nDownloading data:  24%|##3       | 1.67G/7.07G [00:26&lt;01:28, 61.0MB/s]\n\nDownloading data:  24%|##3       | 1.68G/7.07G [00:27&lt;01:28, 61.2MB/s]\n\nDownloading data:  24%|##3       | 1.69G/7.07G [00:27&lt;01:27, 61.6MB/s]\n\nDownloading data:  24%|##3       | 1.69G/7.07G [00:27&lt;01:28, 61.0MB/s]\n\nDownloading data:  24%|##4       | 1.70G/7.07G [00:27&lt;01:29, 60.1MB/s]\n\nDownloading data:  24%|##4       | 1.71G/7.07G [00:27&lt;01:29, 60.2MB/s]\n\nDownloading data:  24%|##4       | 1.71G/7.07G [00:27&lt;01:28, 60.3MB/s]\n\nDownloading data:  24%|##4       | 1.72G/7.07G [00:27&lt;01:27, 61.0MB/s]\n\nDownloading data:  24%|##4       | 1.72G/7.07G [00:27&lt;01:26, 61.6MB/s]\n\nDownloading data:  24%|##4       | 1.73G/7.07G [00:27&lt;01:27, 61.3MB/s]\n\nDownloading data:  25%|##4       | 1.74G/7.07G [00:27&lt;01:26, 61.5MB/s]\n\nDownloading data:  25%|##4       | 1.74G/7.07G [00:28&lt;01:26, 61.9MB/s]\n\nDownloading data:  25%|##4       | 1.75G/7.07G [00:28&lt;01:25, 62.1MB/s]\n\nDownloading data:  25%|##4       | 1.75G/7.07G [00:28&lt;01:26, 61.5MB/s]\n\nDownloading data:  25%|##4       | 1.76G/7.07G [00:28&lt;01:28, 60.1MB/s]\n\nDownloading data:  25%|##4       | 1.77G/7.07G [00:28&lt;01:27, 60.4MB/s]\n\nDownloading data:  25%|##5       | 1.77G/7.07G [00:28&lt;01:26, 61.1MB/s]\n\nDownloading data:  25%|##5       | 1.78G/7.07G [00:28&lt;01:26, 61.3MB/s]\n\nDownloading data:  25%|##5       | 1.79G/7.07G [00:28&lt;01:27, 60.1MB/s]\n\nDownloading data:  25%|##5       | 1.79G/7.07G [00:28&lt;01:26, 60.9MB/s]\n\nDownloading data:  25%|##5       | 1.80G/7.07G [00:28&lt;01:25, 61.4MB/s]\n\nDownloading data:  26%|##5       | 1.80G/7.07G [00:29&lt;01:26, 61.2MB/s]\n\nDownloading data:  26%|##5       | 1.81G/7.07G [00:29&lt;01:26, 60.6MB/s]\n\nDownloading data:  26%|##5       | 1.82G/7.07G [00:29&lt;01:27, 60.2MB/s]\n\nDownloading data:  26%|##5       | 1.82G/7.07G [00:29&lt;01:26, 60.6MB/s]\n\nDownloading data:  26%|##5       | 1.83G/7.07G [00:29&lt;01:25, 61.0MB/s]\n\nDownloading data:  26%|##5       | 1.84G/7.07G [00:29&lt;01:25, 61.4MB/s]\n\nDownloading data:  26%|##6       | 1.84G/7.07G [00:29&lt;01:25, 61.5MB/s]\n\nDownloading data:  26%|##6       | 1.85G/7.07G [00:29&lt;01:25, 60.8MB/s]\n\nDownloading data:  26%|##6       | 1.85G/7.07G [00:29&lt;01:25, 61.0MB/s]\n\nDownloading data:  26%|##6       | 1.86G/7.07G [00:29&lt;01:25, 60.8MB/s]\n\nDownloading data:  26%|##6       | 1.87G/7.07G [00:30&lt;01:26, 60.4MB/s]\n\nDownloading data:  26%|##6       | 1.87G/7.07G [00:30&lt;01:26, 59.9MB/s]\n\nDownloading data:  27%|##6       | 1.88G/7.07G [00:30&lt;01:26, 59.7MB/s]\n\nDownloading data:  27%|##6       | 1.88G/7.07G [00:30&lt;01:25, 60.4MB/s]\n\nDownloading data:  27%|##6       | 1.89G/7.07G [00:30&lt;01:24, 61.0MB/s]\n\nDownloading data:  27%|##6       | 1.90G/7.07G [00:30&lt;01:23, 61.9MB/s]\n\nDownloading data:  27%|##6       | 1.90G/7.07G [00:30&lt;01:22, 62.7MB/s]\n\nDownloading data:  27%|##7       | 1.91G/7.07G [00:30&lt;01:21, 63.5MB/s]\n\nDownloading data:  27%|##7       | 1.92G/7.07G [00:30&lt;01:20, 64.0MB/s]\n\nDownloading data:  27%|##7       | 1.92G/7.07G [00:31&lt;01:20, 64.2MB/s]\n\nDownloading data:  27%|##7       | 1.93G/7.07G [00:31&lt;01:20, 64.1MB/s]\n\nDownloading data:  27%|##7       | 1.94G/7.07G [00:31&lt;01:20, 63.6MB/s]\n\nDownloading data:  27%|##7       | 1.94G/7.07G [00:31&lt;01:21, 62.8MB/s]\n\nDownloading data:  28%|##7       | 1.95G/7.07G [00:31&lt;01:21, 63.0MB/s]\n\nDownloading data:  28%|##7       | 1.95G/7.07G [00:31&lt;01:27, 58.8MB/s]\n\nDownloading data:  28%|##7       | 1.96G/7.07G [00:31&lt;01:31, 55.6MB/s]\n\nDownloading data:  28%|##7       | 1.97G/7.07G [00:31&lt;01:36, 53.0MB/s]\n\nDownloading data:  28%|##7       | 1.97G/7.07G [00:31&lt;01:39, 51.2MB/s]\n\nDownloading data:  28%|##7       | 1.98G/7.07G [00:31&lt;01:41, 50.0MB/s]\n\nDownloading data:  28%|##8       | 1.98G/7.07G [00:32&lt;01:41, 50.3MB/s]\n\nDownloading data:  28%|##8       | 1.99G/7.07G [00:32&lt;01:42, 49.8MB/s]\n\nDownloading data:  28%|##8       | 1.99G/7.07G [00:32&lt;01:45, 48.2MB/s]\n\nDownloading data:  28%|##8       | 2.00G/7.07G [00:32&lt;01:45, 48.2MB/s]\n\nDownloading data:  28%|##8       | 2.00G/7.07G [00:32&lt;01:45, 48.2MB/s]\n\nDownloading data:  28%|##8       | 2.01G/7.07G [00:32&lt;01:45, 48.2MB/s]\n\nDownloading data:  28%|##8       | 2.01G/7.07G [00:32&lt;01:45, 48.0MB/s]\n\nDownloading data:  29%|##8       | 2.02G/7.07G [00:32&lt;01:45, 48.0MB/s]\n\nDownloading data:  29%|##8       | 2.02G/7.07G [00:32&lt;01:44, 48.4MB/s]\n\nDownloading data:  29%|##8       | 2.03G/7.07G [00:33&lt;01:43, 48.7MB/s]\n\nDownloading data:  29%|##8       | 2.03G/7.07G [00:33&lt;01:44, 48.5MB/s]\n\nDownloading data:  29%|##8       | 2.04G/7.07G [00:33&lt;01:44, 48.4MB/s]\n\nDownloading data:  29%|##8       | 2.04G/7.07G [00:33&lt;01:41, 49.5MB/s]\n\nDownloading data:  29%|##8       | 2.05G/7.07G [00:33&lt;01:42, 49.2MB/s]\n\nDownloading data:  29%|##8       | 2.05G/7.07G [00:33&lt;01:42, 49.0MB/s]\n\nDownloading data:  29%|##9       | 2.06G/7.07G [00:33&lt;01:43, 48.5MB/s]\n\nDownloading data:  29%|##9       | 2.06G/7.07G [00:33&lt;01:44, 48.1MB/s]\n\nDownloading data:  29%|##9       | 2.07G/7.07G [00:33&lt;02:29, 33.6MB/s]\n\nDownloading data:  29%|##9       | 2.07G/7.07G [00:34&lt;02:13, 37.5MB/s]\n\nDownloading data:  29%|##9       | 2.08G/7.07G [00:34&lt;02:04, 40.3MB/s]\n\nDownloading data:  29%|##9       | 2.08G/7.07G [00:34&lt;01:57, 42.4MB/s]\n\nDownloading data:  29%|##9       | 2.09G/7.07G [00:34&lt;02:37, 31.6MB/s]\n\nDownloading data:  30%|##9       | 2.09G/7.07G [00:34&lt;02:21, 35.3MB/s]\n\nDownloading data:  30%|##9       | 2.09G/7.07G [00:34&lt;02:09, 38.6MB/s]\n\nDownloading data:  30%|##9       | 2.10G/7.07G [00:34&lt;02:45, 30.1MB/s]\n\nDownloading data:  30%|##9       | 2.10G/7.07G [00:35&lt;02:25, 34.1MB/s]\n\nDownloading data:  30%|##9       | 2.11G/7.07G [00:35&lt;02:15, 36.7MB/s]\n\nDownloading data:  30%|##9       | 2.11G/7.07G [00:35&lt;02:05, 39.6MB/s]\n\nDownloading data:  30%|##9       | 2.12G/7.07G [00:35&lt;01:57, 42.2MB/s]\n\nDownloading data:  30%|###       | 2.12G/7.07G [00:35&lt;01:52, 44.0MB/s]\n\nDownloading data:  30%|###       | 2.13G/7.07G [00:35&lt;01:51, 44.5MB/s]\n\nDownloading data:  30%|###       | 2.13G/7.07G [00:35&lt;01:45, 46.8MB/s]\n\nDownloading data:  30%|###       | 2.14G/7.07G [00:35&lt;01:44, 47.3MB/s]\n\nDownloading data:  30%|###       | 2.14G/7.07G [00:35&lt;01:42, 47.9MB/s]\n\nDownloading data:  30%|###       | 2.15G/7.07G [00:35&lt;01:41, 48.3MB/s]\n\nDownloading data:  30%|###       | 2.15G/7.07G [00:36&lt;01:41, 48.6MB/s]\n\nDownloading data:  31%|###       | 2.16G/7.07G [00:36&lt;01:40, 48.9MB/s]\n\nDownloading data:  31%|###       | 2.16G/7.07G [00:36&lt;01:40, 48.8MB/s]\n\nDownloading data:  31%|###       | 2.17G/7.07G [00:36&lt;01:40, 48.9MB/s]\n\nDownloading data:  31%|###       | 2.17G/7.07G [00:36&lt;01:39, 49.2MB/s]\n\nDownloading data:  31%|###       | 2.18G/7.07G [00:36&lt;01:42, 47.5MB/s]\n\nDownloading data:  31%|###       | 2.18G/7.07G [00:36&lt;01:42, 47.8MB/s]\n\nDownloading data:  31%|###       | 2.19G/7.07G [00:36&lt;01:41, 48.3MB/s]\n\nDownloading data:  31%|###1      | 2.19G/7.07G [00:36&lt;01:41, 48.1MB/s]\n\nDownloading data:  31%|###1      | 2.20G/7.07G [00:36&lt;01:40, 48.6MB/s]\n\nDownloading data:  31%|###1      | 2.20G/7.07G [00:37&lt;01:40, 48.7MB/s]\n\nDownloading data:  31%|###1      | 2.21G/7.07G [00:37&lt;01:39, 48.8MB/s]\n\nDownloading data:  31%|###1      | 2.21G/7.07G [00:37&lt;01:39, 48.9MB/s]\n\nDownloading data:  31%|###1      | 2.22G/7.07G [00:37&lt;01:39, 48.8MB/s]\n\nDownloading data:  31%|###1      | 2.22G/7.07G [00:37&lt;01:38, 49.1MB/s]\n\nDownloading data:  31%|###1      | 2.23G/7.07G [00:37&lt;01:39, 48.7MB/s]\n\nDownloading data:  32%|###1      | 2.23G/7.07G [00:37&lt;01:37, 49.6MB/s]\n\nDownloading data:  32%|###1      | 2.24G/7.07G [00:37&lt;01:37, 49.5MB/s]\n\nDownloading data:  32%|###1      | 2.24G/7.07G [00:37&lt;01:37, 49.3MB/s]\n\nDownloading data:  32%|###1      | 2.25G/7.07G [00:38&lt;01:41, 47.6MB/s]\n\nDownloading data:  32%|###1      | 2.25G/7.07G [00:38&lt;01:40, 48.0MB/s]\n\nDownloading data:  32%|###1      | 2.26G/7.07G [00:38&lt;01:39, 48.4MB/s]\n\nDownloading data:  32%|###1      | 2.26G/7.07G [00:38&lt;01:39, 48.4MB/s]\n\nDownloading data:  32%|###2      | 2.27G/7.07G [00:38&lt;01:38, 48.6MB/s]\n\nDownloading data:  32%|###2      | 2.27G/7.07G [00:38&lt;01:38, 48.7MB/s]\n\nDownloading data:  32%|###2      | 2.28G/7.07G [00:38&lt;01:38, 48.8MB/s]\n\nDownloading data:  32%|###2      | 2.28G/7.07G [00:38&lt;01:38, 48.7MB/s]\n\nDownloading data:  32%|###2      | 2.29G/7.07G [00:38&lt;01:39, 48.3MB/s]\n\nDownloading data:  32%|###2      | 2.29G/7.07G [00:38&lt;01:36, 49.4MB/s]\n\nDownloading data:  32%|###2      | 2.30G/7.07G [00:39&lt;01:36, 49.5MB/s]\n\nDownloading data:  33%|###2      | 2.30G/7.07G [00:39&lt;01:36, 49.5MB/s]\n\nDownloading data:  33%|###2      | 2.31G/7.07G [00:39&lt;01:36, 49.5MB/s]\n\nDownloading data:  33%|###2      | 2.31G/7.07G [00:39&lt;01:36, 49.4MB/s]\n\nDownloading data:  33%|###2      | 2.32G/7.07G [00:39&lt;01:37, 48.8MB/s]\n\nDownloading data:  33%|###2      | 2.32G/7.07G [00:39&lt;01:39, 47.8MB/s]\n\nDownloading data:  33%|###2      | 2.33G/7.07G [00:39&lt;01:40, 47.3MB/s]\n\nDownloading data:  33%|###2      | 2.33G/7.07G [00:39&lt;01:37, 48.7MB/s]\n\nDownloading data:  33%|###3      | 2.34G/7.07G [00:39&lt;01:37, 48.8MB/s]\n\nDownloading data:  33%|###3      | 2.34G/7.07G [00:39&lt;01:36, 49.0MB/s]\n\nDownloading data:  33%|###3      | 2.35G/7.07G [00:40&lt;01:36, 49.2MB/s]\n\nDownloading data:  33%|###3      | 2.35G/7.07G [00:40&lt;01:35, 49.5MB/s]\n\nDownloading data:  33%|###3      | 2.36G/7.07G [00:40&lt;01:35, 49.4MB/s]\n\nDownloading data:  33%|###3      | 2.36G/7.07G [00:40&lt;01:35, 49.3MB/s]\n\nDownloading data:  33%|###3      | 2.37G/7.07G [00:40&lt;01:35, 49.5MB/s]\n\nDownloading data:  34%|###3      | 2.37G/7.07G [00:40&lt;01:34, 49.8MB/s]\n\nDownloading data:  34%|###3      | 2.38G/7.07G [00:40&lt;01:34, 49.9MB/s]\n\nDownloading data:  34%|###3      | 2.38G/7.07G [00:40&lt;01:34, 49.5MB/s]\n\nDownloading data:  34%|###3      | 2.39G/7.07G [00:40&lt;01:34, 49.7MB/s]\n\nDownloading data:  34%|###3      | 2.39G/7.07G [00:40&lt;01:34, 49.5MB/s]\n\nDownloading data:  34%|###3      | 2.40G/7.07G [00:41&lt;01:34, 49.7MB/s]\n\nDownloading data:  34%|###3      | 2.40G/7.07G [00:41&lt;01:33, 49.8MB/s]\n\nDownloading data:  34%|###4      | 2.41G/7.07G [00:41&lt;01:34, 49.5MB/s]\n\nDownloading data:  34%|###4      | 2.41G/7.07G [00:41&lt;01:33, 50.0MB/s]\n\nDownloading data:  34%|###4      | 2.42G/7.07G [00:41&lt;01:33, 49.9MB/s]\n\nDownloading data:  34%|###4      | 2.42G/7.07G [00:41&lt;01:33, 49.8MB/s]\n\nDownloading data:  34%|###4      | 2.43G/7.07G [00:41&lt;01:33, 49.8MB/s]\n\nDownloading data:  34%|###4      | 2.43G/7.07G [00:41&lt;01:37, 47.7MB/s]\n\nDownloading data:  34%|###4      | 2.44G/7.07G [00:41&lt;01:29, 51.6MB/s]\n\nDownloading data:  35%|###4      | 2.44G/7.07G [00:41&lt;01:29, 51.6MB/s]\n\nDownloading data:  35%|###4      | 2.45G/7.07G [00:42&lt;01:29, 51.6MB/s]\n\nDownloading data:  35%|###4      | 2.45G/7.07G [00:42&lt;01:29, 51.6MB/s]\n\nDownloading data:  35%|###4      | 2.46G/7.07G [00:42&lt;01:30, 51.2MB/s]\n\nDownloading data:  35%|###4      | 2.46G/7.07G [00:42&lt;01:31, 50.5MB/s]\n\nDownloading data:  35%|###4      | 2.47G/7.07G [00:42&lt;01:29, 51.2MB/s]\n\nDownloading data:  35%|###4      | 2.47G/7.07G [00:42&lt;01:30, 51.0MB/s]\n\nDownloading data:  35%|###5      | 2.48G/7.07G [00:42&lt;01:30, 50.9MB/s]\n\nDownloading data:  35%|###5      | 2.48G/7.07G [00:42&lt;01:29, 51.5MB/s]\n\nDownloading data:  35%|###5      | 2.49G/7.07G [00:42&lt;01:32, 49.7MB/s]\n\nDownloading data:  35%|###5      | 2.50G/7.07G [00:42&lt;01:26, 53.0MB/s]\n\nDownloading data:  35%|###5      | 2.50G/7.07G [00:43&lt;01:26, 53.2MB/s]\n\nDownloading data:  35%|###5      | 2.51G/7.07G [00:43&lt;01:25, 53.3MB/s]\n\nDownloading data:  36%|###5      | 2.51G/7.07G [00:43&lt;01:26, 53.0MB/s]\n\nDownloading data:  36%|###5      | 2.52G/7.07G [00:43&lt;01:25, 53.0MB/s]\n\nDownloading data:  36%|###5      | 2.52G/7.07G [00:43&lt;01:25, 52.9MB/s]\n\nDownloading data:  36%|###5      | 2.53G/7.07G [00:43&lt;01:25, 53.0MB/s]\n\nDownloading data:  36%|###5      | 2.53G/7.07G [00:43&lt;01:25, 53.0MB/s]\n\nDownloading data:  36%|###5      | 2.54G/7.07G [00:43&lt;01:25, 52.8MB/s]\n\nDownloading data:  36%|###5      | 2.54G/7.07G [00:43&lt;01:24, 53.5MB/s]\n\nDownloading data:  36%|###6      | 2.55G/7.07G [00:43&lt;01:24, 53.6MB/s]\n\nDownloading data:  36%|###6      | 2.55G/7.07G [00:44&lt;01:24, 53.6MB/s]\n\nDownloading data:  36%|###6      | 2.56G/7.07G [00:44&lt;01:23, 53.8MB/s]\n\nDownloading data:  36%|###6      | 2.57G/7.07G [00:44&lt;01:22, 54.7MB/s]\n\nDownloading data:  36%|###6      | 2.57G/7.07G [00:44&lt;01:21, 54.9MB/s]\n\nDownloading data:  36%|###6      | 2.58G/7.07G [00:44&lt;01:21, 55.2MB/s]\n\nDownloading data:  37%|###6      | 2.58G/7.07G [00:44&lt;01:22, 54.5MB/s]\n\nDownloading data:  37%|###6      | 2.59G/7.07G [00:44&lt;01:21, 54.9MB/s]\n\nDownloading data:  37%|###6      | 2.59G/7.07G [00:44&lt;01:20, 55.4MB/s]\n\nDownloading data:  37%|###6      | 2.60G/7.07G [00:44&lt;01:19, 56.0MB/s]\n\nDownloading data:  37%|###6      | 2.60G/7.07G [00:44&lt;01:18, 56.8MB/s]\n\nDownloading data:  37%|###6      | 2.61G/7.07G [00:45&lt;01:18, 56.8MB/s]\n\nDownloading data:  37%|###6      | 2.62G/7.07G [00:45&lt;01:20, 55.2MB/s]\n\nDownloading data:  37%|###7      | 2.62G/7.07G [00:45&lt;01:19, 55.9MB/s]\n\nDownloading data:  37%|###7      | 2.63G/7.07G [00:45&lt;01:19, 56.3MB/s]\n\nDownloading data:  37%|###7      | 2.63G/7.07G [00:45&lt;01:17, 57.0MB/s]\n\nDownloading data:  37%|###7      | 2.64G/7.07G [00:45&lt;01:16, 57.7MB/s]\n\nDownloading data:  37%|###7      | 2.65G/7.07G [00:45&lt;01:17, 56.9MB/s]\n\nDownloading data:  37%|###7      | 2.65G/7.07G [00:45&lt;01:17, 56.8MB/s]\n\nDownloading data:  38%|###7      | 2.66G/7.07G [00:45&lt;01:17, 57.3MB/s]\n\nDownloading data:  38%|###7      | 2.66G/7.07G [00:45&lt;01:16, 57.6MB/s]\n\nDownloading data:  38%|###7      | 2.67G/7.07G [00:46&lt;01:16, 57.5MB/s]\n\nDownloading data:  38%|###7      | 2.67G/7.07G [00:46&lt;01:16, 57.2MB/s]\n\nDownloading data:  38%|###7      | 2.68G/7.07G [00:46&lt;01:17, 56.9MB/s]\n\nDownloading data:  38%|###7      | 2.69G/7.07G [00:46&lt;01:15, 58.0MB/s]\n\nDownloading data:  38%|###8      | 2.69G/7.07G [00:46&lt;01:14, 58.5MB/s]\n\nDownloading data:  38%|###8      | 2.70G/7.07G [00:46&lt;01:14, 58.7MB/s]\n\nDownloading data:  38%|###8      | 2.70G/7.07G [00:46&lt;01:14, 58.7MB/s]\n\nDownloading data:  38%|###8      | 2.71G/7.07G [00:46&lt;01:14, 58.4MB/s]\n\nDownloading data:  38%|###8      | 2.72G/7.07G [00:46&lt;01:14, 58.2MB/s]\n\nDownloading data:  38%|###8      | 2.72G/7.07G [00:47&lt;01:14, 58.4MB/s]\n\nDownloading data:  39%|###8      | 2.73G/7.07G [00:47&lt;01:14, 58.4MB/s]\n\nDownloading data:  39%|###8      | 2.73G/7.07G [00:47&lt;01:14, 58.0MB/s]\n\nDownloading data:  39%|###8      | 2.74G/7.07G [00:47&lt;01:14, 58.3MB/s]\n\nDownloading data:  39%|###8      | 2.74G/7.07G [00:47&lt;01:12, 59.4MB/s]\n\nDownloading data:  39%|###8      | 2.75G/7.07G [00:47&lt;01:12, 59.9MB/s]\n\nDownloading data:  39%|###8      | 2.76G/7.07G [00:47&lt;01:11, 60.4MB/s]\n\nDownloading data:  39%|###9      | 2.76G/7.07G [00:47&lt;01:10, 61.1MB/s]\n\nDownloading data:  39%|###9      | 2.77G/7.07G [00:47&lt;01:09, 61.5MB/s]\n\nDownloading data:  39%|###9      | 2.78G/7.07G [00:47&lt;01:09, 61.9MB/s]\n\nDownloading data:  39%|###9      | 2.78G/7.07G [00:48&lt;01:22, 51.7MB/s]\n\nDownloading data:  39%|###9      | 2.79G/7.07G [00:48&lt;01:16, 55.9MB/s]\n\nDownloading data:  40%|###9      | 2.80G/7.07G [00:48&lt;01:13, 58.3MB/s]\n\nDownloading data:  40%|###9      | 2.80G/7.07G [00:48&lt;01:11, 59.7MB/s]\n\nDownloading data:  40%|###9      | 2.81G/7.07G [00:48&lt;01:10, 60.6MB/s]\n\nDownloading data:  40%|###9      | 2.81G/7.07G [00:48&lt;01:08, 62.3MB/s]\n\nDownloading data:  40%|###9      | 2.82G/7.07G [00:48&lt;01:07, 63.1MB/s]\n\nDownloading data:  40%|###9      | 2.83G/7.07G [00:48&lt;01:06, 63.5MB/s]\n\nDownloading data:  40%|####      | 2.83G/7.07G [00:48&lt;01:05, 64.5MB/s]\n\nDownloading data:  40%|####      | 2.84G/7.07G [00:48&lt;01:05, 64.8MB/s]\n\nDownloading data:  40%|####      | 2.85G/7.07G [00:49&lt;01:03, 66.3MB/s]\n\nDownloading data:  40%|####      | 2.85G/7.07G [00:49&lt;01:03, 66.5MB/s]\n\nDownloading data:  40%|####      | 2.86G/7.07G [00:49&lt;01:03, 66.8MB/s]\n\nDownloading data:  41%|####      | 2.87G/7.07G [00:49&lt;01:03, 66.0MB/s]\n\nDownloading data:  41%|####      | 2.87G/7.07G [00:49&lt;01:03, 66.6MB/s]\n\nDownloading data:  41%|####      | 2.88G/7.07G [00:49&lt;01:02, 66.9MB/s]\n\nDownloading data:  41%|####      | 2.89G/7.07G [00:49&lt;01:02, 67.1MB/s]\n\nDownloading data:  41%|####      | 2.90G/7.07G [00:49&lt;01:02, 66.9MB/s]\n\nDownloading data:  41%|####1     | 2.90G/7.07G [00:49&lt;01:04, 64.3MB/s]\n\nDownloading data:  41%|####1     | 2.91G/7.07G [00:50&lt;01:08, 60.8MB/s]\n\nDownloading data:  41%|####1     | 2.91G/7.07G [00:50&lt;01:11, 58.2MB/s]\n\nDownloading data:  41%|####1     | 2.92G/7.07G [00:50&lt;01:11, 58.1MB/s]\n\nDownloading data:  41%|####1     | 2.93G/7.07G [00:50&lt;01:11, 58.1MB/s]\n\nDownloading data:  41%|####1     | 2.93G/7.07G [00:50&lt;01:11, 58.0MB/s]\n\nDownloading data:  42%|####1     | 2.94G/7.07G [00:50&lt;01:11, 57.6MB/s]\n\nDownloading data:  42%|####1     | 2.94G/7.07G [00:50&lt;01:11, 58.0MB/s]\n\nDownloading data:  42%|####1     | 2.95G/7.07G [00:50&lt;01:13, 56.1MB/s]\n\nDownloading data:  42%|####1     | 2.96G/7.07G [00:50&lt;01:09, 59.1MB/s]\n\nDownloading data:  42%|####1     | 2.96G/7.07G [00:50&lt;01:09, 59.5MB/s]\n\nDownloading data:  42%|####1     | 2.97G/7.07G [00:51&lt;01:08, 59.7MB/s]\n\nDownloading data:  42%|####2     | 2.97G/7.07G [00:51&lt;01:08, 60.2MB/s]\n\nDownloading data:  42%|####2     | 2.98G/7.07G [00:51&lt;01:08, 60.0MB/s]\n\nDownloading data:  42%|####2     | 2.99G/7.07G [00:51&lt;01:08, 59.9MB/s]\n\nDownloading data:  42%|####2     | 2.99G/7.07G [00:51&lt;01:07, 60.4MB/s]\n\nDownloading data:  42%|####2     | 3.00G/7.07G [00:51&lt;01:07, 60.0MB/s]\n\nDownloading data:  42%|####2     | 3.00G/7.07G [00:51&lt;01:08, 59.7MB/s]\n\nDownloading data:  43%|####2     | 3.01G/7.07G [00:51&lt;01:07, 60.5MB/s]\n\nDownloading data:  43%|####2     | 3.02G/7.07G [00:51&lt;01:06, 61.1MB/s]\n\nDownloading data:  43%|####2     | 3.02G/7.07G [00:51&lt;01:06, 60.9MB/s]\n\nDownloading data:  43%|####2     | 3.03G/7.07G [00:52&lt;01:06, 60.6MB/s]\n\nDownloading data:  43%|####2     | 3.04G/7.07G [00:52&lt;01:05, 62.1MB/s]\n\nDownloading data:  43%|####3     | 3.04G/7.07G [00:52&lt;01:04, 62.6MB/s]\n\nDownloading data:  43%|####3     | 3.05G/7.07G [00:52&lt;01:04, 62.8MB/s]\n\nDownloading data:  43%|####3     | 3.05G/7.07G [00:52&lt;01:03, 63.0MB/s]\n\nDownloading data:  43%|####3     | 3.06G/7.07G [00:52&lt;01:04, 62.1MB/s]\n\nDownloading data:  43%|####3     | 3.07G/7.07G [00:52&lt;01:04, 62.4MB/s]\n\nDownloading data:  43%|####3     | 3.07G/7.07G [00:52&lt;01:02, 63.7MB/s]\n\nDownloading data:  44%|####3     | 3.08G/7.07G [00:52&lt;01:02, 64.2MB/s]\n\nDownloading data:  44%|####3     | 3.09G/7.07G [00:52&lt;01:01, 64.9MB/s]\n\nDownloading data:  44%|####3     | 3.09G/7.07G [00:53&lt;01:01, 64.5MB/s]\n\nDownloading data:  44%|####3     | 3.10G/7.07G [00:53&lt;01:01, 64.9MB/s]\n\nDownloading data:  44%|####3     | 3.11G/7.07G [00:53&lt;01:00, 65.5MB/s]\n\nDownloading data:  44%|####4     | 3.11G/7.07G [00:53&lt;01:00, 65.9MB/s]\n\nDownloading data:  44%|####4     | 3.12G/7.07G [00:53&lt;00:59, 65.9MB/s]\n\nDownloading data:  44%|####4     | 3.13G/7.07G [00:53&lt;00:59, 66.5MB/s]\n\nDownloading data:  44%|####4     | 3.13G/7.07G [00:53&lt;00:59, 66.3MB/s]\n\nDownloading data:  44%|####4     | 3.14G/7.07G [00:53&lt;00:59, 65.9MB/s]\n\nDownloading data:  45%|####4     | 3.15G/7.07G [00:53&lt;00:58, 66.9MB/s]\n\nDownloading data:  45%|####4     | 3.15G/7.07G [00:53&lt;00:59, 66.1MB/s]\n\nDownloading data:  45%|####4     | 3.16G/7.07G [00:54&lt;00:58, 67.3MB/s]\n\nDownloading data:  45%|####4     | 3.17G/7.07G [00:54&lt;00:57, 67.4MB/s]\n\nDownloading data:  45%|####4     | 3.18G/7.07G [00:54&lt;00:56, 68.6MB/s]\n\nDownloading data:  45%|####4     | 3.18G/7.07G [00:54&lt;00:57, 67.7MB/s]\n\nDownloading data:  45%|####5     | 3.19G/7.07G [00:54&lt;00:56, 68.2MB/s]\n\nDownloading data:  45%|####5     | 3.20G/7.07G [00:54&lt;00:56, 68.1MB/s]\n\nDownloading data:  45%|####5     | 3.20G/7.07G [00:54&lt;00:56, 68.5MB/s]\n\nDownloading data:  45%|####5     | 3.21G/7.07G [00:54&lt;00:56, 68.0MB/s]\n\nDownloading data:  45%|####5     | 3.22G/7.07G [00:54&lt;00:57, 67.0MB/s]\n\nDownloading data:  46%|####5     | 3.22G/7.07G [00:54&lt;00:56, 67.6MB/s]\n\nDownloading data:  46%|####5     | 3.23G/7.07G [00:55&lt;00:56, 67.5MB/s]\n\nDownloading data:  46%|####5     | 3.24G/7.07G [00:55&lt;00:56, 67.9MB/s]\n\nDownloading data:  46%|####5     | 3.24G/7.07G [00:55&lt;00:56, 68.2MB/s]\n\nDownloading data:  46%|####5     | 3.25G/7.07G [00:55&lt;00:55, 69.2MB/s]\n\nDownloading data:  46%|####6     | 3.26G/7.07G [00:55&lt;00:54, 69.8MB/s]\n\nDownloading data:  46%|####6     | 3.27G/7.07G [00:55&lt;00:54, 70.4MB/s]\n\nDownloading data:  46%|####6     | 3.27G/7.07G [00:55&lt;00:56, 67.4MB/s]\n\nDownloading data:  46%|####6     | 3.28G/7.07G [00:55&lt;00:56, 67.7MB/s]\n\nDownloading data:  46%|####6     | 3.29G/7.07G [00:55&lt;00:55, 67.6MB/s]\n\nDownloading data:  47%|####6     | 3.29G/7.07G [00:56&lt;00:55, 68.3MB/s]\n\nDownloading data:  47%|####6     | 3.30G/7.07G [00:56&lt;00:54, 69.3MB/s]\n\nDownloading data:  47%|####6     | 3.31G/7.07G [00:56&lt;00:54, 69.0MB/s]\n\nDownloading data:  47%|####6     | 3.31G/7.07G [00:56&lt;00:55, 67.6MB/s]\n\nDownloading data:  47%|####6     | 3.32G/7.07G [00:56&lt;00:55, 67.2MB/s]\n\nDownloading data:  47%|####7     | 3.33G/7.07G [00:56&lt;00:56, 66.8MB/s]\n\nDownloading data:  47%|####7     | 3.33G/7.07G [00:56&lt;00:55, 67.5MB/s]\n\nDownloading data:  47%|####7     | 3.34G/7.07G [00:56&lt;00:54, 69.0MB/s]\n\nDownloading data:  47%|####7     | 3.35G/7.07G [00:56&lt;00:53, 69.5MB/s]\n\nDownloading data:  47%|####7     | 3.36G/7.07G [00:56&lt;00:52, 70.2MB/s]\n\nDownloading data:  48%|####7     | 3.36G/7.07G [00:57&lt;00:52, 70.0MB/s]\n\nDownloading data:  48%|####7     | 3.37G/7.07G [00:57&lt;00:52, 70.5MB/s]\n\nDownloading data:  48%|####7     | 3.38G/7.07G [00:57&lt;00:59, 61.9MB/s]\n\nDownloading data:  48%|####7     | 3.38G/7.07G [00:57&lt;01:03, 58.2MB/s]\n\nDownloading data:  48%|####7     | 3.39G/7.07G [00:57&lt;01:04, 57.4MB/s]\n\nDownloading data:  48%|####8     | 3.40G/7.07G [00:57&lt;01:04, 56.9MB/s]\n\nDownloading data:  48%|####8     | 3.40G/7.07G [00:57&lt;01:07, 54.6MB/s]\n\nDownloading data:  48%|####8     | 3.41G/7.07G [00:57&lt;01:07, 53.9MB/s]\n\nDownloading data:  48%|####8     | 3.41G/7.07G [00:57&lt;01:06, 55.1MB/s]\n\nDownloading data:  48%|####8     | 3.42G/7.07G [00:58&lt;01:05, 55.6MB/s]\n\nDownloading data:  48%|####8     | 3.42G/7.07G [00:58&lt;01:05, 55.8MB/s]\n\nDownloading data:  48%|####8     | 3.43G/7.07G [00:58&lt;01:06, 54.8MB/s]\n\nDownloading data:  49%|####8     | 3.44G/7.07G [00:58&lt;01:04, 56.2MB/s]\n\nDownloading data:  49%|####8     | 3.44G/7.07G [00:58&lt;01:04, 56.7MB/s]\n\nDownloading data:  49%|####8     | 3.45G/7.07G [00:58&lt;01:03, 57.3MB/s]\n\nDownloading data:  49%|####8     | 3.45G/7.07G [00:58&lt;01:03, 56.8MB/s]\n\nDownloading data:  49%|####8     | 3.46G/7.07G [00:58&lt;01:03, 57.4MB/s]\n\nDownloading data:  49%|####8     | 3.46G/7.07G [00:58&lt;01:02, 57.4MB/s]\n\nDownloading data:  49%|####9     | 3.47G/7.07G [00:58&lt;01:02, 57.3MB/s]\n\nDownloading data:  49%|####9     | 3.48G/7.07G [00:59&lt;01:03, 57.1MB/s]\n\nDownloading data:  49%|####9     | 3.48G/7.07G [00:59&lt;01:02, 57.3MB/s]\n\nDownloading data:  49%|####9     | 3.49G/7.07G [00:59&lt;01:02, 57.4MB/s]\n\nDownloading data:  49%|####9     | 3.49G/7.07G [00:59&lt;01:02, 57.5MB/s]\n\nDownloading data:  49%|####9     | 3.50G/7.07G [00:59&lt;01:01, 57.9MB/s]\n\nDownloading data:  50%|####9     | 3.51G/7.07G [00:59&lt;01:01, 58.1MB/s]\n\nDownloading data:  50%|####9     | 3.51G/7.07G [00:59&lt;01:00, 58.5MB/s]\n\nDownloading data:  50%|####9     | 3.52G/7.07G [00:59&lt;01:00, 58.6MB/s]\n\nDownloading data:  50%|####9     | 3.52G/7.07G [00:59&lt;01:00, 58.8MB/s]\n\nDownloading data:  50%|####9     | 3.53G/7.07G [00:59&lt;01:00, 58.9MB/s]\n\nDownloading data:  50%|####9     | 3.53G/7.07G [01:00&lt;01:00, 58.9MB/s]\n\nDownloading data:  50%|#####     | 3.54G/7.07G [01:00&lt;01:00, 58.8MB/s]\n\nDownloading data:  50%|#####     | 3.55G/7.07G [01:00&lt;00:59, 59.5MB/s]\n\nDownloading data:  50%|#####     | 3.55G/7.07G [01:00&lt;00:58, 60.1MB/s]\n\nDownloading data:  50%|#####     | 3.56G/7.07G [01:00&lt;00:58, 60.0MB/s]\n\nDownloading data:  50%|#####     | 3.56G/7.07G [01:00&lt;00:58, 59.6MB/s]\n\nDownloading data:  50%|#####     | 3.57G/7.07G [01:00&lt;00:58, 60.0MB/s]\n\nDownloading data:  51%|#####     | 3.58G/7.07G [01:00&lt;00:58, 59.8MB/s]\n\nDownloading data:  51%|#####     | 3.58G/7.07G [01:00&lt;00:57, 60.4MB/s]\n\nDownloading data:  51%|#####     | 3.59G/7.07G [01:00&lt;00:58, 59.1MB/s]\n\nDownloading data:  51%|#####     | 3.60G/7.07G [01:01&lt;00:57, 60.9MB/s]\n\nDownloading data:  51%|#####     | 3.60G/7.07G [01:01&lt;00:57, 60.5MB/s]\n\nDownloading data:  51%|#####1    | 3.61G/7.07G [01:01&lt;00:56, 61.0MB/s]\n\nDownloading data:  51%|#####1    | 3.61G/7.07G [01:01&lt;01:00, 57.4MB/s]\n\nDownloading data:  51%|#####1    | 3.62G/7.07G [01:01&lt;00:57, 60.2MB/s]\n\nDownloading data:  51%|#####1    | 3.63G/7.07G [01:01&lt;00:56, 61.3MB/s]\n\nDownloading data:  51%|#####1    | 3.63G/7.07G [01:01&lt;00:55, 62.1MB/s]\n\nDownloading data:  51%|#####1    | 3.64G/7.07G [01:01&lt;00:55, 62.0MB/s]\n\nDownloading data:  52%|#####1    | 3.65G/7.07G [01:01&lt;00:55, 62.1MB/s]\n\nDownloading data:  52%|#####1    | 3.65G/7.07G [01:01&lt;00:54, 62.3MB/s]\n\nDownloading data:  52%|#####1    | 3.66G/7.07G [01:02&lt;00:54, 62.3MB/s]\n\nDownloading data:  52%|#####1    | 3.66G/7.07G [01:02&lt;00:54, 62.3MB/s]\n\nDownloading data:  52%|#####1    | 3.67G/7.07G [01:02&lt;00:54, 62.4MB/s]\n\nDownloading data:  52%|#####1    | 3.68G/7.07G [01:02&lt;00:54, 62.3MB/s]\n\nDownloading data:  52%|#####2    | 3.68G/7.07G [01:02&lt;00:54, 61.9MB/s]\n\nDownloading data:  52%|#####2    | 3.69G/7.07G [01:02&lt;00:54, 62.5MB/s]\n\nDownloading data:  52%|#####2    | 3.70G/7.07G [01:02&lt;00:53, 63.0MB/s]\n\nDownloading data:  52%|#####2    | 3.70G/7.07G [01:02&lt;00:53, 63.0MB/s]\n\nDownloading data:  52%|#####2    | 3.71G/7.07G [01:02&lt;00:53, 62.7MB/s]\n\nDownloading data:  53%|#####2    | 3.72G/7.07G [01:02&lt;00:54, 61.9MB/s]\n\nDownloading data:  53%|#####2    | 3.72G/7.07G [01:03&lt;00:53, 62.5MB/s]\n\nDownloading data:  53%|#####2    | 3.73G/7.07G [01:03&lt;00:53, 62.5MB/s]\n\nDownloading data:  53%|#####2    | 3.73G/7.07G [01:03&lt;00:53, 62.8MB/s]\n\nDownloading data:  53%|#####2    | 3.74G/7.07G [01:03&lt;00:53, 62.8MB/s]\n\nDownloading data:  53%|#####2    | 3.75G/7.07G [01:03&lt;00:53, 62.6MB/s]\n\nDownloading data:  53%|#####3    | 3.75G/7.07G [01:03&lt;00:53, 61.5MB/s]\n\nDownloading data:  53%|#####3    | 3.76G/7.07G [01:03&lt;00:52, 63.1MB/s]\n\nDownloading data:  53%|#####3    | 3.77G/7.07G [01:03&lt;00:52, 63.2MB/s]\n\nDownloading data:  53%|#####3    | 3.77G/7.07G [01:03&lt;00:52, 62.8MB/s]\n\nDownloading data:  53%|#####3    | 3.78G/7.07G [01:03&lt;00:52, 62.8MB/s]\n\nDownloading data:  54%|#####3    | 3.78G/7.07G [01:04&lt;00:52, 63.1MB/s]\n\nDownloading data:  54%|#####3    | 3.79G/7.07G [01:04&lt;00:52, 63.1MB/s]\n\nDownloading data:  54%|#####3    | 3.80G/7.07G [01:04&lt;00:51, 63.2MB/s]\n\nDownloading data:  54%|#####3    | 3.80G/7.07G [01:04&lt;00:52, 62.7MB/s]\n\nDownloading data:  54%|#####3    | 3.81G/7.07G [01:04&lt;00:51, 63.1MB/s]\n\nDownloading data:  54%|#####3    | 3.82G/7.07G [01:04&lt;00:51, 63.0MB/s]\n\nDownloading data:  54%|#####4    | 3.82G/7.07G [01:04&lt;00:50, 63.8MB/s]\n\nDownloading data:  54%|#####4    | 3.83G/7.07G [01:04&lt;00:52, 62.2MB/s]\n\nDownloading data:  54%|#####4    | 3.84G/7.07G [01:04&lt;00:51, 63.2MB/s]\n\nDownloading data:  54%|#####4    | 3.84G/7.07G [01:05&lt;00:51, 63.3MB/s]\n\nDownloading data:  54%|#####4    | 3.85G/7.07G [01:05&lt;00:50, 63.4MB/s]\n\nDownloading data:  55%|#####4    | 3.86G/7.07G [01:05&lt;00:50, 63.2MB/s]\n\nDownloading data:  55%|#####4    | 3.86G/7.07G [01:05&lt;00:51, 62.8MB/s]\n\nDownloading data:  55%|#####4    | 3.87G/7.07G [01:05&lt;00:51, 62.2MB/s]\n\nDownloading data:  55%|#####4    | 3.87G/7.07G [01:05&lt;00:50, 63.0MB/s]\n\nDownloading data:  55%|#####4    | 3.88G/7.07G [01:05&lt;00:51, 62.1MB/s]\n\nDownloading data:  55%|#####4    | 3.89G/7.07G [01:05&lt;00:50, 62.8MB/s]\n\nDownloading data:  55%|#####5    | 3.89G/7.07G [01:05&lt;00:50, 63.1MB/s]\n\nDownloading data:  55%|#####5    | 3.90G/7.07G [01:05&lt;00:49, 63.5MB/s]\n\nDownloading data:  55%|#####5    | 3.91G/7.07G [01:06&lt;00:49, 63.5MB/s]\n\nDownloading data:  55%|#####5    | 3.91G/7.07G [01:06&lt;00:49, 63.7MB/s]\n\nDownloading data:  55%|#####5    | 3.92G/7.07G [01:06&lt;00:50, 62.4MB/s]\n\nDownloading data:  56%|#####5    | 3.93G/7.07G [01:06&lt;00:49, 63.0MB/s]\n\nDownloading data:  56%|#####5    | 3.93G/7.07G [01:06&lt;00:49, 63.3MB/s]\n\nDownloading data:  56%|#####5    | 3.94G/7.07G [01:06&lt;00:49, 63.5MB/s]\n\nDownloading data:  56%|#####5    | 3.94G/7.07G [01:06&lt;00:49, 63.2MB/s]\n\nDownloading data:  56%|#####5    | 3.95G/7.07G [01:06&lt;00:49, 63.1MB/s]\n\nDownloading data:  56%|#####5    | 3.96G/7.07G [01:06&lt;00:51, 61.1MB/s]\n\nDownloading data:  56%|#####6    | 3.96G/7.07G [01:06&lt;00:49, 62.6MB/s]\n\nDownloading data:  56%|#####6    | 3.97G/7.07G [01:07&lt;00:48, 63.5MB/s]\n\nDownloading data:  56%|#####6    | 3.98G/7.07G [01:07&lt;00:48, 63.7MB/s]\n\nDownloading data:  56%|#####6    | 3.98G/7.07G [01:07&lt;00:49, 61.9MB/s]\n\nDownloading data:  56%|#####6    | 3.99G/7.07G [01:07&lt;00:49, 61.8MB/s]\n\nDownloading data:  57%|#####6    | 4.00G/7.07G [01:07&lt;00:48, 63.1MB/s]\n\nDownloading data:  57%|#####6    | 4.00G/7.07G [01:07&lt;00:49, 62.1MB/s]\n\nDownloading data:  57%|#####6    | 4.01G/7.07G [01:07&lt;00:49, 62.3MB/s]\n\nDownloading data:  57%|#####6    | 4.02G/7.07G [01:07&lt;00:48, 63.4MB/s]\n\nDownloading data:  57%|#####6    | 4.02G/7.07G [01:07&lt;00:49, 61.9MB/s]\n\nDownloading data:  57%|#####6    | 4.03G/7.07G [01:07&lt;00:48, 62.6MB/s]\n\nDownloading data:  57%|#####7    | 4.03G/7.07G [01:08&lt;00:48, 62.5MB/s]\n\nDownloading data:  57%|#####7    | 4.04G/7.07G [01:08&lt;00:48, 63.0MB/s]\n\nDownloading data:  57%|#####7    | 4.05G/7.07G [01:08&lt;00:48, 62.6MB/s]\n\nDownloading data:  57%|#####7    | 4.05G/7.07G [01:08&lt;00:48, 62.8MB/s]\n\nDownloading data:  57%|#####7    | 4.06G/7.07G [01:08&lt;00:48, 62.5MB/s]\n\nDownloading data:  57%|#####7    | 4.07G/7.07G [01:08&lt;00:48, 62.5MB/s]\n\nDownloading data:  58%|#####7    | 4.07G/7.07G [01:08&lt;00:48, 62.1MB/s]\n\nDownloading data:  58%|#####7    | 4.08G/7.07G [01:08&lt;00:48, 61.4MB/s]\n\nDownloading data:  58%|#####7    | 4.09G/7.07G [01:08&lt;00:47, 62.4MB/s]\n\nDownloading data:  58%|#####7    | 4.09G/7.07G [01:08&lt;00:47, 62.1MB/s]\n\nDownloading data:  58%|#####7    | 4.10G/7.07G [01:09&lt;00:48, 60.7MB/s]\n\nDownloading data:  58%|#####8    | 4.10G/7.07G [01:09&lt;00:47, 62.4MB/s]\n\nDownloading data:  58%|#####8    | 4.11G/7.07G [01:09&lt;00:47, 62.2MB/s]\n\nDownloading data:  58%|#####8    | 4.12G/7.07G [01:09&lt;00:47, 62.8MB/s]\n\nDownloading data:  58%|#####8    | 4.12G/7.07G [01:09&lt;00:47, 61.7MB/s]\n\nDownloading data:  58%|#####8    | 4.13G/7.07G [01:09&lt;00:48, 60.6MB/s]\n\nDownloading data:  58%|#####8    | 4.14G/7.07G [01:09&lt;00:46, 62.5MB/s]\n\nDownloading data:  59%|#####8    | 4.14G/7.07G [01:09&lt;00:46, 63.3MB/s]\n\nDownloading data:  59%|#####8    | 4.15G/7.07G [01:09&lt;00:46, 63.0MB/s]\n\nDownloading data:  59%|#####8    | 4.16G/7.07G [01:10&lt;00:46, 62.4MB/s]\n\nDownloading data:  59%|#####8    | 4.16G/7.07G [01:10&lt;00:46, 62.0MB/s]\n\nDownloading data:  59%|#####8    | 4.17G/7.07G [01:10&lt;00:46, 62.2MB/s]\n\nDownloading data:  59%|#####9    | 4.17G/7.07G [01:10&lt;00:46, 62.2MB/s]\n\nDownloading data:  59%|#####9    | 4.18G/7.07G [01:10&lt;00:46, 61.8MB/s]\n\nDownloading data:  59%|#####9    | 4.19G/7.07G [01:10&lt;00:46, 61.7MB/s]\n\nDownloading data:  59%|#####9    | 4.19G/7.07G [01:10&lt;00:46, 61.6MB/s]\n\nDownloading data:  59%|#####9    | 4.20G/7.07G [01:10&lt;00:46, 62.0MB/s]\n\nDownloading data:  59%|#####9    | 4.21G/7.07G [01:10&lt;00:46, 61.8MB/s]\n\nDownloading data:  60%|#####9    | 4.21G/7.07G [01:10&lt;00:46, 62.0MB/s]\n\nDownloading data:  60%|#####9    | 4.22G/7.07G [01:11&lt;00:46, 62.0MB/s]\n\nDownloading data:  60%|#####9    | 4.22G/7.07G [01:11&lt;00:46, 61.1MB/s]\n\nDownloading data:  60%|#####9    | 4.23G/7.07G [01:11&lt;00:45, 62.7MB/s]\n\nDownloading data:  60%|#####9    | 4.24G/7.07G [01:11&lt;00:45, 62.6MB/s]\n\nDownloading data:  60%|#####9    | 4.24G/7.07G [01:11&lt;00:45, 62.0MB/s]\n\nDownloading data:  60%|######    | 4.25G/7.07G [01:11&lt;00:45, 61.8MB/s]\n\nDownloading data:  60%|######    | 4.26G/7.07G [01:11&lt;00:45, 62.0MB/s]\n\nDownloading data:  60%|######    | 4.26G/7.07G [01:11&lt;00:45, 62.1MB/s]\n\nDownloading data:  60%|######    | 4.27G/7.07G [01:11&lt;00:45, 62.0MB/s]\n\nDownloading data:  60%|######    | 4.27G/7.07G [01:11&lt;00:45, 62.0MB/s]\n\nDownloading data:  61%|######    | 4.28G/7.07G [01:12&lt;00:45, 61.7MB/s]\n\nDownloading data:  61%|######    | 4.29G/7.07G [01:12&lt;00:44, 62.1MB/s]\n\nDownloading data:  61%|######    | 4.29G/7.07G [01:12&lt;00:44, 62.2MB/s]\n\nDownloading data:  61%|######    | 4.30G/7.07G [01:12&lt;00:45, 61.3MB/s]\n\nDownloading data:  61%|######    | 4.31G/7.07G [01:12&lt;00:45, 61.0MB/s]\n\nDownloading data:  61%|######    | 4.31G/7.07G [01:12&lt;00:45, 61.3MB/s]\n\nDownloading data:  61%|######1   | 4.32G/7.07G [01:12&lt;00:44, 61.6MB/s]\n\nDownloading data:  61%|######1   | 4.32G/7.07G [01:12&lt;00:45, 59.9MB/s]\n\nDownloading data:  61%|######1   | 4.33G/7.07G [01:12&lt;00:46, 58.5MB/s]\n\nDownloading data:  61%|######1   | 4.34G/7.07G [01:12&lt;00:46, 58.6MB/s]\n\nDownloading data:  61%|######1   | 4.34G/7.07G [01:13&lt;00:47, 57.6MB/s]\n\nDownloading data:  61%|######1   | 4.35G/7.07G [01:13&lt;00:47, 57.6MB/s]\n\nDownloading data:  62%|######1   | 4.35G/7.07G [01:13&lt;00:47, 57.2MB/s]\n\nDownloading data:  62%|######1   | 4.36G/7.07G [01:13&lt;00:47, 57.0MB/s]\n\nDownloading data:  62%|######1   | 4.36G/7.07G [01:13&lt;00:47, 57.3MB/s]\n\nDownloading data:  62%|######1   | 4.37G/7.07G [01:13&lt;00:46, 57.9MB/s]\n\nDownloading data:  62%|######1   | 4.38G/7.07G [01:13&lt;00:45, 58.9MB/s]\n\nDownloading data:  62%|######1   | 4.38G/7.07G [01:13&lt;00:45, 59.2MB/s]\n\nDownloading data:  62%|######2   | 4.39G/7.07G [01:13&lt;00:45, 59.2MB/s]\n\nDownloading data:  62%|######2   | 4.39G/7.07G [01:13&lt;00:44, 59.8MB/s]\n\nDownloading data:  62%|######2   | 4.40G/7.07G [01:14&lt;00:44, 60.2MB/s]\n\nDownloading data:  62%|######2   | 4.41G/7.07G [01:14&lt;00:44, 59.8MB/s]\n\nDownloading data:  62%|######2   | 4.41G/7.07G [01:14&lt;00:45, 59.1MB/s]\n\nDownloading data:  62%|######2   | 4.42G/7.07G [01:14&lt;00:45, 57.7MB/s]\n\nDownloading data:  63%|######2   | 4.42G/7.07G [01:14&lt;00:46, 56.6MB/s]\n\nDownloading data:  63%|######2   | 4.43G/7.07G [01:14&lt;00:47, 56.0MB/s]\n\nDownloading data:  63%|######2   | 4.44G/7.07G [01:14&lt;00:46, 56.3MB/s]\n\nDownloading data:  63%|######2   | 4.44G/7.07G [01:14&lt;00:46, 56.2MB/s]\n\nDownloading data:  63%|######2   | 4.45G/7.07G [01:14&lt;00:46, 56.0MB/s]\n\nDownloading data:  63%|######2   | 4.45G/7.07G [01:14&lt;00:46, 56.4MB/s]\n\nDownloading data:  63%|######3   | 4.46G/7.07G [01:15&lt;00:45, 57.6MB/s]\n\nDownloading data:  63%|######3   | 4.47G/7.07G [01:15&lt;00:44, 58.9MB/s]\n\nDownloading data:  63%|######3   | 4.47G/7.07G [01:15&lt;00:43, 59.9MB/s]\n\nDownloading data:  63%|######3   | 4.48G/7.07G [01:15&lt;00:43, 60.2MB/s]\n\nDownloading data:  63%|######3   | 4.48G/7.07G [01:15&lt;00:42, 60.9MB/s]\n\nDownloading data:  63%|######3   | 4.49G/7.07G [01:15&lt;00:41, 61.5MB/s]\n\nDownloading data:  64%|######3   | 4.50G/7.07G [01:15&lt;00:41, 61.7MB/s]\n\nDownloading data:  64%|######3   | 4.50G/7.07G [01:15&lt;00:41, 61.9MB/s]\n\nDownloading data:  64%|######3   | 4.51G/7.07G [01:15&lt;00:41, 62.1MB/s]\n\nDownloading data:  64%|######3   | 4.52G/7.07G [01:15&lt;00:40, 63.2MB/s]\n\nDownloading data:  64%|######3   | 4.52G/7.07G [01:16&lt;00:40, 63.6MB/s]\n\nDownloading data:  64%|######4   | 4.53G/7.07G [01:16&lt;00:40, 63.4MB/s]\n\nDownloading data:  64%|######4   | 4.53G/7.07G [01:16&lt;00:40, 63.1MB/s]\n\nDownloading data:  64%|######4   | 4.54G/7.07G [01:16&lt;00:40, 62.6MB/s]\n\nDownloading data:  64%|######4   | 4.55G/7.07G [01:16&lt;00:40, 63.0MB/s]\n\nDownloading data:  64%|######4   | 4.55G/7.07G [01:16&lt;00:39, 64.1MB/s]\n\nDownloading data:  64%|######4   | 4.56G/7.07G [01:16&lt;00:39, 64.2MB/s]\n\nDownloading data:  65%|######4   | 4.57G/7.07G [01:16&lt;00:38, 64.6MB/s]\n\nDownloading data:  65%|######4   | 4.57G/7.07G [01:16&lt;00:38, 65.0MB/s]\n\nDownloading data:  65%|######4   | 4.58G/7.07G [01:16&lt;00:38, 65.2MB/s]\n\nDownloading data:  65%|######4   | 4.59G/7.07G [01:17&lt;00:37, 65.6MB/s]\n\nDownloading data:  65%|######4   | 4.59G/7.07G [01:17&lt;00:37, 66.0MB/s]\n\nDownloading data:  65%|######5   | 4.60G/7.07G [01:17&lt;00:37, 66.0MB/s]\n\nDownloading data:  65%|######5   | 4.61G/7.07G [01:17&lt;00:37, 66.0MB/s]\n\nDownloading data:  65%|######5   | 4.61G/7.07G [01:17&lt;00:37, 66.0MB/s]\n\nDownloading data:  65%|######5   | 4.62G/7.07G [01:17&lt;00:37, 66.1MB/s]\n\nDownloading data:  65%|######5   | 4.63G/7.07G [01:17&lt;00:37, 65.7MB/s]\n\nDownloading data:  66%|######5   | 4.63G/7.07G [01:17&lt;00:37, 65.9MB/s]\n\nDownloading data:  66%|######5   | 4.64G/7.07G [01:17&lt;00:36, 66.1MB/s]\n\nDownloading data:  66%|######5   | 4.65G/7.07G [01:17&lt;00:36, 66.2MB/s]\n\nDownloading data:  66%|######5   | 4.65G/7.07G [01:18&lt;00:36, 66.4MB/s]\n\nDownloading data:  66%|######5   | 4.66G/7.07G [01:18&lt;00:36, 66.6MB/s]\n\nDownloading data:  66%|######5   | 4.67G/7.07G [01:18&lt;00:36, 66.4MB/s]\n\nDownloading data:  66%|######6   | 4.67G/7.07G [01:18&lt;00:36, 66.5MB/s]\n\nDownloading data:  66%|######6   | 4.68G/7.07G [01:18&lt;00:35, 66.5MB/s]\n\nDownloading data:  66%|######6   | 4.69G/7.07G [01:18&lt;00:35, 66.3MB/s]\n\nDownloading data:  66%|######6   | 4.69G/7.07G [01:18&lt;00:35, 66.6MB/s]\n\nDownloading data:  66%|######6   | 4.70G/7.07G [01:18&lt;00:35, 66.7MB/s]\n\nDownloading data:  67%|######6   | 4.71G/7.07G [01:18&lt;00:35, 66.5MB/s]\n\nDownloading data:  67%|######6   | 4.71G/7.07G [01:18&lt;00:35, 66.5MB/s]\n\nDownloading data:  67%|######6   | 4.72G/7.07G [01:19&lt;00:35, 66.5MB/s]\n\nDownloading data:  67%|######6   | 4.73G/7.07G [01:19&lt;00:35, 66.2MB/s]\n\nDownloading data:  67%|######6   | 4.73G/7.07G [01:19&lt;00:35, 66.3MB/s]\n\nDownloading data:  67%|######7   | 4.74G/7.07G [01:19&lt;00:35, 66.5MB/s]\n\nDownloading data:  67%|######7   | 4.75G/7.07G [01:19&lt;00:34, 66.7MB/s]\n\nDownloading data:  67%|######7   | 4.75G/7.07G [01:19&lt;00:34, 66.4MB/s]\n\nDownloading data:  67%|######7   | 4.76G/7.07G [01:19&lt;00:34, 66.3MB/s]\n\nDownloading data:  67%|######7   | 4.77G/7.07G [01:19&lt;00:34, 65.9MB/s]\n\nDownloading data:  67%|######7   | 4.77G/7.07G [01:19&lt;00:34, 65.9MB/s]\n\nDownloading data:  68%|######7   | 4.78G/7.07G [01:20&lt;00:34, 65.8MB/s]\n\nDownloading data:  68%|######7   | 4.79G/7.07G [01:20&lt;00:34, 65.9MB/s]\n\nDownloading data:  68%|######7   | 4.79G/7.07G [01:20&lt;00:34, 65.6MB/s]\n\nDownloading data:  68%|######7   | 4.80G/7.07G [01:20&lt;00:34, 66.0MB/s]\n\nDownloading data:  68%|######7   | 4.81G/7.07G [01:20&lt;00:34, 66.2MB/s]\n\nDownloading data:  68%|######8   | 4.81G/7.07G [01:20&lt;00:34, 66.5MB/s]\n\nDownloading data:  68%|######8   | 4.82G/7.07G [01:20&lt;00:33, 66.4MB/s]\n\nDownloading data:  68%|######8   | 4.83G/7.07G [01:20&lt;00:33, 66.2MB/s]\n\nDownloading data:  68%|######8   | 4.83G/7.07G [01:20&lt;00:33, 66.4MB/s]\n\nDownloading data:  68%|######8   | 4.84G/7.07G [01:20&lt;00:33, 66.6MB/s]\n\nDownloading data:  69%|######8   | 4.85G/7.07G [01:21&lt;00:33, 66.6MB/s]\n\nDownloading data:  69%|######8   | 4.85G/7.07G [01:21&lt;00:33, 66.6MB/s]\n\nDownloading data:  69%|######8   | 4.86G/7.07G [01:21&lt;00:33, 66.0MB/s]\n\nDownloading data:  69%|######8   | 4.87G/7.07G [01:21&lt;00:33, 66.1MB/s]\n\nDownloading data:  69%|######8   | 4.87G/7.07G [01:21&lt;00:33, 65.5MB/s]\n\nDownloading data:  69%|######8   | 4.88G/7.07G [01:21&lt;00:34, 63.6MB/s]\n\nDownloading data:  69%|######9   | 4.89G/7.07G [01:21&lt;00:34, 62.6MB/s]\n\nDownloading data:  69%|######9   | 4.89G/7.07G [01:21&lt;00:35, 61.8MB/s]\n\nDownloading data:  69%|######9   | 4.90G/7.07G [01:21&lt;00:35, 61.9MB/s]\n\nDownloading data:  69%|######9   | 4.90G/7.07G [01:21&lt;00:34, 63.4MB/s]\n\nDownloading data:  69%|######9   | 4.91G/7.07G [01:22&lt;00:33, 64.2MB/s]\n\nDownloading data:  70%|######9   | 4.92G/7.07G [01:22&lt;00:49, 43.8MB/s]\n\nDownloading data:  70%|######9   | 4.92G/7.07G [01:22&lt;00:44, 48.1MB/s]\n\nDownloading data:  70%|######9   | 4.93G/7.07G [01:22&lt;00:40, 52.9MB/s]\n\nDownloading data:  70%|######9   | 4.94G/7.07G [01:22&lt;00:37, 56.8MB/s]\n\nDownloading data:  70%|######9   | 4.94G/7.07G [01:22&lt;00:36, 58.8MB/s]\n\nDownloading data:  70%|######9   | 4.95G/7.07G [01:22&lt;00:35, 60.4MB/s]\n\nDownloading data:  70%|#######   | 4.96G/7.07G [01:22&lt;00:33, 62.3MB/s]\n\nDownloading data:  70%|#######   | 4.96G/7.07G [01:22&lt;00:32, 64.1MB/s]\n\nDownloading data:  70%|#######   | 4.97G/7.07G [01:23&lt;00:32, 65.4MB/s]\n\nDownloading data:  70%|#######   | 4.98G/7.07G [01:23&lt;00:31, 66.2MB/s]\n\nDownloading data:  70%|#######   | 4.98G/7.07G [01:23&lt;00:31, 66.6MB/s]\n\nDownloading data:  71%|#######   | 4.99G/7.07G [01:23&lt;00:31, 66.8MB/s]\n\nDownloading data:  71%|#######   | 5.00G/7.07G [01:23&lt;00:31, 65.6MB/s]\n\nDownloading data:  71%|#######   | 5.00G/7.07G [01:23&lt;00:31, 64.9MB/s]\n\nDownloading data:  71%|#######   | 5.01G/7.07G [01:23&lt;00:32, 64.3MB/s]\n\nDownloading data:  71%|#######   | 5.02G/7.07G [01:23&lt;00:32, 63.5MB/s]\n\nDownloading data:  71%|#######1  | 5.02G/7.07G [01:23&lt;00:32, 63.3MB/s]\n\nDownloading data:  71%|#######1  | 5.03G/7.07G [01:24&lt;00:31, 64.4MB/s]\n\nDownloading data:  71%|#######1  | 5.04G/7.07G [01:24&lt;00:31, 65.1MB/s]\n\nDownloading data:  71%|#######1  | 5.04G/7.07G [01:24&lt;00:31, 65.0MB/s]\n\nDownloading data:  71%|#######1  | 5.05G/7.07G [01:24&lt;00:30, 65.8MB/s]\n\nDownloading data:  72%|#######1  | 5.06G/7.07G [01:24&lt;00:30, 66.4MB/s]\n\nDownloading data:  72%|#######1  | 5.06G/7.07G [01:24&lt;00:30, 66.8MB/s]\n\nDownloading data:  72%|#######1  | 5.07G/7.07G [01:24&lt;00:29, 66.8MB/s]\n\nDownloading data:  72%|#######1  | 5.08G/7.07G [01:24&lt;00:29, 67.3MB/s]\n\nDownloading data:  72%|#######1  | 5.08G/7.07G [01:24&lt;00:29, 67.2MB/s]\n\nDownloading data:  72%|#######1  | 5.09G/7.07G [01:24&lt;00:30, 64.3MB/s]\n\nDownloading data:  72%|#######2  | 5.10G/7.07G [01:25&lt;00:31, 62.0MB/s]\n\nDownloading data:  72%|#######2  | 5.10G/7.07G [01:25&lt;00:33, 59.1MB/s]\n\nDownloading data:  72%|#######2  | 5.11G/7.07G [01:25&lt;00:33, 58.2MB/s]\n\nDownloading data:  72%|#######2  | 5.12G/7.07G [01:25&lt;00:33, 57.7MB/s]\n\nDownloading data:  72%|#######2  | 5.12G/7.07G [01:25&lt;00:34, 56.9MB/s]\n\nDownloading data:  73%|#######2  | 5.13G/7.07G [01:25&lt;00:33, 58.0MB/s]\n\nDownloading data:  73%|#######2  | 5.13G/7.07G [01:25&lt;00:33, 57.6MB/s]\n\nDownloading data:  73%|#######2  | 5.14G/7.07G [01:25&lt;00:33, 57.1MB/s]\n\nDownloading data:  73%|#######2  | 5.14G/7.07G [01:25&lt;00:34, 56.4MB/s]\n\nDownloading data:  73%|#######2  | 5.15G/7.07G [01:25&lt;00:33, 56.7MB/s]\n\nDownloading data:  73%|#######2  | 5.16G/7.07G [01:26&lt;00:33, 56.8MB/s]\n\nDownloading data:  73%|#######2  | 5.16G/7.07G [01:26&lt;00:33, 57.4MB/s]\n\nDownloading data:  73%|#######3  | 5.17G/7.07G [01:26&lt;00:33, 57.0MB/s]\n\nDownloading data:  73%|#######3  | 5.17G/7.07G [01:26&lt;00:33, 57.4MB/s]\n\nDownloading data:  73%|#######3  | 5.18G/7.07G [01:26&lt;00:32, 57.8MB/s]\n\nDownloading data:  73%|#######3  | 5.19G/7.07G [01:26&lt;00:32, 57.9MB/s]\n\nDownloading data:  73%|#######3  | 5.19G/7.07G [01:26&lt;00:32, 57.7MB/s]\n\nDownloading data:  73%|#######3  | 5.20G/7.07G [01:26&lt;00:32, 58.0MB/s]\n\nDownloading data:  74%|#######3  | 5.20G/7.07G [01:26&lt;00:32, 57.8MB/s]\n\nDownloading data:  74%|#######3  | 5.21G/7.07G [01:26&lt;00:32, 57.8MB/s]\n\nDownloading data:  74%|#######3  | 5.21G/7.07G [01:27&lt;00:31, 58.4MB/s]\n\nDownloading data:  74%|#######3  | 5.22G/7.07G [01:27&lt;00:31, 58.2MB/s]\n\nDownloading data:  74%|#######3  | 5.23G/7.07G [01:27&lt;00:31, 58.5MB/s]\n\nDownloading data:  74%|#######3  | 5.23G/7.07G [01:27&lt;00:31, 58.5MB/s]\n\nDownloading data:  74%|#######4  | 5.24G/7.07G [01:27&lt;00:31, 58.7MB/s]\n\nDownloading data:  74%|#######4  | 5.24G/7.07G [01:27&lt;00:31, 58.9MB/s]\n\nDownloading data:  74%|#######4  | 5.25G/7.07G [01:27&lt;00:40, 44.9MB/s]\n\nDownloading data:  74%|#######4  | 5.25G/7.07G [01:27&lt;00:41, 43.9MB/s]\n\nDownloading data:  74%|#######4  | 5.26G/7.07G [01:28&lt;00:36, 49.3MB/s]\n\nDownloading data:  74%|#######4  | 5.27G/7.07G [01:28&lt;00:32, 54.7MB/s]\n\nDownloading data:  75%|#######4  | 5.28G/7.07G [01:28&lt;00:30, 58.9MB/s]\n\nDownloading data:  75%|#######4  | 5.28G/7.07G [01:28&lt;00:28, 62.0MB/s]\n\nDownloading data:  75%|#######4  | 5.29G/7.07G [01:28&lt;00:27, 64.1MB/s]\n\nDownloading data:  75%|#######4  | 5.30G/7.07G [01:28&lt;00:27, 65.7MB/s]\n\nDownloading data:  75%|#######4  | 5.30G/7.07G [01:28&lt;00:26, 67.0MB/s]\n\nDownloading data:  75%|#######5  | 5.31G/7.07G [01:28&lt;00:25, 68.0MB/s]\n\nDownloading data:  75%|#######5  | 5.32G/7.07G [01:28&lt;00:26, 65.5MB/s]\n\nDownloading data:  75%|#######5  | 5.32G/7.07G [01:28&lt;00:27, 64.1MB/s]\n\nDownloading data:  75%|#######5  | 5.33G/7.07G [01:29&lt;00:27, 63.2MB/s]\n\nDownloading data:  75%|#######5  | 5.34G/7.07G [01:29&lt;00:28, 61.8MB/s]\n\nDownloading data:  76%|#######5  | 5.34G/7.07G [01:29&lt;00:34, 50.9MB/s]\n\nDownloading data:  76%|#######5  | 5.35G/7.07G [01:29&lt;00:33, 51.2MB/s]\n\nDownloading data:  76%|#######5  | 5.35G/7.07G [01:29&lt;00:32, 53.1MB/s]\n\nDownloading data:  76%|#######5  | 5.36G/7.07G [01:29&lt;00:30, 55.3MB/s]\n\nDownloading data:  76%|#######5  | 5.37G/7.07G [01:29&lt;00:30, 55.7MB/s]\n\nDownloading data:  76%|#######5  | 5.37G/7.07G [01:29&lt;00:29, 57.5MB/s]\n\nDownloading data:  76%|#######6  | 5.38G/7.07G [01:29&lt;00:29, 57.9MB/s]\n\nDownloading data:  76%|#######6  | 5.38G/7.07G [01:30&lt;00:28, 58.8MB/s]\n\nDownloading data:  76%|#######6  | 5.39G/7.07G [01:30&lt;00:28, 58.9MB/s]\n\nDownloading data:  76%|#######6  | 5.40G/7.07G [01:30&lt;00:28, 59.4MB/s]\n\nDownloading data:  76%|#######6  | 5.40G/7.07G [01:30&lt;00:27, 59.7MB/s]\n\nDownloading data:  76%|#######6  | 5.41G/7.07G [01:30&lt;00:27, 59.8MB/s]\n\nDownloading data:  77%|#######6  | 5.41G/7.07G [01:30&lt;00:27, 60.1MB/s]\n\nDownloading data:  77%|#######6  | 5.42G/7.07G [01:30&lt;00:27, 59.7MB/s]\n\nDownloading data:  77%|#######6  | 5.43G/7.07G [01:30&lt;00:27, 59.6MB/s]\n\nDownloading data:  77%|#######6  | 5.43G/7.07G [01:30&lt;00:27, 60.6MB/s]\n\nDownloading data:  77%|#######6  | 5.44G/7.07G [01:30&lt;00:27, 60.3MB/s]\n\nDownloading data:  77%|#######6  | 5.44G/7.07G [01:31&lt;00:27, 59.8MB/s]\n\nDownloading data:  77%|#######7  | 5.45G/7.07G [01:31&lt;00:27, 59.9MB/s]\n\nDownloading data:  77%|#######7  | 5.46G/7.07G [01:31&lt;00:27, 58.9MB/s]\n\nDownloading data:  77%|#######7  | 5.46G/7.07G [01:31&lt;00:27, 59.3MB/s]\n\nDownloading data:  77%|#######7  | 5.47G/7.07G [01:31&lt;00:26, 59.6MB/s]\n\nDownloading data:  77%|#######7  | 5.48G/7.07G [01:31&lt;00:26, 59.6MB/s]\n\nDownloading data:  78%|#######7  | 5.48G/7.07G [01:31&lt;00:26, 60.0MB/s]\n\nDownloading data:  78%|#######7  | 5.49G/7.07G [01:31&lt;00:26, 59.7MB/s]\n\nDownloading data:  78%|#######7  | 5.49G/7.07G [01:31&lt;00:26, 59.9MB/s]\n\nDownloading data:  78%|#######7  | 5.50G/7.07G [01:31&lt;00:26, 59.4MB/s]\n\nDownloading data:  78%|#######7  | 5.51G/7.07G [01:32&lt;00:26, 59.6MB/s]\n\nDownloading data:  78%|#######7  | 5.51G/7.07G [01:32&lt;00:25, 60.3MB/s]\n\nDownloading data:  78%|#######8  | 5.52G/7.07G [01:32&lt;00:25, 60.1MB/s]\n\nDownloading data:  78%|#######8  | 5.52G/7.07G [01:32&lt;00:25, 59.8MB/s]\n\nDownloading data:  78%|#######8  | 5.53G/7.07G [01:32&lt;00:25, 60.4MB/s]\n\nDownloading data:  78%|#######8  | 5.54G/7.07G [01:32&lt;00:25, 60.2MB/s]\n\nDownloading data:  78%|#######8  | 5.54G/7.07G [01:32&lt;00:25, 60.4MB/s]\n\nDownloading data:  78%|#######8  | 5.55G/7.07G [01:32&lt;00:25, 58.8MB/s]\n\nDownloading data:  79%|#######8  | 5.55G/7.07G [01:32&lt;00:25, 59.5MB/s]\n\nDownloading data:  79%|#######8  | 5.56G/7.07G [01:32&lt;00:25, 59.6MB/s]\n\nDownloading data:  79%|#######8  | 5.57G/7.07G [01:33&lt;00:25, 59.5MB/s]\n\nDownloading data:  79%|#######8  | 5.57G/7.07G [01:33&lt;00:25, 59.8MB/s]\n\nDownloading data:  79%|#######8  | 5.58G/7.07G [01:33&lt;00:27, 53.5MB/s]\n\nDownloading data:  79%|#######8  | 5.59G/7.07G [01:33&lt;00:25, 58.9MB/s]\n\nDownloading data:  79%|#######9  | 5.59G/7.07G [01:33&lt;00:23, 62.3MB/s]\n\nDownloading data:  79%|#######9  | 5.60G/7.07G [01:33&lt;00:23, 61.6MB/s]\n\nDownloading data:  79%|#######9  | 5.61G/7.07G [01:33&lt;00:23, 61.4MB/s]\n\nDownloading data:  79%|#######9  | 5.61G/7.07G [01:33&lt;00:23, 61.2MB/s]\n\nDownloading data:  79%|#######9  | 5.62G/7.07G [01:33&lt;00:23, 60.7MB/s]\n\nDownloading data:  80%|#######9  | 5.62G/7.07G [01:34&lt;00:23, 61.0MB/s]\n\nDownloading data:  80%|#######9  | 5.63G/7.07G [01:34&lt;00:23, 60.8MB/s]\n\nDownloading data:  80%|#######9  | 5.64G/7.07G [01:34&lt;00:23, 60.6MB/s]\n\nDownloading data:  80%|#######9  | 5.64G/7.07G [01:34&lt;00:23, 60.4MB/s]\n\nDownloading data:  80%|#######9  | 5.65G/7.07G [01:34&lt;00:23, 60.2MB/s]\n\nDownloading data:  80%|#######9  | 5.65G/7.07G [01:34&lt;00:23, 60.4MB/s]\n\nDownloading data:  80%|########  | 5.66G/7.07G [01:34&lt;00:23, 60.2MB/s]\n\nDownloading data:  80%|########  | 5.67G/7.07G [01:34&lt;00:23, 59.9MB/s]\n\nDownloading data:  80%|########  | 5.67G/7.07G [01:34&lt;00:23, 59.2MB/s]\n\nDownloading data:  80%|########  | 5.68G/7.07G [01:34&lt;00:23, 59.5MB/s]\n\nDownloading data:  80%|########  | 5.68G/7.07G [01:35&lt;00:23, 60.1MB/s]\n\nDownloading data:  80%|########  | 5.69G/7.07G [01:35&lt;00:23, 60.0MB/s]\n\nDownloading data:  81%|########  | 5.70G/7.07G [01:35&lt;00:22, 60.2MB/s]\n\nDownloading data:  81%|########  | 5.70G/7.07G [01:35&lt;00:22, 60.2MB/s]\n\nDownloading data:  81%|########  | 5.71G/7.07G [01:35&lt;00:22, 59.9MB/s]\n\nDownloading data:  81%|########  | 5.71G/7.07G [01:35&lt;00:22, 60.0MB/s]\n\nDownloading data:  81%|########  | 5.72G/7.07G [01:35&lt;00:22, 59.8MB/s]\n\nDownloading data:  81%|########  | 5.73G/7.07G [01:35&lt;00:22, 60.6MB/s]\n\nDownloading data:  81%|########1 | 5.73G/7.07G [01:35&lt;00:22, 59.3MB/s]\n\nDownloading data:  81%|########1 | 5.74G/7.07G [01:35&lt;00:21, 60.8MB/s]\n\nDownloading data:  81%|########1 | 5.75G/7.07G [01:36&lt;00:22, 59.1MB/s]\n\nDownloading data:  81%|########1 | 5.75G/7.07G [01:36&lt;00:22, 59.9MB/s]\n\nDownloading data:  81%|########1 | 5.76G/7.07G [01:36&lt;00:21, 60.2MB/s]\n\nDownloading data:  81%|########1 | 5.76G/7.07G [01:36&lt;00:21, 59.8MB/s]\n\nDownloading data:  82%|########1 | 5.77G/7.07G [01:36&lt;00:21, 60.6MB/s]\n\nDownloading data:  82%|########1 | 5.78G/7.07G [01:36&lt;00:21, 60.0MB/s]\n\nDownloading data:  82%|########1 | 5.78G/7.07G [01:36&lt;00:21, 60.0MB/s]\n\nDownloading data:  82%|########1 | 5.79G/7.07G [01:36&lt;00:21, 60.3MB/s]\n\nDownloading data:  82%|########1 | 5.79G/7.07G [01:36&lt;00:21, 59.7MB/s]\n\nDownloading data:  82%|########2 | 5.80G/7.07G [01:36&lt;00:21, 60.5MB/s]\n\nDownloading data:  82%|########2 | 5.81G/7.07G [01:37&lt;00:20, 60.7MB/s]\n\nDownloading data:  82%|########2 | 5.81G/7.07G [01:37&lt;00:20, 60.8MB/s]\n\nDownloading data:  82%|########2 | 5.82G/7.07G [01:37&lt;00:20, 60.9MB/s]\n\nDownloading data:  82%|########2 | 5.82G/7.07G [01:37&lt;00:20, 60.7MB/s]\n\nDownloading data:  82%|########2 | 5.83G/7.07G [01:37&lt;00:20, 60.7MB/s]\n\nDownloading data:  83%|########2 | 5.84G/7.07G [01:37&lt;00:20, 60.3MB/s]\n\nDownloading data:  83%|########2 | 5.84G/7.07G [01:37&lt;00:20, 61.2MB/s]\n\nDownloading data:  83%|########2 | 5.85G/7.07G [01:37&lt;00:19, 61.7MB/s]\n\nDownloading data:  83%|########2 | 5.86G/7.07G [01:37&lt;00:20, 60.8MB/s]\n\nDownloading data:  83%|########2 | 5.86G/7.07G [01:37&lt;00:19, 60.9MB/s]\n\nDownloading data:  83%|########2 | 5.87G/7.07G [01:38&lt;00:19, 61.2MB/s]\n\nDownloading data:  83%|########3 | 5.87G/7.07G [01:38&lt;00:19, 61.1MB/s]\n\nDownloading data:  83%|########3 | 5.88G/7.07G [01:38&lt;00:19, 61.5MB/s]\n\nDownloading data:  83%|########3 | 5.89G/7.07G [01:38&lt;00:19, 61.1MB/s]\n\nDownloading data:  83%|########3 | 5.89G/7.07G [01:38&lt;00:19, 61.7MB/s]\n\nDownloading data:  83%|########3 | 5.90G/7.07G [01:38&lt;00:18, 62.1MB/s]\n\nDownloading data:  83%|########3 | 5.91G/7.07G [01:38&lt;00:19, 61.0MB/s]\n\nDownloading data:  84%|########3 | 5.91G/7.07G [01:38&lt;00:18, 63.2MB/s]\n\nDownloading data:  84%|########3 | 5.92G/7.07G [01:38&lt;00:18, 62.8MB/s]\n\nDownloading data:  84%|########3 | 5.92G/7.07G [01:38&lt;00:18, 62.9MB/s]\n\nDownloading data:  84%|########3 | 5.93G/7.07G [01:39&lt;00:18, 61.9MB/s]\n\nDownloading data:  84%|########3 | 5.94G/7.07G [01:39&lt;00:18, 60.8MB/s]\n\nDownloading data:  84%|########4 | 5.94G/7.07G [01:39&lt;00:17, 63.4MB/s]\n\nDownloading data:  84%|########4 | 5.95G/7.07G [01:39&lt;00:17, 63.0MB/s]\n\nDownloading data:  84%|########4 | 5.96G/7.07G [01:39&lt;00:17, 63.8MB/s]\n\nDownloading data:  84%|########4 | 5.96G/7.07G [01:39&lt;00:17, 63.4MB/s]\n\nDownloading data:  84%|########4 | 5.97G/7.07G [01:39&lt;00:17, 63.6MB/s]\n\nDownloading data:  85%|########4 | 5.98G/7.07G [01:39&lt;00:17, 63.8MB/s]\n\nDownloading data:  85%|########4 | 5.98G/7.07G [01:39&lt;00:17, 64.0MB/s]\n\nDownloading data:  85%|########4 | 5.99G/7.07G [01:40&lt;00:16, 64.1MB/s]\n\nDownloading data:  85%|########4 | 6.00G/7.07G [01:40&lt;00:16, 64.7MB/s]\n\nDownloading data:  85%|########4 | 6.00G/7.07G [01:40&lt;00:16, 64.6MB/s]\n\nDownloading data:  85%|########4 | 6.01G/7.07G [01:40&lt;00:16, 64.1MB/s]\n\nDownloading data:  85%|########5 | 6.02G/7.07G [01:40&lt;00:27, 38.4MB/s]\n\nDownloading data:  85%|########5 | 6.02G/7.07G [01:40&lt;00:23, 44.2MB/s]\n\nDownloading data:  85%|########5 | 6.03G/7.07G [01:40&lt;00:23, 43.8MB/s]\n\nDownloading data:  85%|########5 | 6.03G/7.07G [01:40&lt;00:23, 45.0MB/s]\n\nDownloading data:  85%|########5 | 6.04G/7.07G [01:41&lt;00:21, 47.1MB/s]\n\nDownloading data:  85%|########5 | 6.04G/7.07G [01:41&lt;00:21, 47.2MB/s]\n\nDownloading data:  86%|########5 | 6.05G/7.07G [01:41&lt;00:21, 46.7MB/s]\n\nDownloading data:  86%|########5 | 6.05G/7.07G [01:41&lt;00:21, 47.4MB/s]\n\nDownloading data:  86%|########5 | 6.06G/7.07G [01:41&lt;00:21, 47.7MB/s]\n\nDownloading data:  86%|########5 | 6.06G/7.07G [01:41&lt;00:20, 48.5MB/s]\n\nDownloading data:  86%|########5 | 6.07G/7.07G [01:41&lt;00:20, 49.5MB/s]\n\nDownloading data:  86%|########5 | 6.07G/7.07G [01:41&lt;00:19, 49.9MB/s]\n\nDownloading data:  86%|########5 | 6.08G/7.07G [01:41&lt;00:19, 50.4MB/s]\n\nDownloading data:  86%|########6 | 6.08G/7.07G [01:41&lt;00:19, 49.8MB/s]\n\nDownloading data:  86%|########6 | 6.09G/7.07G [01:42&lt;00:19, 50.5MB/s]\n\nDownloading data:  86%|########6 | 6.09G/7.07G [01:42&lt;00:19, 51.1MB/s]\n\nDownloading data:  86%|########6 | 6.10G/7.07G [01:42&lt;00:19, 50.3MB/s]\n\nDownloading data:  86%|########6 | 6.10G/7.07G [01:42&lt;00:19, 50.2MB/s]\n\nDownloading data:  86%|########6 | 6.11G/7.07G [01:42&lt;00:18, 51.5MB/s]\n\nDownloading data:  86%|########6 | 6.12G/7.07G [01:42&lt;00:18, 52.1MB/s]\n\nDownloading data:  87%|########6 | 6.12G/7.07G [01:42&lt;00:18, 52.7MB/s]\n\nDownloading data:  87%|########6 | 6.13G/7.07G [01:42&lt;00:17, 53.4MB/s]\n\nDownloading data:  87%|########6 | 6.13G/7.07G [01:42&lt;00:17, 53.6MB/s]\n\nDownloading data:  87%|########6 | 6.14G/7.07G [01:43&lt;00:17, 54.6MB/s]\n\nDownloading data:  87%|########6 | 6.14G/7.07G [01:43&lt;00:17, 54.2MB/s]\n\nDownloading data:  87%|########6 | 6.15G/7.07G [01:43&lt;00:16, 55.3MB/s]\n\nDownloading data:  87%|########7 | 6.15G/7.07G [01:43&lt;00:16, 55.6MB/s]\n\nDownloading data:  87%|########7 | 6.16G/7.07G [01:43&lt;00:16, 56.4MB/s]\n\nDownloading data:  87%|########7 | 6.17G/7.07G [01:43&lt;00:15, 56.7MB/s]\n\nDownloading data:  87%|########7 | 6.17G/7.07G [01:43&lt;00:16, 55.3MB/s]\n\nDownloading data:  87%|########7 | 6.18G/7.07G [01:43&lt;00:16, 55.8MB/s]\n\nDownloading data:  87%|########7 | 6.18G/7.07G [01:43&lt;00:15, 56.5MB/s]\n\nDownloading data:  88%|########7 | 6.19G/7.07G [01:43&lt;00:15, 57.2MB/s]\n\nDownloading data:  88%|########7 | 6.19G/7.07G [01:44&lt;00:15, 57.1MB/s]\n\nDownloading data:  88%|########7 | 6.20G/7.07G [01:44&lt;00:15, 56.6MB/s]\n\nDownloading data:  88%|########7 | 6.21G/7.07G [01:44&lt;00:15, 57.0MB/s]\n\nDownloading data:  88%|########7 | 6.21G/7.07G [01:44&lt;00:14, 58.6MB/s]\n\nDownloading data:  88%|########7 | 6.22G/7.07G [01:44&lt;00:14, 58.5MB/s]\n\nDownloading data:  88%|########8 | 6.22G/7.07G [01:44&lt;00:14, 59.0MB/s]\n\nDownloading data:  88%|########8 | 6.23G/7.07G [01:44&lt;00:14, 59.0MB/s]\n\nDownloading data:  88%|########8 | 6.24G/7.07G [01:44&lt;00:14, 57.6MB/s]\n\nDownloading data:  88%|########8 | 6.24G/7.07G [01:44&lt;00:14, 58.3MB/s]\n\nDownloading data:  88%|########8 | 6.25G/7.07G [01:44&lt;00:14, 58.8MB/s]\n\nDownloading data:  88%|########8 | 6.25G/7.07G [01:45&lt;00:13, 59.5MB/s]\n\nDownloading data:  89%|########8 | 6.26G/7.07G [01:45&lt;00:13, 60.2MB/s]\n\nDownloading data:  89%|########8 | 6.27G/7.07G [01:45&lt;00:13, 60.2MB/s]\n\nDownloading data:  89%|########8 | 6.27G/7.07G [01:45&lt;00:13, 60.6MB/s]\n\nDownloading data:  89%|########8 | 6.28G/7.07G [01:45&lt;00:13, 60.1MB/s]\n\nDownloading data:  89%|########8 | 6.29G/7.07G [01:45&lt;00:12, 60.6MB/s]\n\nDownloading data:  89%|########8 | 6.29G/7.07G [01:45&lt;00:12, 61.2MB/s]\n\nDownloading data:  89%|########9 | 6.30G/7.07G [01:45&lt;00:12, 60.8MB/s]\n\nDownloading data:  89%|########9 | 6.30G/7.07G [01:45&lt;00:12, 61.3MB/s]\n\nDownloading data:  89%|########9 | 6.31G/7.07G [01:45&lt;00:12, 61.1MB/s]\n\nDownloading data:  89%|########9 | 6.32G/7.07G [01:46&lt;00:12, 61.2MB/s]\n\nDownloading data:  89%|########9 | 6.32G/7.07G [01:46&lt;00:12, 61.2MB/s]\n\nDownloading data:  89%|########9 | 6.33G/7.07G [01:46&lt;00:12, 61.4MB/s]\n\nDownloading data:  90%|########9 | 6.33G/7.07G [01:46&lt;00:12, 60.9MB/s]\n\nDownloading data:  90%|########9 | 6.34G/7.07G [01:46&lt;00:11, 61.7MB/s]\n\nDownloading data:  90%|########9 | 6.35G/7.07G [01:46&lt;00:11, 62.2MB/s]\n\nDownloading data:  90%|########9 | 6.35G/7.07G [01:46&lt;00:11, 62.1MB/s]\n\nDownloading data:  90%|########9 | 6.36G/7.07G [01:46&lt;00:11, 62.0MB/s]\n\nDownloading data:  90%|######### | 6.37G/7.07G [01:46&lt;00:11, 62.4MB/s]\n\nDownloading data:  90%|######### | 6.37G/7.07G [01:46&lt;00:11, 62.4MB/s]\n\nDownloading data:  90%|######### | 6.38G/7.07G [01:47&lt;00:11, 61.1MB/s]\n\nDownloading data:  90%|######### | 6.38G/7.07G [01:47&lt;00:11, 61.2MB/s]\n\nDownloading data:  90%|######### | 6.39G/7.07G [01:47&lt;00:11, 60.9MB/s]\n\nDownloading data:  90%|######### | 6.40G/7.07G [01:47&lt;00:10, 61.8MB/s]\n\nDownloading data:  91%|######### | 6.40G/7.07G [01:47&lt;00:10, 62.0MB/s]\n\nDownloading data:  91%|######### | 6.41G/7.07G [01:47&lt;00:10, 62.4MB/s]\n\nDownloading data:  91%|######### | 6.42G/7.07G [01:47&lt;00:10, 61.8MB/s]\n\nDownloading data:  91%|######### | 6.42G/7.07G [01:47&lt;00:10, 62.2MB/s]\n\nDownloading data:  91%|######### | 6.43G/7.07G [01:47&lt;00:10, 61.7MB/s]\n\nDownloading data:  91%|######### | 6.43G/7.07G [01:47&lt;00:10, 62.4MB/s]\n\nDownloading data:  91%|#########1| 6.44G/7.07G [01:48&lt;00:10, 62.0MB/s]\n\nDownloading data:  91%|#########1| 6.45G/7.07G [01:48&lt;00:10, 61.7MB/s]\n\nDownloading data:  91%|#########1| 6.45G/7.07G [01:48&lt;00:09, 62.6MB/s]\n\nDownloading data:  91%|#########1| 6.46G/7.07G [01:48&lt;00:09, 62.3MB/s]\n\nDownloading data:  91%|#########1| 6.47G/7.07G [01:48&lt;00:09, 62.4MB/s]\n\nDownloading data:  92%|#########1| 6.47G/7.07G [01:48&lt;00:09, 61.5MB/s]\n\nDownloading data:  92%|#########1| 6.48G/7.07G [01:48&lt;00:09, 62.0MB/s]\n\nDownloading data:  92%|#########1| 6.49G/7.07G [01:48&lt;00:09, 61.9MB/s]\n\nDownloading data:  92%|#########1| 6.49G/7.07G [01:48&lt;00:09, 61.7MB/s]\n\nDownloading data:  92%|#########1| 6.50G/7.07G [01:48&lt;00:09, 61.9MB/s]\n\nDownloading data:  92%|#########1| 6.50G/7.07G [01:49&lt;00:09, 61.8MB/s]\n\nDownloading data:  92%|#########2| 6.51G/7.07G [01:49&lt;00:09, 61.8MB/s]\n\nDownloading data:  92%|#########2| 6.52G/7.07G [01:49&lt;00:09, 61.7MB/s]\n\nDownloading data:  92%|#########2| 6.52G/7.07G [01:49&lt;00:08, 61.8MB/s]\n\nDownloading data:  92%|#########2| 6.53G/7.07G [01:49&lt;00:08, 63.1MB/s]\n\nDownloading data:  92%|#########2| 6.54G/7.07G [01:49&lt;00:08, 63.6MB/s]\n\nDownloading data:  93%|#########2| 6.54G/7.07G [01:49&lt;00:08, 64.3MB/s]\n\nDownloading data:  93%|#########2| 6.55G/7.07G [01:49&lt;00:08, 64.3MB/s]\n\nDownloading data:  93%|#########2| 6.56G/7.07G [01:49&lt;00:08, 64.6MB/s]\n\nDownloading data:  93%|#########2| 6.56G/7.07G [01:49&lt;00:07, 64.0MB/s]\n\nDownloading data:  93%|#########2| 6.57G/7.07G [01:50&lt;00:07, 65.1MB/s]\n\nDownloading data:  93%|#########2| 6.57G/7.07G [01:50&lt;00:07, 65.1MB/s]\n\nDownloading data:  93%|#########3| 6.58G/7.07G [01:50&lt;00:07, 65.2MB/s]\n\nDownloading data:  93%|#########3| 6.59G/7.07G [01:50&lt;00:07, 63.4MB/s]\n\nDownloading data:  93%|#########3| 6.60G/7.07G [01:50&lt;00:07, 66.5MB/s]\n\nDownloading data:  93%|#########3| 6.60G/7.07G [01:50&lt;00:07, 67.0MB/s]\n\nDownloading data:  93%|#########3| 6.61G/7.07G [01:50&lt;00:07, 65.3MB/s]\n\nDownloading data:  94%|#########3| 6.62G/7.07G [01:50&lt;00:07, 64.9MB/s]\n\nDownloading data:  94%|#########3| 6.62G/7.07G [01:50&lt;00:06, 65.4MB/s]\n\nDownloading data:  94%|#########3| 6.63G/7.07G [01:51&lt;00:06, 64.9MB/s]\n\nDownloading data:  94%|#########3| 6.64G/7.07G [01:51&lt;00:06, 64.5MB/s]\n\nDownloading data:  94%|#########3| 6.64G/7.07G [01:51&lt;00:06, 64.9MB/s]\n\nDownloading data:  94%|#########4| 6.65G/7.07G [01:51&lt;00:06, 64.8MB/s]\n\nDownloading data:  94%|#########4| 6.65G/7.07G [01:51&lt;00:06, 65.0MB/s]\n\nDownloading data:  94%|#########4| 6.66G/7.07G [01:51&lt;00:06, 65.1MB/s]\n\nDownloading data:  94%|#########4| 6.67G/7.07G [01:51&lt;00:06, 65.2MB/s]\n\nDownloading data:  94%|#########4| 6.67G/7.07G [01:51&lt;00:06, 64.7MB/s]\n\nDownloading data:  94%|#########4| 6.68G/7.07G [01:51&lt;00:06, 56.9MB/s]\n\nDownloading data:  95%|#########4| 6.69G/7.07G [01:51&lt;00:06, 58.1MB/s]\n\nDownloading data:  95%|#########4| 6.69G/7.07G [01:52&lt;00:06, 60.3MB/s]\n\nDownloading data:  95%|#########4| 6.70G/7.07G [01:52&lt;00:06, 61.5MB/s]\n\nDownloading data:  95%|#########4| 6.71G/7.07G [01:52&lt;00:05, 61.9MB/s]\n\nDownloading data:  95%|#########4| 6.71G/7.07G [01:52&lt;00:05, 63.3MB/s]\n\nDownloading data:  95%|#########5| 6.72G/7.07G [01:52&lt;00:05, 63.1MB/s]\n\nDownloading data:  95%|#########5| 6.73G/7.07G [01:52&lt;00:05, 64.4MB/s]\n\nDownloading data:  95%|#########5| 6.73G/7.07G [01:52&lt;00:05, 63.6MB/s]\n\nDownloading data:  95%|#########5| 6.74G/7.07G [01:52&lt;00:05, 64.7MB/s]\n\nDownloading data:  95%|#########5| 6.75G/7.07G [01:52&lt;00:04, 66.2MB/s]\n\nDownloading data:  95%|#########5| 6.75G/7.07G [01:52&lt;00:04, 65.7MB/s]\n\nDownloading data:  96%|#########5| 6.76G/7.07G [01:53&lt;00:04, 65.9MB/s]\n\nDownloading data:  96%|#########5| 6.77G/7.07G [01:53&lt;00:04, 65.7MB/s]\n\nDownloading data:  96%|#########5| 6.77G/7.07G [01:53&lt;00:04, 66.6MB/s]\n\nDownloading data:  96%|#########5| 6.78G/7.07G [01:53&lt;00:04, 65.5MB/s]\n\nDownloading data:  96%|#########5| 6.79G/7.07G [01:53&lt;00:04, 65.5MB/s]\n\nDownloading data:  96%|#########6| 6.79G/7.07G [01:53&lt;00:04, 65.5MB/s]\n\nDownloading data:  96%|#########6| 6.80G/7.07G [01:53&lt;00:04, 65.2MB/s]\n\nDownloading data:  96%|#########6| 6.81G/7.07G [01:53&lt;00:04, 65.3MB/s]\n\nDownloading data:  96%|#########6| 6.81G/7.07G [01:53&lt;00:04, 64.0MB/s]\n\nDownloading data:  96%|#########6| 6.82G/7.07G [01:53&lt;00:03, 63.8MB/s]\n\nDownloading data:  97%|#########6| 6.83G/7.07G [01:54&lt;00:03, 62.9MB/s]\n\nDownloading data:  97%|#########6| 6.83G/7.07G [01:54&lt;00:03, 64.1MB/s]\n\nDownloading data:  97%|#########6| 6.84G/7.07G [01:54&lt;00:03, 66.6MB/s]\n\nDownloading data:  97%|#########6| 6.85G/7.07G [01:54&lt;00:03, 65.9MB/s]\n\nDownloading data:  97%|#########6| 6.85G/7.07G [01:54&lt;00:05, 38.9MB/s]\n\nDownloading data:  97%|#########6| 6.86G/7.07G [01:54&lt;00:04, 44.4MB/s]\n\nDownloading data:  97%|#########7| 6.87G/7.07G [01:54&lt;00:04, 48.7MB/s]\n\nDownloading data:  97%|#########7| 6.87G/7.07G [01:55&lt;00:03, 52.3MB/s]\n\nDownloading data:  97%|#########7| 6.88G/7.07G [01:55&lt;00:03, 55.6MB/s]\n\nDownloading data:  97%|#########7| 6.88G/7.07G [01:55&lt;00:03, 58.4MB/s]\n\nDownloading data:  97%|#########7| 6.89G/7.07G [01:55&lt;00:03, 59.7MB/s]\n\nDownloading data:  98%|#########7| 6.90G/7.07G [01:55&lt;00:02, 61.5MB/s]\n\nDownloading data:  98%|#########7| 6.90G/7.07G [01:55&lt;00:02, 62.6MB/s]\n\nDownloading data:  98%|#########7| 6.91G/7.07G [01:55&lt;00:02, 63.6MB/s]\n\nDownloading data:  98%|#########7| 6.92G/7.07G [01:55&lt;00:02, 64.0MB/s]\n\nDownloading data:  98%|#########7| 6.92G/7.07G [01:55&lt;00:02, 64.1MB/s]\n\nDownloading data:  98%|#########7| 6.93G/7.07G [01:55&lt;00:02, 62.6MB/s]\n\nDownloading data:  98%|#########8| 6.94G/7.07G [01:56&lt;00:02, 66.0MB/s]\n\nDownloading data:  98%|#########8| 6.95G/7.07G [01:56&lt;00:01, 67.0MB/s]\n\nDownloading data:  98%|#########8| 6.95G/7.07G [01:56&lt;00:01, 66.3MB/s]\n\nDownloading data:  98%|#########8| 6.96G/7.07G [01:56&lt;00:01, 64.9MB/s]\n\nDownloading data:  98%|#########8| 6.97G/7.07G [01:56&lt;00:01, 64.5MB/s]\n\nDownloading data:  99%|#########8| 6.97G/7.07G [01:56&lt;00:01, 65.9MB/s]\n\nDownloading data:  99%|#########8| 6.98G/7.07G [01:56&lt;00:01, 56.6MB/s]\n\nDownloading data:  99%|#########8| 6.98G/7.07G [01:56&lt;00:01, 57.1MB/s]\n\nDownloading data:  99%|#########8| 6.99G/7.07G [01:56&lt;00:01, 58.9MB/s]\n\nDownloading data:  99%|#########8| 7.00G/7.07G [01:57&lt;00:01, 61.8MB/s]\n\nDownloading data:  99%|#########9| 7.00G/7.07G [01:57&lt;00:01, 62.6MB/s]\n\nDownloading data:  99%|#########9| 7.01G/7.07G [01:57&lt;00:00, 62.9MB/s]\n\nDownloading data:  99%|#########9| 7.02G/7.07G [01:57&lt;00:00, 64.0MB/s]\n\nDownloading data:  99%|#########9| 7.02G/7.07G [01:57&lt;00:00, 64.7MB/s]\n\nDownloading data:  99%|#########9| 7.03G/7.07G [01:57&lt;00:00, 64.6MB/s]\n\nDownloading data:  99%|#########9| 7.04G/7.07G [01:57&lt;00:00, 64.8MB/s]\n\nDownloading data: 100%|#########9| 7.04G/7.07G [01:57&lt;00:00, 65.8MB/s]\n\nDownloading data: 100%|#########9| 7.05G/7.07G [01:57&lt;00:00, 65.2MB/s]\n\nDownloading data: 100%|#########9| 7.06G/7.07G [01:57&lt;00:00, 65.5MB/s]\n\nDownloading data: 100%|#########9| 7.06G/7.07G [01:58&lt;00:00, 62.6MB/s]\n\nDownloading data: 100%|#########9| 7.07G/7.07G [01:58&lt;00:00, 64.0MB/s]\nDownloading data: 100%|##########| 7.07G/7.07G [01:58&lt;00:00, 59.9MB/s]\n\nDownloading data files:  80%|########  | 4/5 [02:03&lt;00:47, 47.67s/it]\n\nDownloading data:   0%|          | 0.00/970M [00:00&lt;?, ?B/s]\n\nDownloading data:   0%|          | 52.2k/970M [00:00&lt;34:19, 471kB/s]\n\nDownloading data:   0%|          | 191k/970M [00:00&lt;17:29, 925kB/s]\n\nDownloading data:   0%|          | 818k/970M [00:00&lt;05:05, 3.17MB/s]\n\nDownloading data:   0%|          | 3.21M/970M [00:00&lt;01:30, 10.7MB/s]\n\nDownloading data:   1%|          | 8.66M/970M [00:00&lt;00:37, 25.8MB/s]\n\nDownloading data:   2%|1         | 15.0M/970M [00:00&lt;00:25, 38.0MB/s]\n\nDownloading data:   2%|2         | 21.8M/970M [00:00&lt;00:19, 47.7MB/s]\n\nDownloading data:   3%|2         | 28.6M/970M [00:00&lt;00:17, 54.3MB/s]\n\nDownloading data:   4%|3         | 35.1M/970M [00:00&lt;00:16, 57.6MB/s]\n\nDownloading data:   4%|4         | 41.0M/970M [00:01&lt;00:17, 52.0MB/s]\n\nDownloading data:   5%|4         | 46.3M/970M [00:01&lt;00:19, 47.2MB/s]\n\nDownloading data:   5%|5         | 52.3M/970M [00:01&lt;00:18, 50.5MB/s]\n\nDownloading data:   6%|6         | 58.7M/970M [00:01&lt;00:16, 54.2MB/s]\n\nDownloading data:   7%|6         | 64.2M/970M [00:01&lt;00:20, 44.9MB/s]\n\nDownloading data:   7%|7         | 69.3M/970M [00:01&lt;00:19, 46.5MB/s]\n\nDownloading data:   8%|7         | 75.4M/970M [00:01&lt;00:17, 50.3MB/s]\n\nDownloading data:   8%|8         | 80.7M/970M [00:02&lt;00:23, 37.2MB/s]\n\nDownloading data:   9%|8         | 86.3M/970M [00:02&lt;00:21, 41.4MB/s]\n\nDownloading data:   9%|9         | 92.1M/970M [00:02&lt;00:31, 27.8MB/s]\n\nDownloading data:  10%|#         | 98.4M/970M [00:02&lt;00:25, 33.9MB/s]\n\nDownloading data:  11%|#         | 104M/970M [00:02&lt;00:22, 38.7MB/s]\n\nDownloading data:  11%|#1        | 109M/970M [00:02&lt;00:23, 36.0MB/s]\n\nDownloading data:  12%|#1        | 116M/970M [00:02&lt;00:20, 41.2MB/s]\n\nDownloading data:  13%|#2        | 122M/970M [00:03&lt;00:18, 46.8MB/s]\n\nDownloading data:  13%|#3        | 127M/970M [00:03&lt;00:18, 44.8MB/s]\n\nDownloading data:  14%|#3        | 134M/970M [00:03&lt;00:16, 50.3MB/s]\n\nDownloading data:  15%|#4        | 141M/970M [00:03&lt;00:15, 54.3MB/s]\n\nDownloading data:  15%|#5        | 148M/970M [00:03&lt;00:14, 58.5MB/s]\n\nDownloading data:  16%|#5        | 154M/970M [00:03&lt;00:13, 58.6MB/s]\n\nDownloading data:  17%|#6        | 160M/970M [00:03&lt;00:13, 60.4MB/s]\n\nDownloading data:  17%|#7        | 167M/970M [00:03&lt;00:13, 58.5MB/s]\n\nDownloading data:  18%|#7        | 173M/970M [00:03&lt;00:12, 61.3MB/s]\n\nDownloading data:  19%|#8        | 180M/970M [00:04&lt;00:12, 63.4MB/s]\n\nDownloading data:  19%|#9        | 187M/970M [00:04&lt;00:12, 64.2MB/s]\n\nDownloading data:  20%|#9        | 193M/970M [00:04&lt;00:12, 64.1MB/s]\n\nDownloading data:  21%|##        | 200M/970M [00:04&lt;00:11, 64.4MB/s]\n\nDownloading data:  21%|##1       | 206M/970M [00:04&lt;00:12, 63.4MB/s]\n\nDownloading data:  22%|##1       | 213M/970M [00:04&lt;00:11, 63.9MB/s]\n\nDownloading data:  23%|##2       | 219M/970M [00:04&lt;00:11, 64.7MB/s]\n\nDownloading data:  23%|##3       | 226M/970M [00:04&lt;00:11, 64.9MB/s]\n\nDownloading data:  24%|##3       | 233M/970M [00:04&lt;00:11, 65.5MB/s]\n\nDownloading data:  25%|##4       | 239M/970M [00:04&lt;00:12, 60.3MB/s]\n\nDownloading data:  25%|##5       | 246M/970M [00:05&lt;00:11, 61.8MB/s]\n\nDownloading data:  26%|##6       | 252M/970M [00:05&lt;00:11, 62.9MB/s]\n\nDownloading data:  27%|##6       | 259M/970M [00:05&lt;00:11, 63.9MB/s]\n\nDownloading data:  27%|##7       | 266M/970M [00:05&lt;00:10, 64.7MB/s]\n\nDownloading data:  28%|##8       | 272M/970M [00:05&lt;00:10, 65.4MB/s]\n\nDownloading data:  29%|##8       | 279M/970M [00:05&lt;00:10, 65.2MB/s]\n\nDownloading data:  29%|##9       | 285M/970M [00:05&lt;00:10, 65.4MB/s]\n\nDownloading data:  30%|###       | 292M/970M [00:05&lt;00:10, 65.4MB/s]\n\nDownloading data:  31%|###       | 299M/970M [00:05&lt;00:10, 65.7MB/s]\n\nDownloading data:  31%|###1      | 305M/970M [00:05&lt;00:10, 66.1MB/s]\n\nDownloading data:  32%|###2      | 312M/970M [00:06&lt;00:09, 66.2MB/s]\n\nDownloading data:  33%|###2      | 319M/970M [00:06&lt;00:09, 66.2MB/s]\n\nDownloading data:  34%|###3      | 325M/970M [00:06&lt;00:09, 65.8MB/s]\n\nDownloading data:  34%|###4      | 332M/970M [00:06&lt;00:09, 66.0MB/s]\n\nDownloading data:  35%|###4      | 339M/970M [00:06&lt;00:09, 66.9MB/s]\n\nDownloading data:  36%|###5      | 346M/970M [00:06&lt;00:09, 69.2MB/s]\n\nDownloading data:  36%|###6      | 354M/970M [00:06&lt;00:08, 70.6MB/s]\n\nDownloading data:  37%|###7      | 361M/970M [00:06&lt;00:08, 71.6MB/s]\n\nDownloading data:  38%|###7      | 368M/970M [00:06&lt;00:09, 62.7MB/s]\n\nDownloading data:  39%|###8      | 375M/970M [00:06&lt;00:09, 62.9MB/s]\n\nDownloading data:  39%|###9      | 382M/970M [00:07&lt;00:09, 64.4MB/s]\n\nDownloading data:  40%|####      | 388M/970M [00:07&lt;00:08, 64.7MB/s]\n\nDownloading data:  41%|####      | 395M/970M [00:07&lt;00:08, 65.4MB/s]\n\nDownloading data:  41%|####1     | 402M/970M [00:07&lt;00:08, 66.4MB/s]\n\nDownloading data:  42%|####2     | 409M/970M [00:07&lt;00:08, 66.1MB/s]\n\nDownloading data:  43%|####2     | 416M/970M [00:07&lt;00:08, 67.1MB/s]\n\nDownloading data:  44%|####3     | 422M/970M [00:07&lt;00:08, 66.1MB/s]\n\nDownloading data:  44%|####4     | 429M/970M [00:07&lt;00:08, 65.5MB/s]\n\nDownloading data:  45%|####4     | 436M/970M [00:07&lt;00:08, 65.1MB/s]\n\nDownloading data:  46%|####5     | 442M/970M [00:08&lt;00:08, 65.5MB/s]\n\nDownloading data:  46%|####6     | 449M/970M [00:08&lt;00:08, 63.3MB/s]\n\nDownloading data:  47%|####6     | 456M/970M [00:08&lt;00:07, 64.9MB/s]\n\nDownloading data:  48%|####7     | 463M/970M [00:08&lt;00:07, 65.0MB/s]\n\nDownloading data:  48%|####8     | 470M/970M [00:08&lt;00:07, 67.1MB/s]\n\nDownloading data:  49%|####9     | 477M/970M [00:08&lt;00:07, 67.6MB/s]\n\nDownloading data:  50%|####9     | 484M/970M [00:08&lt;00:07, 66.4MB/s]\n\nDownloading data:  51%|#####     | 491M/970M [00:08&lt;00:07, 68.4MB/s]\n\nDownloading data:  51%|#####1    | 498M/970M [00:08&lt;00:08, 58.8MB/s]\n\nDownloading data:  52%|#####1    | 505M/970M [00:09&lt;00:07, 60.7MB/s]\n\nDownloading data:  53%|#####2    | 512M/970M [00:09&lt;00:07, 63.4MB/s]\n\nDownloading data:  53%|#####3    | 518M/970M [00:09&lt;00:07, 62.8MB/s]\n\nDownloading data:  54%|#####4    | 525M/970M [00:09&lt;00:06, 64.4MB/s]\n\nDownloading data:  55%|#####4    | 532M/970M [00:09&lt;00:06, 64.6MB/s]\n\nDownloading data:  55%|#####5    | 538M/970M [00:09&lt;00:06, 64.1MB/s]\n\nDownloading data:  56%|#####6    | 544M/970M [00:09&lt;00:06, 61.0MB/s]\n\nDownloading data:  57%|#####6    | 551M/970M [00:09&lt;00:06, 62.2MB/s]\n\nDownloading data:  58%|#####7    | 558M/970M [00:09&lt;00:06, 64.2MB/s]\n\nDownloading data:  58%|#####8    | 564M/970M [00:09&lt;00:06, 62.9MB/s]\n\nDownloading data:  59%|#####8    | 571M/970M [00:10&lt;00:06, 60.5MB/s]\n\nDownloading data:  60%|#####9    | 578M/970M [00:10&lt;00:06, 62.6MB/s]\n\nDownloading data:  60%|######    | 584M/970M [00:10&lt;00:06, 60.3MB/s]\n\nDownloading data:  61%|######    | 591M/970M [00:10&lt;00:05, 63.6MB/s]\n\nDownloading data:  62%|######1   | 598M/970M [00:10&lt;00:05, 66.0MB/s]\n\nDownloading data:  62%|######2   | 605M/970M [00:10&lt;00:05, 64.9MB/s]\n\nDownloading data:  63%|######3   | 611M/970M [00:10&lt;00:05, 62.6MB/s]\n\nDownloading data:  64%|######3   | 618M/970M [00:10&lt;00:05, 60.8MB/s]\n\nDownloading data:  64%|######4   | 625M/970M [00:10&lt;00:05, 64.1MB/s]\n\nDownloading data:  65%|######5   | 631M/970M [00:11&lt;00:05, 63.6MB/s]\n\nDownloading data:  66%|######5   | 638M/970M [00:11&lt;00:05, 63.7MB/s]\n\nDownloading data:  66%|######6   | 644M/970M [00:11&lt;00:05, 61.5MB/s]\n\nDownloading data:  67%|######7   | 651M/970M [00:11&lt;00:04, 64.4MB/s]\n\nDownloading data:  68%|######7   | 658M/970M [00:11&lt;00:04, 65.3MB/s]\n\nDownloading data:  69%|######8   | 665M/970M [00:11&lt;00:04, 63.0MB/s]\n\nDownloading data:  69%|######9   | 672M/970M [00:11&lt;00:04, 64.9MB/s]\n\nDownloading data:  70%|######9   | 678M/970M [00:11&lt;00:04, 62.5MB/s]\n\nDownloading data:  71%|#######   | 684M/970M [00:11&lt;00:04, 61.2MB/s]\n\nDownloading data:  71%|#######1  | 691M/970M [00:11&lt;00:04, 62.6MB/s]\n\nDownloading data:  72%|#######1  | 698M/970M [00:12&lt;00:04, 64.3MB/s]\n\nDownloading data:  73%|#######2  | 704M/970M [00:12&lt;00:04, 63.8MB/s]\n\nDownloading data:  73%|#######3  | 711M/970M [00:12&lt;00:04, 63.3MB/s]\n\nDownloading data:  74%|#######3  | 717M/970M [00:12&lt;00:03, 63.4MB/s]\n\nDownloading data:  75%|#######4  | 723M/970M [00:12&lt;00:04, 61.6MB/s]\n\nDownloading data:  75%|#######5  | 730M/970M [00:12&lt;00:03, 61.1MB/s]\n\nDownloading data:  76%|#######5  | 736M/970M [00:12&lt;00:05, 45.8MB/s]\n\nDownloading data:  76%|#######6  | 742M/970M [00:12&lt;00:04, 49.0MB/s]\n\nDownloading data:  77%|#######7  | 748M/970M [00:12&lt;00:04, 52.8MB/s]\n\nDownloading data:  78%|#######7  | 754M/970M [00:13&lt;00:03, 55.9MB/s]\n\nDownloading data:  78%|#######8  | 761M/970M [00:13&lt;00:03, 58.1MB/s]\n\nDownloading data:  79%|#######9  | 768M/970M [00:13&lt;00:03, 61.4MB/s]\n\nDownloading data:  80%|#######9  | 774M/970M [00:13&lt;00:03, 61.3MB/s]\n\nDownloading data:  80%|########  | 780M/970M [00:13&lt;00:03, 60.7MB/s]\n\nDownloading data:  81%|########1 | 787M/970M [00:13&lt;00:03, 60.0MB/s]\n\nDownloading data:  82%|########1 | 794M/970M [00:13&lt;00:02, 63.5MB/s]\n\nDownloading data:  82%|########2 | 800M/970M [00:13&lt;00:02, 61.9MB/s]\n\nDownloading data:  83%|########3 | 807M/970M [00:13&lt;00:02, 62.6MB/s]\n\nDownloading data:  84%|########3 | 814M/970M [00:14&lt;00:02, 64.8MB/s]\n\nDownloading data:  85%|########4 | 820M/970M [00:14&lt;00:02, 64.2MB/s]\n\nDownloading data:  85%|########5 | 827M/970M [00:14&lt;00:02, 62.0MB/s]\n\nDownloading data:  86%|########5 | 834M/970M [00:14&lt;00:02, 65.0MB/s]\n\nDownloading data:  87%|########6 | 840M/970M [00:14&lt;00:02, 60.8MB/s]\n\nDownloading data:  87%|########7 | 847M/970M [00:14&lt;00:02, 61.3MB/s]\n\nDownloading data:  88%|########7 | 853M/970M [00:14&lt;00:02, 57.9MB/s]\n\nDownloading data:  89%|########8 | 860M/970M [00:14&lt;00:01, 62.5MB/s]\n\nDownloading data:  89%|########9 | 867M/970M [00:14&lt;00:01, 61.2MB/s]\n\nDownloading data:  90%|######### | 873M/970M [00:14&lt;00:01, 63.1MB/s]\n\nDownloading data:  91%|######### | 880M/970M [00:15&lt;00:01, 65.0MB/s]\n\nDownloading data:  92%|#########1| 888M/970M [00:15&lt;00:01, 68.0MB/s]\n\nDownloading data:  92%|#########2| 895M/970M [00:15&lt;00:01, 67.1MB/s]\n\nDownloading data:  93%|#########2| 902M/970M [00:15&lt;00:01, 67.2MB/s]\n\nDownloading data:  94%|#########3| 908M/970M [00:15&lt;00:00, 64.0MB/s]\n\nDownloading data:  94%|#########4| 915M/970M [00:15&lt;00:01, 54.1MB/s]\n\nDownloading data:  95%|#########4| 920M/970M [00:15&lt;00:01, 47.6MB/s]\n\nDownloading data:  95%|#########5| 926M/970M [00:15&lt;00:00, 48.3MB/s]\n\nDownloading data:  96%|#########6| 932M/970M [00:16&lt;00:00, 48.8MB/s]\n\nDownloading data:  97%|#########6| 937M/970M [00:16&lt;00:00, 50.8MB/s]\n\nDownloading data:  97%|#########7| 942M/970M [00:16&lt;00:00, 50.1MB/s]\n\nDownloading data:  98%|#########7| 947M/970M [00:16&lt;00:00, 49.6MB/s]\n\nDownloading data:  98%|#########8| 953M/970M [00:16&lt;00:00, 49.5MB/s]\n\nDownloading data:  99%|#########8| 958M/970M [00:16&lt;00:00, 49.0MB/s]\n\nDownloading data:  99%|#########9| 963M/970M [00:16&lt;00:00, 50.7MB/s]\n\nDownloading data: 100%|#########9| 968M/970M [00:16&lt;00:00, 51.1MB/s]\nDownloading data: 100%|##########| 970M/970M [00:16&lt;00:00, 57.7MB/s]\n\nDownloading data files: 100%|##########| 5/5 [02:21&lt;00:00, 36.83s/it]\nDownloading data files: 100%|##########| 5/5 [02:21&lt;00:00, 28.22s/it]\n\nExtracting data files:   0%|          | 0/5 [00:00&lt;?, ?it/s]\nExtracting data files:  80%|########  | 4/5 [00:28&lt;00:07,  7.04s/it]\nExtracting data files: 100%|##########| 5/5 [00:32&lt;00:00,  6.20s/it]\nExtracting data files: 100%|##########| 5/5 [00:32&lt;00:00,  6.40s/it]\n\nGenerating train split:   0%|          | 0/34602 [00:00&lt;?, ? examples/s]\nGenerating train split:   0%|          | 1/34602 [00:00&lt;6:28:41,  1.48 examples/s]\nGenerating train split:   1%|1         | 379/34602 [00:00&lt;00:51, 661.87 examples/s]\nGenerating train split:   2%|2         | 768/34602 [00:00&lt;00:25, 1305.96 examples/s]\nGenerating train split:   3%|3         | 1198/34602 [00:01&lt;00:20, 1624.78 examples/s]\nGenerating train split:   5%|4         | 1598/34602 [00:01&lt;00:15, 2116.36 examples/s]\nGenerating train split:   6%|5         | 2000/34602 [00:01&lt;00:13, 2447.35 examples/s]\nGenerating train split:   7%|6         | 2395/34602 [00:01&lt;00:11, 2803.76 examples/s]\nGenerating train split:   8%|8         | 2790/34602 [00:01&lt;00:10, 3091.92 examples/s]\nGenerating train split:  10%|9         | 3300/34602 [00:01&lt;00:09, 3197.93 examples/s]\nGenerating train split:  11%|#         | 3693/34602 [00:01&lt;00:09, 3378.51 examples/s]\nGenerating train split:  12%|#2        | 4216/34602 [00:01&lt;00:08, 3414.68 examples/s]\nGenerating train split:  13%|#3        | 4619/34602 [00:01&lt;00:08, 3566.97 examples/s]\nGenerating train split:  14%|#4        | 5000/34602 [00:02&lt;00:08, 3493.18 examples/s]\nGenerating train split:  16%|#5        | 5394/34602 [00:02&lt;00:08, 3609.95 examples/s]\nGenerating train split:  17%|#6        | 5795/34602 [00:02&lt;00:07, 3716.85 examples/s]\nGenerating train split:  18%|#8        | 6311/34602 [00:02&lt;00:07, 3608.36 examples/s]\nGenerating train split:  19%|#9        | 6701/34602 [00:02&lt;00:07, 3683.15 examples/s]\nGenerating train split:  21%|##        | 7221/34602 [00:02&lt;00:07, 3599.75 examples/s]\nGenerating train split:  22%|##2       | 7624/34602 [00:02&lt;00:07, 3706.21 examples/s]\nGenerating train split:  24%|##3       | 8198/34602 [00:02&lt;00:07, 3633.41 examples/s]\nGenerating train split:  25%|##4       | 8601/34602 [00:03&lt;00:06, 3728.54 examples/s]\nGenerating train split:  26%|##6       | 9000/34602 [00:03&lt;00:07, 3648.25 examples/s]\nGenerating train split:  27%|##7       | 9402/34602 [00:03&lt;00:06, 3743.27 examples/s]\nGenerating train split:  28%|##8       | 9797/34602 [00:03&lt;00:06, 3797.32 examples/s]\nGenerating train split:  30%|##9       | 10324/34602 [00:03&lt;00:06, 3689.01 examples/s]\nGenerating train split:  31%|###       | 10708/34602 [00:03&lt;00:06, 3724.76 examples/s]\nGenerating train split:  32%|###2      | 11217/34602 [00:03&lt;00:06, 3604.20 examples/s]\nGenerating train split:  34%|###3      | 11615/34602 [00:03&lt;00:06, 3696.31 examples/s]\nGenerating train split:  35%|###4      | 11995/34602 [00:03&lt;00:06, 3721.10 examples/s]\nGenerating train split:  36%|###6      | 12484/34602 [00:04&lt;00:06, 3552.63 examples/s]\nGenerating train split:  37%|###7      | 12888/34602 [00:04&lt;00:05, 3676.47 examples/s]\nGenerating train split:  39%|###8      | 13415/34602 [00:04&lt;00:05, 3614.38 examples/s]\nGenerating train split:  40%|###9      | 13824/34602 [00:04&lt;00:05, 3733.15 examples/s]\nGenerating train split:  41%|####1     | 14204/34602 [00:04&lt;00:05, 3628.00 examples/s]\nGenerating train split:  42%|####2     | 14605/34602 [00:04&lt;00:05, 3729.74 examples/s]\nGenerating train split:  43%|####3     | 14996/34602 [00:04&lt;00:05, 3774.74 examples/s]\nGenerating train split:  45%|####4     | 15482/34602 [00:04&lt;00:05, 3570.56 examples/s]\nGenerating train split:  46%|####5     | 15880/34602 [00:05&lt;00:05, 3674.60 examples/s]\nGenerating train split:  47%|####7     | 16420/34602 [00:05&lt;00:04, 3644.19 examples/s]\nGenerating train split:  49%|####8     | 16829/34602 [00:05&lt;00:04, 3754.54 examples/s]\nGenerating train split:  50%|#####     | 17412/34602 [00:05&lt;00:04, 3705.24 examples/s]\nGenerating train split:  51%|#####1    | 17815/34602 [00:05&lt;00:04, 3782.97 examples/s]\nGenerating train split:  53%|#####2    | 18324/34602 [00:05&lt;00:04, 3638.13 examples/s]\nGenerating train split:  54%|#####4    | 18724/34602 [00:05&lt;00:04, 3726.06 examples/s]\nGenerating train split:  55%|#####5    | 19165/34602 [00:05&lt;00:04, 3410.93 examples/s]\nGenerating train split:  57%|#####6    | 19641/34602 [00:06&lt;00:04, 3328.50 examples/s]\nGenerating train split:  58%|#####7    | 20040/34602 [00:06&lt;00:04, 3111.96 examples/s]\nGenerating train split:  59%|#####9    | 20507/34602 [00:06&lt;00:04, 3110.11 examples/s]\nGenerating train split:  61%|######    | 20986/34602 [00:06&lt;00:04, 3132.31 examples/s]\nGenerating train split:  62%|######1   | 21401/34602 [00:06&lt;00:04, 3018.20 examples/s]\nGenerating train split:  63%|######2   | 21732/34602 [00:06&lt;00:04, 3081.14 examples/s]\nGenerating train split:  64%|######4   | 22214/34602 [00:06&lt;00:03, 3120.59 examples/s]\nGenerating train split:  65%|######5   | 22582/34602 [00:07&lt;00:03, 3250.29 examples/s]\nGenerating train split:  66%|######6   | 22919/34602 [00:07&lt;00:03, 3278.15 examples/s]\nGenerating train split:  68%|######7   | 23401/34602 [00:07&lt;00:03, 3254.65 examples/s]\nGenerating train split:  69%|######8   | 23767/34602 [00:07&lt;00:03, 3352.62 examples/s]\nGenerating train split:  70%|#######   | 24252/34602 [00:07&lt;00:03, 3308.31 examples/s]\nGenerating train split:  71%|#######1  | 24637/34602 [00:07&lt;00:02, 3441.39 examples/s]\nGenerating train split:  72%|#######2  | 25000/34602 [00:07&lt;00:02, 3382.84 examples/s]\nGenerating train split:  73%|#######3  | 25379/34602 [00:07&lt;00:02, 3489.01 examples/s]\nGenerating train split:  74%|#######4  | 25761/34602 [00:07&lt;00:02, 3577.25 examples/s]\nGenerating train split:  76%|#######5  | 26288/34602 [00:08&lt;00:02, 3550.82 examples/s]\nGenerating train split:  77%|#######7  | 26687/34602 [00:08&lt;00:02, 3663.77 examples/s]\nGenerating train split:  79%|#######8  | 27214/34602 [00:08&lt;00:02, 3608.48 examples/s]\nGenerating train split:  80%|#######9  | 27617/34602 [00:08&lt;00:01, 3713.01 examples/s]\nGenerating train split:  81%|########  | 28003/34602 [00:08&lt;00:02, 2361.62 examples/s]\nGenerating train split:  82%|########1 | 28366/34602 [00:08&lt;00:02, 2605.37 examples/s]\nGenerating train split:  83%|########3 | 28754/34602 [00:09&lt;00:02, 2879.97 examples/s]\nGenerating train split:  85%|########4 | 29267/34602 [00:09&lt;00:01, 3048.78 examples/s]\nGenerating train split:  86%|########5 | 29658/34602 [00:09&lt;00:01, 3245.88 examples/s]\nGenerating train split:  87%|########7 | 30203/34602 [00:09&lt;00:01, 3315.18 examples/s]\nGenerating train split:  88%|########8 | 30600/34602 [00:09&lt;00:01, 3468.41 examples/s]\nGenerating train split:  90%|########9 | 30988/34602 [00:09&lt;00:01, 3570.03 examples/s]\nGenerating train split:  91%|#########1| 31525/34602 [00:09&lt;00:00, 3570.08 examples/s]\nGenerating train split:  92%|#########2| 31906/34602 [00:09&lt;00:00, 3628.23 examples/s]\nGenerating train split:  94%|#########3| 32359/34602 [00:10&lt;00:00, 3412.88 examples/s]\nGenerating train split:  95%|#########4| 32731/34602 [00:10&lt;00:00, 3485.80 examples/s]\nGenerating train split:  96%|#########5| 33216/34602 [00:10&lt;00:00, 3392.70 examples/s]\nGenerating train split:  97%|#########7| 33603/34602 [00:10&lt;00:00, 3509.93 examples/s]\nGenerating train split:  98%|#########8| 34000/34602 [00:10&lt;00:00, 3467.02 examples/s]\nGenerating train split:  99%|#########9| 34390/34602 [00:10&lt;00:00, 3578.57 examples/s]\n\n\nGenerating validation split:   0%|          | 0/5000 [00:00&lt;?, ? examples/s]\nGenerating validation split:   4%|3         | 199/5000 [00:00&lt;00:02, 1907.76 examples/s]\nGenerating validation split:  12%|#1        | 597/5000 [00:00&lt;00:01, 3101.72 examples/s]\nGenerating validation split:  20%|#9        | 984/5000 [00:00&lt;00:01, 3444.02 examples/s]\nGenerating validation split:  30%|##9       | 1499/5000 [00:00&lt;00:01, 3435.02 examples/s]\nGenerating validation split:  38%|###7      | 1894/5000 [00:00&lt;00:00, 3597.72 examples/s]\nGenerating validation split:  48%|####8     | 2407/5000 [00:00&lt;00:00, 3522.12 examples/s]\nGenerating validation split:  56%|#####6    | 2800/5000 [00:00&lt;00:00, 3634.53 examples/s]\nGenerating validation split:  66%|######6   | 3315/5000 [00:00&lt;00:00, 3553.00 examples/s]\nGenerating validation split:  74%|#######4  | 3705/5000 [00:01&lt;00:00, 3640.83 examples/s]\nGenerating validation split:  84%|########4 | 4217/5000 [00:01&lt;00:00, 3558.38 examples/s]\nGenerating validation split:  92%|#########1| 4585/5000 [00:01&lt;00:00, 3586.99 examples/s]\nGenerating validation split:  99%|#########9| 4958/5000 [00:01&lt;00:00, 3622.42 examples/s]\n\n\nGenerating test split:   0%|          | 0/5734 [00:00&lt;?, ? examples/s]\nGenerating test split:   5%|4         | 263/5734 [00:00&lt;00:02, 2617.06 examples/s]\nGenerating test split:  12%|#1        | 677/5734 [00:00&lt;00:01, 3506.36 examples/s]\nGenerating test split:  21%|##1       | 1210/5734 [00:00&lt;00:01, 3485.67 examples/s]\nGenerating test split:  27%|##7       | 1574/5734 [00:00&lt;00:01, 3534.72 examples/s]\nGenerating test split:  35%|###4      | 1981/5734 [00:00&lt;00:01, 3709.14 examples/s]\nGenerating test split:  44%|####4     | 2529/5734 [00:00&lt;00:00, 3682.74 examples/s]\nGenerating test split:  51%|#####1    | 2947/5734 [00:00&lt;00:00, 3819.82 examples/s]\nGenerating test split:  61%|######    | 3488/5734 [00:00&lt;00:00, 3737.10 examples/s]\nGenerating test split:  68%|######7   | 3896/5734 [00:01&lt;00:00, 3824.52 examples/s]\nGenerating test split:  77%|#######7  | 4418/5734 [00:01&lt;00:00, 3698.92 examples/s]\nGenerating test split:  84%|########4 | 4822/5734 [00:01&lt;00:00, 3783.61 examples/s]\nGenerating test split:  94%|#########4| 5404/5734 [00:01&lt;00:00, 3712.76 examples/s]\n\nDataset textvqa downloaded and prepared to /var/lib/jenkins/.cache/huggingface/datasets/textvqa/textvqa/0.5.1/9b89037cc122c3b495b155a1bce4170851829843454e88f236bb8715d977c027. Subsequent calls will reuse this data.\n\n  0%|          | 0/3 [00:00&lt;?, ?it/s]\n100%|##########| 3/3 [00:00&lt;00:00, 260.09it/s]",
                        "code"
                    ],
                    [
                        "Lets display a sample entry from the dataset:",
                        "markdown"
                    ],
                    [
                        "import matplotlib.pyplot as plt\nimport numpy as np\nidx = 5\nprint(\"Question: \", dataset[\"train\"][idx][\"question\"])\nprint(\"Answers: \" ,dataset[\"train\"][idx][\"answers\"])\nim = np.asarray(dataset[\"train\"][idx][\"image\"].resize((500,500)))\nplt.imshow(im)\nplt.show()\n\n\n<img alt=\"flava finetuning tutorial\" class=\"sphx-glr-single-img\" src=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\" srcset=\"../_images/sphx_glr_flava_finetuning_tutorial_001.png\"/>",
                        "code"
                    ],
                    [
                        "Question:  what year is shown in the photo?\nAnswers:  ['2011', '2011', '2011', '2011', '2011', '2011', '2011', '2011', '2011', '2011']",
                        "code"
                    ],
                    [
                        "3. Next, we write the transform function to convert the image and text into\nTensors consumable by our model - For images, we use the transforms from\ntorchvision to convert to Tensor and resize to uniform sizes - For text,\nwe tokenize (and pad) them using the BertTokenizer from HuggingFace -\nFor answers (i.e. labels), we take the most frequently occuring answer\nas the label to train with:",
                        "markdown"
                    ],
                    [
                        "import torch\nfrom torchvision import transforms\nfrom collections import defaultdict\nfrom transformers import BertTokenizer\nfrom functools import partial\n\ndef transform(tokenizer, input):\n  batch = {}\n  image_transform = ([(), ([224,224])])\n  image = image_transform(input[\"image\"][0].convert(\"RGB\"))\n  batch[\"image\"] = [image]\n\n  tokenized=tokenizer(input[\"question\"],return_tensors='pt',padding=\"max_length\",max_length=512)\n  batch.update(tokenized)\n\n\n  ans_to_count = defaultdict(int)\n  for ans in input[\"answers\"][0]:\n    ans_to_count[ans] += 1\n  max_value = max(ans_to_count, key=ans_to_count.get)\n  ans_idx = answer_to_idx.get(max_value,0)\n  batch[\"answers\"] = ([ans_idx])\n  return batch\n\ntokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\",padding=\"max_length\",max_length=512)\ntransform=partial(transform,tokenizer)\ndataset.set_transform(transform)",
                        "code"
                    ],
                    [
                        "Downloading (\u2026)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00&lt;?, ?B/s]\nDownloading (\u2026)solve/main/vocab.txt: 100%|##########| 232k/232k [00:00&lt;00:00, 2.68MB/s]\n\nDownloading (\u2026)okenizer_config.json:   0%|          | 0.00/28.0 [00:00&lt;?, ?B/s]\nDownloading (\u2026)okenizer_config.json: 100%|##########| 28.0/28.0 [00:00&lt;00:00, 22.7kB/s]\n\nDownloading (\u2026)lve/main/config.json:   0%|          | 0.00/570 [00:00&lt;?, ?B/s]\nDownloading (\u2026)lve/main/config.json: 100%|##########| 570/570 [00:00&lt;00:00, 623kB/s]",
                        "code"
                    ],
                    [
                        "4. Finally, we import the flava_model_for_classification from\ntorchmultimodal. It loads the pretrained flava checkpoint by default and\nincludes a classification head.",
                        "markdown"
                    ],
                    [
                        "The model forward function passes the image through the visual encoder\nand the question through the text encoder. The image and question\nembeddings are then passed through the multimodal encoder. The final\nembedding corresponding to the CLS token is passed through a MLP head\nwhich finally gives the probability distribution over each possible\nanswers.",
                        "markdown"
                    ],
                    [
                        "from torchmultimodal.models.flava.model import flava_model_for_classification\nmodel = flava_model_for_classification(num_classes=len(vocab))",
                        "code"
                    ],
                    [
                        "flava_for_pretraining_unified_text_encoder.pt: 0.00B [00:00, ?B/s]\nflava_for_pretraining_unified_text_encoder.pt:   0%|          | 8.19k/1.43G [00:00&lt;13:26:06, 29.6kB/s]\nflava_for_pretraining_unified_text_encoder.pt:   0%|          | 5.50M/1.43G [00:00&lt;01:16, 18.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   1%|          | 11.0M/1.43G [00:00&lt;00:46, 30.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   1%|1         | 14.8M/1.43G [00:00&lt;00:43, 32.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   1%|1         | 18.4M/1.43G [00:00&lt;00:41, 33.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   2%|1         | 22.6M/1.43G [00:00&lt;00:39, 36.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   2%|1         | 25.8M/1.43G [00:00&lt;00:40, 34.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   2%|2         | 30.3M/1.43G [00:00&lt;00:37, 37.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   3%|2         | 35.8M/1.43G [00:01&lt;00:32, 42.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   3%|2         | 40.5M/1.43G [00:01&lt;00:31, 44.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   3%|3         | 45.1M/1.43G [00:01&lt;00:30, 44.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   4%|3         | 50.6M/1.43G [00:01&lt;00:28, 47.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   4%|3         | 54.2M/1.43G [00:01&lt;00:31, 44.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   4%|4         | 59.7M/1.43G [00:01&lt;00:28, 47.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   5%|4         | 65.6M/1.43G [00:01&lt;00:26, 50.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   5%|4         | 70.7M/1.43G [00:01&lt;00:27, 50.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   5%|5         | 75.1M/1.43G [00:01&lt;00:28, 48.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   6%|5         | 79.4M/1.43G [00:01&lt;00:29, 46.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   6%|5         | 83.9M/1.43G [00:02&lt;00:29, 46.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   6%|6         | 87.3M/1.43G [00:02&lt;00:31, 42.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   6%|6         | 92.0M/1.43G [00:02&lt;00:35, 38.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   7%|6         | 96.8M/1.43G [00:02&lt;00:34, 38.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   7%|7         | 100M/1.43G [00:02&lt;00:35, 37.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   7%|7         | 105M/1.43G [00:02&lt;00:33, 40.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   8%|7         | 111M/1.43G [00:02&lt;00:28, 46.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   8%|8         | 115M/1.43G [00:02&lt;00:29, 45.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   8%|8         | 121M/1.43G [00:02&lt;00:27, 47.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   9%|8         | 125M/1.43G [00:03&lt;00:29, 44.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   9%|9         | 131M/1.43G [00:03&lt;00:26, 49.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:   9%|9         | 135M/1.43G [00:03&lt;00:27, 46.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  10%|9         | 138M/1.43G [00:03&lt;00:31, 40.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  10%|#         | 144M/1.43G [00:03&lt;00:28, 44.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  10%|#         | 149M/1.43G [00:03&lt;00:28, 45.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  11%|#         | 152M/1.43G [00:03&lt;00:29, 43.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  11%|#         | 157M/1.43G [00:03&lt;00:29, 43.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  11%|#1        | 162M/1.43G [00:03&lt;00:27, 45.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  12%|#1        | 167M/1.43G [00:03&lt;00:26, 48.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  12%|#2        | 173M/1.43G [00:04&lt;00:24, 51.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  12%|#2        | 177M/1.43G [00:04&lt;00:25, 48.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  13%|#2        | 182M/1.43G [00:04&lt;00:26, 47.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  13%|#2        | 185M/1.43G [00:04&lt;00:28, 43.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  13%|#3        | 187M/1.43G [00:04&lt;00:34, 36.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  13%|#3        | 192M/1.43G [00:04&lt;00:31, 39.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  14%|#3        | 195M/1.43G [00:04&lt;00:33, 36.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  14%|#4        | 202M/1.43G [00:04&lt;00:27, 45.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  14%|#4        | 207M/1.43G [00:04&lt;00:25, 48.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  15%|#4        | 213M/1.43G [00:05&lt;00:24, 49.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  15%|#5        | 218M/1.43G [00:05&lt;00:23, 50.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  16%|#5        | 223M/1.43G [00:05&lt;00:24, 49.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  16%|#5        | 227M/1.43G [00:05&lt;00:28, 42.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  16%|#6        | 232M/1.43G [00:05&lt;00:27, 44.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  17%|#6        | 237M/1.43G [00:05&lt;00:26, 45.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  17%|#6        | 242M/1.43G [00:05&lt;00:24, 48.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  17%|#7        | 247M/1.43G [00:05&lt;00:25, 47.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  18%|#7        | 253M/1.43G [00:05&lt;00:23, 50.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  18%|#8        | 259M/1.43G [00:05&lt;00:21, 53.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  19%|#8        | 265M/1.43G [00:06&lt;00:21, 55.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  19%|#8        | 269M/1.43G [00:06&lt;00:23, 50.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  19%|#9        | 273M/1.43G [00:06&lt;00:24, 47.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  20%|#9        | 279M/1.43G [00:06&lt;00:22, 51.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  20%|#9        | 286M/1.43G [00:06&lt;00:20, 56.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  20%|##        | 290M/1.43G [00:06&lt;00:21, 52.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  21%|##        | 295M/1.43G [00:06&lt;00:21, 52.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  21%|##        | 299M/1.43G [00:06&lt;00:23, 47.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  21%|##1       | 304M/1.43G [00:06&lt;00:23, 48.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  22%|##1       | 310M/1.43G [00:06&lt;00:22, 50.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  22%|##1       | 311M/1.43G [00:07&lt;00:29, 38.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  22%|##1       | 312M/1.43G [00:07&lt;00:37, 30.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  22%|##1       | 315M/1.43G [00:07&lt;00:38, 29.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  22%|##2       | 320M/1.43G [00:07&lt;00:30, 36.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  23%|##2       | 326M/1.43G [00:07&lt;00:26, 42.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  23%|##3       | 331M/1.43G [00:07&lt;00:24, 44.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  23%|##3       | 336M/1.43G [00:07&lt;00:23, 47.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  24%|##3       | 341M/1.43G [00:07&lt;00:22, 48.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  24%|##4       | 346M/1.43G [00:07&lt;00:22, 47.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  25%|##4       | 351M/1.43G [00:07&lt;00:22, 48.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  25%|##4       | 356M/1.43G [00:08&lt;00:22, 47.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  25%|##5       | 361M/1.43G [00:08&lt;00:21, 50.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  26%|##5       | 367M/1.43G [00:08&lt;00:19, 54.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  26%|##6       | 373M/1.43G [00:08&lt;00:19, 53.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  26%|##6       | 378M/1.43G [00:08&lt;00:19, 52.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  27%|##6       | 381M/1.43G [00:08&lt;00:22, 46.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  27%|##7       | 387M/1.43G [00:08&lt;00:21, 48.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  27%|##7       | 391M/1.43G [00:08&lt;00:21, 47.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  28%|##7       | 397M/1.43G [00:08&lt;00:20, 51.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  28%|##8       | 402M/1.43G [00:08&lt;00:20, 50.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  28%|##8       | 408M/1.43G [00:09&lt;00:19, 52.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  29%|##8       | 413M/1.43G [00:09&lt;00:19, 53.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  29%|##9       | 419M/1.43G [00:09&lt;00:18, 55.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  30%|##9       | 425M/1.43G [00:09&lt;00:18, 54.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  30%|###       | 429M/1.43G [00:09&lt;00:19, 51.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  31%|###       | 437M/1.43G [00:09&lt;00:16, 59.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  31%|###       | 443M/1.43G [00:09&lt;00:16, 58.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  31%|###1      | 448M/1.43G [00:09&lt;00:17, 57.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  32%|###1      | 453M/1.43G [00:09&lt;00:18, 53.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  32%|###2      | 459M/1.43G [00:09&lt;00:17, 54.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  32%|###2      | 464M/1.43G [00:10&lt;00:17, 54.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  33%|###2      | 467M/1.43G [00:10&lt;00:20, 45.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  33%|###3      | 473M/1.43G [00:10&lt;00:18, 50.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  33%|###3      | 478M/1.43G [00:10&lt;00:18, 50.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  34%|###3      | 484M/1.43G [00:10&lt;00:18, 51.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  34%|###4      | 488M/1.43G [00:10&lt;00:18, 49.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  34%|###4      | 492M/1.43G [00:10&lt;00:22, 42.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  35%|###4      | 495M/1.43G [00:10&lt;00:23, 39.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  35%|###4      | 500M/1.43G [00:10&lt;00:21, 42.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  35%|###5      | 505M/1.43G [00:11&lt;00:20, 46.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  36%|###5      | 509M/1.43G [00:11&lt;00:21, 42.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  36%|###5      | 514M/1.43G [00:11&lt;00:19, 46.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  36%|###6      | 520M/1.43G [00:11&lt;00:17, 50.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  37%|###6      | 525M/1.43G [00:11&lt;00:18, 49.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  37%|###6      | 529M/1.43G [00:11&lt;00:20, 43.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  37%|###7      | 534M/1.43G [00:11&lt;00:19, 45.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  38%|###7      | 540M/1.43G [00:11&lt;00:17, 50.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  38%|###8      | 545M/1.43G [00:11&lt;00:18, 47.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  38%|###8      | 548M/1.43G [00:11&lt;00:19, 44.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  39%|###8      | 553M/1.43G [00:12&lt;00:18, 46.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  39%|###9      | 560M/1.43G [00:12&lt;00:16, 52.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  40%|###9      | 568M/1.43G [00:12&lt;00:14, 60.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  40%|####      | 575M/1.43G [00:12&lt;00:13, 63.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  41%|####      | 581M/1.43G [00:12&lt;00:13, 62.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  41%|####1     | 587M/1.43G [00:12&lt;00:13, 62.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  42%|####1     | 594M/1.43G [00:12&lt;00:12, 64.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  42%|####2     | 601M/1.43G [00:12&lt;00:12, 66.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  42%|####2     | 608M/1.43G [00:12&lt;00:12, 63.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  43%|####2     | 612M/1.43G [00:12&lt;00:14, 54.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  43%|####2     | 615M/1.43G [00:13&lt;00:16, 48.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  43%|####3     | 620M/1.43G [00:13&lt;00:16, 48.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  44%|####3     | 625M/1.43G [00:13&lt;00:16, 49.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  44%|####4     | 630M/1.43G [00:13&lt;00:16, 48.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  44%|####4     | 633M/1.43G [00:13&lt;00:17, 44.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  45%|####4     | 641M/1.43G [00:13&lt;00:14, 53.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  45%|####5     | 647M/1.43G [00:13&lt;00:13, 57.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  46%|####5     | 654M/1.43G [00:13&lt;00:13, 59.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  46%|####6     | 661M/1.43G [00:13&lt;00:12, 63.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  46%|####6     | 665M/1.43G [00:13&lt;00:13, 56.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  47%|####6     | 672M/1.43G [00:14&lt;00:12, 61.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  47%|####7     | 679M/1.43G [00:14&lt;00:12, 62.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  48%|####7     | 685M/1.43G [00:14&lt;00:12, 61.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  48%|####8     | 691M/1.43G [00:14&lt;00:12, 60.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  49%|####8     | 697M/1.43G [00:14&lt;00:12, 60.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  49%|####9     | 704M/1.43G [00:14&lt;00:11, 63.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  50%|####9     | 711M/1.43G [00:14&lt;00:11, 64.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  50%|#####     | 717M/1.43G [00:14&lt;00:11, 64.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  51%|#####     | 723M/1.43G [00:14&lt;00:11, 63.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  51%|#####     | 730M/1.43G [00:14&lt;00:11, 62.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  51%|#####1    | 735M/1.43G [00:15&lt;00:12, 57.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  52%|#####1    | 740M/1.43G [00:15&lt;00:12, 53.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  52%|#####2    | 746M/1.43G [00:15&lt;00:11, 57.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  53%|#####2    | 752M/1.43G [00:15&lt;00:11, 58.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  53%|#####3    | 759M/1.43G [00:15&lt;00:11, 59.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  53%|#####3    | 763M/1.43G [00:15&lt;00:12, 55.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  54%|#####3    | 768M/1.43G [00:15&lt;00:12, 52.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  54%|#####4    | 773M/1.43G [00:15&lt;00:12, 52.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  54%|#####4    | 779M/1.43G [00:15&lt;00:12, 53.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  55%|#####4    | 781M/1.43G [00:15&lt;00:15, 43.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  55%|#####4    | 786M/1.43G [00:16&lt;00:13, 46.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  55%|#####5    | 789M/1.43G [00:16&lt;00:17, 37.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  56%|#####5    | 795M/1.43G [00:16&lt;00:15, 41.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  56%|#####5    | 801M/1.43G [00:16&lt;00:13, 46.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  56%|#####6    | 806M/1.43G [00:16&lt;00:12, 49.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  57%|#####6    | 812M/1.43G [00:16&lt;00:11, 52.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  57%|#####7    | 818M/1.43G [00:16&lt;00:11, 54.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  58%|#####7    | 823M/1.43G [00:16&lt;00:11, 53.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  58%|#####7    | 829M/1.43G [00:16&lt;00:10, 55.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  58%|#####8    | 835M/1.43G [00:17&lt;00:10, 55.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  59%|#####8    | 839M/1.43G [00:17&lt;00:11, 50.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  59%|#####8    | 842M/1.43G [00:17&lt;00:13, 44.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  59%|#####9    | 846M/1.43G [00:17&lt;00:13, 42.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  59%|#####9    | 851M/1.43G [00:17&lt;00:13, 43.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  60%|#####9    | 855M/1.43G [00:17&lt;00:12, 44.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  60%|######    | 862M/1.43G [00:17&lt;00:11, 50.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  61%|######    | 868M/1.43G [00:17&lt;00:10, 53.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  61%|######1   | 874M/1.43G [00:17&lt;00:09, 56.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  62%|######1   | 880M/1.43G [00:17&lt;00:09, 56.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  62%|######1   | 883M/1.43G [00:18&lt;00:11, 48.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  62%|######2   | 887M/1.43G [00:18&lt;00:11, 46.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  62%|######2   | 893M/1.43G [00:18&lt;00:10, 49.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  63%|######2   | 896M/1.43G [00:18&lt;00:11, 44.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  63%|######2   | 901M/1.43G [00:18&lt;00:11, 45.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  63%|######3   | 907M/1.43G [00:18&lt;00:10, 48.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  64%|######3   | 913M/1.43G [00:18&lt;00:09, 52.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  64%|######4   | 918M/1.43G [00:18&lt;00:09, 52.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  64%|######4   | 920M/1.43G [00:18&lt;00:12, 42.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  65%|######4   | 924M/1.43G [00:18&lt;00:11, 42.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  65%|######4   | 930M/1.43G [00:19&lt;00:11, 45.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  65%|######5   | 933M/1.43G [00:19&lt;00:11, 43.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  66%|######5   | 938M/1.43G [00:19&lt;00:11, 43.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  66%|######5   | 943M/1.43G [00:19&lt;00:10, 44.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  66%|######6   | 946M/1.43G [00:19&lt;00:12, 39.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  66%|######6   | 949M/1.43G [00:19&lt;00:13, 36.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  67%|######6   | 953M/1.43G [00:19&lt;00:11, 39.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  67%|######6   | 958M/1.43G [00:19&lt;00:11, 41.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  67%|######7   | 963M/1.43G [00:19&lt;00:10, 45.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  68%|######7   | 968M/1.43G [00:19&lt;00:09, 46.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  68%|######7   | 972M/1.43G [00:20&lt;00:10, 42.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  68%|######8   | 978M/1.43G [00:20&lt;00:09, 47.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  69%|######8   | 983M/1.43G [00:20&lt;00:09, 47.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  69%|######9   | 988M/1.43G [00:20&lt;00:09, 48.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  69%|######9   | 993M/1.43G [00:20&lt;00:09, 48.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  70%|######9   | 997M/1.43G [00:20&lt;00:09, 45.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  70%|######9   | 1.00G/1.43G [00:20&lt;00:10, 42.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  70%|#######   | 1.00G/1.43G [00:20&lt;00:11, 38.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  70%|#######   | 1.01G/1.43G [00:20&lt;00:10, 39.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  71%|#######   | 1.01G/1.43G [00:20&lt;00:08, 48.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  71%|#######1  | 1.02G/1.43G [00:21&lt;00:07, 51.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  72%|#######1  | 1.03G/1.43G [00:21&lt;00:07, 51.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  72%|#######2  | 1.03G/1.43G [00:21&lt;00:07, 54.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  72%|#######2  | 1.04G/1.43G [00:21&lt;00:07, 50.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  73%|#######2  | 1.04G/1.43G [00:21&lt;00:07, 51.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  73%|#######3  | 1.05G/1.43G [00:21&lt;00:07, 52.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  73%|#######3  | 1.05G/1.43G [00:21&lt;00:07, 47.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  74%|#######3  | 1.06G/1.43G [00:21&lt;00:07, 47.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  74%|#######4  | 1.06G/1.43G [00:21&lt;00:07, 48.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  74%|#######4  | 1.07G/1.43G [00:22&lt;00:07, 48.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  75%|#######4  | 1.07G/1.43G [00:22&lt;00:08, 43.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  75%|#######4  | 1.07G/1.43G [00:22&lt;00:11, 31.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  75%|#######5  | 1.08G/1.43G [00:22&lt;00:09, 39.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  75%|#######5  | 1.08G/1.43G [00:22&lt;00:11, 30.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  75%|#######5  | 1.08G/1.43G [00:22&lt;00:15, 22.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  76%|#######5  | 1.08G/1.43G [00:22&lt;00:12, 28.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  76%|#######6  | 1.09G/1.43G [00:22&lt;00:09, 35.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  76%|#######6  | 1.09G/1.43G [00:22&lt;00:08, 39.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  77%|#######6  | 1.10G/1.43G [00:23&lt;00:08, 40.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  77%|#######6  | 1.10G/1.43G [00:23&lt;00:08, 38.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  77%|#######7  | 1.10G/1.43G [00:23&lt;00:09, 35.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  78%|#######7  | 1.11G/1.43G [00:23&lt;00:08, 39.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  78%|#######7  | 1.11G/1.43G [00:23&lt;00:07, 44.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  78%|#######8  | 1.12G/1.43G [00:23&lt;00:06, 45.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  78%|#######8  | 1.12G/1.43G [00:23&lt;00:07, 38.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  79%|#######8  | 1.13G/1.43G [00:23&lt;00:07, 38.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  79%|#######9  | 1.13G/1.43G [00:23&lt;00:06, 43.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  80%|#######9  | 1.14G/1.43G [00:23&lt;00:06, 47.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  80%|#######9  | 1.14G/1.43G [00:24&lt;00:05, 50.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  80%|########  | 1.15G/1.43G [00:24&lt;00:05, 50.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  81%|########  | 1.15G/1.43G [00:24&lt;00:05, 49.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  81%|########  | 1.16G/1.43G [00:24&lt;00:05, 48.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  81%|########1 | 1.16G/1.43G [00:24&lt;00:05, 51.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  82%|########1 | 1.17G/1.43G [00:24&lt;00:05, 51.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  82%|########2 | 1.17G/1.43G [00:24&lt;00:04, 52.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  82%|########2 | 1.18G/1.43G [00:24&lt;00:04, 51.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  83%|########2 | 1.18G/1.43G [00:24&lt;00:05, 48.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  83%|########2 | 1.19G/1.43G [00:24&lt;00:06, 38.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  83%|########3 | 1.19G/1.43G [00:25&lt;00:06, 39.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  83%|########3 | 1.19G/1.43G [00:25&lt;00:06, 38.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  84%|########3 | 1.20G/1.43G [00:25&lt;00:06, 37.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  84%|########3 | 1.20G/1.43G [00:25&lt;00:06, 34.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  84%|########4 | 1.20G/1.43G [00:25&lt;00:06, 33.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  84%|########4 | 1.21G/1.43G [00:25&lt;00:06, 32.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  85%|########4 | 1.21G/1.43G [00:25&lt;00:06, 32.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  85%|########4 | 1.21G/1.43G [00:25&lt;00:06, 35.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  85%|########5 | 1.22G/1.43G [00:25&lt;00:05, 37.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  85%|########5 | 1.22G/1.43G [00:25&lt;00:05, 37.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  86%|########5 | 1.23G/1.43G [00:26&lt;00:05, 40.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  86%|########5 | 1.23G/1.43G [00:26&lt;00:05, 34.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  86%|########6 | 1.23G/1.43G [00:26&lt;00:04, 42.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  87%|########6 | 1.24G/1.43G [00:26&lt;00:04, 45.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  87%|########7 | 1.25G/1.43G [00:26&lt;00:03, 48.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  87%|########7 | 1.25G/1.43G [00:26&lt;00:03, 47.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  88%|########7 | 1.25G/1.43G [00:26&lt;00:03, 46.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  88%|########8 | 1.26G/1.43G [00:26&lt;00:03, 49.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  88%|########8 | 1.26G/1.43G [00:26&lt;00:04, 40.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  89%|########8 | 1.27G/1.43G [00:26&lt;00:03, 43.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  89%|########8 | 1.27G/1.43G [00:27&lt;00:03, 42.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  89%|########9 | 1.28G/1.43G [00:27&lt;00:03, 44.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  90%|########9 | 1.28G/1.43G [00:27&lt;00:03, 49.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  90%|########9 | 1.29G/1.43G [00:27&lt;00:02, 48.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  90%|######### | 1.29G/1.43G [00:27&lt;00:02, 46.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  91%|######### | 1.30G/1.43G [00:27&lt;00:02, 52.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  91%|#########1| 1.30G/1.43G [00:27&lt;00:02, 52.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  92%|#########1| 1.31G/1.43G [00:27&lt;00:02, 56.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  92%|#########1| 1.32G/1.43G [00:27&lt;00:01, 58.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  92%|#########2| 1.32G/1.43G [00:27&lt;00:02, 53.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  93%|#########2| 1.33G/1.43G [00:28&lt;00:01, 53.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  93%|#########3| 1.33G/1.43G [00:28&lt;00:01, 53.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  93%|#########3| 1.34G/1.43G [00:28&lt;00:01, 52.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  94%|#########3| 1.34G/1.43G [00:28&lt;00:01, 54.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  94%|#########4| 1.35G/1.43G [00:28&lt;00:01, 49.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  94%|#########4| 1.35G/1.43G [00:28&lt;00:01, 47.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  95%|#########4| 1.36G/1.43G [00:28&lt;00:01, 51.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  95%|#########5| 1.36G/1.43G [00:28&lt;00:01, 56.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  96%|#########5| 1.37G/1.43G [00:28&lt;00:01, 55.5MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  96%|#########5| 1.37G/1.43G [00:28&lt;00:01, 48.7MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  96%|#########6| 1.37G/1.43G [00:29&lt;00:01, 39.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  96%|#########6| 1.38G/1.43G [00:29&lt;00:01, 42.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  97%|#########6| 1.38G/1.43G [00:29&lt;00:01, 41.2MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  97%|#########6| 1.39G/1.43G [00:29&lt;00:01, 42.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  97%|#########7| 1.39G/1.43G [00:29&lt;00:00, 43.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  98%|#########7| 1.40G/1.43G [00:29&lt;00:00, 49.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  98%|#########8| 1.40G/1.43G [00:29&lt;00:00, 47.3MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  98%|#########8| 1.41G/1.43G [00:29&lt;00:00, 51.1MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  99%|#########8| 1.41G/1.43G [00:29&lt;00:00, 49.9MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  99%|#########9| 1.42G/1.43G [00:30&lt;00:00, 50.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt:  99%|#########9| 1.42G/1.43G [00:30&lt;00:00, 45.8MB/s]\nflava_for_pretraining_unified_text_encoder.pt: 100%|#########9| 1.43G/1.43G [00:30&lt;00:00, 40.6MB/s]\nflava_for_pretraining_unified_text_encoder.pt: 100%|#########9| 1.43G/1.43G [00:30&lt;00:00, 35.0MB/s]\nflava_for_pretraining_unified_text_encoder.pt: 100%|#########9| 1.43G/1.43G [00:30&lt;00:00, 23.4MB/s]\nflava_for_pretraining_unified_text_encoder.pt: 1.43GB [00:30, 46.9MB/s]",
                        "code"
                    ],
                    [
                        "5. We put together the dataset and model in a toy training loop to\ndemonstrate how to train the model for 3 iterations:",
                        "markdown"
                    ],
                    [
                        "from torch import nn\nBATCH_SIZE = 2\nMAX_STEPS = 3\nfrom torch.utils.data import \n\n = (dataset[\"train\"], batch_size= BATCH_SIZE)\n = (())\n\n\nepochs = 1\nfor _ in range(epochs):\n  for idx, batch in enumerate():\n    ()\n    out = model(text = batch[\"input_ids\"], image = batch[\"image\"], labels = batch[\"answers\"])\n     = \n    ()\n    ()\n    print(f\"Loss at step {idx} = {}\")\n    if idx &gt; MAX_STEPS-1:\n      break",
                        "code"
                    ],
                    [
                        "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning:\n\nThe default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n\nLoss at step 0 = 8.314010620117188\nLoss at step 1 = 8.259458541870117\nLoss at step 2 = 8.271486282348633\nLoss at step 3 = 8.26636791229248",
                        "code"
                    ]
                ]
            },
            {
                "Conclusion": [
                    [
                        "This tutorial introduced the basics around how to finetune on a\nmultimodal task using FLAVA from TorchMultimodal. Please also check out\nother examples from the library like\n\nwhich is a multimodal model for object detection and\n\nwhich is multitask model spanning image, video and 3d classification.",
                        "markdown"
                    ],
                    [
                        "<strong>Total running time of the script:</strong> ( 4 minutes  12.837 seconds)",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ],
                    [
                        "",
                        "markdown"
                    ]
                ]
            }
        ]
    }
}