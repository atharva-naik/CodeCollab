{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bf96253",
   "metadata": {},
   "source": [
    "Importing all the necessary modules, with the usual aliases. See readme for graphviz ImportError resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a2a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = [14.0, 12.0] #Adjust as necessary, the plots are tiny if left at the default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eabb681",
   "metadata": {},
   "source": [
    "For some reason Python converts some of the columns to type = Object, so here I'm forcing them all to be numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6bf179",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/PythonProjects/ThesisPy/ThesisData3.csv')\n",
    "df = df.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b4cbe",
   "metadata": {},
   "source": [
    "First, I'll subset the data a little bit.\n",
    "Next, we need to point python at a decompiler so that xgboost will import correctly. See readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6612867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.loc[:,'WFT1':'FamSup']\n",
    "mingw_path = 'C:\\\\Program Files\\\\mingw-w64\\\\x86_64-7.2.0-posix-seh-rt_v5-rev1\\\\mingw64\\\\bin'\n",
    "os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "import xgboost as xgb\n",
    "df_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa5d91a",
   "metadata": {},
   "source": [
    "Subsetting removes a few categorical, total-score, and transformed columns, leaving us with 414 rows of 177 columns. \n",
    "Not exactly big data, but enough for our purposes.\n",
    "Next, I'll slice our new df to separate the target (outcome) variable from the feature (prediction) variables and make a\n",
    "xgboost-centric data object from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf6c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_1.loc[:,'WFT1':'Industry']\n",
    "y = df_1.loc[:,'WF4Omni']\n",
    "mydmatrix = xgb.DMatrix(data=X, label=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a32af",
   "metadata": {},
   "source": [
    "First, we'll make 10 trees using 80% of our rows, and then see how well that tree predicts our outcome on the remaining 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost model with linear learner\n",
    "xg_reg = xgb.XGBRegressor(seed=123, objective = \"reg:linear\", n_estimators=30)\n",
    "xg_tre = xgb.XGBModel(seed =123)\n",
    "\n",
    "xg_reg.fit(X_train, y_train)\n",
    "preds=xg_reg.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9315d6e",
   "metadata": {},
   "source": [
    "RMSE (our error metric) is about 30. Not great, but it's just a baseline for now. Next, we'll plot the last (10th) tree xgboost created for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d44071",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(xg_reg, num_trees = 9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a626a5",
   "metadata": {},
   "source": [
    "Now to make some boosted trees. The xgboost API takes a dictionary of model parameters, and uses the DMatrix we created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13514e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"objective\":\"reg:linear\", \"max_depth\":5}\n",
    "xg_reg = xgb.train(params=params, dtrain=mydmatrix, num_boost_round=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024566bf",
   "metadata": {},
   "source": [
    "We can plot the relative importance of the variables xgboost used to plot our trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_importance(xg_reg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df75859",
   "metadata": {},
   "source": [
    "Using graphviz, we can also plot individual trees - the second tree, in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc4f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.plot_tree(xg_reg, num_trees=1, rankdir = 'LR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64711eab",
   "metadata": {},
   "source": [
    "Now let's try some optimization and validation. We create a range of lambda values from 0.001 to 1 (really 1.001) and our xgboost parameter dictionary. Then we initialize an empty list to store our rmse values, then iterate over a for-loop to run our models. \n",
    "(10-fold x-validation and 10 trees by default here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd9272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_params = np.arange(0.001, 1.10, 0.10)\n",
    "params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "rmses_l2 = []\n",
    "for reg in reg_params:\n",
    "\n",
    "    params[\"lambda\"] = reg\n",
    "    cv_results_rmse = xgb.cv(dtrain=mydmatrix, params=params, nfold=10, num_boost_round=10, metrics=\"rmse\", \n",
    "                             as_pandas=True, seed=123)\n",
    "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee68ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best rmse as a function of l2:\")\n",
    "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\", \"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a187b8",
   "metadata": {},
   "source": [
    "The best value of lambda here was 0.001, with an associated RMSE of 5.9. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918baeb",
   "metadata": {},
   "source": [
    "Here there be dragons. Testing sklearn API and further optimization. Updates to follow."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
