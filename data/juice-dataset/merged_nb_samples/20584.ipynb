{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df8bb67e",
   "metadata": {},
   "source": [
    "# Comparing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6633320e",
   "metadata": {},
   "source": [
    "- Run the models 3 times with different random sees (24, 45, 93) at T/T-split to get 3 different R2-scores\n",
    "- Take the STD of the 3 R2-scores and make error bars for the different models.\n",
    "\n",
    "T/T-split with random seed was done in notebook 12, so get the first metrics from there.\n",
    "This notebook will provide metrics for random seed 2 and 3. \n",
    "\n",
    "I have tried to clean the code a little bit to be able to make it run faster.\n",
    "There are less explenation in this notebook than in notebook nr 12. That is bc I do the same thing 2 more times. To get more explenations to things please look at notebook 12."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877abe5c",
   "metadata": {},
   "source": [
    "## Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9510b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-bright')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975e984",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9035677",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data_top10c_more_lyrics.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f97fd71",
   "metadata": {},
   "source": [
    "## Fix a little bit with the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d124d1",
   "metadata": {},
   "source": [
    "#### Get dummies for the Country column + include in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_dum = pd.get_dummies(data['Country'])\n",
    "\n",
    "data_c = pd.concat([data, country_dum], axis=1)\n",
    "\n",
    "data_c.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c726720",
   "metadata": {},
   "source": [
    "#### Get dummies for the Position column  + include in the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336cc8b2",
   "metadata": {},
   "source": [
    "**Gradient #3 - TF-IDF + top 3 coefs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d032c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X_train26 = indep_train_tvec[['Energy', 'Acousticness', 'Tempo']]\n",
    "y_train26 = dep_train\n",
    "X_test26 = indep_test_tvec[['Energy', 'Acousticness', 'Tempo']]\n",
    "y_test26 = dep_test\n",
    "\n",
    "# RandomizedSearch\n",
    "params = {'n_estimators': np.arange(10, 100, 10),\n",
    "        'loss': ('ls', 'lad', 'huber', 'quantile'),\n",
    "        'max_depth': np.arange(2, 20, 3),\n",
    "        'max_features' : ('auto', 'sqrt', 'log2'),\n",
    "        'verbose' : np.arange(0, 1)}\n",
    "\n",
    "grad6 = GradientBoostingRegressor(random_state=24)\n",
    "\n",
    "get_best_hype(grad6, params, X_train26, y_train26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc46b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose model and use best hyperparameters (from gridsearchCV)\n",
    "grad6 = GradientBoostingRegressor(random_state=24, n_estimators=70, loss='huber', max_depth=2, max_features='auto',\n",
    "                                 verbose=0)\n",
    "\n",
    "# call function\n",
    "evaluate_model(grad6, X_train26, X_test26, y_train26, y_test26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22369821",
   "metadata": {},
   "source": [
    "## Compare model visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e8f15",
   "metadata": {},
   "source": [
    "R2-score and STD of 3 R2-scores with different random seed in the Train/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d47230",
   "metadata": {},
   "source": [
    "https://matplotlib.org/1.2.1/examples/pylab_examples/errorbar_demo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd7fdab",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/22364565/python-pylab-scatter-plot-error-bars-the-error-on-each-point-is-unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f2149",
   "metadata": {},
   "source": [
    "### Independet varables (X):\n",
    "### Spotify API, Countries, Position, AvgStreams, AvgPosition, TextBlob, NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc18e19",
   "metadata": {},
   "source": [
    "Spotify API: Acousticness, Energy, Instrumentalness, Mode, Tempo, Valence<BR />\n",
    "Countries: au, ca, de, fr, gb, it, nl, us<BR />\n",
    "Position: 1-200<BR />\n",
    "TextBlob: Polarity, Subjectivity<BR />\n",
    "NLP: CountVec / TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b2010c",
   "metadata": {},
   "source": [
    "**LinearRegressor**\n",
    "Random_State(24, 45, 93)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbfe76f",
   "metadata": {},
   "source": [
    "Get the STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102dc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features - cvec\n",
    "print(np.std([0.013486410677653882,0.018647294642352596,0.027289480032708258]))\n",
    "\n",
    "# Top 10 features - cvec\n",
    "print(np.std([0.15703025345048383,0.0027594332154159407,0.010353867606340716]))\n",
    "\n",
    "# Top 3 features - cvec\n",
    "print(np.std([0.14873571822331177,-0.007729319613470675,0.005940599633448396]))\n",
    "\n",
    "# All features - tvec\n",
    "print(np.std([0.021622186821517175,0.06084852472490698,0.04931998355658407]))\n",
    "\n",
    "# Top 10 features - tvec\n",
    "print(np.std([0.15924513606808388,0.1421096962323608,0.16564254674555934]))\n",
    "\n",
    "# Top 3 features - tvec\n",
    "print(np.std([0.15789602776517975,0.1214683266365274,0.14426340969288443]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e60207",
   "metadata": {},
   "source": [
    "Get average R2-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f716a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_hype(model, params, X_train, y_train):  \n",
    "    # standardize\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_train)\n",
    "    X_train_s = ss.transform(X_train)\n",
    "     \n",
    "    # Best Hyperparameters\n",
    "    rs = RandomizedSearchCV(model, params, n_iter=27)\n",
    "    \n",
    "    # fit\n",
    "    rs.fit(X_train_s, y_train)\n",
    "     \n",
    "    return {'best_score': rs.best_score_,'best_params': rs.best_params_} \n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    # standardize the predictors\n",
    "    ss = StandardScaler()\n",
    "    ss.fit(X_train)\n",
    "    X_train_s = ss.transform(X_train)\n",
    "    X_test_s = ss.transform(X_test)\n",
    "    \n",
    "    # fit\n",
    "    model.fit(X_train_s, y_train)\n",
    "    \n",
    "    # Evaluate: predict\n",
    "    y_pred = model.predict(X_test_s)\n",
    "    y_true = y_test\n",
    "    \n",
    "    mean_square_error = np.sqrt(sklearn.metrics.mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    # Evaluate: score\n",
    "    score = model.score(X_test_s, y_test)\n",
    "    \n",
    "    return {'Score (R^2)': score.mean(), 'MSE': mean_square_error}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c0e84",
   "metadata": {},
   "source": [
    "**Ada #1 - CountVec + all coefs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19774834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare indep and dep\n",
    "X_train23 = indep_train_tvec.drop(['Track Name', 'Artist', 'ID', 'Lyrics', 'Valence'], axis=1)\n",
    "y_train23 = dep_train\n",
    "X_test23 = indep_test_tvec.drop(['Track Name', 'Artist', 'ID', 'Lyrics', 'Valence'], axis=1)\n",
    "y_test23 = dep_test\n",
    "\n",
    "# RandomizedSearch\n",
    "params = {'n_estimators': np.arange(10, 100, 10),\n",
    "        'loss': ('linear', 'square', 'exponential')}\n",
    "\n",
    "ada2 = AdaBoostRegressor(random_state=24) \n",
    "# base_estimator = DecisionTreeRegressor() (default)\n",
    "# learning_rate: (default=1.0)\n",
    "\n",
    "get_best_hype(ada2, params, X_train23, y_train23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50022fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose model and use best hyperparameters (from gridsearchCV)\n",
    "ada2 = AdaBoostRegressor(n_estimators=10, loss='linear', random_state=24)\n",
    "# I'm going with the default base_estimator, bc to choose a rfr is taking too long time\n",
    "\n",
    "# call function\n",
    "evaluate_model(ada2, X_train23, X_test23, y_train23, y_test23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bcc37c",
   "metadata": {},
   "source": [
    "**Gradient #1 - TF-IDF + all coefs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26063dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declare indep and dep\n",
    "X_train24 = indep_train_tvec.drop(['Track Name', 'Artist', 'ID', 'Lyrics', 'Valence'], axis=1)\n",
    "y_train24 = dep_train\n",
    "X_test24 = indep_test_tvec.drop(['Track Name', 'Artist', 'ID', 'Lyrics', 'Valence'], axis=1)\n",
    "y_test24 = dep_test\n",
    "\n",
    "# RandomizedSearch\n",
    "params = {'n_estimators': np.arange(10, 100, 10),\n",
    "        'loss': ('ls', 'lad', 'huber', 'quantile'),\n",
    "        'max_depth': np.arange(2, 20, 3),\n",
    "        'max_features' : ('auto', 'sqrt', 'log2'),\n",
    "        'verbose' : np.arange(0, 1)}\n",
    "\n",
    "grad4 = GradientBoostingRegressor(random_state=24) \n",
    "# learning_rate: (default=1.0)\n",
    "# init = BaseEstimator, default = None (loss.init_estimator)\n",
    "\n",
    "get_best_hype(grad4, params, X_train24, y_train24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f2cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose model and use best hyperparameters (from gridsearchCV)\n",
    "grad4 = GradientBoostingRegressor(random_state=24, n_estimators=70, loss='ls', max_depth=5, max_features='auto',\n",
    "                                 verbose=0)\n",
    "# if loss = huber or quantile we need a value for alpha\n",
    "\n",
    "# call function\n",
    "evaluate_model(grad4, X_train24, X_test24, y_train24, y_test24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f562e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the feature importance with coef_\n",
    "pd.Series(dict(zip(X_train24.columns,grad4.feature_importances_ ))).abs().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf4f5a",
   "metadata": {},
   "source": [
    "**Gradient #2 - TF-IDF + top 10 coefs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd1b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X_train25 = indep_train_tvec[['Energy', 'Acousticness', 'Subjectivity', 'baby', 'Instrumentalness', 'Tempo', \n",
    "                              'girl', 'moi', 'avg_Position', 'hab']]\n",
    "y_train25 = dep_train\n",
    "X_test25 = indep_test_tvec[['Energy', 'Acousticness', 'Subjectivity', 'baby', 'Instrumentalness', 'Tempo', \n",
    "                              'girl', 'moi', 'avg_Position', 'hab']]\n",
    "y_test25 = dep_test\n",
    "\n",
    "# RandomizedSearch\n",
    "params = {'n_estimators': np.arange(10, 100, 10),\n",
    "        'loss': ('ls', 'lad', 'huber', 'quantile'),\n",
    "        'max_depth': np.arange(2, 20, 3),\n",
    "        'max_features' : ('auto', 'sqrt', 'log2'),\n",
    "        'verbose' : np.arange(0, 1)}\n",
    "\n",
    "grad5 = GradientBoostingRegressor(random_state=24)\n",
    "\n",
    "get_best_hype(grad5, params, X_train25, y_train25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf7adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose model and use best hyperparameters (from gridsearchCV)\n",
    "grad5 = GradientBoostingRegressor(random_state=24, n_estimators=90, loss='lad', max_depth=8, max_features='log2',\n",
    "                                 verbose=0)\n",
    "\n",
    "# call function\n",
    "evaluate_model(grad5, X_train25, X_test25, y_train25, y_test25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfeddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_dum = pd.get_dummies(data['Position'])\n",
    "\n",
    "data_c_p = pd.concat([data_c, position_dum], axis=1)\n",
    "\n",
    "data_c_p.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4006064",
   "metadata": {},
   "source": [
    "#### Group by ID, sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13280ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cp_groupbyID = data_c_p.groupby('ID').sum()\n",
    "data_cp_groupbyID.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688455d1",
   "metadata": {},
   "source": [
    "#### Make 2 new columns for average Streams and average Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare indep and dep\n",
    "X_train10 = indep_train_cvec.drop(['Track Name', 'Artist', 'ID', 'Lyrics', 'Valence'], axis=1)\n",
    "y_train10 = dep_train\n",
    "X_test10 = indep_test_cvec.drop(['Track Name', 'Artist', 'ID', 'Lyrics', 'Valence'], axis=1)\n",
    "y_test10 = dep_test\n",
    "\n",
    "# RandomizedSearch\n",
    "params = {'n_estimators': np.arange(10, 100, 10),\n",
    "        'loss': ('linear', 'square', 'exponential')}\n",
    "\n",
    "ada = AdaBoostRegressor(random_state=24) \n",
    "# base_estimator = DecisionTreeRegressor() (default)\n",
    "# learning_rate: (default=1.0)\n",
    "\n",
    "get_best_hype(ada, params, X_train10, y_train10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3982b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose model and use best hyperparameters (from gridsearchCV)\n",
    "ada = AdaBoostRegressor(n_estimators=10, loss='exponential', random_state=24)\n",
    "# I'm going with the default base_estimator, bc to choose a rfr is taking too long time\n",
    "\n",
    "# call function\n",
    "evaluate_model(ada, X_train10, X_test10, y_train10, y_test10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab1c36",
   "metadata": {},
   "source": [
    "**Gradient #1 - CountVec + all coefs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c83d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare indep and dep\n",
    "X_train11 = indep_train_cvec.drop(['Track Name', 'Artist', 'ID', 'Lyrics', 'Valence'], axis=1)\n",
    "y_train11 = dep_train\n",
    "X_test11 = indep_test_cvec.drop(['Track Name', 'Artist', 'ID', 'Lyrics', 'Valence'], axis=1)\n",
    "y_test11 = dep_test\n",
    "\n",
    "# RandomizedSearch\n",
    "params = {'n_estimators': np.arange(10, 100, 10),\n",
    "        'loss': ('ls', 'lad', 'huber', 'quantile'),\n",
    "        'max_depth': np.arange(2, 20, 3),\n",
    "        'max_features' : ('auto', 'sqrt', 'log2'),\n",
    "        'verbose' : np.arange(0, 1)}\n",
    "\n",
    "\n",
    "grad = GradientBoostingRegressor(random_state=24) \n",
    "# learning_rate: (default=1.0)\n",
    "# init = BaseEstimator, default = None (loss.init_estimator)\n",
    "\n",
    "get_best_hype(grad, params, X_train11, y_train11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b11123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose model and use best hyperparameters (from gridsearchCV)\n",
    "grad = GradientBoostingRegressor(random_state=24, n_estimators=30, loss='lad', max_depth=17 , max_features='auto',\n",
    "                                 verbose=0)\n",
    "\n",
    "# call function\n",
    "evaluate_model(grad, X_train11, X_test11, y_train11, y_test11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the feature importance with coef_\n",
    "pd.Series(dict(zip(X_train11.columns,grad.feature_importances_ ))).abs().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a1a6b9",
   "metadata": {},
   "source": [
    "**Gradient #2 - CountVec + top 10 coefs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ffd662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X_train12 = indep_train_cvec[['Energy', 'Acousticness', 'Tempo', 'Subjectivity', 'Polarity','avg_Streams', \n",
    "                              'Instrumentalness', 'avg_Position', 'baby', 'know']]\n",
    "y_train12 = dep_train\n",
    "X_test12 = indep_test_cvec[['Energy', 'Acousticness', 'Tempo', 'Subjectivity', 'Polarity','avg_Streams', \n",
    "                              'Instrumentalness', 'avg_Position', 'baby', 'know']]\n",
    "y_test12 = dep_test\n",
    "\n",
    "# RandomizedSearch\n",
    "params = {'n_estimators': np.arange(10, 100, 10),\n",
    "        'loss': ('ls', 'lad', 'huber', 'quantile'),\n",
    "        'max_depth': np.arange(2, 20, 3),\n",
    "        'max_features' : ('auto', 'sqrt', 'log2'),\n",
    "        'verbose' : np.arange(0, 1)}\n",
    "\n",
    "grad2 = GradientBoostingRegressor(random_state=24)\n",
    "\n",
    "get_best_hype(grad2, params, X_train12, y_train12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2a1aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose model and use best hyperparameters (from gridsearchCV)\n",
    "grad2 = GradientBoostingRegressor(random_state=24, n_estimators=90, loss='huber', max_depth=2, max_features='auto',\n",
    "                                 verbose=0)\n",
    "\n",
    "# call function\n",
    "evaluate_model(grad2, X_train12, X_test12, y_train12, y_test12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc6397",
   "metadata": {},
   "source": [
    "**Gradient #3 - CountVec + top 3 coefs**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
