{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9d97bc",
   "metadata": {},
   "source": [
    "### Intuition behind approach:\n",
    "* The essence to properly do the one class classification on this dataset is to learn the underlying sequence strcture of the names\n",
    "* The sequence can be learnt by approximating what letters occur in Hindi Names\n",
    "* But that is not much useful as english letters are in total 26 and most of the languages and their words around the world can be represented in english\n",
    "* This brings to an important observation:\n",
    "    - We can learn the combination of letters that are there in names\n",
    "    - These combinations of one language might be different from another language names\n",
    "    - If we can properly learn mappings (vectors) of tokens(token : combinations of letters) of names that belongs to a particular language\n",
    "    , then we can represent each name ( a sum of such combinations) as a vector too.\n",
    "    - These vectors can then be fed to our classifier for modelling. \n",
    "* Evaluations:\n",
    "    - For evalation I can use False Positive rate for negative_class files.\n",
    "    - I can also use accuracy as a metric if I mix some instances of target calss with outlier class.\n",
    "    \n",
    "<br/>\n",
    "** PS: In this model I have used tokens and ngram interchangable. In the next approach of word2vec I make the distinction between them. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670902de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_table('hindu_baby_names.txt',sep='\\n',names=['Names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c834cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc7cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Names'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ff7a6",
   "metadata": {},
   "source": [
    "#### Split data in training and testing\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2bfd2a",
   "metadata": {},
   "source": [
    "* Since I already have very less data for training\n",
    "* I will keep very less part for testing from target data set\n",
    "* Will test on other testing data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a copy of data to test\n",
    "dataCopy=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4208348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Size is the percentage of data in training set\n",
    "def splitData(dataCopy,splitSize):\n",
    "    trainData=[]\n",
    "    testData=[]\n",
    "    trainNames=dataCopy[:int(.01*splitSize*len(dataCopy))]\n",
    "    testNames=dataCopy[len(trainNames):]\n",
    "    \n",
    "    for i in range(len(trainNames)):\n",
    "        trainData.extend(trainNames.iloc[i])\n",
    "    for i in range(len(testNames)):\n",
    "        testData.extend(testNames.iloc[i])\n",
    "        \n",
    "    print(\"Number of names in training Set: \"+str(len(trainData)))\n",
    "    print(\"Number of names in test Set: \"+str(len(testData)))\n",
    "    return trainData,testData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f96ad",
   "metadata": {},
   "source": [
    "#### Form test Set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dcab13",
   "metadata": {},
   "source": [
    "* following code will be used to join names in the other two files provided with the names\n",
    "in the test set of the target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85075458",
   "metadata": {},
   "outputs": [],
   "source": [
    "testError=m.predict(testNamesFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8e3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "testErrorNegativeClass=m.predict(testFilesCorpusFS)\n",
    "print(\"Accuracy Socre On Negative Class: \",accuracy_score(y1,testErrorNegativeClass))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e6843c",
   "metadata": {},
   "source": [
    "* We are able to classify outlier class with 96% accuracy as outliers\n",
    "* This means we have very little False Positives and more True Negatives predicted\n",
    "* Lets look at the confusion matrix of the total Test Set with some instances of target class present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974db33",
   "metadata": {},
   "source": [
    "* Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f93d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the confusion matrix on the training set\n",
    "confusion_matrix(testNamesTarget,testError)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37fd691",
   "metadata": {},
   "source": [
    "* True Negatives count(True State: Outlier -> Predicted State: Outlier is at (0,0)\n",
    "* False Negative Count (True State: Target -> Predicted State: Outlier is at (1,0)\n",
    "* True Positive Count (True State: Target -> Predicted State: Target is at (1,1)\n",
    "* False Postive Count (True State: Outlier -> Predicted State: Target is at (0,1)\n",
    "* Accuracy : TP+TN/(TP+TN+FP+FN)\n",
    "* False Positive Rate: FP/FP+TN\n",
    "* Precision : TP/TP+FP \n",
    "* Recall : TP/TP+FN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193fefe1",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "* We can see that the False Positive count is way low than that of the true negative\n",
    "* This suggests that 2gram OneClass SVM is able to filter most part of the outlier class as outliers\n",
    "* But the count of True positives is very less\n",
    "* This maybe due to underfitting \n",
    "* Lets try using a polynomial kernel with default hyperparameters\n",
    "* Accuracy : .90 -> Model gives good accuracy on mix of target and outliers\n",
    "* FPR ~ .03 -> Model able to filter out most of the outliers\n",
    "* Precision ~ 222/(222+298) = .42\n",
    "* Recall ~ 222/819 = .27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a486a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "polySVM=OneClassSVM(kernel='poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2882aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting confusion matrix \n",
    "polySVM.fit(trainNames2GramFS)\n",
    "testError2=polySVM.predict(testNamesFS)\n",
    "confusion_matrix(testNamesTarget,testError2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18675b8c",
   "metadata": {},
   "source": [
    "* This has nearly the same results as linear svm.\n",
    "* this may be due to the training data being too sparse\n",
    "* This may also be due to not being able to capture hidden sequence structure of the names\n",
    "* Next I will try using three grams of names to check if 3grams is ideal than 2gram to capture the sequence structure\n",
    "* Accuracy: .90 ~ Behaves well too\n",
    "* FPR: .03 \n",
    "* Precision ~ .41\n",
    "* Recall ~ .25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ddc1d",
   "metadata": {},
   "source": [
    "### 3Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6462c91",
   "metadata": {},
   "source": [
    "#### Intuition:\n",
    "* Maybe taking bigger letter window of names to form tokens can help us catch better sequence structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8480f59e",
   "metadata": {},
   "source": [
    "* The following plot plots learning curve for an estimator\n",
    "* The training and validation errors are plotted for the number of training examples \n",
    "represented on the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39640d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLearningCurve(estimator,title,X,y,n_jobs=1,scoring=\"accuracy\"):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(scoring)\n",
    "    train_sizes,train_scores,test_scores=learning_curve(estimator,\n",
    "                                                    X,y,\n",
    "                                                   n_jobs=2,scoring=\"accuracy\")\n",
    "    train_scores_mean=np.mean(train_scores,axis=1)\n",
    "    train_scores_std=np.std(train_scores,axis=1)\n",
    "    test_scores_mean=np.mean(test_scores,axis=1)\n",
    "    test_scores_std=np.std(test_scores,axis=1)\n",
    "    \n",
    "    plt.fill_between(train_sizes,train_scores_mean-train_scores_std,\n",
    "                    train_scores_mean+train_scores_std,alpha=0.1,color='g')\n",
    "    \n",
    "    plt.fill_between(train_sizes,test_scores_mean-test_scores_std,\n",
    "                    test_scores_mean+test_scores_std,alpha=0.1,color='r')\n",
    "    plt.plot(train_sizes,train_scores_mean,'o-',color='g',label=\"Training Score\")\n",
    "    plt.plot(train_sizes,test_scores_mean,'o-',color='r',label=\"Cross Validation Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194dd95c",
   "metadata": {},
   "source": [
    "### 2gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad2556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training Names\n",
    "print(trainNames[:5])\n",
    "print(\"Length of training data: \"+str(len(trainNames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d446321",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Form 2gram object of text class\n",
    "trainNames2GramObj=text(trainNames,2)\n",
    "print(trainNames2GramObj.data[:5])\n",
    "print(\"Length of data in trainNames2GramObj: \"+str(len(trainNames2GramObj.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eefb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create token2Index mapping of unique tokens from trainNames2GramObj's data\n",
    "token2Index2Gram=createToken2Index(trainNames2GramObj.data)\n",
    "token2Index2Gram['aa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabdacda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Feature Set of names in TrainNames\n",
    "trainNames2GramFS=getFeatureSet(trainNames,trainNames2GramObj.tokenSize,token2Index2Gram)\n",
    "print(\"Shape of feature set of trainNames: \",trainNames2GramFS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb302439",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNames2GramTarget=[1 for _ in range(len(trainNames))]\n",
    "print(\"Shape of Target label for trainNames Feature Set: \",len(trainNames2GramTarget))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02c2196",
   "metadata": {},
   "source": [
    "* Now I have feature set of 7371 names with 428 features for each name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa33e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizePCA(trainNames2GramFS,trainNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b086510",
   "metadata": {},
   "source": [
    "* We can infer that there are  cluster of names that exist if 2grams are used\n",
    "* Next I will create a oneClassSVM model from sklearn to train on this feature set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21f30c0",
   "metadata": {},
   "source": [
    "#### OneClassSVM\n",
    "* This model assumes a hypersphere exists around the target data in the vector space of target class\n",
    "* It tries to find the boundary of that hypersphere\n",
    "* It is suitable for data set with very few instances as it has some bias\n",
    "* If we had a huge dataset to train on then density estimation techniques would work better\n",
    "* OneClassSvm is a boundary detection technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411591ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa909a2",
   "metadata": {},
   "source": [
    "\n",
    "* Trying out linear kernel classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44672f53",
   "metadata": {},
   "source": [
    "##### Analysis:\n",
    "* The training score is maintained at a level\n",
    "* The cross validation score increases with number of instances\n",
    "* This plot suggests that with more data, we could train our linear kernel SVM with\n",
    "better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dbaf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Feature set of test instances\n",
    "testFilesCorpusFS=getFeatureSet(testFilesCorpus,trainNames2GramObj.tokenSize,token2Index2Gram)\n",
    "testNamesFS=getFeatureSet(testNames,trainNames2GramObj.tokenSize,token2Index2Gram)\n",
    "testNamesFS=testNamesFS.append(testFilesCorpusFS,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e15cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=[-1 for _ in range(len(testFilesCorpus))]\n",
    "y2=[1 for _ in range(len(testNames))]\n",
    "testNamesTarget=y2+y1   \n",
    "print(testNamesTarget[:5],testNamesTarget[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d5d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5065e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=OneClassSVM(kernel=\"linear\")\n",
    "m.fit(trainNames2GramFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e133b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating object of text class with total names and 3 as size of each gram\n",
    "threeGramsObj=text(trainNames,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "threeGramsObj.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of index to token\n",
    "threeGram2Index=createToken2Index(threeGramsObj.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f63d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create feature set of all train names using this dictionary\n",
    "trainNames3GramFS=getFeatureSet(trainNames,threeGramsObj.tokenSize,threeGram2Index)\n",
    "trainNames3GramTarget=[1 for _ in range(len(trainNames))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16f2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNames3GramFS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a37f22",
   "metadata": {},
   "source": [
    "##### Analysis:\n",
    "* The feature set for three gram model has increased\n",
    "* The number of tokens is more\n",
    "* This would result in a more sparse feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a80b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizePCA(trainNames3GramFS,trainNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050997f",
   "metadata": {},
   "source": [
    "* We can see that there are more number of dissimilar names here than 2grams\n",
    "* This maybe due to more uniqueness in 3grams\n",
    "* Names similar here are more strongly related \n",
    "* Though this increases more strong similarity, but the feature set is getting more sparse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc9d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying Linear Kernel first\n",
    "linearSVM3Gram=OneClassSVM(kernel='linear',random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e51088",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLearningCurve(linearSVM3Gram,\"Linear Kernel One SVM\",trainNames3GramFS,\n",
    "                  trainNames3GramTarget,1,\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be68e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e5797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Feature set of test instances\n",
    "testFilesCorpusFS=getFeatureSet(testFilesCorpus,threeGramsObj.tokenSize,threeGram2Index)\n",
    "testNamesFS=getFeatureSet(testNames,threeGramsObj.tokenSize,threeGram2Index)\n",
    "testNamesFS=testNamesFS.append(testFilesCorpusFS,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c625100",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Plotting confusion matrix \n",
    "m3=OneClassSVM(kernel=\"linear\")\n",
    "m3.fit(trainNames3GramFS)\n",
    "testError3=m3.predict(testNamesFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a9a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(testNamesTarget,testError2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8353a1",
   "metadata": {},
   "source": [
    "* True Negatives count(True State: Outlier -> Predicted State: Outlier is at (0,0)\n",
    "* False Negative Count (True State: Target -> Predicted State: Outlier is at (1,0)\n",
    "* True Positive Count (True State: Target -> Predicted State: Target is at (1,1)\n",
    "* False Postive Count (True State: Outlier -> Predicted State: Target is at (0,1)\n",
    "* Accuracy : TP+TN/(TP+TN+FP+FN)\n",
    "* False Positive Rate: FP/FP+TN\n",
    "* Precision : TP/TP+FP \n",
    "* Recall : TP/TP+FN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df6fa9e",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "* We can see that the false Negative count has risen\n",
    "* The False positive count is lower than linear of 2gram. This may be because the feature set is more sparse than the 2gram model\n",
    "* Maybe using both 2gram and 3gram as features of the model will result in better feature set\n",
    "* Accuracy: .90 -> Gives us pretty good accuracy\n",
    "* FPR: .03 -> Most of the outlier instance are rejected\n",
    "* Precision ~ .5\n",
    "* Recall ~ .23\n",
    "* Using 3Grams has helped increase the precision. This can be inferred as that this model lowers the positives. But the precision is increased by filtering out most of the outliers that were predicted as targets in earlier models.\n",
    "* Maybe grid search will help to tune the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7476c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "testFilesCorpus=[]\n",
    "for i in range(len(df1)):\n",
    "    word=df1.iloc[i]\n",
    "    testFilesCorpus.extend(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2612b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total names for testing: \" + str(len(testFilesCorpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cc061a",
   "metadata": {},
   "source": [
    "#### Clean the names from common punctuation and chosen noisy words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183012a",
   "metadata": {},
   "source": [
    "* For proper learning and catching tokens that are actually there in names we can remove the punctuations present\n",
    "* We can also remove specific words that arent part of names but occur as a separate word alongwith the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af62ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(t):\n",
    "    return ''.join(l.strip(',.\"!?;:()/\\'') for l in t)\n",
    "\n",
    "def removeNoisyWords(text):\n",
    "    stopList=set('or a the and it is if to in for of'.split())\n",
    "    result=[]\n",
    "    for name in text:\n",
    "        words=name.lower().split()\n",
    "        temp=[]\n",
    "        for i in words:\n",
    "            if i not in stopList:\n",
    "                temp.extend(i)\n",
    "        result.append(temp)   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01303e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNames,testNames=splitData(dataCopy,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e623e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312859e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNames=removeNoisyWords(trainNames)\n",
    "trainNames=[cleanText(n) for n in trainNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3663b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The total Names in the corpus\n",
    "totalNames=trainNames+testNames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a82c79",
   "metadata": {},
   "source": [
    "\n",
    "#### Creating Tokens of n letters of each names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578043f2",
   "metadata": {},
   "source": [
    "* This function divided each name into combinations of n letters each\n",
    "* Sometimes referred to as ngram \n",
    "* I will try to test with n=2 and n=3\n",
    "* This is done because the essence of learning names or words from a particular \n",
    "language, we can look into combination of letters occuring right next to each other.\n",
    "Such combinations of letters are unique to each language. For example, in name \"aditya\"\n",
    "the letter combinations \"it\" and \"ty\" are maybe unique to Hindi Language and rare in \n",
    "names of other Languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ed0938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PolySVM\n",
    "polySVM=OneClassSVM(kernel='poly')\n",
    "# Plotting confusion matrix \n",
    "polySVM.fit(trainNames3GramFS)\n",
    "testError3=polySVM.predict(testNamesFS)\n",
    "confusion_matrix(testNamesTarget,testError3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab0c1e1",
   "metadata": {},
   "source": [
    "###  2Grams and 3Grams combines Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c60374",
   "metadata": {},
   "source": [
    "#### Intuition:\n",
    "* In this approach, we take both 2grams and 3grams to form features of the names\n",
    "* This may be helpful to catch better sequence structure\n",
    "* But this may also produce more sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linearSVM=OneClassSVM(kernel='linear',random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec3320b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLearningCurve(linearSVM,\"Linear Kernel One SVM\",trainNames2GramFS,\n",
    "                  trainNames2GramTarget,1,\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773238b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTokens(text,n):\n",
    "    letters=text\n",
    "    output=[]\n",
    "    for i in range(len(letters)-n+1):\n",
    "        output.append(letters[i:i+n])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2923f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "class text:\n",
    "    def __init__(self,data,tokenSize=2):\n",
    "        self.data=[]\n",
    "        for name in data:\n",
    "            self.data.extend(generateTokens(name.lower(),tokenSize))\n",
    "        self.tokenSize=tokenSize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3cfb4b",
   "metadata": {},
   "source": [
    "\n",
    "#### Create a Mapping of all unique tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fff274",
   "metadata": {},
   "source": [
    "* Creating a list which contains unique tokens in the entire corpus of names\n",
    "* This list will provide us a mapping of index -> token\n",
    "* Using this list, a input vector of the size of this list for each name can be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce6d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createList(data):\n",
    "    s=set(data)\n",
    "    lst=list(s)\n",
    "    return lst\n",
    "\n",
    "def createToken2Index(data):\n",
    "    token2Index={}\n",
    "    lst=createList(data)\n",
    "    for index,token in enumerate(lst):\n",
    "        token2Index[token]=index\n",
    "    return token2Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8201ddfe",
   "metadata": {},
   "source": [
    "\n",
    "#### Generate one hot embeddings for each name in trainData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5b9305",
   "metadata": {},
   "source": [
    "* Each name is allotted a vector of size same as the list created in last portion\n",
    "* This vector will represent the feature set of each name in the corpus\n",
    "* For each token in a name, the index of that token can be set \n",
    "* Thus, a feature set of name contains a set bit at the index of its token and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ff82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTokenVector(data,tokenSize,token2Index):\n",
    "    trainVector=[]\n",
    "    for name in data:\n",
    "        tokens=generateTokens(name,tokenSize)\n",
    "        vector=np.zeros((len(token2Index.keys(),)))\n",
    "        for i in tokens:\n",
    "            if i in token2Index:\n",
    "                index=token2Index[i]\n",
    "                vector[index]=1\n",
    "        trainVector.append([name,vector])\n",
    "    return pd.DataFrame(trainVector)\n",
    "\n",
    "#function to create the feature set of the names\n",
    "def getFeatureSet(data,tokenSize,token2Index):\n",
    "    rowList=[]\n",
    "    textDF=createTokenVector(data,tokenSize,token2Index)\n",
    "    for i in range(len(textDF)):\n",
    "        row=textDF.iloc[i][1]\n",
    "        rowList.append(row)\n",
    "    featureSet=pd.DataFrame(rowList)\n",
    "    return featureSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c0cb52",
   "metadata": {},
   "source": [
    "#### Visualizing Embeddings in 2d space using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08b434",
   "metadata": {},
   "source": [
    "* Trying to check which names are closer when the dimensionality is reduced to two\n",
    "* This graph provides an intuition as to how names are placed on the basis of similarity of the feature set we have created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbef08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9236b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizePCA(df,names):\n",
    "    #using 2 components for PCA to visualize in 2 dimension\n",
    "    pca=PCA(n_components=2)\n",
    "    result=pca.fit_transform(df)\n",
    "    plt.scatter(result[:,0],result[:,1],alpha=.2)\n",
    "    for i in range(0,len(names),100):\n",
    "        name=names[i]\n",
    "        plt.annotate(name,xy=(result[i,0],result[i,1]))\n",
    "    plt.title(\"Model 2-d Visualization\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e99157",
   "metadata": {},
   "source": [
    "* The following function plots training and validation curve for an estimator.\n",
    "* The plots are done by varying a single param with a range provided explicitly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d8de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotValidationCurve(estimator,title,X,y,param,param_range,cv=None,scoring=\"accuracy\",n_jobs=1):\n",
    "    train_scores, test_scores = validation_curve(\n",
    "    estimator, X, y, param_name=param, param_range=param_range,\n",
    "    cv=cv, scoring=scoring, n_jobs=n_jobs)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(param)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    lw = 2\n",
    "    plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "    plt.semilogx(param_range, test_scores_mean, label=\"Cross Validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5587e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train23Gram[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f6ed1",
   "metadata": {},
   "source": [
    "* Now we have data set which holds both 2 and 3 grams\n",
    "* Next step is to generate the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabacfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dictionary of token2Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f7af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "token2Index23=createToken2Index(train23Gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6b8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(token2Index23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd6e1a2",
   "metadata": {},
   "source": [
    "* Now we have 3142 total ngrams\n",
    "* Generate encoding for each name using these tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed80aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fUNCTION to create token vector using both tokensize = 2 and 3\n",
    "def createTokenVector23(data,tokenSize1,tokenSize2,token2Index):\n",
    "    trainVector=[]\n",
    "    for name in data:\n",
    "        tokens1=generateTokens(name,tokenSize1)\n",
    "        tokens2=generateTokens(name,tokenSize2)\n",
    "        tokens=tokens1+tokens2\n",
    "        vector=np.zeros((len(token2Index.keys(),)))\n",
    "        for i in tokens:\n",
    "            if i in token2Index:\n",
    "                index=token2Index[i]\n",
    "                vector[index]=1\n",
    "        trainVector.append([name,vector])\n",
    "    return pd.DataFrame(trainVector)\n",
    "\n",
    "#function to create the feature set of the names\n",
    "def getFeatureSet23(data,tokenSize1,tokenSize2,token2Index):\n",
    "    rowList=[]\n",
    "    textDF=createTokenVector23(data,tokenSize1,tokenSize2,token2Index)\n",
    "    for i in range(len(textDF)):\n",
    "        row=textDF.iloc[i][1]\n",
    "        rowList.append(row)\n",
    "    featureSet=pd.DataFrame(rowList)\n",
    "    return featureSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2cd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train23GramFS=getFeatureSet23(trainN,2,3,token2Index23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2ae2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train23GramFS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[1 for _ in range(len(train23GramFS))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b1f80",
   "metadata": {},
   "source": [
    "#### Trying SVM with different kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e08b2",
   "metadata": {},
   "source": [
    "* The following Code took 4 hours to run\n",
    "* Models and valudation curves have taken too much  time to train and plot\n",
    "* This has been a bottleneck to my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "polySVM=OneClassSVM(kernel='poly',random_state=10)\n",
    "param=\"nu\"\n",
    "param_range=[.2,.3,.4,.5]\n",
    "plotValidationCurve(polySVM,\"Polynomial One class SVM\",train23GramFS,y,param,param_range,None,\"accuracy\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db7112",
   "metadata": {},
   "source": [
    "#### Analysis:\n",
    "* Training with both 2 and 3 grams has increased our accuracy\n",
    "* Inference can be made that nu ~ .3 gives best performance\n",
    "* Trying grid search to tune hyperparameters nu and gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19bf51",
   "metadata": {},
   "source": [
    "#### Tuning more hyperparameters using Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27758913",
   "metadata": {},
   "source": [
    "* Trying Grid Search to find best poly kernel parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15528cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53003a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"kernel\":('linear','poly','rbf'),'nu':[.23,.30,.50,.60],'gamma':np.logspace(-3,3,7)}\n",
    "grid=GridSearchCV(OneClassSVM(),param_grid=parameters,scoring='accuracy')\n",
    "grid.fit(train23GramFS,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7b8d51",
   "metadata": {},
   "source": [
    "### Analysis:\n",
    "* Since I have less computing power, models have taken too much time to train\n",
    "* But we can see that from one validation plot of poly kernel svm with nu~.3 gives a very good performance\n",
    "* Due to this computing power bottleneck I was not able to run the grid search to tune the parameters of the polySVM.\n",
    "* **My approach**: \n",
    "    - To compute tokens for each name \n",
    "    - Get unique tokens for the entire corpus\n",
    "    - Form embeddings for each name using this token\n",
    "    - Using OneClassSVM model I can classify the names on test set\n",
    "    - PolySVM with nu~.3 gave me good performance\n",
    "    - was able to vary only one parameter due to less computing power of my laptop\n",
    "* **Next Steps**:\n",
    "    - To vary other parameters and plot validation curves\n",
    "    - To perform grid search and find the most optimal parameters\n",
    "    - Try the following kmeans algorithm with a correct number of clusters (can be done by checking metric for each cluster)\n",
    "    - develop a threshold value using some heuristic\n",
    "    - classify points based on this distance threshold from its assigned cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a247d2",
   "metadata": {},
   "source": [
    "#### Trying kMeans and mixture models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce97b55",
   "metadata": {},
   "source": [
    "Intuition:\n",
    "    * there seemed to be clusters of names when PCA(2 dimensions) was done\n",
    "    * Can we find clusters of names which are very similar based on the 2gram or 3gram embeddings?"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
